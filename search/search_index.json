{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyhDToolkit \u267b\ufe0f An all-in-one package for Python work in my PhD \u267b\ufe0f Link to documentation . License Copyright \u00a9 2019 Felix Soubelet. MIT License","title":"Home"},{"location":"#license","text":"Copyright \u00a9 2019 Felix Soubelet. MIT License","title":"License"},{"location":"docs/About_PyhDToolkit/","text":"About PyhDToolkit Purpose This package is an all-in-one collection of baseline utilities I use in my PhD work. Most of the codes here have their use in my day-to-day work, but not necessarily in our team's softwares. Functionality For now, PyhDToolkit provides some of the following features: A cpymadtools module with tools building on cpymad , a Python bindings library for the MAD-X code; including generators, matching routines, plotting utilities etc. A maths module for useful methods used in analysis. An optics module for particle accelerator physics related calculations and analysis. A plotting module for various matplotlib helpers. A utils module for various Python and UNIX utilities.","title":"About"},{"location":"docs/About_PyhDToolkit/#about-pyhdtoolkit","text":"","title":"About PyhDToolkit"},{"location":"docs/About_PyhDToolkit/#purpose","text":"This package is an all-in-one collection of baseline utilities I use in my PhD work. Most of the codes here have their use in my day-to-day work, but not necessarily in our team's softwares.","title":"Purpose"},{"location":"docs/About_PyhDToolkit/#functionality","text":"For now, PyhDToolkit provides some of the following features: A cpymadtools module with tools building on cpymad , a Python bindings library for the MAD-X code; including generators, matching routines, plotting utilities etc. A maths module for useful methods used in analysis. An optics module for particle accelerator physics related calculations and analysis. A plotting module for various matplotlib helpers. A utils module for various Python and UNIX utilities.","title":"Functionality"},{"location":"docs/Getting_Started/","text":"Getting Started Installation This package is tested for and supports Python 3.7+ , although it should be compatible with Python 3.6 . You can install it simply in a virtual environment with: pip install pyhdtoolkit Installation in a virtual environment Don't know what a virtual environment is or how to set it up? Here is a good primer on virtual environments by RealPython. How about a development environment? Sure thing. This repository uses Poetry as a packaging and build tool. To set yourself up, get a local copy through VCS and run: poetry install This repository follows the Google docstring format, uses Black as a code formatter with a default enforced line length of 100 characters, and Pylint as a linter. You can format the code with make format and lint it (which will format first) with make lint . Testing builds are ensured after each commit through Github Actions. You can run tests locally with the predefined make tests , or through poetry run pytest <options> for customized options. How can I easily reproduce your research done with this? This repository comes with an environment.yml file to reproduce my work conda environment, feel free to use it. If you checked out from version control, you can install this environment and add it to your ipython kernel by running make condaenv . If you are comfortable with containers, a fully-fetched one is provided and accessible through Docker as explained below. Using With Docker Docker provides an easy way to get access to a fully-fledged environment identical to the one I use, for reproducibility of my results. You can directly pull a pre-built image from Dockerhub with: docker pull fsoubelet/simenv You can then run a server from within the container and bind a local directory to work on. Assuming you pulled the provided image from Dockerhub, run a jupyterlab server on port 8888 with the command: docker run --rm -p 8888 :8888 -e JUPYTER_ENABLE_LAB = yes -v <host_dir_to_mount>:/home/jovyan/work fsoubelet/simenv Any jupyter notebook or Python files in the mounted directory can then be used / ran with an environment identical to mine. Citing If you have a use of these codes, please consider citing them. The repository has a DOI provided by Zenodo , and all versions can be cited with the following BibTeX entry: @software { pyhdtoolkit , author = {Felix Soubelet} , title = {fsoubelet/PyhDToolkit} , publisher = {Zenodo} , doi = {10.5281/zenodo.4268804} , url = {https://doi.org/10.5281/zenodo.4268804} } To cite a specific version, select the version on the package's page on Zotero and export the BibTex entry at the bottom right of the page.","title":"Getting Started"},{"location":"docs/Getting_Started/#getting-started","text":"","title":"Getting Started"},{"location":"docs/Getting_Started/#installation","text":"This package is tested for and supports Python 3.7+ , although it should be compatible with Python 3.6 . You can install it simply in a virtual environment with: pip install pyhdtoolkit Installation in a virtual environment Don't know what a virtual environment is or how to set it up? Here is a good primer on virtual environments by RealPython. How about a development environment? Sure thing. This repository uses Poetry as a packaging and build tool. To set yourself up, get a local copy through VCS and run: poetry install This repository follows the Google docstring format, uses Black as a code formatter with a default enforced line length of 100 characters, and Pylint as a linter. You can format the code with make format and lint it (which will format first) with make lint . Testing builds are ensured after each commit through Github Actions. You can run tests locally with the predefined make tests , or through poetry run pytest <options> for customized options. How can I easily reproduce your research done with this? This repository comes with an environment.yml file to reproduce my work conda environment, feel free to use it. If you checked out from version control, you can install this environment and add it to your ipython kernel by running make condaenv . If you are comfortable with containers, a fully-fetched one is provided and accessible through Docker as explained below.","title":"Installation"},{"location":"docs/Getting_Started/#using-with-docker","text":"Docker provides an easy way to get access to a fully-fledged environment identical to the one I use, for reproducibility of my results. You can directly pull a pre-built image from Dockerhub with: docker pull fsoubelet/simenv You can then run a server from within the container and bind a local directory to work on. Assuming you pulled the provided image from Dockerhub, run a jupyterlab server on port 8888 with the command: docker run --rm -p 8888 :8888 -e JUPYTER_ENABLE_LAB = yes -v <host_dir_to_mount>:/home/jovyan/work fsoubelet/simenv Any jupyter notebook or Python files in the mounted directory can then be used / ran with an environment identical to mine.","title":"Using With Docker"},{"location":"docs/Getting_Started/#citing","text":"If you have a use of these codes, please consider citing them. The repository has a DOI provided by Zenodo , and all versions can be cited with the following BibTeX entry: @software { pyhdtoolkit , author = {Felix Soubelet} , title = {fsoubelet/PyhDToolkit} , publisher = {Zenodo} , doi = {10.5281/zenodo.4268804} , url = {https://doi.org/10.5281/zenodo.4268804} } To cite a specific version, select the version on the package's page on Zotero and export the BibTex entry at the bottom right of the page.","title":"Citing"},{"location":"reference/pyhdtoolkit/","text":"Module pyhdtoolkit pyhdtoolkit Library ~ ~ ~ ~ ~ ~ ~ pyhdtoolkit is a utility library, written in Python, for my PhD needs. Mainly particle accelerator physics studies and plotting. View Source \"\"\" pyhdtoolkit Library ~~~~~~~~~~~~~~~~~~~ pyhdtoolkit is a utility library, written in Python, for my PhD needs. Mainly particle accelerator physics studies and plotting. :copyright: (c) 2019-2020 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" # Set default logging handler to avoid \"No handler found\" warnings. __title__ = \"pyhdtoolkit\" __description__ = \"An all-in-one toolkit package to easy my Python work in my PhD.\" __url__ = \"https://github.com/fsoubelet/PyhDToolkit\" __version__ = \"0.10.0\" __author__ = \"Felix Soubelet\" __author_email__ = \"felix.soubelet@cern.ch\" __license__ = \"MIT\" Sub-modules pyhdtoolkit.cpymadtools pyhdtoolkit.maths pyhdtoolkit.models pyhdtoolkit.optics pyhdtoolkit.plotting pyhdtoolkit.utils","title":"Index"},{"location":"reference/pyhdtoolkit/#module-pyhdtoolkit","text":"pyhdtoolkit Library ~ ~ ~ ~ ~ ~ ~ pyhdtoolkit is a utility library, written in Python, for my PhD needs. Mainly particle accelerator physics studies and plotting. View Source \"\"\" pyhdtoolkit Library ~~~~~~~~~~~~~~~~~~~ pyhdtoolkit is a utility library, written in Python, for my PhD needs. Mainly particle accelerator physics studies and plotting. :copyright: (c) 2019-2020 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" # Set default logging handler to avoid \"No handler found\" warnings. __title__ = \"pyhdtoolkit\" __description__ = \"An all-in-one toolkit package to easy my Python work in my PhD.\" __url__ = \"https://github.com/fsoubelet/PyhDToolkit\" __version__ = \"0.10.0\" __author__ = \"Felix Soubelet\" __author_email__ = \"felix.soubelet@cern.ch\" __license__ = \"MIT\"","title":"Module pyhdtoolkit"},{"location":"reference/pyhdtoolkit/#sub-modules","text":"pyhdtoolkit.cpymadtools pyhdtoolkit.maths pyhdtoolkit.models pyhdtoolkit.optics pyhdtoolkit.plotting pyhdtoolkit.utils","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/cpymadtools/","text":"Module pyhdtoolkit.cpymadtools cpymadtools package ~ ~ ~ ~ ~ ~ ~ cpymadtools is a collection of utilities that integrate within my workflow with the cpymad library. View Source \"\"\" cpymadtools package ~~~~~~~~~~~~~~~~~~~ cpymadtools is a collection of utilities that integrate within my workflow with the `cpymad` library. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .errors import misalign_lhc_ir_quadrupoles , misalign_lhc_triplets , switch_magnetic_errors from .generators import LatticeGenerator from .latwiss import plot_latwiss , plot_machine_survey from .matching import get_closest_tune_approach , get_lhc_tune_and_chroma_knobs , match_tunes_and_chromaticities from .orbit import correct_lhc_orbit , get_current_orbit_setup , lhc_orbit_variables , setup_lhc_orbit from .parameters import query_beam_attributes from .plotters import AperturePlotter , DynamicAperturePlotter , PhaseSpacePlotter , TuneDiagramPlotter from .ptc import get_amplitude_detuning , get_rdts from .special import ( apply_lhc_colinearity_knob , apply_lhc_coupling_knob , apply_lhc_rigidity_waist_shift_knob , deactivate_lhc_arc_sextupoles , install_ac_dipole , make_lhc_beams , make_lhc_thin , make_sixtrack_output , power_landau_octupoles , re_cycle_sequence , vary_independent_ir_quadrupoles , ) from .track import track_single_particle from .tune import make_footprint_table from .twiss import get_ips_twiss , get_ir_twiss , get_twiss_tfs from .utils import get_table_tfs Sub-modules pyhdtoolkit.cpymadtools.constants pyhdtoolkit.cpymadtools.errors pyhdtoolkit.cpymadtools.generators pyhdtoolkit.cpymadtools.latwiss pyhdtoolkit.cpymadtools.matching pyhdtoolkit.cpymadtools.orbit pyhdtoolkit.cpymadtools.parameters pyhdtoolkit.cpymadtools.plotters pyhdtoolkit.cpymadtools.ptc pyhdtoolkit.cpymadtools.special pyhdtoolkit.cpymadtools.track pyhdtoolkit.cpymadtools.tune pyhdtoolkit.cpymadtools.twiss pyhdtoolkit.cpymadtools.utils","title":"Index"},{"location":"reference/pyhdtoolkit/cpymadtools/#module-pyhdtoolkitcpymadtools","text":"cpymadtools package ~ ~ ~ ~ ~ ~ ~ cpymadtools is a collection of utilities that integrate within my workflow with the cpymad library. View Source \"\"\" cpymadtools package ~~~~~~~~~~~~~~~~~~~ cpymadtools is a collection of utilities that integrate within my workflow with the `cpymad` library. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .errors import misalign_lhc_ir_quadrupoles , misalign_lhc_triplets , switch_magnetic_errors from .generators import LatticeGenerator from .latwiss import plot_latwiss , plot_machine_survey from .matching import get_closest_tune_approach , get_lhc_tune_and_chroma_knobs , match_tunes_and_chromaticities from .orbit import correct_lhc_orbit , get_current_orbit_setup , lhc_orbit_variables , setup_lhc_orbit from .parameters import query_beam_attributes from .plotters import AperturePlotter , DynamicAperturePlotter , PhaseSpacePlotter , TuneDiagramPlotter from .ptc import get_amplitude_detuning , get_rdts from .special import ( apply_lhc_colinearity_knob , apply_lhc_coupling_knob , apply_lhc_rigidity_waist_shift_knob , deactivate_lhc_arc_sextupoles , install_ac_dipole , make_lhc_beams , make_lhc_thin , make_sixtrack_output , power_landau_octupoles , re_cycle_sequence , vary_independent_ir_quadrupoles , ) from .track import track_single_particle from .tune import make_footprint_table from .twiss import get_ips_twiss , get_ir_twiss , get_twiss_tfs from .utils import get_table_tfs","title":"Module pyhdtoolkit.cpymadtools"},{"location":"reference/pyhdtoolkit/cpymadtools/#sub-modules","text":"pyhdtoolkit.cpymadtools.constants pyhdtoolkit.cpymadtools.errors pyhdtoolkit.cpymadtools.generators pyhdtoolkit.cpymadtools.latwiss pyhdtoolkit.cpymadtools.matching pyhdtoolkit.cpymadtools.orbit pyhdtoolkit.cpymadtools.parameters pyhdtoolkit.cpymadtools.plotters pyhdtoolkit.cpymadtools.ptc pyhdtoolkit.cpymadtools.special pyhdtoolkit.cpymadtools.track pyhdtoolkit.cpymadtools.tune pyhdtoolkit.cpymadtools.twiss pyhdtoolkit.cpymadtools.utils","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/cpymadtools/constants/","text":"Module pyhdtoolkit.cpymadtools.constants Module cpymadtools.constants Created on 2020.02.02 View Source \"\"\" Module cpymadtools.constants ---------------------------- Created on 2020.02.02 :author: Felix Soubelet (felix.soubelet@cern.ch) Specific constants to be used in cpymadtools functions, to help with consistency. \"\"\" DEFAULT_TWISS_COLUMNS = [ \"name\" , \"s\" , \"x\" , \"y\" , \"px\" , \"py\" , \"betx\" , \"bety\" , \"alfx\" , \"alfy\" , \"dx\" , \"dy\" , \"mux\" , \"muy\" , \"r11\" , \"r12\" , \"r21\" , \"r22\" , \"beta11\" , \"beta12\" , \"beta21\" , \"beta22\" , ] LHC_CROSSING_SCHEMES = { \"flat\" : {}, \"lhc_inj\" : { \"on_x1\" : - 170 , \"on_sep1\" : - 2 , \"on_x2\" : 170 , \"on_sep2\" : 3.5 , \"on_x5\" : 170 , \"on_sep5\" : 2 , \"on_x8\" : - 170 , \"on_sep8\" : - 3.5 , \"phi_IR1\" : 90 , \"phi_IR5\" : 0 , }, \"lhc_top\" : { \"on_x1\" : - 160 , \"on_sep1\" : - 0.55 , \"on_x2\" : 200 , \"on_sep2\" : 1.4 , \"on_x5\" : 160 , \"on_sep5\" : 0.55 , \"on_oh5\" : - 1.8 , \"on_x8\" : - 250 , \"on_sep8\" : - 1 , \"phi_IR1\" : 90 , \"phi_IR5\" : 0 , }, \"hllhc_inj\" : { \"on_x1\" : 295 , \"on_sep1\" : - 2 , \"on_x2\" : 170 , \"on_sep2\" : 3.5 , \"on_x5\" : 295 , \"on_sep5\" : 2 , \"on_x8\" : - 170 , \"on_sep8\" : - 3.5 , # phis should be set by optics }, \"hllhc_top\" : { \"on_x1\" : 250 , \"on_sep1\" : - 0.75 , # 0.55 \"on_x2\" : 170 , \"on_sep2\" : 1 , # 1.4 \"on_x5\" : 250 , \"on_sep5\" : 0.75 , # 0.55 # 'on_oh5': -1.8, \"on_x8\" : - 200 , # - 250 \"on_sep8\" : - 1 , \"on_crab1\" : - 190 , \"on_crab5\" : - 190 , # phis should be set by optics }, } # All values are defined as multiples of 0.3 / Energy CORRECTOR_LIMITS = { \"HLLHC\" : dict ( # MQSX1 = mvars [ 'kmax_MQSXF'], MQSX1 = 0.600 / 0.050 , # 0.6 T . m @ 50 mm in IR1&IR5 MQSX2 = 1.360 / 0.017 , # 1.36 T @ 17 mm in IR2&IR8 # MCSX1 = mvars [ 'kmax_MCSXF'], MCSX1 = 0.050 * 2 / ( 0.050 ** 2 ), # 0.050 Tm @ 50 mm in IR1&IR5 MCSX2 = 0.028 * 2 / ( 0.017 ** 2 ), # 0.028 T @ 17 mm in IR2&IR8 # MCSSX1 = mvars [ 'kmax_MCSSXF'], MCSSX1 = 0.050 * 2 / ( 0.050 ** 2 ), # 0.050 Tm @ 50 mm in IR1&IR5 MCSSX2 = 0.11 * 2 / ( 0.017 ** 2 ), # 0.11 T @ 17 mm in IR2&IR8 # MCOX1 = mvars [ 'kmax_MCOXF'], MCOX1 = 0.030 * 6 / ( 0.050 ** 3 ), # 0.030 Tm @ 50 mm in IR1&IR5 MCOX2 = 0.045 * 6 / ( 0.017 ** 3 ), # 0.045 T @ 17 mm in IR2&IR8 # MCOSX1 = mvars [ 'kmax_MCOSXF'], MCOSX1 = 0.030 * 6 / ( 0.050 ** 3 ), # 0.030 Tm @ 50 mm in IR1&IR5 MCOSX2 = 0.048 * 6 / ( 0.017 ** 3 ), # 0.048 T @ 17 mm in IR2&IR8 # MCDX1 = mvars [ 'kmax_MCDXF'], MCDX1 = 0.030 * 24 / ( 0.050 ** 4 ), # 0.030 Tm @ 50 mm in IR1&IR5 # MCDSX1 = mvars [ 'kmax_MCDSXF'], MCDSX1 = 0.030 * 24 / ( 0.050 ** 4 ), # 0.030 Tm @ 50 mm in IR1&IR5 # MCTX1 = mvars [ 'kmax_MCTXF'], MCTX1 = 0.07 * 120 / ( 0.050 ** 5 ), # 0.070 Tm @ 50 mm in IR1&IR5 MCTX2 = 0.01 * 120 / ( 0.017 ** 5 ), # 0.010 Tm @ 17 mm in IR1&IR5 # MCTSX1 = mvars [ 'kmax_MCTSXF'], MCTSX1 = 0.07 * 120 / ( 0.050 ** 5 ), # 0.070 Tm @ 50 mm in IR1&IR5 MQT = 120 , # 120 T / m MQS = 120 , # 120 T / m MS = 1.280 * 2 / ( 0.017 ** 2 ), # 1.28 T @ 17 mm MSS = 1.280 * 2 / ( 0.017 ** 2 ), # 1.28 T @ 17 mm MCS = 0.471 * 2 / ( 0.017 ** 2 ), # 0.471 T @ 17 mm MCO = 0.040 * 6 / ( 0.017 ** 3 ), # 0.04 T @ 17 mm MCD = 0.100 * 24 / ( 0.017 ** 4 ), # 0.1 T @ 17 mm MO = 0.29 * 6 / ( 0.017 ** 3 ), # 0.29 T @ 17 mm ) } FD_FAMILIES = { \"MO\" , \"MS\" , \"MQT\" } # Magnets that have F and D families TWO_FAMILIES = { \"MS\" } # Magnets that have 1 and 2 families SPECIAL_FAMILIES = { \"MQS\" } # Magnets in every second arc Variables CORRECTOR_LIMITS DEFAULT_TWISS_COLUMNS FD_FAMILIES LHC_CROSSING_SCHEMES SPECIAL_FAMILIES TWO_FAMILIES","title":"Constants"},{"location":"reference/pyhdtoolkit/cpymadtools/constants/#module-pyhdtoolkitcpymadtoolsconstants","text":"Module cpymadtools.constants Created on 2020.02.02 View Source \"\"\" Module cpymadtools.constants ---------------------------- Created on 2020.02.02 :author: Felix Soubelet (felix.soubelet@cern.ch) Specific constants to be used in cpymadtools functions, to help with consistency. \"\"\" DEFAULT_TWISS_COLUMNS = [ \"name\" , \"s\" , \"x\" , \"y\" , \"px\" , \"py\" , \"betx\" , \"bety\" , \"alfx\" , \"alfy\" , \"dx\" , \"dy\" , \"mux\" , \"muy\" , \"r11\" , \"r12\" , \"r21\" , \"r22\" , \"beta11\" , \"beta12\" , \"beta21\" , \"beta22\" , ] LHC_CROSSING_SCHEMES = { \"flat\" : {}, \"lhc_inj\" : { \"on_x1\" : - 170 , \"on_sep1\" : - 2 , \"on_x2\" : 170 , \"on_sep2\" : 3.5 , \"on_x5\" : 170 , \"on_sep5\" : 2 , \"on_x8\" : - 170 , \"on_sep8\" : - 3.5 , \"phi_IR1\" : 90 , \"phi_IR5\" : 0 , }, \"lhc_top\" : { \"on_x1\" : - 160 , \"on_sep1\" : - 0.55 , \"on_x2\" : 200 , \"on_sep2\" : 1.4 , \"on_x5\" : 160 , \"on_sep5\" : 0.55 , \"on_oh5\" : - 1.8 , \"on_x8\" : - 250 , \"on_sep8\" : - 1 , \"phi_IR1\" : 90 , \"phi_IR5\" : 0 , }, \"hllhc_inj\" : { \"on_x1\" : 295 , \"on_sep1\" : - 2 , \"on_x2\" : 170 , \"on_sep2\" : 3.5 , \"on_x5\" : 295 , \"on_sep5\" : 2 , \"on_x8\" : - 170 , \"on_sep8\" : - 3.5 , # phis should be set by optics }, \"hllhc_top\" : { \"on_x1\" : 250 , \"on_sep1\" : - 0.75 , # 0.55 \"on_x2\" : 170 , \"on_sep2\" : 1 , # 1.4 \"on_x5\" : 250 , \"on_sep5\" : 0.75 , # 0.55 # 'on_oh5': -1.8, \"on_x8\" : - 200 , # - 250 \"on_sep8\" : - 1 , \"on_crab1\" : - 190 , \"on_crab5\" : - 190 , # phis should be set by optics }, } # All values are defined as multiples of 0.3 / Energy CORRECTOR_LIMITS = { \"HLLHC\" : dict ( # MQSX1 = mvars [ 'kmax_MQSXF'], MQSX1 = 0.600 / 0.050 , # 0.6 T . m @ 50 mm in IR1&IR5 MQSX2 = 1.360 / 0.017 , # 1.36 T @ 17 mm in IR2&IR8 # MCSX1 = mvars [ 'kmax_MCSXF'], MCSX1 = 0.050 * 2 / ( 0.050 ** 2 ), # 0.050 Tm @ 50 mm in IR1&IR5 MCSX2 = 0.028 * 2 / ( 0.017 ** 2 ), # 0.028 T @ 17 mm in IR2&IR8 # MCSSX1 = mvars [ 'kmax_MCSSXF'], MCSSX1 = 0.050 * 2 / ( 0.050 ** 2 ), # 0.050 Tm @ 50 mm in IR1&IR5 MCSSX2 = 0.11 * 2 / ( 0.017 ** 2 ), # 0.11 T @ 17 mm in IR2&IR8 # MCOX1 = mvars [ 'kmax_MCOXF'], MCOX1 = 0.030 * 6 / ( 0.050 ** 3 ), # 0.030 Tm @ 50 mm in IR1&IR5 MCOX2 = 0.045 * 6 / ( 0.017 ** 3 ), # 0.045 T @ 17 mm in IR2&IR8 # MCOSX1 = mvars [ 'kmax_MCOSXF'], MCOSX1 = 0.030 * 6 / ( 0.050 ** 3 ), # 0.030 Tm @ 50 mm in IR1&IR5 MCOSX2 = 0.048 * 6 / ( 0.017 ** 3 ), # 0.048 T @ 17 mm in IR2&IR8 # MCDX1 = mvars [ 'kmax_MCDXF'], MCDX1 = 0.030 * 24 / ( 0.050 ** 4 ), # 0.030 Tm @ 50 mm in IR1&IR5 # MCDSX1 = mvars [ 'kmax_MCDSXF'], MCDSX1 = 0.030 * 24 / ( 0.050 ** 4 ), # 0.030 Tm @ 50 mm in IR1&IR5 # MCTX1 = mvars [ 'kmax_MCTXF'], MCTX1 = 0.07 * 120 / ( 0.050 ** 5 ), # 0.070 Tm @ 50 mm in IR1&IR5 MCTX2 = 0.01 * 120 / ( 0.017 ** 5 ), # 0.010 Tm @ 17 mm in IR1&IR5 # MCTSX1 = mvars [ 'kmax_MCTSXF'], MCTSX1 = 0.07 * 120 / ( 0.050 ** 5 ), # 0.070 Tm @ 50 mm in IR1&IR5 MQT = 120 , # 120 T / m MQS = 120 , # 120 T / m MS = 1.280 * 2 / ( 0.017 ** 2 ), # 1.28 T @ 17 mm MSS = 1.280 * 2 / ( 0.017 ** 2 ), # 1.28 T @ 17 mm MCS = 0.471 * 2 / ( 0.017 ** 2 ), # 0.471 T @ 17 mm MCO = 0.040 * 6 / ( 0.017 ** 3 ), # 0.04 T @ 17 mm MCD = 0.100 * 24 / ( 0.017 ** 4 ), # 0.1 T @ 17 mm MO = 0.29 * 6 / ( 0.017 ** 3 ), # 0.29 T @ 17 mm ) } FD_FAMILIES = { \"MO\" , \"MS\" , \"MQT\" } # Magnets that have F and D families TWO_FAMILIES = { \"MS\" } # Magnets that have 1 and 2 families SPECIAL_FAMILIES = { \"MQS\" } # Magnets in every second arc","title":"Module pyhdtoolkit.cpymadtools.constants"},{"location":"reference/pyhdtoolkit/cpymadtools/constants/#variables","text":"CORRECTOR_LIMITS DEFAULT_TWISS_COLUMNS FD_FAMILIES LHC_CROSSING_SCHEMES SPECIAL_FAMILIES TWO_FAMILIES","title":"Variables"},{"location":"reference/pyhdtoolkit/cpymadtools/errors/","text":"Module pyhdtoolkit.cpymadtools.errors Module cpymadtools.errors Created on 2020.02.03 View Source \"\"\" Module cpymadtools.errors ------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X errors setups and manipulatioins with a cpymad.madx.Madx object, mainly for LHC and HLLHC machines. \"\"\" from typing import Dict , List , Sequence from cpymad.madx import Madx from loguru import logger # ----- Constants ----- # # After number 10 are either MQ or MQT quadrupole elements, which officially belong to the arcs IR_QUADS_PATTERNS : Dict [ int , List [ str ]] = { 1 : [ \"^MQXA.1 {side}{ip:d} \" , \"^MQXFA.[AB]1 {side}{ip:d} \" ], # Q1 LHC, Q1A & Q1B HL-LHC 2 : [ \"^MQXB.[AB]2 {side}{ip:d} \" , \"^MQXB.[AB]2 {side}{ip:d} \" ], # Q2A & Q2B LHC, Q2A & Q2B HL-LHC 3 : [ \"^MQXA.3 {side}{ip:d} \" , \"^MQXFA.[AB]3 {side}{ip:d} \" ], # Q3 LHC, Q3A & Q3B HL-LHC 4 : [ \"^MQY.4 {side}{ip:d} .B {beam:d} \" ], # Q4 LHC & HL-LHC 5 : [ \"^MQML.5 {side}{ip:d} .B {beam:d} \" ], # Q5 LHC & HL-LHC 6 : [ \"^MQML.6 {side}{ip:d} .B {beam:d} \" ], # Q6 LHC & HL-LHC 7 : [ \"^MQM.[AB]7 {side}{ip:d} .B {beam:d} \" ], # Q7A & Q7B LHC & HL-LHC 8 : [ \"^MQML.8 {side}{ip:d} .B {beam:d} \" ], # Q8 LHC & HL-LHC 9 : [ \"^MQM.9 {side}{ip:d} .B {beam:d} \" , \"^MQMC.9 {side}{ip:d} .B {beam:d} \" ], # Q9 3.4m then 2.4m LHC & HL-LHC 10 : [ \"^MQML.10 {side}{ip:d} .B {beam:d} \" ], # Q10 4.8m LHC & HL-LHC } # ----- Utilites ----- # def switch_magnetic_errors ( madx : Madx , ** kwargs ) -> None : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Applies magnetic field orders. This will only work for LHC and HLLHC machines. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Keyword Args: default: sets global default to this value. Defaults to `False`. AB#: sets the default for all of that order, the order being the # number. A# or B#: sets the default for systematic and random of this id. A#s, B#r etc.: sets the specific value. In all kwargs, the order # should be in the range (1...15), where 1 == dipolar field. \"\"\" logger . debug ( \"Setting magnetic errors\" ) global_default = kwargs . get ( \"default\" , False ) for order in range ( 1 , 16 ): logger . trace ( f \"Setting up for order { order } \" ) order_default = kwargs . get ( f \"AB { order : d } \" , global_default ) for ab in \"AB\" : ab_default = kwargs . get ( f \" { ab }{ order : d } \" , order_default ) for sr in \"sr\" : name = f \" { ab }{ order : d }{ sr } \" error_value = int ( kwargs . get ( name , ab_default )) logger . trace ( f \"Setting global for 'ON_ { name } ' to { error_value } \" ) madx . globals [ f \"ON_ { name } \" ] = error_value def misalign_lhc_ir_quadrupoles ( madx : Madx , ip : int , beam : int , quadrupoles : Sequence [ int ], sides : Sequence [ str ] = ( \"r\" , \"l\" ), table : str = \"ir_quads_errors\" , ** kwargs , ) -> None : \"\"\" Apply misalignment errors to IR quadrupoles on a given side of a given IP. In case of a sliced lattice, this will misalign all slices of the magnet together. According to the equipment codes main system, those are Q1 to Q10 included, quads beyond are MQ or MQT which are considered arc elements. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ip (int): the interaction point around which to apply errors. beam (int): beam number to apply the errors to. Unlike triplet quadrupoles which are single aperture, Q4 to Q10 are not and will need this information. quadrupoles (Sequence[int]): the number of quadrupoles to apply errors to. sides (Sequence[str]): sides of the IP to apply error on the triplets, either L or R or both. Defaults to both. table (str): the name of the internal table that will save the assigned errors. Defaults to 'ir_quads_errors'. Keyword Args: Any keyword argument to give to the EALIGN command, including the error to apply (DX, DY, DPSI etc) as a string, like it would be given directly into MAD-X. Warning: One should avoid issuing different errors with several uses of this command as it is unclear to me how MAD-X chooses to handle this internally. Instead, it is advised to give all errors in the same command, which is guaranteed to work. See the last provided example below. Examples: For systematic DX misalignment: misalign_lhc_ir_quadrupoles( madx, ip=1, quadrupoles=[1, 2, 3, 4, 5, 6], beam=1, sides=\"RL\", dx=\"1E-5\" ) For a tilt distribution centered on 1mrad: misalign_lhc_ir_quadrupoles( madx, ip=5, quadrupoles=[7, 8, 9, 10], beam=1, sides=\"RL\", dpsi=\"1E-3 + 8E-4 * TGAUSS(2.5)\" ) For several error types on the elements, here DY and DPSI: misalign_lhc_ir_quadrupoles( madx, ip=1, quadrupoles=list(range(1, 11)), beam=1, sides=\"RL\", dy=1e-5, # ok too as cpymad converts this to a string first dpsi=\"1E-3 + 8E-4 * TGAUSS(2.5)\" ) \"\"\" if ip not in ( 1 , 2 , 5 , 8 ): logger . error ( \"The IP number provided is invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'ip'parameter\" ) if beam and beam not in ( 1 , 2 , 3 , 4 ): logger . error ( \"The beam number provided is invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'beam' parameter\" ) if any ( side . upper () not in ( \"R\" , \"L\" ) for side in sides ): logger . error ( \"The side provided is invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'sides' parameter\" ) sides = [ side . upper () for side in sides ] logger . trace ( \"Clearing error flag\" ) madx . select ( flag = \"error\" , clear = True ) logger . info ( f \"Applying alignment errors to IR quads ' { quadrupoles } ', with arguments { kwargs } \" ) for side in sides : for quad_number in quadrupoles : for quad_pattern in IR_QUADS_PATTERNS [ quad_number ]: # Triplets are single aperture and don't need beam information, others do if quad_number <= 3 : madx . select ( flag = \"error\" , pattern = quad_pattern . format ( side = side , ip = ip )) else : madx . select ( flag = \"error\" , pattern = quad_pattern . format ( side = side , ip = ip , beam = beam )) madx . command . ealign ( ** kwargs ) table = table if table else \"etable\" # guarantee etable command won't fail if someone gives `table=None` logger . debug ( f \"Saving assigned errors in internal table ' { table } '\" ) madx . command . etable ( table = table ) logger . trace ( \"Clearing up error flag\" ) madx . select ( flag = \"error\" , clear = True ) def misalign_lhc_triplets ( madx : Madx , ip : int , sides : Sequence [ str ] = ( \"r\" , \"l\" ), table : str = \"triplet_errors\" , ** kwargs ) -> None : \"\"\" Apply misalignment errors to triplet quadrupoles on a given side of a given IP. In case of a sliced lattice, this will misalign all slices of the magnet together. This is a convenience wrapper around the `misalign_lhc_ir_quadrupoles` function, see that function's docstring for more information. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ip (int): the interaction point around which to apply errors. sides (Sequence[str]): sides of the IP to apply error on the triplets, either L or R or both. Defaults to both. table (str): the name of the internal table that will save the assigned errors. Defaults to 'triplet_errors'. Keyword Args: Any keyword argument to give to the EALIGN command, including the error to apply (DX, DY, DPSI etc) as a string, like it would be given directly into MAD-X. Examples: misalign_lhc_triplets(madx, ip=1, sides=\"RL\", dx=\"1E-5 * TGAUSS(2.5)\") misalign_lhc_triplets(madx, ip=5, sides=\"RL\", dpsi=\"0.001 * TGAUSS(2.5)\") \"\"\" misalign_lhc_ir_quadrupoles ( madx , ip = ip , beam = None , quadrupoles = ( 1 , 2 , 3 ), sides = sides , table = table , ** kwargs ) Variables IR_QUADS_PATTERNS Functions misalign_lhc_ir_quadrupoles def misalign_lhc_ir_quadrupoles ( madx : cpymad . madx . Madx , ip : int , beam : int , quadrupoles : Sequence [ int ], sides : Sequence [ str ] = ( 'r' , 'l' ), table : str = 'ir_quads_errors' , ** kwargs ) -> None Apply misalignment errors to IR quadrupoles on a given side of a given IP. In case of a sliced lattice, this will misalign all slices of the magnet together. According to the equipment codes main system, those are Q1 to Q10 included, quads beyond are MQ or MQT which are considered arc elements. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ip (int): the interaction point around which to apply errors. beam (int): beam number to apply the errors to. Unlike triplet quadrupoles which are single aperture, Q4 to Q10 are not and will need this information. quadrupoles (Sequence[int]): the number of quadrupoles to apply errors to. sides (Sequence[str]): sides of the IP to apply error on the triplets, either L or R or both. Defaults to both. table (str): the name of the internal table that will save the assigned errors. Defaults to 'ir_quads_errors'. Keyword Args: Any keyword argument to give to the EALIGN command, including the error to apply (DX, DY, DPSI etc) as a string, like it would be given directly into MAD-X. Warning: One should avoid issuing different errors with several uses of this command as it is unclear to me how MAD-X chooses to handle this internally. Instead, it is advised to give all errors in the same command, which is guaranteed to work. See the last provided example below. Examples: For systematic DX misalignment: misalign_lhc_ir_quadrupoles( madx, ip=1, quadrupoles=[1, 2, 3, 4, 5, 6], beam=1, sides=\"RL\", dx=\"1E-5\" ) For a tilt distribution centered on 1mrad: misalign_lhc_ir_quadrupoles( madx, ip=5, quadrupoles=[7, 8, 9, 10], beam=1, sides=\"RL\", dpsi=\"1E-3 + 8E-4 * TGAUSS(2.5)\" ) For several error types on the elements, here DY and DPSI: misalign_lhc_ir_quadrupoles( madx, ip=1, quadrupoles=list(range(1, 11)), beam=1, sides=\"RL\", dy=1e-5, # ok too as cpymad converts this to a string first dpsi=\"1E-3 + 8E-4 * TGAUSS(2.5)\" ) View Source def misalign_lhc_ir_quadrupoles ( madx : Madx , ip : int , beam : int , quadrupoles : Sequence [ int ] , sides : Sequence [ str ] = ( \"r\" , \"l\" ), table : str = \"ir_quads_errors\" , ** kwargs , ) -> None : \"\"\" Apply misalignment errors to IR quadrupoles on a given side of a given IP. In case of a sliced lattice, this will misalign all slices of the magnet together. According to the equipment codes main system, those are Q1 to Q10 included, quads beyond are MQ or MQT which are considered arc elements. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ip (int): the interaction point around which to apply errors. beam (int): beam number to apply the errors to. Unlike triplet quadrupoles which are single aperture, Q4 to Q10 are not and will need this information. quadrupoles (Sequence[int]): the number of quadrupoles to apply errors to. sides (Sequence[str]): sides of the IP to apply error on the triplets, either L or R or both. Defaults to both. table (str): the name of the internal table that will save the assigned errors. Defaults to 'ir_quads_errors'. Keyword Args: Any keyword argument to give to the EALIGN command, including the error to apply (DX, DY, DPSI etc) as a string, like it would be given directly into MAD-X. Warning: One should avoid issuing different errors with several uses of this command as it is unclear to me how MAD-X chooses to handle this internally. Instead, it is advised to give all errors in the same command, which is guaranteed to work. See the last provided example below. Examples: For systematic DX misalignment: misalign_lhc_ir_quadrupoles( madx, ip=1, quadrupoles=[1, 2, 3, 4, 5, 6], beam=1, sides=\" RL \", dx=\" 1E-5 \" ) For a tilt distribution centered on 1mrad: misalign_lhc_ir_quadrupoles( madx, ip=5, quadrupoles=[7, 8, 9, 10], beam=1, sides=\" RL \", dpsi=\" 1E-3 + 8E-4 * TGAUSS ( 2.5 ) \" ) For several error types on the elements, here DY and DPSI: misalign_lhc_ir_quadrupoles( madx, ip=1, quadrupoles=list(range(1, 11)), beam=1, sides=\" RL \", dy=1e-5, # ok too as cpymad converts this to a string first dpsi=\" 1E-3 + 8E-4 * TGAUSS ( 2.5 ) \" ) \"\"\" if ip not in ( 1 , 2 , 5 , 8 ) : logger . error ( \"The IP number provided is invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'ip'parameter\" ) if beam and beam not in ( 1 , 2 , 3 , 4 ) : logger . error ( \"The beam number provided is invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'beam' parameter\" ) if any ( side . upper () not in ( \"R\" , \"L\" ) for side in sides ) : logger . error ( \"The side provided is invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'sides' parameter\" ) sides = [ side.upper() for side in sides ] logger . trace ( \"Clearing error flag\" ) madx . select ( flag = \"error\" , clear = True ) logger . info ( f \"Applying alignment errors to IR quads '{quadrupoles}', with arguments {kwargs}\" ) for side in sides : for quad_number in quadrupoles : for quad_pattern in IR_QUADS_PATTERNS [ quad_number ] : # Triplets are single aperture and don 't need beam information, others do if quad_number <= 3: madx.select(flag=\"error\", pattern=quad_pattern.format(side=side, ip=ip)) else: madx.select(flag=\"error\", pattern=quad_pattern.format(side=side, ip=ip, beam=beam)) madx.command.ealign(**kwargs) table = table if table else \"etable\" # guarantee etable command won' t fail if someone gives ` table = None ` logger . debug ( f \"Saving assigned errors in internal table '{table}'\" ) madx . command . etable ( table = table ) logger . trace ( \"Clearing up error flag\" ) madx . select ( flag = \"error\" , clear = True ) misalign_lhc_triplets def misalign_lhc_triplets ( madx : cpymad . madx . Madx , ip : int , sides : Sequence [ str ] = ( 'r' , 'l' ), table : str = 'triplet_errors' , ** kwargs ) -> None Apply misalignment errors to triplet quadrupoles on a given side of a given IP. In case of a sliced lattice, this will misalign all slices of the magnet together. This is a convenience wrapper around the misalign_lhc_ir_quadrupoles function, see that function's docstring for more information. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ip (int): the interaction point around which to apply errors. sides (Sequence[str]): sides of the IP to apply error on the triplets, either L or R or both. Defaults to both. table (str): the name of the internal table that will save the assigned errors. Defaults to 'triplet_errors'. Keyword Args: Any keyword argument to give to the EALIGN command, including the error to apply (DX, DY, DPSI etc) as a string, like it would be given directly into MAD-X. Examples: misalign_lhc_triplets(madx, ip=1, sides=\"RL\", dx=\"1E-5 * TGAUSS(2.5)\") misalign_lhc_triplets(madx, ip=5, sides=\"RL\", dpsi=\"0.001 * TGAUSS(2.5)\") View Source def misalign_lhc_triplets ( madx : Madx , ip : int , sides : Sequence [ str ] = ( \"r\" , \"l\" ), table : str = \"triplet_errors\" , ** kwargs ) -> None : \"\"\" Apply misalignment errors to triplet quadrupoles on a given side of a given IP. In case of a sliced lattice, this will misalign all slices of the magnet together. This is a convenience wrapper around the `misalign_lhc_ir_quadrupoles` function, see that function's docstring for more information. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ip (int): the interaction point around which to apply errors. sides (Sequence[str]): sides of the IP to apply error on the triplets, either L or R or both. Defaults to both. table (str): the name of the internal table that will save the assigned errors. Defaults to 'triplet_errors'. Keyword Args: Any keyword argument to give to the EALIGN command, including the error to apply (DX, DY, DPSI etc) as a string, like it would be given directly into MAD-X. Examples: misalign_lhc_triplets(madx, ip=1, sides=\" RL \", dx=\" 1E-5 * TGAUSS ( 2.5 ) \") misalign_lhc_triplets(madx, ip=5, sides=\" RL \", dpsi=\" 0.001 * TGAUSS ( 2.5 ) \") \"\"\" misalign_lhc_ir_quadrupoles ( madx , ip = ip , beam = None , quadrupoles = ( 1 , 2 , 3 ), sides = sides , table = table , ** kwargs ) switch_magnetic_errors def switch_magnetic_errors ( madx : cpymad . madx . Madx , ** kwargs ) -> None INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Applies magnetic field orders. This will only work for LHC and HLLHC machines. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. Keyword Args: None default None sets global default to this value. Defaults to False . False AB# None sets the default for all of that order, the order being the # number. None A# or B# None sets the default for systematic and random of this id. None A#s, B#r etc. None sets the specific value. In all kwargs, the order # should be in the range (1...15), where 1 == dipolar field. None View Source def switch_magnetic_errors ( madx : Madx , ** kwargs ) -> None : \" \"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Applies magnetic field orders. This will only work for LHC and HLLHC machines. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Keyword Args: default: sets global default to this value. Defaults to `False`. AB#: sets the default for all of that order, the order being the # number. A# or B#: sets the default for systematic and random of this id. A#s, B#r etc.: sets the specific value. In all kwargs, the order # should be in the range (1...15), where 1 == dipolar field. \"\" \" logger . debug ( \"Setting magnetic errors\" ) global_default = kwargs . get ( \"default\" , False ) for order in range ( 1 , 16 ) : logger . trace ( f \"Setting up for order {order}\" ) order_default = kwargs . get ( f \"AB{order:d}\" , global_default ) for ab in \"AB\" : ab_default = kwargs . get ( f \"{ab}{order:d}\" , order_default ) for sr in \"sr\" : name = f \"{ab}{order:d}{sr}\" error_value = int ( kwargs . get ( name , ab_default )) logger . trace ( f \"Setting global for 'ON_{name}' to {error_value}\" ) madx . globals [ f \"ON_{name}\" ] = error_value","title":"Errors"},{"location":"reference/pyhdtoolkit/cpymadtools/errors/#module-pyhdtoolkitcpymadtoolserrors","text":"Module cpymadtools.errors Created on 2020.02.03 View Source \"\"\" Module cpymadtools.errors ------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X errors setups and manipulatioins with a cpymad.madx.Madx object, mainly for LHC and HLLHC machines. \"\"\" from typing import Dict , List , Sequence from cpymad.madx import Madx from loguru import logger # ----- Constants ----- # # After number 10 are either MQ or MQT quadrupole elements, which officially belong to the arcs IR_QUADS_PATTERNS : Dict [ int , List [ str ]] = { 1 : [ \"^MQXA.1 {side}{ip:d} \" , \"^MQXFA.[AB]1 {side}{ip:d} \" ], # Q1 LHC, Q1A & Q1B HL-LHC 2 : [ \"^MQXB.[AB]2 {side}{ip:d} \" , \"^MQXB.[AB]2 {side}{ip:d} \" ], # Q2A & Q2B LHC, Q2A & Q2B HL-LHC 3 : [ \"^MQXA.3 {side}{ip:d} \" , \"^MQXFA.[AB]3 {side}{ip:d} \" ], # Q3 LHC, Q3A & Q3B HL-LHC 4 : [ \"^MQY.4 {side}{ip:d} .B {beam:d} \" ], # Q4 LHC & HL-LHC 5 : [ \"^MQML.5 {side}{ip:d} .B {beam:d} \" ], # Q5 LHC & HL-LHC 6 : [ \"^MQML.6 {side}{ip:d} .B {beam:d} \" ], # Q6 LHC & HL-LHC 7 : [ \"^MQM.[AB]7 {side}{ip:d} .B {beam:d} \" ], # Q7A & Q7B LHC & HL-LHC 8 : [ \"^MQML.8 {side}{ip:d} .B {beam:d} \" ], # Q8 LHC & HL-LHC 9 : [ \"^MQM.9 {side}{ip:d} .B {beam:d} \" , \"^MQMC.9 {side}{ip:d} .B {beam:d} \" ], # Q9 3.4m then 2.4m LHC & HL-LHC 10 : [ \"^MQML.10 {side}{ip:d} .B {beam:d} \" ], # Q10 4.8m LHC & HL-LHC } # ----- Utilites ----- # def switch_magnetic_errors ( madx : Madx , ** kwargs ) -> None : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Applies magnetic field orders. This will only work for LHC and HLLHC machines. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Keyword Args: default: sets global default to this value. Defaults to `False`. AB#: sets the default for all of that order, the order being the # number. A# or B#: sets the default for systematic and random of this id. A#s, B#r etc.: sets the specific value. In all kwargs, the order # should be in the range (1...15), where 1 == dipolar field. \"\"\" logger . debug ( \"Setting magnetic errors\" ) global_default = kwargs . get ( \"default\" , False ) for order in range ( 1 , 16 ): logger . trace ( f \"Setting up for order { order } \" ) order_default = kwargs . get ( f \"AB { order : d } \" , global_default ) for ab in \"AB\" : ab_default = kwargs . get ( f \" { ab }{ order : d } \" , order_default ) for sr in \"sr\" : name = f \" { ab }{ order : d }{ sr } \" error_value = int ( kwargs . get ( name , ab_default )) logger . trace ( f \"Setting global for 'ON_ { name } ' to { error_value } \" ) madx . globals [ f \"ON_ { name } \" ] = error_value def misalign_lhc_ir_quadrupoles ( madx : Madx , ip : int , beam : int , quadrupoles : Sequence [ int ], sides : Sequence [ str ] = ( \"r\" , \"l\" ), table : str = \"ir_quads_errors\" , ** kwargs , ) -> None : \"\"\" Apply misalignment errors to IR quadrupoles on a given side of a given IP. In case of a sliced lattice, this will misalign all slices of the magnet together. According to the equipment codes main system, those are Q1 to Q10 included, quads beyond are MQ or MQT which are considered arc elements. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ip (int): the interaction point around which to apply errors. beam (int): beam number to apply the errors to. Unlike triplet quadrupoles which are single aperture, Q4 to Q10 are not and will need this information. quadrupoles (Sequence[int]): the number of quadrupoles to apply errors to. sides (Sequence[str]): sides of the IP to apply error on the triplets, either L or R or both. Defaults to both. table (str): the name of the internal table that will save the assigned errors. Defaults to 'ir_quads_errors'. Keyword Args: Any keyword argument to give to the EALIGN command, including the error to apply (DX, DY, DPSI etc) as a string, like it would be given directly into MAD-X. Warning: One should avoid issuing different errors with several uses of this command as it is unclear to me how MAD-X chooses to handle this internally. Instead, it is advised to give all errors in the same command, which is guaranteed to work. See the last provided example below. Examples: For systematic DX misalignment: misalign_lhc_ir_quadrupoles( madx, ip=1, quadrupoles=[1, 2, 3, 4, 5, 6], beam=1, sides=\"RL\", dx=\"1E-5\" ) For a tilt distribution centered on 1mrad: misalign_lhc_ir_quadrupoles( madx, ip=5, quadrupoles=[7, 8, 9, 10], beam=1, sides=\"RL\", dpsi=\"1E-3 + 8E-4 * TGAUSS(2.5)\" ) For several error types on the elements, here DY and DPSI: misalign_lhc_ir_quadrupoles( madx, ip=1, quadrupoles=list(range(1, 11)), beam=1, sides=\"RL\", dy=1e-5, # ok too as cpymad converts this to a string first dpsi=\"1E-3 + 8E-4 * TGAUSS(2.5)\" ) \"\"\" if ip not in ( 1 , 2 , 5 , 8 ): logger . error ( \"The IP number provided is invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'ip'parameter\" ) if beam and beam not in ( 1 , 2 , 3 , 4 ): logger . error ( \"The beam number provided is invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'beam' parameter\" ) if any ( side . upper () not in ( \"R\" , \"L\" ) for side in sides ): logger . error ( \"The side provided is invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'sides' parameter\" ) sides = [ side . upper () for side in sides ] logger . trace ( \"Clearing error flag\" ) madx . select ( flag = \"error\" , clear = True ) logger . info ( f \"Applying alignment errors to IR quads ' { quadrupoles } ', with arguments { kwargs } \" ) for side in sides : for quad_number in quadrupoles : for quad_pattern in IR_QUADS_PATTERNS [ quad_number ]: # Triplets are single aperture and don't need beam information, others do if quad_number <= 3 : madx . select ( flag = \"error\" , pattern = quad_pattern . format ( side = side , ip = ip )) else : madx . select ( flag = \"error\" , pattern = quad_pattern . format ( side = side , ip = ip , beam = beam )) madx . command . ealign ( ** kwargs ) table = table if table else \"etable\" # guarantee etable command won't fail if someone gives `table=None` logger . debug ( f \"Saving assigned errors in internal table ' { table } '\" ) madx . command . etable ( table = table ) logger . trace ( \"Clearing up error flag\" ) madx . select ( flag = \"error\" , clear = True ) def misalign_lhc_triplets ( madx : Madx , ip : int , sides : Sequence [ str ] = ( \"r\" , \"l\" ), table : str = \"triplet_errors\" , ** kwargs ) -> None : \"\"\" Apply misalignment errors to triplet quadrupoles on a given side of a given IP. In case of a sliced lattice, this will misalign all slices of the magnet together. This is a convenience wrapper around the `misalign_lhc_ir_quadrupoles` function, see that function's docstring for more information. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ip (int): the interaction point around which to apply errors. sides (Sequence[str]): sides of the IP to apply error on the triplets, either L or R or both. Defaults to both. table (str): the name of the internal table that will save the assigned errors. Defaults to 'triplet_errors'. Keyword Args: Any keyword argument to give to the EALIGN command, including the error to apply (DX, DY, DPSI etc) as a string, like it would be given directly into MAD-X. Examples: misalign_lhc_triplets(madx, ip=1, sides=\"RL\", dx=\"1E-5 * TGAUSS(2.5)\") misalign_lhc_triplets(madx, ip=5, sides=\"RL\", dpsi=\"0.001 * TGAUSS(2.5)\") \"\"\" misalign_lhc_ir_quadrupoles ( madx , ip = ip , beam = None , quadrupoles = ( 1 , 2 , 3 ), sides = sides , table = table , ** kwargs )","title":"Module pyhdtoolkit.cpymadtools.errors"},{"location":"reference/pyhdtoolkit/cpymadtools/errors/#variables","text":"IR_QUADS_PATTERNS","title":"Variables"},{"location":"reference/pyhdtoolkit/cpymadtools/errors/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/errors/#misalign_lhc_ir_quadrupoles","text":"def misalign_lhc_ir_quadrupoles ( madx : cpymad . madx . Madx , ip : int , beam : int , quadrupoles : Sequence [ int ], sides : Sequence [ str ] = ( 'r' , 'l' ), table : str = 'ir_quads_errors' , ** kwargs ) -> None Apply misalignment errors to IR quadrupoles on a given side of a given IP. In case of a sliced lattice, this will misalign all slices of the magnet together. According to the equipment codes main system, those are Q1 to Q10 included, quads beyond are MQ or MQT which are considered arc elements. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ip (int): the interaction point around which to apply errors. beam (int): beam number to apply the errors to. Unlike triplet quadrupoles which are single aperture, Q4 to Q10 are not and will need this information. quadrupoles (Sequence[int]): the number of quadrupoles to apply errors to. sides (Sequence[str]): sides of the IP to apply error on the triplets, either L or R or both. Defaults to both. table (str): the name of the internal table that will save the assigned errors. Defaults to 'ir_quads_errors'. Keyword Args: Any keyword argument to give to the EALIGN command, including the error to apply (DX, DY, DPSI etc) as a string, like it would be given directly into MAD-X. Warning: One should avoid issuing different errors with several uses of this command as it is unclear to me how MAD-X chooses to handle this internally. Instead, it is advised to give all errors in the same command, which is guaranteed to work. See the last provided example below. Examples: For systematic DX misalignment: misalign_lhc_ir_quadrupoles( madx, ip=1, quadrupoles=[1, 2, 3, 4, 5, 6], beam=1, sides=\"RL\", dx=\"1E-5\" ) For a tilt distribution centered on 1mrad: misalign_lhc_ir_quadrupoles( madx, ip=5, quadrupoles=[7, 8, 9, 10], beam=1, sides=\"RL\", dpsi=\"1E-3 + 8E-4 * TGAUSS(2.5)\" ) For several error types on the elements, here DY and DPSI: misalign_lhc_ir_quadrupoles( madx, ip=1, quadrupoles=list(range(1, 11)), beam=1, sides=\"RL\", dy=1e-5, # ok too as cpymad converts this to a string first dpsi=\"1E-3 + 8E-4 * TGAUSS(2.5)\" ) View Source def misalign_lhc_ir_quadrupoles ( madx : Madx , ip : int , beam : int , quadrupoles : Sequence [ int ] , sides : Sequence [ str ] = ( \"r\" , \"l\" ), table : str = \"ir_quads_errors\" , ** kwargs , ) -> None : \"\"\" Apply misalignment errors to IR quadrupoles on a given side of a given IP. In case of a sliced lattice, this will misalign all slices of the magnet together. According to the equipment codes main system, those are Q1 to Q10 included, quads beyond are MQ or MQT which are considered arc elements. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ip (int): the interaction point around which to apply errors. beam (int): beam number to apply the errors to. Unlike triplet quadrupoles which are single aperture, Q4 to Q10 are not and will need this information. quadrupoles (Sequence[int]): the number of quadrupoles to apply errors to. sides (Sequence[str]): sides of the IP to apply error on the triplets, either L or R or both. Defaults to both. table (str): the name of the internal table that will save the assigned errors. Defaults to 'ir_quads_errors'. Keyword Args: Any keyword argument to give to the EALIGN command, including the error to apply (DX, DY, DPSI etc) as a string, like it would be given directly into MAD-X. Warning: One should avoid issuing different errors with several uses of this command as it is unclear to me how MAD-X chooses to handle this internally. Instead, it is advised to give all errors in the same command, which is guaranteed to work. See the last provided example below. Examples: For systematic DX misalignment: misalign_lhc_ir_quadrupoles( madx, ip=1, quadrupoles=[1, 2, 3, 4, 5, 6], beam=1, sides=\" RL \", dx=\" 1E-5 \" ) For a tilt distribution centered on 1mrad: misalign_lhc_ir_quadrupoles( madx, ip=5, quadrupoles=[7, 8, 9, 10], beam=1, sides=\" RL \", dpsi=\" 1E-3 + 8E-4 * TGAUSS ( 2.5 ) \" ) For several error types on the elements, here DY and DPSI: misalign_lhc_ir_quadrupoles( madx, ip=1, quadrupoles=list(range(1, 11)), beam=1, sides=\" RL \", dy=1e-5, # ok too as cpymad converts this to a string first dpsi=\" 1E-3 + 8E-4 * TGAUSS ( 2.5 ) \" ) \"\"\" if ip not in ( 1 , 2 , 5 , 8 ) : logger . error ( \"The IP number provided is invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'ip'parameter\" ) if beam and beam not in ( 1 , 2 , 3 , 4 ) : logger . error ( \"The beam number provided is invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'beam' parameter\" ) if any ( side . upper () not in ( \"R\" , \"L\" ) for side in sides ) : logger . error ( \"The side provided is invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'sides' parameter\" ) sides = [ side.upper() for side in sides ] logger . trace ( \"Clearing error flag\" ) madx . select ( flag = \"error\" , clear = True ) logger . info ( f \"Applying alignment errors to IR quads '{quadrupoles}', with arguments {kwargs}\" ) for side in sides : for quad_number in quadrupoles : for quad_pattern in IR_QUADS_PATTERNS [ quad_number ] : # Triplets are single aperture and don 't need beam information, others do if quad_number <= 3: madx.select(flag=\"error\", pattern=quad_pattern.format(side=side, ip=ip)) else: madx.select(flag=\"error\", pattern=quad_pattern.format(side=side, ip=ip, beam=beam)) madx.command.ealign(**kwargs) table = table if table else \"etable\" # guarantee etable command won' t fail if someone gives ` table = None ` logger . debug ( f \"Saving assigned errors in internal table '{table}'\" ) madx . command . etable ( table = table ) logger . trace ( \"Clearing up error flag\" ) madx . select ( flag = \"error\" , clear = True )","title":"misalign_lhc_ir_quadrupoles"},{"location":"reference/pyhdtoolkit/cpymadtools/errors/#misalign_lhc_triplets","text":"def misalign_lhc_triplets ( madx : cpymad . madx . Madx , ip : int , sides : Sequence [ str ] = ( 'r' , 'l' ), table : str = 'triplet_errors' , ** kwargs ) -> None Apply misalignment errors to triplet quadrupoles on a given side of a given IP. In case of a sliced lattice, this will misalign all slices of the magnet together. This is a convenience wrapper around the misalign_lhc_ir_quadrupoles function, see that function's docstring for more information. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ip (int): the interaction point around which to apply errors. sides (Sequence[str]): sides of the IP to apply error on the triplets, either L or R or both. Defaults to both. table (str): the name of the internal table that will save the assigned errors. Defaults to 'triplet_errors'. Keyword Args: Any keyword argument to give to the EALIGN command, including the error to apply (DX, DY, DPSI etc) as a string, like it would be given directly into MAD-X. Examples: misalign_lhc_triplets(madx, ip=1, sides=\"RL\", dx=\"1E-5 * TGAUSS(2.5)\") misalign_lhc_triplets(madx, ip=5, sides=\"RL\", dpsi=\"0.001 * TGAUSS(2.5)\") View Source def misalign_lhc_triplets ( madx : Madx , ip : int , sides : Sequence [ str ] = ( \"r\" , \"l\" ), table : str = \"triplet_errors\" , ** kwargs ) -> None : \"\"\" Apply misalignment errors to triplet quadrupoles on a given side of a given IP. In case of a sliced lattice, this will misalign all slices of the magnet together. This is a convenience wrapper around the `misalign_lhc_ir_quadrupoles` function, see that function's docstring for more information. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ip (int): the interaction point around which to apply errors. sides (Sequence[str]): sides of the IP to apply error on the triplets, either L or R or both. Defaults to both. table (str): the name of the internal table that will save the assigned errors. Defaults to 'triplet_errors'. Keyword Args: Any keyword argument to give to the EALIGN command, including the error to apply (DX, DY, DPSI etc) as a string, like it would be given directly into MAD-X. Examples: misalign_lhc_triplets(madx, ip=1, sides=\" RL \", dx=\" 1E-5 * TGAUSS ( 2.5 ) \") misalign_lhc_triplets(madx, ip=5, sides=\" RL \", dpsi=\" 0.001 * TGAUSS ( 2.5 ) \") \"\"\" misalign_lhc_ir_quadrupoles ( madx , ip = ip , beam = None , quadrupoles = ( 1 , 2 , 3 ), sides = sides , table = table , ** kwargs )","title":"misalign_lhc_triplets"},{"location":"reference/pyhdtoolkit/cpymadtools/errors/#switch_magnetic_errors","text":"def switch_magnetic_errors ( madx : cpymad . madx . Madx , ** kwargs ) -> None INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Applies magnetic field orders. This will only work for LHC and HLLHC machines. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. Keyword Args: None default None sets global default to this value. Defaults to False . False AB# None sets the default for all of that order, the order being the # number. None A# or B# None sets the default for systematic and random of this id. None A#s, B#r etc. None sets the specific value. In all kwargs, the order # should be in the range (1...15), where 1 == dipolar field. None View Source def switch_magnetic_errors ( madx : Madx , ** kwargs ) -> None : \" \"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Applies magnetic field orders. This will only work for LHC and HLLHC machines. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Keyword Args: default: sets global default to this value. Defaults to `False`. AB#: sets the default for all of that order, the order being the # number. A# or B#: sets the default for systematic and random of this id. A#s, B#r etc.: sets the specific value. In all kwargs, the order # should be in the range (1...15), where 1 == dipolar field. \"\" \" logger . debug ( \"Setting magnetic errors\" ) global_default = kwargs . get ( \"default\" , False ) for order in range ( 1 , 16 ) : logger . trace ( f \"Setting up for order {order}\" ) order_default = kwargs . get ( f \"AB{order:d}\" , global_default ) for ab in \"AB\" : ab_default = kwargs . get ( f \"{ab}{order:d}\" , order_default ) for sr in \"sr\" : name = f \"{ab}{order:d}{sr}\" error_value = int ( kwargs . get ( name , ab_default )) logger . trace ( f \"Setting global for 'ON_{name}' to {error_value}\" ) madx . globals [ f \"ON_{name}\" ] = error_value","title":"switch_magnetic_errors"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/","text":"Module pyhdtoolkit.cpymadtools.generators Module cpymadtools.generators Created on 2019.06.15 View Source \"\"\" Module cpymadtools.generators ----------------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions for generating different lattices for cpymad.madx.Madx input. \"\"\" # ----- Utlites ----- # class LatticeGenerator : \"\"\" A simple class to handle said functions. \"\"\" @staticmethod def generate_base_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_onesext_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, ks1, 0}; mod: multipole, knl:={0, 0, ks2, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_oneoct_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, 0, koct}; mod: multipole, knl:={0, 0, 0, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_tripleterrors_study_reference () -> str : \"\"\" Generate generic script for reference Twiss, to use in a `cpymad.madx.Madx` object. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 twiss; \"\"\" @staticmethod def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str : \"\"\" Generate generic script for tf_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. tf_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For field errors ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma Rr = 0.05; ! Radius for field errors (??) ON_B2R = 1; ! Activate field errors B2r = {tf_error}; ! Set field errors magnitude -> Units of B2 error (will be in 1E-4) exec, SetEfcomp_Q; ! Assign field errors !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" @staticmethod def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str : \"\"\" Generate generic script for ms_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. ms_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet MSErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For longitudinal missalignments ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma ealign, ds := {ms_error} * 1E-3 * TGAUSS(GCUTR); ! Gaussian missalignments in meters !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" Classes LatticeGenerator class LatticeGenerator ( / , * args , ** kwargs ) View Source class LatticeGenerator : \"\"\" A simple class to handle said functions. \"\"\" @staticmethod def generate_base_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_onesext_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, ks1, 0}; mod: multipole, knl:={0, 0, ks2, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_oneoct_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, 0, koct}; mod: multipole, knl:={0, 0, 0, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_tripleterrors_study_reference () -> str : \"\"\" Generate generic script for reference Twiss, to use in a `cpymad.madx.Madx` object. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 twiss; \"\"\" @staticmethod def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str : \"\"\" Generate generic script for tf_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. tf_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For field errors ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma Rr = 0.05; ! Radius for field errors (??) ON_B2R = 1; ! Activate field errors B2r = {tf_error}; ! Set field errors magnitude -> Units of B2 error (will be in 1E-4) exec, SetEfcomp_Q; ! Assign field errors !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" @staticmethod def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str : \"\"\" Generate generic script for ms_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. ms_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet MSErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For longitudinal missalignments ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma ealign, ds := {ms_error} * 1E-3 * TGAUSS(GCUTR); ! Gaussian missalignments in meters !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" Static methods generate_base_cas_lattice def generate_base_cas_lattice ( ) -> str Simple function to help unclutter the notebook. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_base_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" generate_oneoct_cas_lattice def generate_oneoct_cas_lattice ( ) -> str Simple function to help unclutter the notebook. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_oneoct_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, 0, koct}; mod: multipole, knl:={0, 0, 0, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" generate_onesext_cas_lattice def generate_onesext_cas_lattice ( ) -> str Simple function to help unclutter the notebook. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_onesext_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, ks1, 0}; mod: multipole, knl:={0, 0, ks2, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" generate_tripleterrors_study_mserror_job def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str Generate generic script for ms_error Twiss, to use in a cpymad.madx.Madx object. Parameters: Name Type Description Default rand_seed str the random seed to provide MAD for the errors distributions. None ms_error str the misalignment error value (along the s axis). None Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str : \"\"\" Generate generic script for ms_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. ms_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet MSErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For longitudinal missalignments ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma ealign, ds := {ms_error} * 1E-3 * TGAUSS(GCUTR); ! Gaussian missalignments in meters !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" generate_tripleterrors_study_reference def generate_tripleterrors_study_reference ( ) -> str Generate generic script for reference Twiss, to use in a cpymad.madx.Madx object. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_tripleterrors_study_reference () -> str : \"\"\" Generate generic script for reference Twiss, to use in a `cpymad.madx.Madx` object. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 twiss; \"\"\" generate_tripleterrors_study_tferror_job def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str Generate generic script for tf_error Twiss, to use in a cpymad.madx.Madx object. Parameters: Name Type Description Default rand_seed str the random seed to provide MAD for the errors distributions. None tf_error str the misalignment error value (along the s axis). None Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str : \"\"\" Generate generic script for tf_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. tf_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For field errors ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma Rr = 0.05; ! Radius for field errors (??) ON_B2R = 1; ! Activate field errors B2r = {tf_error}; ! Set field errors magnitude -> Units of B2 error (will be in 1E-4) exec, SetEfcomp_Q; ! Assign field errors !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\"","title":"Generators"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#module-pyhdtoolkitcpymadtoolsgenerators","text":"Module cpymadtools.generators Created on 2019.06.15 View Source \"\"\" Module cpymadtools.generators ----------------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions for generating different lattices for cpymad.madx.Madx input. \"\"\" # ----- Utlites ----- # class LatticeGenerator : \"\"\" A simple class to handle said functions. \"\"\" @staticmethod def generate_base_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_onesext_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, ks1, 0}; mod: multipole, knl:={0, 0, ks2, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_oneoct_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, 0, koct}; mod: multipole, knl:={0, 0, 0, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_tripleterrors_study_reference () -> str : \"\"\" Generate generic script for reference Twiss, to use in a `cpymad.madx.Madx` object. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 twiss; \"\"\" @staticmethod def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str : \"\"\" Generate generic script for tf_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. tf_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For field errors ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma Rr = 0.05; ! Radius for field errors (??) ON_B2R = 1; ! Activate field errors B2r = {tf_error}; ! Set field errors magnitude -> Units of B2 error (will be in 1E-4) exec, SetEfcomp_Q; ! Assign field errors !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" @staticmethod def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str : \"\"\" Generate generic script for ms_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. ms_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet MSErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For longitudinal missalignments ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma ealign, ds := {ms_error} * 1E-3 * TGAUSS(GCUTR); ! Gaussian missalignments in meters !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\"","title":"Module pyhdtoolkit.cpymadtools.generators"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#latticegenerator","text":"class LatticeGenerator ( / , * args , ** kwargs ) View Source class LatticeGenerator : \"\"\" A simple class to handle said functions. \"\"\" @staticmethod def generate_base_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_onesext_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, ks1, 0}; mod: multipole, knl:={0, 0, ks2, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_oneoct_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, 0, koct}; mod: multipole, knl:={0, 0, 0, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_tripleterrors_study_reference () -> str : \"\"\" Generate generic script for reference Twiss, to use in a `cpymad.madx.Madx` object. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 twiss; \"\"\" @staticmethod def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str : \"\"\" Generate generic script for tf_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. tf_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For field errors ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma Rr = 0.05; ! Radius for field errors (??) ON_B2R = 1; ! Activate field errors B2r = {tf_error}; ! Set field errors magnitude -> Units of B2 error (will be in 1E-4) exec, SetEfcomp_Q; ! Assign field errors !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" @staticmethod def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str : \"\"\" Generate generic script for ms_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. ms_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet MSErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For longitudinal missalignments ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma ealign, ds := {ms_error} * 1E-3 * TGAUSS(GCUTR); ! Gaussian missalignments in meters !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\"","title":"LatticeGenerator"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#generate_base_cas_lattice","text":"def generate_base_cas_lattice ( ) -> str Simple function to help unclutter the notebook. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_base_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\"","title":"generate_base_cas_lattice"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#generate_oneoct_cas_lattice","text":"def generate_oneoct_cas_lattice ( ) -> str Simple function to help unclutter the notebook. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_oneoct_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, 0, koct}; mod: multipole, knl:={0, 0, 0, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\"","title":"generate_oneoct_cas_lattice"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#generate_onesext_cas_lattice","text":"def generate_onesext_cas_lattice ( ) -> str Simple function to help unclutter the notebook. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_onesext_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, ks1, 0}; mod: multipole, knl:={0, 0, ks2, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\"","title":"generate_onesext_cas_lattice"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#generate_tripleterrors_study_mserror_job","text":"def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str Generate generic script for ms_error Twiss, to use in a cpymad.madx.Madx object. Parameters: Name Type Description Default rand_seed str the random seed to provide MAD for the errors distributions. None ms_error str the misalignment error value (along the s axis). None Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str : \"\"\" Generate generic script for ms_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. ms_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet MSErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For longitudinal missalignments ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma ealign, ds := {ms_error} * 1E-3 * TGAUSS(GCUTR); ! Gaussian missalignments in meters !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\"","title":"generate_tripleterrors_study_mserror_job"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#generate_tripleterrors_study_reference","text":"def generate_tripleterrors_study_reference ( ) -> str Generate generic script for reference Twiss, to use in a cpymad.madx.Madx object. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_tripleterrors_study_reference () -> str : \"\"\" Generate generic script for reference Twiss, to use in a `cpymad.madx.Madx` object. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 twiss; \"\"\"","title":"generate_tripleterrors_study_reference"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#generate_tripleterrors_study_tferror_job","text":"def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str Generate generic script for tf_error Twiss, to use in a cpymad.madx.Madx object. Parameters: Name Type Description Default rand_seed str the random seed to provide MAD for the errors distributions. None tf_error str the misalignment error value (along the s axis). None Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str : \"\"\" Generate generic script for tf_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. tf_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For field errors ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma Rr = 0.05; ! Radius for field errors (??) ON_B2R = 1; ! Activate field errors B2r = {tf_error}; ! Set field errors magnitude -> Units of B2 error (will be in 1E-4) exec, SetEfcomp_Q; ! Assign field errors !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\"","title":"generate_tripleterrors_study_tferror_job"},{"location":"reference/pyhdtoolkit/cpymadtools/latwiss/","text":"Module pyhdtoolkit.cpymadtools.latwiss Module cpymadtools.latwiss Created on 2019.06.15 View Source \"\"\" Module cpymadtools.latwiss -------------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions to elegantly plot the Twiss parameters output of a cpymad.madx.Madx instance after it has ran, or machine survey. \"\"\" from typing import Dict , Tuple import matplotlib.axes import matplotlib.patches as patches import matplotlib.pyplot as plt import pandas as pd from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.utils.defaults import PLOT_PARAMS plt . rcParams . update ( PLOT_PARAMS ) plt . rcParams . update ({ \"xtick.direction\" : \"in\" , \"ytick.direction\" : \"in\" }) # need to reiterate these somehow # ----- Plotters ----- # def plot_latwiss ( madx : Madx , title : str , figsize : Tuple [ int , int ] = ( 18 , 11 ), savefig : str = None , xoffset : float = 0 , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_bpms : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ), k2l_lim : Tuple [ float , float ] = None , ** kwargs , ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is very heavily refactored code, inspired by code from Guido Sterbini. WARNING: This WILL FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xoffset (float): An offset applied to the S coordinate before plotting. This is useful is you want to center a plot around a specific point or element, which would then become located at s = 0. Beware this offset is applied before applying the `xlimits`. Offset defaults to 0 (no change). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_bpms (bool): if True, additional patches will be plotted on the layout subplot to represent Beam Position Monitors. BPMs are plotted in dark grey. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). k2l_lim (Tuple[float, float]): if given, sextupole patches will be plotted on the layout subplot of the figure, and the provided values act as vertical axis limits for the k2l values used for the height of sextupole patches. Keyword Args: Any keyword argument to be transmitted to `_plot_machine_layout`, later on to `plot_lattice_series` and then `matplotlib.patches.Rectangle`, such as lw etc. WARNING: Currently the function tries to plot legends for the different layout patches. The position of the different legends has been hardcoded in corners and might require users to tweak the axis limits (through `k0l_lim`, `k1l_lim` and `k2l_lim`) to ensure legend labels and plotted elements don't overlap. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint: disable=too-many-arguments # Restrict the span of twiss_df to avoid plotting all elements then cropping when xlimits is given logger . info ( \"Plotting optics functions and machine layout\" ) logger . debug ( \"Getting Twiss dataframe from cpymad\" ) twiss_df = madx . table . twiss . dframe () . copy () twiss_df . s = twiss_df . s - xoffset xlimits = ( twiss_df . s . min (), twiss_df . s . max ()) if xlimits is None else xlimits twiss_df = twiss_df [ twiss_df . s . between ( xlimits [ 0 ], xlimits [ 1 ])] if xlimits else twiss_df # Create a subplot for the lattice patches (takes a third of figure) figure = plt . figure ( figsize = figsize ) quadrupole_patches_axis = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) _plot_machine_layout ( madx , quadrupole_patches_axis = quadrupole_patches_axis , title = title , xoffset = xoffset , xlimits = xlimits , plot_dipoles = plot_dipoles , plot_quadrupoles = plot_quadrupoles , plot_bpms = plot_bpms , k0l_lim = k0l_lim , k1l_lim = k1l_lim , k2l_lim = k2l_lim , ** kwargs , ) # Plotting beta functions on remaining two thirds of the figure logger . debug ( \"Setting up betatron functions subplot\" ) betatron_axis = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 2 , sharex = quadrupole_patches_axis ) betatron_axis . plot ( twiss_df . s , twiss_df . betx , label = \"$ \\\\ beta_x$\" , lw = 2 ) betatron_axis . plot ( twiss_df . s , twiss_df . bety , label = \"$ \\\\ beta_y$\" , lw = 2 ) betatron_axis . legend ( loc = 2 ) betatron_axis . set_ylabel ( \"$ \\\\ beta_{x,y}$ $[m]$\" ) betatron_axis . set_xlabel ( \"$S$ $[m]$\" ) logger . trace ( \"Setting up dispersion functions subplot\" ) dispertion_axis = betatron_axis . twinx () dispertion_axis . plot ( twiss_df . s , twiss_df . dx , color = \"brown\" , label = \"$D_x$\" , lw = 2 ) dispertion_axis . plot ( twiss_df . s , twiss_df . dy , ls = \"-.\" , color = \"sienna\" , label = \"$D_y$\" , lw = 2 ) dispertion_axis . legend ( loc = 1 ) dispertion_axis . set_ylabel ( \"$D_{x,y}$ $[m]$\" , color = \"brown\" ) dispertion_axis . tick_params ( axis = \"y\" , labelcolor = \"brown\" ) dispertion_axis . grid ( False ) if beta_ylim : logger . debug ( \"Setting ylim for betatron functions plot\" ) betatron_axis . set_ylim ( beta_ylim ) if disp_ylim : logger . debug ( \"Setting ylim for dispersion plot\" ) dispertion_axis . set_ylim ( disp_ylim ) if xlimits : logger . debug ( \"Setting xlim for longitudinal coordinate\" ) plt . xlim ( xlimits ) if savefig : logger . info ( f \"Saving latwiss plot as { savefig } \" ) plt . savefig ( savefig ) return figure def plot_machine_survey ( madx : Madx , title : str = \"Machine Layout\" , figsize : Tuple [ int , int ] = ( 16 , 11 ), savefig : str = None , show_elements : bool = False , high_orders : bool = False , ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing the machine geometry in 2D. Original code is from Guido Sterbini. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. show_elements (bool): if True, will try to plot by differentiating elements. Experimental, defaults to False. high_orders (bool): if True, plot sextupoles and octupoles when show_elements is True, otherwise only up to quadrupoles. Defaults to False. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" logger . debug ( \"Getting machine survey from cpymad\" ) madx . command . survey () survey = madx . table . survey . dframe () figure = plt . figure ( figsize = figsize ) if show_elements : logger . debug ( \"Plotting survey with elements differentiation\" ) element_dfs = _make_survey_groups ( survey ) plt . scatter ( element_dfs [ \"dipoles\" ] . z , element_dfs [ \"dipoles\" ] . x , marker = \".\" , c = element_dfs [ \"dipoles\" ] . s , label = \"Dipoles\" , ) plt . scatter ( element_dfs [ \"quad_foc\" ] . z , element_dfs [ \"quad_foc\" ] . x , marker = \"o\" , color = \"blue\" , label = \"QF\" , ) plt . scatter ( element_dfs [ \"quad_defoc\" ] . z , element_dfs [ \"quad_defoc\" ] . x , marker = \"o\" , color = \"red\" , label = \"QD\" , ) if high_orders : logger . debug ( \"Plotting high order magnetic elements (up to octupoles)\" ) plt . scatter ( element_dfs [ \"sextupoles\" ] . z , element_dfs [ \"sextupoles\" ] . x , marker = \".\" , color = \"m\" , label = \"MS\" , ) plt . scatter ( element_dfs [ \"octupoles\" ] . z , element_dfs [ \"octupoles\" ] . x , marker = \".\" , color = \"cyan\" , label = \"MO\" , ) plt . legend ( loc = 2 ) else : logger . debug ( \"Plotting survey without elements differentiation\" ) plt . scatter ( survey . z , survey . x , c = survey . s , marker = \".\" ) plt . axis ( \"equal\" ) plt . colorbar () . set_label ( \"$S$ $[m]$\" ) plt . xlabel ( \"$Z$ $[m]$\" ) plt . ylabel ( \"$X$ $[m]$\" ) plt . title ( title ) if savefig : logger . info ( f \"Saving machine survey plot as { savefig } \" ) plt . savefig ( savefig ) return figure # ----- Utility plotters ----- # def _plot_lattice_series ( ax : matplotlib . axes . Axes , series : pd . DataFrame , height : float = 1.0 , v_offset : float = 0.0 , color : str = \"r\" , alpha : float = 0.5 , ** kwargs , ) -> None : \"\"\" Plots the layout of your machine as a patch of rectangles for different element types. Original code from Guido Sterbini. Args: ax (matplotlib.axes.Axes): an existing matplotlib.axis `Axes` object to act on. series (pd.DataFrame): a dataframe with your elements' data. height (float): value to reach for the patch on the y axis. v_offset (float): vertical offset for the patch. color (str): color kwarg to transmit to pyplot. alpha (float): alpha kwarg to transmit to pyplot. Keyword Args: Any kwarg that can be given to matplotlib.patches.Rectangle(), for instance `lw` for the edge line width. \"\"\" ax . add_patch ( patches . Rectangle ( ( series . s - series . l , v_offset - height / 2.0 ), # anchor point series . l , # width height , # height color = color , alpha = alpha , ** kwargs , ) ) def _plot_machine_layout ( madx : Madx , quadrupole_patches_axis : matplotlib . axes . Axes , title : str , xoffset : float = 0 , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_bpms : bool = False , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ), k2l_lim : Tuple [ float , float ] = None , ** kwargs , ) -> None : \"\"\" Provided with an active Cpymad class after having ran a script, will plot the lattice layout and the on a given axis. This is the function that takes care of the machine layout in `plot_latwiss`, and is in theory a private function, though if you know what you are doing you may use it individually. WARNING: This WILL FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. quadrupole_patches_axis (matplotlib.axes.Axes): the axis on which to plot. Will also create the appropriate new axes with `twinx()` to plot the element orders asked for. title (str): title of your plot. xoffset (float): An offset applied to the S coordinate before plotting. This is useful is you want to center a plot around a specific point or element, which would then become located at s = 0. Beware this offset is applied before applying the `xlimits`. Offset defaults to 0 (no change). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_bpms (bool): if True, additional patches will be plotted on the layout subplot to represent Beam Position Monitors. BPMs are plotted in dark grey. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). k2l_lim (Tuple[float, float]): if given, sextupole patches will be plotted on the layout subplot of the figure, and the provided values act as vertical axis limits for the k2l values used for the height of sextupole patches. Keyword Args: Any keyword argument to be transmitted to `_plot_lattice_series`, and later on to `matplotlib.patches.Rectangle`, such as lw etc. WARNING: Currently the function tries to plot legends for the different layout patches. The position of the different legends has been hardcoded in corners and might require users to tweak the axis limits (through `k0l_lim`, `k1l_lim` and `k2l_lim`) to ensure legend labels and plotted elements don't overlap. \"\"\" # pylint: disable=too-many-arguments # Restrict the span of twiss_df to avoid plotting all elements then cropping when xlimits is given logger . trace ( \"Getting Twiss dataframe from cpymad\" ) twiss_df = madx . table . twiss . dframe () . copy () twiss_df . s = twiss_df . s - xoffset twiss_df = twiss_df [ twiss_df . s . between ( xlimits [ 0 ], xlimits [ 1 ])] if xlimits else twiss_df logger . debug ( \"Plotting machine layout\" ) logger . trace ( f \"Plotting from axis ' { quadrupole_patches_axis } '\" ) quadrupole_patches_axis . set_ylabel ( \"$1/f=K_ {1} L$ $[m^{-1}]$\" , color = \"red\" ) # quadrupole in red quadrupole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"red\" ) quadrupole_patches_axis . set_ylim ( k1l_lim ) quadrupole_patches_axis . set_xlim ( xlimits ) quadrupole_patches_axis . set_title ( title ) quadrupole_patches_axis . plot ( twiss_df . s , 0 * twiss_df . s , \"k\" ) # 0-level line quadrupole_patches_axis . grid ( False ) dipole_patches_axis = quadrupole_patches_axis . twinx () dipole_patches_axis . set_ylabel ( \"$ \\\\ theta=K_ {0} L$ $[rad]$\" , color = \"royalblue\" ) # dipoles in blue dipole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"royalblue\" ) dipole_patches_axis . set_ylim ( k0l_lim ) dipole_patches_axis . grid ( False ) # All elements can be defined as a 'multipole', but dipoles can also be defined as 'rbend' or 'sbend', # quadrupoles as 'quadrupoles' and sextupoles as 'sextupoles'. Function does not handle higher orders. logger . debug ( \"Extracting element-specific dataframes\" ) dipoles_df = twiss_df [ ( twiss_df . keyword . isin ([ \"multipole\" , \"rbend\" , \"sbend\" ])) & ( twiss_df . name . str . contains ( \"B\" , case = False )) ] quadrupoles_df = twiss_df [ ( twiss_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( twiss_df . name . str . contains ( \"Q\" , case = False )) ] sextupoles_df = twiss_df [ ( twiss_df . keyword . isin ([ \"multipole\" , \"sextupole\" ])) & ( twiss_df . name . str . contains ( \"S\" , case = False )) ] bpms_df = twiss_df [( twiss_df . keyword . isin ([ \"monitor\" ])) & ( twiss_df . name . str . contains ( \"BPM\" , case = False ))] if plot_dipoles : # beware 'sbend' and 'rbend' have an 'angle' value and not a 'k0l' logger . debug ( \"Plotting dipole patches\" ) plotted_elements = 0 # will help us not declare a label for legend at every patch for dipole_name , dipole in dipoles_df . iterrows (): # by default twiss.dframe() has names as index if dipole . k0l != 0 or dipole . angle != 0 : logger . trace ( f \"Plotting dipole element ' { dipole_name } '\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . k0l if dipole . k0l != 0 else dipole . angle , v_offset = dipole . k0l / 2 if dipole . k0l != 0 else dipole . angle / 2 , color = \"royalblue\" , label = \"MB\" if plotted_elements == 0 else None , ** kwargs , ) plotted_elements += 1 dipole_patches_axis . legend ( loc = 1 , fontsize = 16 ) if plot_quadrupoles : logger . debug ( \"Plotting quadrupole patches\" ) plotted_elements = 0 for quadrupole_name , quadrupole in quadrupoles_df . iterrows (): logger . trace ( f \"Plotting quadrupole element ' { quadrupole_name } '\" ) _plot_lattice_series ( quadrupole_patches_axis , quadrupole , height = quadrupole . k1l , v_offset = quadrupole . k1l / 2 , color = \"r\" , label = \"MQ\" if plotted_elements == 0 else None , ** kwargs , ) plotted_elements += 1 quadrupole_patches_axis . legend ( loc = 2 , fontsize = 16 ) if k2l_lim : logger . debug ( \"Plotting sextupole patches\" ) sextupoles_patches_axis = quadrupole_patches_axis . twinx () sextupoles_patches_axis . set_ylabel ( \"K2L [m$^{-2}$]\" , color = \"darkgoldenrod\" ) sextupoles_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"darkgoldenrod\" ) sextupoles_patches_axis . spines [ \"right\" ] . set_position (( \"axes\" , 1.1 )) sextupoles_patches_axis . set_ylim ( k2l_lim ) plotted_elements = 0 for sextupole_name , sextupole in sextupoles_df . iterrows (): logger . trace ( f \"Plotting sextupole element ' { sextupole_name } '\" ) _plot_lattice_series ( sextupoles_patches_axis , sextupole , height = sextupole . k2l , v_offset = sextupole . k2l / 2 , color = \"goldenrod\" , label = \"MS\" if plotted_elements == 0 else None , ** kwargs , ) plotted_elements += 1 sextupoles_patches_axis . legend ( loc = 3 , fontsize = 16 ) sextupoles_patches_axis . grid ( False ) if plot_bpms : logger . debug ( \"Plotting BPM patches\" ) bpm_patches_axis = quadrupole_patches_axis . twinx () bpm_patches_axis . set_axis_off () # hide yticks, labels etc bpm_patches_axis . set_ylim ( - 1.6 , 1.6 ) plotted_elements = 0 for bpm_name , bpm in bpms_df . iterrows (): logger . trace ( f \"Plotting BPM element ' { bpm_name } '\" ) _plot_lattice_series ( bpm_patches_axis , bpm , height = 2 , v_offset = 0 , color = \"dimgrey\" , label = \"BPM\" if plotted_elements == 0 else None , ** kwargs , ) plotted_elements += 1 bpm_patches_axis . legend ( loc = 4 , fontsize = 16 ) bpm_patches_axis . grid ( False ) # ----- Helpers ----- # def _make_survey_groups ( survey_df : pd . DataFrame ) -> Dict [ str , pd . DataFrame ]: \"\"\" Gets a survey dataframe and returns different sub-dataframes corresponding to different magnetic elements. Args: survey_df (pd.DataFrame): machine survey dataframe obtained from your Madx instance, with <instance>.table.survey.dframe(). Returns: A dictionary containing a dataframe for dipoles, focusing quadrupoles, defocusing quadrupoles, sextupoles and octupoles. The keys are self-explanatory. \"\"\" logger . debug ( \"Getting different element groups dframes from MAD-X survey\" ) return { \"dipoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"sbend\" , \"rbend\" ])) & ( survey_df . name . str . contains ( \"B\" , case = False )) ], \"quad_foc\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) & ( survey_df . name . str . contains ( \"F\" , case = False )) ], \"quad_defoc\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) & ( survey_df . name . str . contains ( \"D\" , case = False )) ], \"sextupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"sextupole\" ])) & ( survey_df . name . str . contains ( \"S\" , case = False )) ], \"octupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"octupole\" ])) & ( survey_df . name . str . contains ( \"O\" , case = False )) ], } Variables PLOT_PARAMS Functions plot_latwiss def plot_latwiss ( madx : cpymad . madx . Madx , title : str , figsize : Tuple [ int , int ] = ( 18 , 11 ), savefig : str = None , xoffset : float = 0 , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_bpms : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ), k2l_lim : Tuple [ float , float ] = None , ** kwargs ) -> matplotlib . figure . Figure Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is very heavily refactored code, inspired by code from Guido Sterbini. WARNING: This WILL FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xoffset (float): An offset applied to the S coordinate before plotting. This is useful is you want to center a plot around a specific point or element, which would then become located at s = 0. Beware this offset is applied before applying the xlimits . Offset defaults to 0 (no change). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_bpms (bool): if True, additional patches will be plotted on the layout subplot to represent Beam Position Monitors. BPMs are plotted in dark grey. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). k2l_lim (Tuple[float, float]): if given, sextupole patches will be plotted on the layout subplot of the figure, and the provided values act as vertical axis limits for the k2l values used for the height of sextupole patches. Keyword Args: Any keyword argument to be transmitted to _plot_machine_layout , later on to plot_lattice_series and then matplotlib.patches.Rectangle , such as lw etc. WARNING: Currently the function tries to plot legends for the different layout patches. The position of the different legends has been hardcoded in corners and might require users to tweak the axis limits (through k0l_lim , k1l_lim and k2l_lim ) to ensure legend labels and plotted elements don't overlap. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source def plot_latwiss ( madx : Madx , title : str , figsize : Tuple [ int , int ] = ( 18 , 11 ), savefig : str = None , xoffset : float = 0 , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_bpms : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ), k2l_lim : Tuple [ float , float ] = None , ** kwargs , ) -> matplotlib . figure . Figure : \" \"\" Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is very heavily refactored code, inspired by code from Guido Sterbini. WARNING: This WILL FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xoffset (float): An offset applied to the S coordinate before plotting. This is useful is you want to center a plot around a specific point or element, which would then become located at s = 0. Beware this offset is applied before applying the `xlimits`. Offset defaults to 0 (no change). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_bpms (bool): if True, additional patches will be plotted on the layout subplot to represent Beam Position Monitors. BPMs are plotted in dark grey. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). k2l_lim (Tuple[float, float]): if given, sextupole patches will be plotted on the layout subplot of the figure, and the provided values act as vertical axis limits for the k2l values used for the height of sextupole patches. Keyword Args: Any keyword argument to be transmitted to `_plot_machine_layout`, later on to `plot_lattice_series` and then `matplotlib.patches.Rectangle`, such as lw etc. WARNING: Currently the function tries to plot legends for the different layout patches. The position of the different legends has been hardcoded in corners and might require users to tweak the axis limits (through `k0l_lim`, `k1l_lim` and `k2l_lim`) to ensure legend labels and plotted elements don't overlap. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\" \" # pylint: disable=too-many-arguments # Restrict the span of twiss_df to avoid plotting all elements then cropping when xlimits is given logger . info ( \"Plotting optics functions and machine layout\" ) logger . debug ( \"Getting Twiss dataframe from cpymad\" ) twiss_df = madx . table . twiss . dframe (). copy () twiss_df . s = twiss_df . s - xoffset xlimits = ( twiss_df . s . min (), twiss_df . s . max ()) if xlimits is None else xlimits twiss_df = twiss_df [ twiss_df . s . between ( xlimits [ 0 ] , xlimits [ 1 ] ) ] if xlimits else twiss_df # Create a subplot for the lattice patches (takes a third of figure) figure = plt . figure ( figsize = figsize ) quadrupole_patches_axis = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) _plot_machine_layout ( madx , quadrupole_patches_axis = quadrupole_patches_axis , title = title , xoffset = xoffset , xlimits = xlimits , plot_dipoles = plot_dipoles , plot_quadrupoles = plot_quadrupoles , plot_bpms = plot_bpms , k0l_lim = k0l_lim , k1l_lim = k1l_lim , k2l_lim = k2l_lim , ** kwargs , ) # Plotting beta functions on remaining two thirds of the figure logger . debug ( \"Setting up betatron functions subplot\" ) betatron_axis = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 2 , sharex = quadrupole_patches_axis ) betatron_axis . plot ( twiss_df . s , twiss_df . betx , label = \"$ \\\\ beta_x$\" , lw = 2 ) betatron_axis . plot ( twiss_df . s , twiss_df . bety , label = \"$ \\\\ beta_y$\" , lw = 2 ) betatron_axis . legend ( loc = 2 ) betatron_axis . set _ylabel ( \"$ \\\\ beta_{x,y}$ $[m]$\" ) betatron_axis . set _xlabel ( \"$S$ $[m]$\" ) logger . trace ( \"Setting up dispersion functions subplot\" ) dispertion_axis = betatron_axis . twinx () dispertion_axis . plot ( twiss_df . s , twiss_df . dx , color = \"brown\" , label = \"$D_x$\" , lw = 2 ) dispertion_axis . plot ( twiss_df . s , twiss_df . dy , ls = \"-.\" , color = \"sienna\" , label = \"$D_y$\" , lw = 2 ) dispertion_axis . legend ( loc = 1 ) dispertion_axis . set _ylabel ( \"$D_{x,y}$ $[m]$\" , color = \"brown\" ) dispertion_axis . tick_params ( axis = \"y\" , labelcolor = \"brown\" ) dispertion_axis . grid ( False ) if beta_ylim : logger . debug ( \"Setting ylim for betatron functions plot\" ) betatron_axis . set _ylim ( beta_ylim ) if disp_ylim : logger . debug ( \"Setting ylim for dispersion plot\" ) dispertion_axis . set _ylim ( disp_ylim ) if xlimits : logger . debug ( \"Setting xlim for longitudinal coordinate\" ) plt . xlim ( xlimits ) if savefig : logger . info ( f \"Saving latwiss plot as {savefig}\" ) plt . savefig ( savefig ) return figure plot_machine_survey def plot_machine_survey ( madx : cpymad . madx . Madx , title : str = 'Machine Layout' , figsize : Tuple [ int , int ] = ( 16 , 11 ), savefig : str = None , show_elements : bool = False , high_orders : bool = False ) -> matplotlib . figure . Figure Provided with an active Cpymad class after having ran a script, will create a plot representing the machine geometry in 2D. Original code is from Guido Sterbini. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None title str title of your plot. None figsize Tuple[int, int] size of the figure, defaults to (16, 10). None savefig str will save the figure if this is not None, using the string value passed. None show_elements bool if True, will try to plot by differentiating elements. Experimental, defaults to False. None high_orders bool if True, plot sextupoles and octupoles when show_elements is True, otherwise only up to quadrupoles. Defaults to False. None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source def plot_machine_survey ( madx : Madx , title : str = \"Machine Layout\" , figsize : Tuple [ int, int ] = ( 16 , 11 ), savefig : str = None , show_elements : bool = False , high_orders : bool = False , ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing the machine geometry in 2D. Original code is from Guido Sterbini. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. show_elements (bool): if True, will try to plot by differentiating elements. Experimental, defaults to False. high_orders (bool): if True, plot sextupoles and octupoles when show_elements is True, otherwise only up to quadrupoles. Defaults to False. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" logger . debug ( \"Getting machine survey from cpymad\" ) madx . command . survey () survey = madx . table . survey . dframe () figure = plt . figure ( figsize = figsize ) if show_elements : logger . debug ( \"Plotting survey with elements differentiation\" ) element_dfs = _make_survey_groups ( survey ) plt . scatter ( element_dfs [ \"dipoles\" ] . z , element_dfs [ \"dipoles\" ] . x , marker = \".\" , c = element_dfs [ \"dipoles\" ] . s , label = \"Dipoles\" , ) plt . scatter ( element_dfs [ \"quad_foc\" ] . z , element_dfs [ \"quad_foc\" ] . x , marker = \"o\" , color = \"blue\" , label = \"QF\" , ) plt . scatter ( element_dfs [ \"quad_defoc\" ] . z , element_dfs [ \"quad_defoc\" ] . x , marker = \"o\" , color = \"red\" , label = \"QD\" , ) if high_orders : logger . debug ( \"Plotting high order magnetic elements (up to octupoles)\" ) plt . scatter ( element_dfs [ \"sextupoles\" ] . z , element_dfs [ \"sextupoles\" ] . x , marker = \".\" , color = \"m\" , label = \"MS\" , ) plt . scatter ( element_dfs [ \"octupoles\" ] . z , element_dfs [ \"octupoles\" ] . x , marker = \".\" , color = \"cyan\" , label = \"MO\" , ) plt . legend ( loc = 2 ) else : logger . debug ( \"Plotting survey without elements differentiation\" ) plt . scatter ( survey . z , survey . x , c = survey . s , marker = \".\" ) plt . axis ( \"equal\" ) plt . colorbar (). set_label ( \"$S$ $[m]$\" ) plt . xlabel ( \"$Z$ $[m]$\" ) plt . ylabel ( \"$X$ $[m]$\" ) plt . title ( title ) if savefig : logger . info ( f \"Saving machine survey plot as {savefig}\" ) plt . savefig ( savefig ) return figure","title":"Latwiss"},{"location":"reference/pyhdtoolkit/cpymadtools/latwiss/#module-pyhdtoolkitcpymadtoolslatwiss","text":"Module cpymadtools.latwiss Created on 2019.06.15 View Source \"\"\" Module cpymadtools.latwiss -------------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions to elegantly plot the Twiss parameters output of a cpymad.madx.Madx instance after it has ran, or machine survey. \"\"\" from typing import Dict , Tuple import matplotlib.axes import matplotlib.patches as patches import matplotlib.pyplot as plt import pandas as pd from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.utils.defaults import PLOT_PARAMS plt . rcParams . update ( PLOT_PARAMS ) plt . rcParams . update ({ \"xtick.direction\" : \"in\" , \"ytick.direction\" : \"in\" }) # need to reiterate these somehow # ----- Plotters ----- # def plot_latwiss ( madx : Madx , title : str , figsize : Tuple [ int , int ] = ( 18 , 11 ), savefig : str = None , xoffset : float = 0 , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_bpms : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ), k2l_lim : Tuple [ float , float ] = None , ** kwargs , ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is very heavily refactored code, inspired by code from Guido Sterbini. WARNING: This WILL FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xoffset (float): An offset applied to the S coordinate before plotting. This is useful is you want to center a plot around a specific point or element, which would then become located at s = 0. Beware this offset is applied before applying the `xlimits`. Offset defaults to 0 (no change). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_bpms (bool): if True, additional patches will be plotted on the layout subplot to represent Beam Position Monitors. BPMs are plotted in dark grey. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). k2l_lim (Tuple[float, float]): if given, sextupole patches will be plotted on the layout subplot of the figure, and the provided values act as vertical axis limits for the k2l values used for the height of sextupole patches. Keyword Args: Any keyword argument to be transmitted to `_plot_machine_layout`, later on to `plot_lattice_series` and then `matplotlib.patches.Rectangle`, such as lw etc. WARNING: Currently the function tries to plot legends for the different layout patches. The position of the different legends has been hardcoded in corners and might require users to tweak the axis limits (through `k0l_lim`, `k1l_lim` and `k2l_lim`) to ensure legend labels and plotted elements don't overlap. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint: disable=too-many-arguments # Restrict the span of twiss_df to avoid plotting all elements then cropping when xlimits is given logger . info ( \"Plotting optics functions and machine layout\" ) logger . debug ( \"Getting Twiss dataframe from cpymad\" ) twiss_df = madx . table . twiss . dframe () . copy () twiss_df . s = twiss_df . s - xoffset xlimits = ( twiss_df . s . min (), twiss_df . s . max ()) if xlimits is None else xlimits twiss_df = twiss_df [ twiss_df . s . between ( xlimits [ 0 ], xlimits [ 1 ])] if xlimits else twiss_df # Create a subplot for the lattice patches (takes a third of figure) figure = plt . figure ( figsize = figsize ) quadrupole_patches_axis = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) _plot_machine_layout ( madx , quadrupole_patches_axis = quadrupole_patches_axis , title = title , xoffset = xoffset , xlimits = xlimits , plot_dipoles = plot_dipoles , plot_quadrupoles = plot_quadrupoles , plot_bpms = plot_bpms , k0l_lim = k0l_lim , k1l_lim = k1l_lim , k2l_lim = k2l_lim , ** kwargs , ) # Plotting beta functions on remaining two thirds of the figure logger . debug ( \"Setting up betatron functions subplot\" ) betatron_axis = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 2 , sharex = quadrupole_patches_axis ) betatron_axis . plot ( twiss_df . s , twiss_df . betx , label = \"$ \\\\ beta_x$\" , lw = 2 ) betatron_axis . plot ( twiss_df . s , twiss_df . bety , label = \"$ \\\\ beta_y$\" , lw = 2 ) betatron_axis . legend ( loc = 2 ) betatron_axis . set_ylabel ( \"$ \\\\ beta_{x,y}$ $[m]$\" ) betatron_axis . set_xlabel ( \"$S$ $[m]$\" ) logger . trace ( \"Setting up dispersion functions subplot\" ) dispertion_axis = betatron_axis . twinx () dispertion_axis . plot ( twiss_df . s , twiss_df . dx , color = \"brown\" , label = \"$D_x$\" , lw = 2 ) dispertion_axis . plot ( twiss_df . s , twiss_df . dy , ls = \"-.\" , color = \"sienna\" , label = \"$D_y$\" , lw = 2 ) dispertion_axis . legend ( loc = 1 ) dispertion_axis . set_ylabel ( \"$D_{x,y}$ $[m]$\" , color = \"brown\" ) dispertion_axis . tick_params ( axis = \"y\" , labelcolor = \"brown\" ) dispertion_axis . grid ( False ) if beta_ylim : logger . debug ( \"Setting ylim for betatron functions plot\" ) betatron_axis . set_ylim ( beta_ylim ) if disp_ylim : logger . debug ( \"Setting ylim for dispersion plot\" ) dispertion_axis . set_ylim ( disp_ylim ) if xlimits : logger . debug ( \"Setting xlim for longitudinal coordinate\" ) plt . xlim ( xlimits ) if savefig : logger . info ( f \"Saving latwiss plot as { savefig } \" ) plt . savefig ( savefig ) return figure def plot_machine_survey ( madx : Madx , title : str = \"Machine Layout\" , figsize : Tuple [ int , int ] = ( 16 , 11 ), savefig : str = None , show_elements : bool = False , high_orders : bool = False , ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing the machine geometry in 2D. Original code is from Guido Sterbini. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. show_elements (bool): if True, will try to plot by differentiating elements. Experimental, defaults to False. high_orders (bool): if True, plot sextupoles and octupoles when show_elements is True, otherwise only up to quadrupoles. Defaults to False. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" logger . debug ( \"Getting machine survey from cpymad\" ) madx . command . survey () survey = madx . table . survey . dframe () figure = plt . figure ( figsize = figsize ) if show_elements : logger . debug ( \"Plotting survey with elements differentiation\" ) element_dfs = _make_survey_groups ( survey ) plt . scatter ( element_dfs [ \"dipoles\" ] . z , element_dfs [ \"dipoles\" ] . x , marker = \".\" , c = element_dfs [ \"dipoles\" ] . s , label = \"Dipoles\" , ) plt . scatter ( element_dfs [ \"quad_foc\" ] . z , element_dfs [ \"quad_foc\" ] . x , marker = \"o\" , color = \"blue\" , label = \"QF\" , ) plt . scatter ( element_dfs [ \"quad_defoc\" ] . z , element_dfs [ \"quad_defoc\" ] . x , marker = \"o\" , color = \"red\" , label = \"QD\" , ) if high_orders : logger . debug ( \"Plotting high order magnetic elements (up to octupoles)\" ) plt . scatter ( element_dfs [ \"sextupoles\" ] . z , element_dfs [ \"sextupoles\" ] . x , marker = \".\" , color = \"m\" , label = \"MS\" , ) plt . scatter ( element_dfs [ \"octupoles\" ] . z , element_dfs [ \"octupoles\" ] . x , marker = \".\" , color = \"cyan\" , label = \"MO\" , ) plt . legend ( loc = 2 ) else : logger . debug ( \"Plotting survey without elements differentiation\" ) plt . scatter ( survey . z , survey . x , c = survey . s , marker = \".\" ) plt . axis ( \"equal\" ) plt . colorbar () . set_label ( \"$S$ $[m]$\" ) plt . xlabel ( \"$Z$ $[m]$\" ) plt . ylabel ( \"$X$ $[m]$\" ) plt . title ( title ) if savefig : logger . info ( f \"Saving machine survey plot as { savefig } \" ) plt . savefig ( savefig ) return figure # ----- Utility plotters ----- # def _plot_lattice_series ( ax : matplotlib . axes . Axes , series : pd . DataFrame , height : float = 1.0 , v_offset : float = 0.0 , color : str = \"r\" , alpha : float = 0.5 , ** kwargs , ) -> None : \"\"\" Plots the layout of your machine as a patch of rectangles for different element types. Original code from Guido Sterbini. Args: ax (matplotlib.axes.Axes): an existing matplotlib.axis `Axes` object to act on. series (pd.DataFrame): a dataframe with your elements' data. height (float): value to reach for the patch on the y axis. v_offset (float): vertical offset for the patch. color (str): color kwarg to transmit to pyplot. alpha (float): alpha kwarg to transmit to pyplot. Keyword Args: Any kwarg that can be given to matplotlib.patches.Rectangle(), for instance `lw` for the edge line width. \"\"\" ax . add_patch ( patches . Rectangle ( ( series . s - series . l , v_offset - height / 2.0 ), # anchor point series . l , # width height , # height color = color , alpha = alpha , ** kwargs , ) ) def _plot_machine_layout ( madx : Madx , quadrupole_patches_axis : matplotlib . axes . Axes , title : str , xoffset : float = 0 , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_bpms : bool = False , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ), k2l_lim : Tuple [ float , float ] = None , ** kwargs , ) -> None : \"\"\" Provided with an active Cpymad class after having ran a script, will plot the lattice layout and the on a given axis. This is the function that takes care of the machine layout in `plot_latwiss`, and is in theory a private function, though if you know what you are doing you may use it individually. WARNING: This WILL FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. quadrupole_patches_axis (matplotlib.axes.Axes): the axis on which to plot. Will also create the appropriate new axes with `twinx()` to plot the element orders asked for. title (str): title of your plot. xoffset (float): An offset applied to the S coordinate before plotting. This is useful is you want to center a plot around a specific point or element, which would then become located at s = 0. Beware this offset is applied before applying the `xlimits`. Offset defaults to 0 (no change). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_bpms (bool): if True, additional patches will be plotted on the layout subplot to represent Beam Position Monitors. BPMs are plotted in dark grey. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). k2l_lim (Tuple[float, float]): if given, sextupole patches will be plotted on the layout subplot of the figure, and the provided values act as vertical axis limits for the k2l values used for the height of sextupole patches. Keyword Args: Any keyword argument to be transmitted to `_plot_lattice_series`, and later on to `matplotlib.patches.Rectangle`, such as lw etc. WARNING: Currently the function tries to plot legends for the different layout patches. The position of the different legends has been hardcoded in corners and might require users to tweak the axis limits (through `k0l_lim`, `k1l_lim` and `k2l_lim`) to ensure legend labels and plotted elements don't overlap. \"\"\" # pylint: disable=too-many-arguments # Restrict the span of twiss_df to avoid plotting all elements then cropping when xlimits is given logger . trace ( \"Getting Twiss dataframe from cpymad\" ) twiss_df = madx . table . twiss . dframe () . copy () twiss_df . s = twiss_df . s - xoffset twiss_df = twiss_df [ twiss_df . s . between ( xlimits [ 0 ], xlimits [ 1 ])] if xlimits else twiss_df logger . debug ( \"Plotting machine layout\" ) logger . trace ( f \"Plotting from axis ' { quadrupole_patches_axis } '\" ) quadrupole_patches_axis . set_ylabel ( \"$1/f=K_ {1} L$ $[m^{-1}]$\" , color = \"red\" ) # quadrupole in red quadrupole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"red\" ) quadrupole_patches_axis . set_ylim ( k1l_lim ) quadrupole_patches_axis . set_xlim ( xlimits ) quadrupole_patches_axis . set_title ( title ) quadrupole_patches_axis . plot ( twiss_df . s , 0 * twiss_df . s , \"k\" ) # 0-level line quadrupole_patches_axis . grid ( False ) dipole_patches_axis = quadrupole_patches_axis . twinx () dipole_patches_axis . set_ylabel ( \"$ \\\\ theta=K_ {0} L$ $[rad]$\" , color = \"royalblue\" ) # dipoles in blue dipole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"royalblue\" ) dipole_patches_axis . set_ylim ( k0l_lim ) dipole_patches_axis . grid ( False ) # All elements can be defined as a 'multipole', but dipoles can also be defined as 'rbend' or 'sbend', # quadrupoles as 'quadrupoles' and sextupoles as 'sextupoles'. Function does not handle higher orders. logger . debug ( \"Extracting element-specific dataframes\" ) dipoles_df = twiss_df [ ( twiss_df . keyword . isin ([ \"multipole\" , \"rbend\" , \"sbend\" ])) & ( twiss_df . name . str . contains ( \"B\" , case = False )) ] quadrupoles_df = twiss_df [ ( twiss_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( twiss_df . name . str . contains ( \"Q\" , case = False )) ] sextupoles_df = twiss_df [ ( twiss_df . keyword . isin ([ \"multipole\" , \"sextupole\" ])) & ( twiss_df . name . str . contains ( \"S\" , case = False )) ] bpms_df = twiss_df [( twiss_df . keyword . isin ([ \"monitor\" ])) & ( twiss_df . name . str . contains ( \"BPM\" , case = False ))] if plot_dipoles : # beware 'sbend' and 'rbend' have an 'angle' value and not a 'k0l' logger . debug ( \"Plotting dipole patches\" ) plotted_elements = 0 # will help us not declare a label for legend at every patch for dipole_name , dipole in dipoles_df . iterrows (): # by default twiss.dframe() has names as index if dipole . k0l != 0 or dipole . angle != 0 : logger . trace ( f \"Plotting dipole element ' { dipole_name } '\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . k0l if dipole . k0l != 0 else dipole . angle , v_offset = dipole . k0l / 2 if dipole . k0l != 0 else dipole . angle / 2 , color = \"royalblue\" , label = \"MB\" if plotted_elements == 0 else None , ** kwargs , ) plotted_elements += 1 dipole_patches_axis . legend ( loc = 1 , fontsize = 16 ) if plot_quadrupoles : logger . debug ( \"Plotting quadrupole patches\" ) plotted_elements = 0 for quadrupole_name , quadrupole in quadrupoles_df . iterrows (): logger . trace ( f \"Plotting quadrupole element ' { quadrupole_name } '\" ) _plot_lattice_series ( quadrupole_patches_axis , quadrupole , height = quadrupole . k1l , v_offset = quadrupole . k1l / 2 , color = \"r\" , label = \"MQ\" if plotted_elements == 0 else None , ** kwargs , ) plotted_elements += 1 quadrupole_patches_axis . legend ( loc = 2 , fontsize = 16 ) if k2l_lim : logger . debug ( \"Plotting sextupole patches\" ) sextupoles_patches_axis = quadrupole_patches_axis . twinx () sextupoles_patches_axis . set_ylabel ( \"K2L [m$^{-2}$]\" , color = \"darkgoldenrod\" ) sextupoles_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"darkgoldenrod\" ) sextupoles_patches_axis . spines [ \"right\" ] . set_position (( \"axes\" , 1.1 )) sextupoles_patches_axis . set_ylim ( k2l_lim ) plotted_elements = 0 for sextupole_name , sextupole in sextupoles_df . iterrows (): logger . trace ( f \"Plotting sextupole element ' { sextupole_name } '\" ) _plot_lattice_series ( sextupoles_patches_axis , sextupole , height = sextupole . k2l , v_offset = sextupole . k2l / 2 , color = \"goldenrod\" , label = \"MS\" if plotted_elements == 0 else None , ** kwargs , ) plotted_elements += 1 sextupoles_patches_axis . legend ( loc = 3 , fontsize = 16 ) sextupoles_patches_axis . grid ( False ) if plot_bpms : logger . debug ( \"Plotting BPM patches\" ) bpm_patches_axis = quadrupole_patches_axis . twinx () bpm_patches_axis . set_axis_off () # hide yticks, labels etc bpm_patches_axis . set_ylim ( - 1.6 , 1.6 ) plotted_elements = 0 for bpm_name , bpm in bpms_df . iterrows (): logger . trace ( f \"Plotting BPM element ' { bpm_name } '\" ) _plot_lattice_series ( bpm_patches_axis , bpm , height = 2 , v_offset = 0 , color = \"dimgrey\" , label = \"BPM\" if plotted_elements == 0 else None , ** kwargs , ) plotted_elements += 1 bpm_patches_axis . legend ( loc = 4 , fontsize = 16 ) bpm_patches_axis . grid ( False ) # ----- Helpers ----- # def _make_survey_groups ( survey_df : pd . DataFrame ) -> Dict [ str , pd . DataFrame ]: \"\"\" Gets a survey dataframe and returns different sub-dataframes corresponding to different magnetic elements. Args: survey_df (pd.DataFrame): machine survey dataframe obtained from your Madx instance, with <instance>.table.survey.dframe(). Returns: A dictionary containing a dataframe for dipoles, focusing quadrupoles, defocusing quadrupoles, sextupoles and octupoles. The keys are self-explanatory. \"\"\" logger . debug ( \"Getting different element groups dframes from MAD-X survey\" ) return { \"dipoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"sbend\" , \"rbend\" ])) & ( survey_df . name . str . contains ( \"B\" , case = False )) ], \"quad_foc\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) & ( survey_df . name . str . contains ( \"F\" , case = False )) ], \"quad_defoc\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) & ( survey_df . name . str . contains ( \"D\" , case = False )) ], \"sextupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"sextupole\" ])) & ( survey_df . name . str . contains ( \"S\" , case = False )) ], \"octupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"octupole\" ])) & ( survey_df . name . str . contains ( \"O\" , case = False )) ], }","title":"Module pyhdtoolkit.cpymadtools.latwiss"},{"location":"reference/pyhdtoolkit/cpymadtools/latwiss/#variables","text":"PLOT_PARAMS","title":"Variables"},{"location":"reference/pyhdtoolkit/cpymadtools/latwiss/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/latwiss/#plot_latwiss","text":"def plot_latwiss ( madx : cpymad . madx . Madx , title : str , figsize : Tuple [ int , int ] = ( 18 , 11 ), savefig : str = None , xoffset : float = 0 , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_bpms : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ), k2l_lim : Tuple [ float , float ] = None , ** kwargs ) -> matplotlib . figure . Figure Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is very heavily refactored code, inspired by code from Guido Sterbini. WARNING: This WILL FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xoffset (float): An offset applied to the S coordinate before plotting. This is useful is you want to center a plot around a specific point or element, which would then become located at s = 0. Beware this offset is applied before applying the xlimits . Offset defaults to 0 (no change). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_bpms (bool): if True, additional patches will be plotted on the layout subplot to represent Beam Position Monitors. BPMs are plotted in dark grey. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). k2l_lim (Tuple[float, float]): if given, sextupole patches will be plotted on the layout subplot of the figure, and the provided values act as vertical axis limits for the k2l values used for the height of sextupole patches. Keyword Args: Any keyword argument to be transmitted to _plot_machine_layout , later on to plot_lattice_series and then matplotlib.patches.Rectangle , such as lw etc. WARNING: Currently the function tries to plot legends for the different layout patches. The position of the different legends has been hardcoded in corners and might require users to tweak the axis limits (through k0l_lim , k1l_lim and k2l_lim ) to ensure legend labels and plotted elements don't overlap. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source def plot_latwiss ( madx : Madx , title : str , figsize : Tuple [ int , int ] = ( 18 , 11 ), savefig : str = None , xoffset : float = 0 , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_bpms : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ), k2l_lim : Tuple [ float , float ] = None , ** kwargs , ) -> matplotlib . figure . Figure : \" \"\" Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is very heavily refactored code, inspired by code from Guido Sterbini. WARNING: This WILL FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xoffset (float): An offset applied to the S coordinate before plotting. This is useful is you want to center a plot around a specific point or element, which would then become located at s = 0. Beware this offset is applied before applying the `xlimits`. Offset defaults to 0 (no change). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_bpms (bool): if True, additional patches will be plotted on the layout subplot to represent Beam Position Monitors. BPMs are plotted in dark grey. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). k2l_lim (Tuple[float, float]): if given, sextupole patches will be plotted on the layout subplot of the figure, and the provided values act as vertical axis limits for the k2l values used for the height of sextupole patches. Keyword Args: Any keyword argument to be transmitted to `_plot_machine_layout`, later on to `plot_lattice_series` and then `matplotlib.patches.Rectangle`, such as lw etc. WARNING: Currently the function tries to plot legends for the different layout patches. The position of the different legends has been hardcoded in corners and might require users to tweak the axis limits (through `k0l_lim`, `k1l_lim` and `k2l_lim`) to ensure legend labels and plotted elements don't overlap. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\" \" # pylint: disable=too-many-arguments # Restrict the span of twiss_df to avoid plotting all elements then cropping when xlimits is given logger . info ( \"Plotting optics functions and machine layout\" ) logger . debug ( \"Getting Twiss dataframe from cpymad\" ) twiss_df = madx . table . twiss . dframe (). copy () twiss_df . s = twiss_df . s - xoffset xlimits = ( twiss_df . s . min (), twiss_df . s . max ()) if xlimits is None else xlimits twiss_df = twiss_df [ twiss_df . s . between ( xlimits [ 0 ] , xlimits [ 1 ] ) ] if xlimits else twiss_df # Create a subplot for the lattice patches (takes a third of figure) figure = plt . figure ( figsize = figsize ) quadrupole_patches_axis = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) _plot_machine_layout ( madx , quadrupole_patches_axis = quadrupole_patches_axis , title = title , xoffset = xoffset , xlimits = xlimits , plot_dipoles = plot_dipoles , plot_quadrupoles = plot_quadrupoles , plot_bpms = plot_bpms , k0l_lim = k0l_lim , k1l_lim = k1l_lim , k2l_lim = k2l_lim , ** kwargs , ) # Plotting beta functions on remaining two thirds of the figure logger . debug ( \"Setting up betatron functions subplot\" ) betatron_axis = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 2 , sharex = quadrupole_patches_axis ) betatron_axis . plot ( twiss_df . s , twiss_df . betx , label = \"$ \\\\ beta_x$\" , lw = 2 ) betatron_axis . plot ( twiss_df . s , twiss_df . bety , label = \"$ \\\\ beta_y$\" , lw = 2 ) betatron_axis . legend ( loc = 2 ) betatron_axis . set _ylabel ( \"$ \\\\ beta_{x,y}$ $[m]$\" ) betatron_axis . set _xlabel ( \"$S$ $[m]$\" ) logger . trace ( \"Setting up dispersion functions subplot\" ) dispertion_axis = betatron_axis . twinx () dispertion_axis . plot ( twiss_df . s , twiss_df . dx , color = \"brown\" , label = \"$D_x$\" , lw = 2 ) dispertion_axis . plot ( twiss_df . s , twiss_df . dy , ls = \"-.\" , color = \"sienna\" , label = \"$D_y$\" , lw = 2 ) dispertion_axis . legend ( loc = 1 ) dispertion_axis . set _ylabel ( \"$D_{x,y}$ $[m]$\" , color = \"brown\" ) dispertion_axis . tick_params ( axis = \"y\" , labelcolor = \"brown\" ) dispertion_axis . grid ( False ) if beta_ylim : logger . debug ( \"Setting ylim for betatron functions plot\" ) betatron_axis . set _ylim ( beta_ylim ) if disp_ylim : logger . debug ( \"Setting ylim for dispersion plot\" ) dispertion_axis . set _ylim ( disp_ylim ) if xlimits : logger . debug ( \"Setting xlim for longitudinal coordinate\" ) plt . xlim ( xlimits ) if savefig : logger . info ( f \"Saving latwiss plot as {savefig}\" ) plt . savefig ( savefig ) return figure","title":"plot_latwiss"},{"location":"reference/pyhdtoolkit/cpymadtools/latwiss/#plot_machine_survey","text":"def plot_machine_survey ( madx : cpymad . madx . Madx , title : str = 'Machine Layout' , figsize : Tuple [ int , int ] = ( 16 , 11 ), savefig : str = None , show_elements : bool = False , high_orders : bool = False ) -> matplotlib . figure . Figure Provided with an active Cpymad class after having ran a script, will create a plot representing the machine geometry in 2D. Original code is from Guido Sterbini. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None title str title of your plot. None figsize Tuple[int, int] size of the figure, defaults to (16, 10). None savefig str will save the figure if this is not None, using the string value passed. None show_elements bool if True, will try to plot by differentiating elements. Experimental, defaults to False. None high_orders bool if True, plot sextupoles and octupoles when show_elements is True, otherwise only up to quadrupoles. Defaults to False. None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source def plot_machine_survey ( madx : Madx , title : str = \"Machine Layout\" , figsize : Tuple [ int, int ] = ( 16 , 11 ), savefig : str = None , show_elements : bool = False , high_orders : bool = False , ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing the machine geometry in 2D. Original code is from Guido Sterbini. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. show_elements (bool): if True, will try to plot by differentiating elements. Experimental, defaults to False. high_orders (bool): if True, plot sextupoles and octupoles when show_elements is True, otherwise only up to quadrupoles. Defaults to False. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" logger . debug ( \"Getting machine survey from cpymad\" ) madx . command . survey () survey = madx . table . survey . dframe () figure = plt . figure ( figsize = figsize ) if show_elements : logger . debug ( \"Plotting survey with elements differentiation\" ) element_dfs = _make_survey_groups ( survey ) plt . scatter ( element_dfs [ \"dipoles\" ] . z , element_dfs [ \"dipoles\" ] . x , marker = \".\" , c = element_dfs [ \"dipoles\" ] . s , label = \"Dipoles\" , ) plt . scatter ( element_dfs [ \"quad_foc\" ] . z , element_dfs [ \"quad_foc\" ] . x , marker = \"o\" , color = \"blue\" , label = \"QF\" , ) plt . scatter ( element_dfs [ \"quad_defoc\" ] . z , element_dfs [ \"quad_defoc\" ] . x , marker = \"o\" , color = \"red\" , label = \"QD\" , ) if high_orders : logger . debug ( \"Plotting high order magnetic elements (up to octupoles)\" ) plt . scatter ( element_dfs [ \"sextupoles\" ] . z , element_dfs [ \"sextupoles\" ] . x , marker = \".\" , color = \"m\" , label = \"MS\" , ) plt . scatter ( element_dfs [ \"octupoles\" ] . z , element_dfs [ \"octupoles\" ] . x , marker = \".\" , color = \"cyan\" , label = \"MO\" , ) plt . legend ( loc = 2 ) else : logger . debug ( \"Plotting survey without elements differentiation\" ) plt . scatter ( survey . z , survey . x , c = survey . s , marker = \".\" ) plt . axis ( \"equal\" ) plt . colorbar (). set_label ( \"$S$ $[m]$\" ) plt . xlabel ( \"$Z$ $[m]$\" ) plt . ylabel ( \"$X$ $[m]$\" ) plt . title ( title ) if savefig : logger . info ( f \"Saving machine survey plot as {savefig}\" ) plt . savefig ( savefig ) return figure","title":"plot_machine_survey"},{"location":"reference/pyhdtoolkit/cpymadtools/matching/","text":"Module pyhdtoolkit.cpymadtools.matching Module cpymadtools.matching Created on 2020.02.03 View Source \"\"\" Module cpymadtools.matching --------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X matchings with a cpymad.madx.Madx object. \"\"\" from typing import Dict , Sequence , Tuple from cpymad.madx import Madx from loguru import logger # ----- Utlites ----- # def get_lhc_tune_and_chroma_knobs ( accelerator : str , beam : int = 1 , telescopic_squeeze : bool = False ) -> Tuple [ str , str , str , str ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get names of knobs needed to match tunes and chromaticities as a tuple of strings. Args: accelerator (str): Accelerator either 'LHC' (dQ[xy], dQp[xy] knobs) or 'HLLHC' (kqt[fd], ks[fd] knobs). beam (int): Beam to use, for the knob names. telescopic_squeeze (bool): if set to True, returns the knobs for Telescopic Squeeze configuration. Defaults to False. Returns: Tuple of strings with knobs for `(qx, qy, dqx, dqy)`. \"\"\" beam = 2 if beam == 4 else beam suffix = \"_sq\" if telescopic_squeeze else \"\" if accelerator . upper () not in ( \"LHC\" , \"HLLHC\" ): logger . error ( \"Invalid accelerator name, only 'LHC' and 'HLLHC' implemented\" ) raise NotImplementedError ( f \"Accelerator ' { accelerator } ' not implemented.\" ) return { \"LHC\" : ( f \"dQx.b { beam }{ suffix } \" , f \"dQy.b { beam }{ suffix } \" , f \"dQpx.b { beam }{ suffix } \" , f \"dQpy.b { beam }{ suffix } \" , ), \"HLLHC\" : ( f \"kqtf.b { beam }{ suffix } \" , f \"kqtd.b { beam }{ suffix } \" , f \"ksf.b { beam }{ suffix } \" , f \"ksd.b { beam }{ suffix } \" , ), }[ accelerator . upper ()] def match_tunes_and_chromaticities ( madx : Madx , accelerator : str = None , sequence : str = None , q1_target : float = None , q2_target : float = None , dq1_target : float = None , dq2_target : float = None , varied_knobs : Sequence [ str ] = None , telescopic_squeeze : bool = True , step : float = 1e-7 , calls : int = 100 , tolerance : float = 1e-21 , ) -> None : \"\"\" Provided with an active `cpymad` class after having ran a script, will run an additional matching command to reach the provided values for tunes and chromaticities. Tune matching is always performed. If chromaticity target values are given, then a matching is done for them, followed by an additionnal matching for both tunes and chromaticities. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. q1_target (float): horizontal tune to match to. q2_target (float): vertical tune to match to. dq1_target (float): horizontal chromaticity to match to. dq2_target (float): vertical chromaticity to match to. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. telescopic_squeeze (bool): LHC specific. If set to True, uses the (HL)LHC knobs for Telescopic Squeeze configuration. Defaults to True as of Run III. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default { accelerator . upper () } values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ - 1 ]), telescopic_squeeze = telescopic_squeeze ) def match ( * args , ** kwargs ): \"\"\"Create matching commands for kwarg targets, varying the given args.\"\"\" logger . debug ( f \"Executing matching commands, using sequence ' { sequence } '\" ) madx . command . match ( chrom = True ) logger . trace ( f \"Targets are given as { kwargs } \" ) madx . command . global_ ( sequence = sequence , ** kwargs ) for variable_name in args : logger . trace ( f \"Creating vary command for knob ' { variable_name } '\" ) madx . command . vary ( name = variable_name , step = step ) madx . command . lmdif ( calls = calls , tolerance = tolerance ) madx . command . endmatch () logger . trace ( \"Performing routine TWISS\" ) madx . twiss () # prevents errors if the user forgets to TWISS before querying tables if q1_target is not None and q2_target is not None and dq1_target is not None and dq2_target is not None : logger . info ( f \"Doing combined matching to Qx= { q1_target } , Qy= { q2_target } , \" f \"dqx= { dq1_target } , dqy= { dq2_target } for sequence ' { sequence } '\" ) logger . trace ( f \"Vary knobs sent are { varied_knobs } \" ) match ( * varied_knobs , q1 = q1_target , q2 = q2_target , dq1 = dq1_target , dq2 = dq2_target ) elif q1_target is not None and q2_target is not None : logger . info ( f \"Matching tunes to Qx= { q1_target } , Qy= { q2_target } for sequence ' { sequence } '\" ) logger . trace ( f \"Vary knobs sent are { varied_knobs [: 2 ] } \" ) match ( * varied_knobs [: 2 ], q1 = q1_target , q2 = q2_target ) # first two in varied_knobs are tune knobs def get_closest_tune_approach ( madx : Madx , accelerator : str = None , sequence : str = None , varied_knobs : Sequence [ str ] = None , telescopic_squeeze : bool = False , explicit_targets : Tuple [ float , float ] = None , step : float = 1e-7 , calls : float = 100 , tolerance : float = 1e-21 , ) -> float : \"\"\" Provided with an active `cpymad` class after having ran a script, tries to match the tunes to their mid-fractional tunes. The difference between this mid-tune and the actual matched tune is the closest tune approach. NOTA BENE: This assumes your lattice has previously been matched to desired tunes and chromaticities, as it will determine the appropriate targets from the Madx instance's internal tables. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. telescopic_squeeze (bool): LHC specific. If set to True, uses the (HL)LHC knobs for Telescopic Squeeze configuration. Defaults to False. explicit_targets (Tuple[float, float]): if given, will be used as matching targets for Qx, Qy. Otherwise, the target is determined as the middle of the current fractional tunes. Defaults to None. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. Returns: The closest tune approach, in absolute value. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default { accelerator . upper () } values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ - 1 ]), telescopic_squeeze = telescopic_squeeze ) logger . debug ( \"Saving knob values to restore after closest tune approach\" ) saved_knobs : Dict [ str , float ] = { knob : madx . globals [ knob ] for knob in varied_knobs } logger . trace ( f \"Saved knobs are { saved_knobs } \" ) if explicit_targets : qx_target , qy_target = explicit_targets q1 , q2 = qx_target , qy_target # the integer part is used later on else : logger . trace ( \"Retrieving tunes and chromaticities from internal tables\" ) q1 , q2 = madx . table . summ . q1 [ 0 ], madx . table . summ . q2 [ 0 ] dq1 , dq2 = madx . table . summ . dq1 [ 0 ], madx . table . summ . dq2 [ 0 ] logger . trace ( f \"Retrieved values are q1 = { q1 } , q2 = { q2 } , dq1 = { dq1 } , dq2 = { dq2 } \" ) logger . trace ( \"Determining target tunes for closest approach\" ) middle_of_fractional_tunes = ( _fractional_tune ( q1 ) + _fractional_tune ( q2 )) / 2 qx_target = int ( q1 ) + middle_of_fractional_tunes qy_target = int ( q2 ) + middle_of_fractional_tunes logger . debug ( f \"Targeting tunes Qx = { qx_target } | Qy = { qy_target } \" ) logger . info ( \"Performing closest tune approach routine, matching should fail at DeltaQ = dqmin\" ) match_tunes_and_chromaticities ( madx , accelerator , sequence , qx_target , qy_target , varied_knobs = varied_knobs , step = step , calls = calls , tolerance = tolerance , ) logger . debug ( \"Retrieving tune separation from internal tables\" ) dqmin = madx . table . summ . q1 [ 0 ] - madx . table . summ . q2 [ 0 ] - ( int ( q1 ) - int ( q2 )) logger . info ( \"Restoring saved knobs\" ) with madx . batch (): madx . globals . update ( saved_knobs ) madx . twiss () return abs ( dqmin ) # ----- Helpers ----- # def _fractional_tune ( tune : float ) -> float : \"\"\" Return only the fractional part of a tune value. Args: tune (float): tune value. Returns: The fractional part. \"\"\" return tune - int ( tune ) # ok since int truncates to lower integer Functions get_closest_tune_approach def get_closest_tune_approach ( madx : cpymad . madx . Madx , accelerator : str = None , sequence : str = None , varied_knobs : Sequence [ str ] = None , telescopic_squeeze : bool = False , explicit_targets : Tuple [ float , float ] = None , step : float = 1e-07 , calls : float = 100 , tolerance : float = 1e-21 ) -> float Provided with an active cpymad class after having ran a script, tries to match the tunes to their mid-fractional tunes. The difference between this mid-tune and the actual matched tune is the closest tune approach. NOTA BENE: This assumes your lattice has previously been matched to desired tunes and chromaticities, as it will determine the appropriate targets from the Madx instance's internal tables. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None accelerator str name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. None sequence str name of the sequence you want to activate for the tune matching. None varied_knobs Sequence[str] the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. None telescopic_squeeze bool LHC specific. If set to True, uses the (HL)LHC knobs for Telescopic Squeeze configuration. Defaults to False. None explicit_targets Tuple[float, float] if given, will be used as matching targets for Qx, Qy. Otherwise, the target is determined as the middle of the current fractional tunes. Defaults to None. None step float step size to use when varying knobs. None calls int max number of varying calls to perform. None tolerance float tolerance for successfull matching. None Returns: Type Description None The closest tune approach, in absolute value. View Source def get_closest_tune_approach ( madx : Madx , accelerator : str = None , sequence : str = None , varied_knobs : Sequence [ str ] = None , telescopic_squeeze : bool = False , explicit_targets : Tuple [ float, float ] = None , step : float = 1e-7 , calls : float = 100 , tolerance : float = 1e-21 , ) -> float : \"\"\" Provided with an active `cpymad` class after having ran a script, tries to match the tunes to their mid-fractional tunes. The difference between this mid-tune and the actual matched tune is the closest tune approach. NOTA BENE: This assumes your lattice has previously been matched to desired tunes and chromaticities, as it will determine the appropriate targets from the Madx instance's internal tables. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\" kqf \", \" ksd \", \" kqf \", \" kqd \"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. telescopic_squeeze (bool): LHC specific. If set to True, uses the (HL)LHC knobs for Telescopic Squeeze configuration. Defaults to False. explicit_targets (Tuple[float, float]): if given, will be used as matching targets for Qx, Qy. Otherwise, the target is determined as the middle of the current fractional tunes. Defaults to None. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. Returns: The closest tune approach, in absolute value. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default {accelerator.upper()} values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ -1 ] ), telescopic_squeeze = telescopic_squeeze ) logger . debug ( \"Saving knob values to restore after closest tune approach\" ) saved_knobs : Dict [ str, float ] = { knob : madx . globals [ knob ] for knob in varied_knobs } logger . trace ( f \"Saved knobs are {saved_knobs}\" ) if explicit_targets : qx_target , qy_target = explicit_targets q1 , q2 = qx_target , qy_target # the integer part is used later on else : logger . trace ( \"Retrieving tunes and chromaticities from internal tables\" ) q1 , q2 = madx . table . summ . q1 [ 0 ] , madx . table . summ . q2 [ 0 ] dq1 , dq2 = madx . table . summ . dq1 [ 0 ] , madx . table . summ . dq2 [ 0 ] logger . trace ( f \"Retrieved values are q1 = {q1}, q2 = {q2}, dq1 = {dq1}, dq2 = {dq2}\" ) logger . trace ( \"Determining target tunes for closest approach\" ) middle_of_fractional_tunes = ( _fractional_tune ( q1 ) + _fractional_tune ( q2 )) / 2 qx_target = int ( q1 ) + middle_of_fractional_tunes qy_target = int ( q2 ) + middle_of_fractional_tunes logger . debug ( f \"Targeting tunes Qx = {qx_target} | Qy = {qy_target}\" ) logger . info ( \"Performing closest tune approach routine, matching should fail at DeltaQ = dqmin\" ) match_tunes_and_chromaticities ( madx , accelerator , sequence , qx_target , qy_target , varied_knobs = varied_knobs , step = step , calls = calls , tolerance = tolerance , ) logger . debug ( \"Retrieving tune separation from internal tables\" ) dqmin = madx . table . summ . q1 [ 0 ] - madx . table . summ . q2 [ 0 ] - ( int ( q1 ) - int ( q2 )) logger . info ( \"Restoring saved knobs\" ) with madx . batch () : madx . globals . update ( saved_knobs ) madx . twiss () return abs ( dqmin ) get_lhc_tune_and_chroma_knobs def get_lhc_tune_and_chroma_knobs ( accelerator : str , beam : int = 1 , telescopic_squeeze : bool = False ) -> Tuple [ str , str , str , str ] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get names of knobs needed to match tunes and chromaticities as a tuple of strings. Parameters: Name Type Description Default accelerator str Accelerator either 'LHC' (dQ[xy], dQp[xy] knobs) or 'HLLHC' (kqt[fd], ks[fd] knobs). None beam int Beam to use, for the knob names. None telescopic_squeeze bool if set to True, returns the knobs for Telescopic Squeeze configuration. Defaults to False. None Returns: Type Description None Tuple of strings with knobs for (qx, qy, dqx, dqy) . View Source def get_lhc_tune_and_chroma_knobs ( accelerator : str , beam : int = 1 , telescopic_squeeze : bool = False ) -> Tuple [ str, str, str, str ] : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get names of knobs needed to match tunes and chromaticities as a tuple of strings. Args: accelerator (str): Accelerator either 'LHC' (dQ[xy], dQp[xy] knobs) or 'HLLHC' (kqt[fd], ks[fd] knobs). beam (int): Beam to use, for the knob names. telescopic_squeeze (bool): if set to True, returns the knobs for Telescopic Squeeze configuration. Defaults to False. Returns: Tuple of strings with knobs for `(qx, qy, dqx, dqy)`. \"\"\" beam = 2 if beam == 4 else beam suffix = \"_sq\" if telescopic_squeeze else \"\" if accelerator . upper () not in ( \"LHC\" , \"HLLHC\" ) : logger . error ( \"Invalid accelerator name, only 'LHC' and 'HLLHC' implemented\" ) raise NotImplementedError ( f \"Accelerator '{accelerator}' not implemented.\" ) return { \"LHC\" : ( f \"dQx.b{beam}{suffix}\" , f \"dQy.b{beam}{suffix}\" , f \"dQpx.b{beam}{suffix}\" , f \"dQpy.b{beam}{suffix}\" , ), \"HLLHC\" : ( f \"kqtf.b{beam}{suffix}\" , f \"kqtd.b{beam}{suffix}\" , f \"ksf.b{beam}{suffix}\" , f \"ksd.b{beam}{suffix}\" , ), } [ accelerator.upper() ] match_tunes_and_chromaticities def match_tunes_and_chromaticities ( madx : cpymad . madx . Madx , accelerator : str = None , sequence : str = None , q1_target : float = None , q2_target : float = None , dq1_target : float = None , dq2_target : float = None , varied_knobs : Sequence [ str ] = None , telescopic_squeeze : bool = True , step : float = 1e-07 , calls : int = 100 , tolerance : float = 1e-21 ) -> None Provided with an active cpymad class after having ran a script, will run an additional matching command to reach the provided values for tunes and chromaticities. Tune matching is always performed. If chromaticity target values are given, then a matching is done for them, followed by an additionnal matching for both tunes and chromaticities. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None accelerator str name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. None sequence str name of the sequence you want to activate for the tune matching. None q1_target float horizontal tune to match to. None q2_target float vertical tune to match to. None dq1_target float horizontal chromaticity to match to. None dq2_target float vertical chromaticity to match to. None varied_knobs Sequence[str] the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. None telescopic_squeeze bool LHC specific. If set to True, uses the (HL)LHC knobs for Telescopic Squeeze configuration. Defaults to True as of Run III. None step float step size to use when varying knobs. None calls int max number of varying calls to perform. None tolerance float tolerance for successfull matching. None View Source def match_tunes_and_chromaticities ( madx : Madx , accelerator : str = None , sequence : str = None , q1_target : float = None , q2_target : float = None , dq1_target : float = None , dq2_target : float = None , varied_knobs : Sequence [ str ] = None , telescopic_squeeze : bool = True , step : float = 1e-7 , calls : int = 100 , tolerance : float = 1e-21 , ) -> None : \"\"\" Provided with an active `cpymad` class after having ran a script, will run an additional matching command to reach the provided values for tunes and chromaticities. Tune matching is always performed. If chromaticity target values are given, then a matching is done for them, followed by an additionnal matching for both tunes and chromaticities. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. q1_target (float): horizontal tune to match to. q2_target (float): vertical tune to match to. dq1_target (float): horizontal chromaticity to match to. dq2_target (float): vertical chromaticity to match to. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\" kqf \", \" ksd \", \" kqf \", \" kqd \"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. telescopic_squeeze (bool): LHC specific. If set to True, uses the (HL)LHC knobs for Telescopic Squeeze configuration. Defaults to True as of Run III. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default {accelerator.upper()} values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ -1 ] ), telescopic_squeeze = telescopic_squeeze ) def match ( * args , ** kwargs ) : \"\"\"Create matching commands for kwarg targets, varying the given args.\"\"\" logger . debug ( f \"Executing matching commands, using sequence '{sequence}'\" ) madx . command . match ( chrom = True ) logger . trace ( f \"Targets are given as {kwargs}\" ) madx . command . global_ ( sequence = sequence , ** kwargs ) for variable_name in args : logger . trace ( f \"Creating vary command for knob '{variable_name}'\" ) madx . command . vary ( name = variable_name , step = step ) madx . command . lmdif ( calls = calls , tolerance = tolerance ) madx . command . endmatch () logger . trace ( \"Performing routine TWISS\" ) madx . twiss () # prevents errors if the user forgets to TWISS before querying tables if q1_target is not None and q2_target is not None and dq1_target is not None and dq2_target is not None : logger . info ( f \"Doing combined matching to Qx={q1_target}, Qy={q2_target}, \" f \"dqx={dq1_target}, dqy={dq2_target} for sequence '{sequence}'\" ) logger . trace ( f \"Vary knobs sent are {varied_knobs}\" ) match ( * varied_knobs , q1 = q1_target , q2 = q2_target , dq1 = dq1_target , dq2 = dq2_target ) elif q1_target is not None and q2_target is not None : logger . info ( f \"Matching tunes to Qx={q1_target}, Qy={q2_target} for sequence '{sequence}'\" ) logger . trace ( f \"Vary knobs sent are {varied_knobs[:2]}\" ) match ( * varied_knobs [ :2 ] , q1 = q1_target , q2 = q2_target ) # first two in varied_knobs are tune knobs","title":"Matching"},{"location":"reference/pyhdtoolkit/cpymadtools/matching/#module-pyhdtoolkitcpymadtoolsmatching","text":"Module cpymadtools.matching Created on 2020.02.03 View Source \"\"\" Module cpymadtools.matching --------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X matchings with a cpymad.madx.Madx object. \"\"\" from typing import Dict , Sequence , Tuple from cpymad.madx import Madx from loguru import logger # ----- Utlites ----- # def get_lhc_tune_and_chroma_knobs ( accelerator : str , beam : int = 1 , telescopic_squeeze : bool = False ) -> Tuple [ str , str , str , str ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get names of knobs needed to match tunes and chromaticities as a tuple of strings. Args: accelerator (str): Accelerator either 'LHC' (dQ[xy], dQp[xy] knobs) or 'HLLHC' (kqt[fd], ks[fd] knobs). beam (int): Beam to use, for the knob names. telescopic_squeeze (bool): if set to True, returns the knobs for Telescopic Squeeze configuration. Defaults to False. Returns: Tuple of strings with knobs for `(qx, qy, dqx, dqy)`. \"\"\" beam = 2 if beam == 4 else beam suffix = \"_sq\" if telescopic_squeeze else \"\" if accelerator . upper () not in ( \"LHC\" , \"HLLHC\" ): logger . error ( \"Invalid accelerator name, only 'LHC' and 'HLLHC' implemented\" ) raise NotImplementedError ( f \"Accelerator ' { accelerator } ' not implemented.\" ) return { \"LHC\" : ( f \"dQx.b { beam }{ suffix } \" , f \"dQy.b { beam }{ suffix } \" , f \"dQpx.b { beam }{ suffix } \" , f \"dQpy.b { beam }{ suffix } \" , ), \"HLLHC\" : ( f \"kqtf.b { beam }{ suffix } \" , f \"kqtd.b { beam }{ suffix } \" , f \"ksf.b { beam }{ suffix } \" , f \"ksd.b { beam }{ suffix } \" , ), }[ accelerator . upper ()] def match_tunes_and_chromaticities ( madx : Madx , accelerator : str = None , sequence : str = None , q1_target : float = None , q2_target : float = None , dq1_target : float = None , dq2_target : float = None , varied_knobs : Sequence [ str ] = None , telescopic_squeeze : bool = True , step : float = 1e-7 , calls : int = 100 , tolerance : float = 1e-21 , ) -> None : \"\"\" Provided with an active `cpymad` class after having ran a script, will run an additional matching command to reach the provided values for tunes and chromaticities. Tune matching is always performed. If chromaticity target values are given, then a matching is done for them, followed by an additionnal matching for both tunes and chromaticities. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. q1_target (float): horizontal tune to match to. q2_target (float): vertical tune to match to. dq1_target (float): horizontal chromaticity to match to. dq2_target (float): vertical chromaticity to match to. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. telescopic_squeeze (bool): LHC specific. If set to True, uses the (HL)LHC knobs for Telescopic Squeeze configuration. Defaults to True as of Run III. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default { accelerator . upper () } values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ - 1 ]), telescopic_squeeze = telescopic_squeeze ) def match ( * args , ** kwargs ): \"\"\"Create matching commands for kwarg targets, varying the given args.\"\"\" logger . debug ( f \"Executing matching commands, using sequence ' { sequence } '\" ) madx . command . match ( chrom = True ) logger . trace ( f \"Targets are given as { kwargs } \" ) madx . command . global_ ( sequence = sequence , ** kwargs ) for variable_name in args : logger . trace ( f \"Creating vary command for knob ' { variable_name } '\" ) madx . command . vary ( name = variable_name , step = step ) madx . command . lmdif ( calls = calls , tolerance = tolerance ) madx . command . endmatch () logger . trace ( \"Performing routine TWISS\" ) madx . twiss () # prevents errors if the user forgets to TWISS before querying tables if q1_target is not None and q2_target is not None and dq1_target is not None and dq2_target is not None : logger . info ( f \"Doing combined matching to Qx= { q1_target } , Qy= { q2_target } , \" f \"dqx= { dq1_target } , dqy= { dq2_target } for sequence ' { sequence } '\" ) logger . trace ( f \"Vary knobs sent are { varied_knobs } \" ) match ( * varied_knobs , q1 = q1_target , q2 = q2_target , dq1 = dq1_target , dq2 = dq2_target ) elif q1_target is not None and q2_target is not None : logger . info ( f \"Matching tunes to Qx= { q1_target } , Qy= { q2_target } for sequence ' { sequence } '\" ) logger . trace ( f \"Vary knobs sent are { varied_knobs [: 2 ] } \" ) match ( * varied_knobs [: 2 ], q1 = q1_target , q2 = q2_target ) # first two in varied_knobs are tune knobs def get_closest_tune_approach ( madx : Madx , accelerator : str = None , sequence : str = None , varied_knobs : Sequence [ str ] = None , telescopic_squeeze : bool = False , explicit_targets : Tuple [ float , float ] = None , step : float = 1e-7 , calls : float = 100 , tolerance : float = 1e-21 , ) -> float : \"\"\" Provided with an active `cpymad` class after having ran a script, tries to match the tunes to their mid-fractional tunes. The difference between this mid-tune and the actual matched tune is the closest tune approach. NOTA BENE: This assumes your lattice has previously been matched to desired tunes and chromaticities, as it will determine the appropriate targets from the Madx instance's internal tables. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. telescopic_squeeze (bool): LHC specific. If set to True, uses the (HL)LHC knobs for Telescopic Squeeze configuration. Defaults to False. explicit_targets (Tuple[float, float]): if given, will be used as matching targets for Qx, Qy. Otherwise, the target is determined as the middle of the current fractional tunes. Defaults to None. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. Returns: The closest tune approach, in absolute value. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default { accelerator . upper () } values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ - 1 ]), telescopic_squeeze = telescopic_squeeze ) logger . debug ( \"Saving knob values to restore after closest tune approach\" ) saved_knobs : Dict [ str , float ] = { knob : madx . globals [ knob ] for knob in varied_knobs } logger . trace ( f \"Saved knobs are { saved_knobs } \" ) if explicit_targets : qx_target , qy_target = explicit_targets q1 , q2 = qx_target , qy_target # the integer part is used later on else : logger . trace ( \"Retrieving tunes and chromaticities from internal tables\" ) q1 , q2 = madx . table . summ . q1 [ 0 ], madx . table . summ . q2 [ 0 ] dq1 , dq2 = madx . table . summ . dq1 [ 0 ], madx . table . summ . dq2 [ 0 ] logger . trace ( f \"Retrieved values are q1 = { q1 } , q2 = { q2 } , dq1 = { dq1 } , dq2 = { dq2 } \" ) logger . trace ( \"Determining target tunes for closest approach\" ) middle_of_fractional_tunes = ( _fractional_tune ( q1 ) + _fractional_tune ( q2 )) / 2 qx_target = int ( q1 ) + middle_of_fractional_tunes qy_target = int ( q2 ) + middle_of_fractional_tunes logger . debug ( f \"Targeting tunes Qx = { qx_target } | Qy = { qy_target } \" ) logger . info ( \"Performing closest tune approach routine, matching should fail at DeltaQ = dqmin\" ) match_tunes_and_chromaticities ( madx , accelerator , sequence , qx_target , qy_target , varied_knobs = varied_knobs , step = step , calls = calls , tolerance = tolerance , ) logger . debug ( \"Retrieving tune separation from internal tables\" ) dqmin = madx . table . summ . q1 [ 0 ] - madx . table . summ . q2 [ 0 ] - ( int ( q1 ) - int ( q2 )) logger . info ( \"Restoring saved knobs\" ) with madx . batch (): madx . globals . update ( saved_knobs ) madx . twiss () return abs ( dqmin ) # ----- Helpers ----- # def _fractional_tune ( tune : float ) -> float : \"\"\" Return only the fractional part of a tune value. Args: tune (float): tune value. Returns: The fractional part. \"\"\" return tune - int ( tune ) # ok since int truncates to lower integer","title":"Module pyhdtoolkit.cpymadtools.matching"},{"location":"reference/pyhdtoolkit/cpymadtools/matching/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/matching/#get_closest_tune_approach","text":"def get_closest_tune_approach ( madx : cpymad . madx . Madx , accelerator : str = None , sequence : str = None , varied_knobs : Sequence [ str ] = None , telescopic_squeeze : bool = False , explicit_targets : Tuple [ float , float ] = None , step : float = 1e-07 , calls : float = 100 , tolerance : float = 1e-21 ) -> float Provided with an active cpymad class after having ran a script, tries to match the tunes to their mid-fractional tunes. The difference between this mid-tune and the actual matched tune is the closest tune approach. NOTA BENE: This assumes your lattice has previously been matched to desired tunes and chromaticities, as it will determine the appropriate targets from the Madx instance's internal tables. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None accelerator str name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. None sequence str name of the sequence you want to activate for the tune matching. None varied_knobs Sequence[str] the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. None telescopic_squeeze bool LHC specific. If set to True, uses the (HL)LHC knobs for Telescopic Squeeze configuration. Defaults to False. None explicit_targets Tuple[float, float] if given, will be used as matching targets for Qx, Qy. Otherwise, the target is determined as the middle of the current fractional tunes. Defaults to None. None step float step size to use when varying knobs. None calls int max number of varying calls to perform. None tolerance float tolerance for successfull matching. None Returns: Type Description None The closest tune approach, in absolute value. View Source def get_closest_tune_approach ( madx : Madx , accelerator : str = None , sequence : str = None , varied_knobs : Sequence [ str ] = None , telescopic_squeeze : bool = False , explicit_targets : Tuple [ float, float ] = None , step : float = 1e-7 , calls : float = 100 , tolerance : float = 1e-21 , ) -> float : \"\"\" Provided with an active `cpymad` class after having ran a script, tries to match the tunes to their mid-fractional tunes. The difference between this mid-tune and the actual matched tune is the closest tune approach. NOTA BENE: This assumes your lattice has previously been matched to desired tunes and chromaticities, as it will determine the appropriate targets from the Madx instance's internal tables. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\" kqf \", \" ksd \", \" kqf \", \" kqd \"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. telescopic_squeeze (bool): LHC specific. If set to True, uses the (HL)LHC knobs for Telescopic Squeeze configuration. Defaults to False. explicit_targets (Tuple[float, float]): if given, will be used as matching targets for Qx, Qy. Otherwise, the target is determined as the middle of the current fractional tunes. Defaults to None. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. Returns: The closest tune approach, in absolute value. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default {accelerator.upper()} values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ -1 ] ), telescopic_squeeze = telescopic_squeeze ) logger . debug ( \"Saving knob values to restore after closest tune approach\" ) saved_knobs : Dict [ str, float ] = { knob : madx . globals [ knob ] for knob in varied_knobs } logger . trace ( f \"Saved knobs are {saved_knobs}\" ) if explicit_targets : qx_target , qy_target = explicit_targets q1 , q2 = qx_target , qy_target # the integer part is used later on else : logger . trace ( \"Retrieving tunes and chromaticities from internal tables\" ) q1 , q2 = madx . table . summ . q1 [ 0 ] , madx . table . summ . q2 [ 0 ] dq1 , dq2 = madx . table . summ . dq1 [ 0 ] , madx . table . summ . dq2 [ 0 ] logger . trace ( f \"Retrieved values are q1 = {q1}, q2 = {q2}, dq1 = {dq1}, dq2 = {dq2}\" ) logger . trace ( \"Determining target tunes for closest approach\" ) middle_of_fractional_tunes = ( _fractional_tune ( q1 ) + _fractional_tune ( q2 )) / 2 qx_target = int ( q1 ) + middle_of_fractional_tunes qy_target = int ( q2 ) + middle_of_fractional_tunes logger . debug ( f \"Targeting tunes Qx = {qx_target} | Qy = {qy_target}\" ) logger . info ( \"Performing closest tune approach routine, matching should fail at DeltaQ = dqmin\" ) match_tunes_and_chromaticities ( madx , accelerator , sequence , qx_target , qy_target , varied_knobs = varied_knobs , step = step , calls = calls , tolerance = tolerance , ) logger . debug ( \"Retrieving tune separation from internal tables\" ) dqmin = madx . table . summ . q1 [ 0 ] - madx . table . summ . q2 [ 0 ] - ( int ( q1 ) - int ( q2 )) logger . info ( \"Restoring saved knobs\" ) with madx . batch () : madx . globals . update ( saved_knobs ) madx . twiss () return abs ( dqmin )","title":"get_closest_tune_approach"},{"location":"reference/pyhdtoolkit/cpymadtools/matching/#get_lhc_tune_and_chroma_knobs","text":"def get_lhc_tune_and_chroma_knobs ( accelerator : str , beam : int = 1 , telescopic_squeeze : bool = False ) -> Tuple [ str , str , str , str ] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get names of knobs needed to match tunes and chromaticities as a tuple of strings. Parameters: Name Type Description Default accelerator str Accelerator either 'LHC' (dQ[xy], dQp[xy] knobs) or 'HLLHC' (kqt[fd], ks[fd] knobs). None beam int Beam to use, for the knob names. None telescopic_squeeze bool if set to True, returns the knobs for Telescopic Squeeze configuration. Defaults to False. None Returns: Type Description None Tuple of strings with knobs for (qx, qy, dqx, dqy) . View Source def get_lhc_tune_and_chroma_knobs ( accelerator : str , beam : int = 1 , telescopic_squeeze : bool = False ) -> Tuple [ str, str, str, str ] : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get names of knobs needed to match tunes and chromaticities as a tuple of strings. Args: accelerator (str): Accelerator either 'LHC' (dQ[xy], dQp[xy] knobs) or 'HLLHC' (kqt[fd], ks[fd] knobs). beam (int): Beam to use, for the knob names. telescopic_squeeze (bool): if set to True, returns the knobs for Telescopic Squeeze configuration. Defaults to False. Returns: Tuple of strings with knobs for `(qx, qy, dqx, dqy)`. \"\"\" beam = 2 if beam == 4 else beam suffix = \"_sq\" if telescopic_squeeze else \"\" if accelerator . upper () not in ( \"LHC\" , \"HLLHC\" ) : logger . error ( \"Invalid accelerator name, only 'LHC' and 'HLLHC' implemented\" ) raise NotImplementedError ( f \"Accelerator '{accelerator}' not implemented.\" ) return { \"LHC\" : ( f \"dQx.b{beam}{suffix}\" , f \"dQy.b{beam}{suffix}\" , f \"dQpx.b{beam}{suffix}\" , f \"dQpy.b{beam}{suffix}\" , ), \"HLLHC\" : ( f \"kqtf.b{beam}{suffix}\" , f \"kqtd.b{beam}{suffix}\" , f \"ksf.b{beam}{suffix}\" , f \"ksd.b{beam}{suffix}\" , ), } [ accelerator.upper() ]","title":"get_lhc_tune_and_chroma_knobs"},{"location":"reference/pyhdtoolkit/cpymadtools/matching/#match_tunes_and_chromaticities","text":"def match_tunes_and_chromaticities ( madx : cpymad . madx . Madx , accelerator : str = None , sequence : str = None , q1_target : float = None , q2_target : float = None , dq1_target : float = None , dq2_target : float = None , varied_knobs : Sequence [ str ] = None , telescopic_squeeze : bool = True , step : float = 1e-07 , calls : int = 100 , tolerance : float = 1e-21 ) -> None Provided with an active cpymad class after having ran a script, will run an additional matching command to reach the provided values for tunes and chromaticities. Tune matching is always performed. If chromaticity target values are given, then a matching is done for them, followed by an additionnal matching for both tunes and chromaticities. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None accelerator str name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. None sequence str name of the sequence you want to activate for the tune matching. None q1_target float horizontal tune to match to. None q2_target float vertical tune to match to. None dq1_target float horizontal chromaticity to match to. None dq2_target float vertical chromaticity to match to. None varied_knobs Sequence[str] the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. None telescopic_squeeze bool LHC specific. If set to True, uses the (HL)LHC knobs for Telescopic Squeeze configuration. Defaults to True as of Run III. None step float step size to use when varying knobs. None calls int max number of varying calls to perform. None tolerance float tolerance for successfull matching. None View Source def match_tunes_and_chromaticities ( madx : Madx , accelerator : str = None , sequence : str = None , q1_target : float = None , q2_target : float = None , dq1_target : float = None , dq2_target : float = None , varied_knobs : Sequence [ str ] = None , telescopic_squeeze : bool = True , step : float = 1e-7 , calls : int = 100 , tolerance : float = 1e-21 , ) -> None : \"\"\" Provided with an active `cpymad` class after having ran a script, will run an additional matching command to reach the provided values for tunes and chromaticities. Tune matching is always performed. If chromaticity target values are given, then a matching is done for them, followed by an additionnal matching for both tunes and chromaticities. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. q1_target (float): horizontal tune to match to. q2_target (float): vertical tune to match to. dq1_target (float): horizontal chromaticity to match to. dq2_target (float): vertical chromaticity to match to. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\" kqf \", \" ksd \", \" kqf \", \" kqd \"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. telescopic_squeeze (bool): LHC specific. If set to True, uses the (HL)LHC knobs for Telescopic Squeeze configuration. Defaults to True as of Run III. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default {accelerator.upper()} values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ -1 ] ), telescopic_squeeze = telescopic_squeeze ) def match ( * args , ** kwargs ) : \"\"\"Create matching commands for kwarg targets, varying the given args.\"\"\" logger . debug ( f \"Executing matching commands, using sequence '{sequence}'\" ) madx . command . match ( chrom = True ) logger . trace ( f \"Targets are given as {kwargs}\" ) madx . command . global_ ( sequence = sequence , ** kwargs ) for variable_name in args : logger . trace ( f \"Creating vary command for knob '{variable_name}'\" ) madx . command . vary ( name = variable_name , step = step ) madx . command . lmdif ( calls = calls , tolerance = tolerance ) madx . command . endmatch () logger . trace ( \"Performing routine TWISS\" ) madx . twiss () # prevents errors if the user forgets to TWISS before querying tables if q1_target is not None and q2_target is not None and dq1_target is not None and dq2_target is not None : logger . info ( f \"Doing combined matching to Qx={q1_target}, Qy={q2_target}, \" f \"dqx={dq1_target}, dqy={dq2_target} for sequence '{sequence}'\" ) logger . trace ( f \"Vary knobs sent are {varied_knobs}\" ) match ( * varied_knobs , q1 = q1_target , q2 = q2_target , dq1 = dq1_target , dq2 = dq2_target ) elif q1_target is not None and q2_target is not None : logger . info ( f \"Matching tunes to Qx={q1_target}, Qy={q2_target} for sequence '{sequence}'\" ) logger . trace ( f \"Vary knobs sent are {varied_knobs[:2]}\" ) match ( * varied_knobs [ :2 ] , q1 = q1_target , q2 = q2_target ) # first two in varied_knobs are tune knobs","title":"match_tunes_and_chromaticities"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/","text":"Module pyhdtoolkit.cpymadtools.orbit Module cpymadtools.orbit Created on 2020.02.03 View Source \"\"\" Module cpymadtools.orbit ------------------------ Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X orbit setup with a cpymad.madx.Madx object, mainly for LHC and HLLHC machines. \"\"\" from typing import Dict , List , Tuple from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.cpymadtools.constants import LHC_CROSSING_SCHEMES # ----- Utilities ----- # def lhc_orbit_variables () -> Tuple [ List [ str ], Dict [ str , str ]]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the variable names used for orbit setup in the (HL)LHC. Returns: A tuple with a list of all orbit variables, and a dict of additional variables, that in the default configurations have the same value as another variable. \"\"\" logger . trace ( \"Returning (HL)LHC orbit variables\" ) on_variables = ( \"crab1\" , \"crab5\" , # exists only in HL-LHC \"x1\" , \"sep1\" , \"o1\" , \"oh1\" , \"ov1\" , \"x2\" , \"sep2\" , \"o2\" , \"oe2\" , \"a2\" , \"oh2\" , \"ov2\" , \"x5\" , \"sep5\" , \"o5\" , \"oh5\" , \"ov5\" , \"phi_IR5\" , \"x8\" , \"sep8\" , \"o8\" , \"a8\" , \"sep8h\" , \"x8v\" , \"oh8\" , \"ov8\" , \"alice\" , \"sol_alice\" , \"lhcb\" , \"sol_atlas\" , \"sol_cms\" , ) variables = [ f \"on_ { var } \" for var in on_variables ] + [ f \"phi_IR { ir : d } \" for ir in ( 1 , 2 , 5 , 8 )] special = { \"on_ssep1\" : \"on_sep1\" , \"on_xx1\" : \"on_x1\" , \"on_ssep5\" : \"on_sep5\" , \"on_xx5\" : \"on_x5\" , } return variables , special def setup_lhc_orbit ( madx : Madx , scheme : str = \"flat\" , ** kwargs ) -> Dict [ str , float ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Automated orbit setup for (hl)lhc runs, for some default schemes. Assumed that at least sequence and optics files have been called. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. scheme (str): the default scheme to apply, as defined in `LHC_CROSSING_SCHEMES`. Accepted values are keys of `LHC_CROSSING_SCHEMES`. Defaults to 'flat' (every orbit variable to 0). Keyword Args: All standard crossing scheme variables (on_x1, phi_IR1, etc). Values given here override the values in the default scheme configurations. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" if scheme not in LHC_CROSSING_SCHEMES . keys (): logger . error ( f \"Invalid scheme parameter, should be one of { LHC_CROSSING_SCHEMES . keys () } \" ) raise ValueError ( \"Invalid scheme parameter given\" ) logger . debug ( \"Getting orbit variables\" ) variables , special = lhc_orbit_variables () scheme_dict = LHC_CROSSING_SCHEMES [ scheme ] final_scheme = {} for orbit_variable in variables : variable_value = kwargs . get ( orbit_variable , scheme_dict . get ( orbit_variable , 0 )) logger . trace ( f \"Setting orbit variable ' { orbit_variable } ' to { variable_value } \" ) # Sets value in MAD-X globals & returned dict, taken from scheme dict or kwargs if provided madx . globals [ orbit_variable ] = final_scheme [ orbit_variable ] = variable_value for special_variable , copy_from in special . items (): special_variable_value = kwargs . get ( special_variable , madx . globals [ copy_from ]) logger . trace ( f \"Setting special orbit variable ' { special_variable } ' to { special_variable_value } \" ) # Sets value in MAD-X globals & returned dict, taken from a given global or kwargs if provided madx . globals [ special_variable ] = final_scheme [ special_variable ] = special_variable_value return final_scheme def get_current_orbit_setup ( madx : Madx ) -> Dict [ str , float ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the current values for the (HL)LHC orbit variales. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" logger . debug ( \"Extracting orbit variables from global table\" ) variables , specials = lhc_orbit_variables () return { orbit_variable : madx . globals [ orbit_variable ] for orbit_variable in variables + list ( specials . keys ()) } def correct_lhc_orbit ( madx : Madx , orbit_tolerance : float = 1e-14 , iterations : int = 3 , mode : str = \"svd\" , ** kwargs ) -> None : \"\"\" Routine for orbit correction using 'MCB.*' elements in the LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. orbit_tolerance (float): the tolerance for the correction. Defaults to 1e-14. iterations (int): the number of iterations of the correction to perform. Defaults to 3. mode (str): the method to use for the correction. Defaults to 'svd'. Keyword Args: Any keyword argument that can be given to the MAD-X CORRECT command, such as `mode`, `ncorr`, etc. \"\"\" logger . info ( \"Starting orbit correction\" ) for default_kicker in ( \"kicker\" , \"hkicker\" , \"vkicker\" , \"virtualcorrector\" ): logger . trace ( f \"Disabling default corrector class ' { default_kicker } '\" ) madx . command . usekick ( status = \"off\" , class_ = default_kicker ) logger . debug ( \"Selecting '^MCB.*' correctors\" ) madx . command . usekick ( status = \"on\" , pattern = \"^MCB.*\" ) madx . command . usemonitor ( status = \"on\" , class_ = \"monitor\" ) for _ in range ( iterations ): logger . trace ( \"Doing orbit correction for Y then X plane\" ) madx . twiss ( chrom = True ) madx . command . correct ( plane = \"y\" , flag = \"ring\" , error = orbit_tolerance , mode = mode , ** kwargs ) madx . command . correct ( plane = \"x\" , flag = \"ring\" , error = orbit_tolerance , mode = mode , ** kwargs ) Variables LHC_CROSSING_SCHEMES Functions correct_lhc_orbit def correct_lhc_orbit ( madx : cpymad . madx . Madx , orbit_tolerance : float = 1e-14 , iterations : int = 3 , mode : str = 'svd' , ** kwargs ) -> None Routine for orbit correction using 'MCB.*' elements in the LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. orbit_tolerance (float): the tolerance for the correction. Defaults to 1e-14. iterations (int): the number of iterations of the correction to perform. Defaults to 3. mode (str): the method to use for the correction. Defaults to 'svd'. Keyword Args: Any keyword argument that can be given to the MAD-X CORRECT command, such as mode , ncorr , etc. View Source def correct_lhc_orbit ( madx : Madx , orbit_tolerance : float = 1e-14 , iterations : int = 3 , mode : str = \"svd\" , ** kwargs ) -> None : \" \"\" Routine for orbit correction using 'MCB.*' elements in the LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. orbit_tolerance (float): the tolerance for the correction. Defaults to 1e-14. iterations (int): the number of iterations of the correction to perform. Defaults to 3. mode (str): the method to use for the correction. Defaults to 'svd'. Keyword Args: Any keyword argument that can be given to the MAD-X CORRECT command, such as `mode`, `ncorr`, etc. \"\" \" logger . info ( \"Starting orbit correction\" ) for default_kicker in ( \"kicker\" , \"hkicker\" , \"vkicker\" , \"virtualcorrector\" ) : logger . trace ( f \"Disabling default corrector class '{default_kicker}'\" ) madx . command . usekick ( status = \"off\" , class_ = default_kicker ) logger . debug ( \"Selecting '^MCB.*' correctors\" ) madx . command . usekick ( status = \"on\" , pattern = \"^MCB.*\" ) madx . command . usemonitor ( status = \"on\" , class_ = \"monitor\" ) for _ in range ( iterations ) : logger . trace ( \"Doing orbit correction for Y then X plane\" ) madx . twiss ( chrom = True ) madx . command . correct ( plane = \"y\" , flag = \"ring\" , error = orbit_tolerance , mode = mode , ** kwargs ) madx . command . correct ( plane = \"x\" , flag = \"ring\" , error = orbit_tolerance , mode = mode , ** kwargs ) get_current_orbit_setup def get_current_orbit_setup ( madx : cpymad . madx . Madx ) -> Dict [ str , float ] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the current values for the (HL)LHC orbit variales. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None Returns: Type Description None A dictionary of all orbit variables set, and their values as set in the MAD-X globals. View Source def get_current_orbit_setup ( madx : Madx ) -> Dict [ str, float ] : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the current values for the (HL)LHC orbit variales. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" logger . debug ( \"Extracting orbit variables from global table\" ) variables , specials = lhc_orbit_variables () return { orbit_variable : madx . globals [ orbit_variable ] for orbit_variable in variables + list ( specials . keys ()) } lhc_orbit_variables def lhc_orbit_variables ( ) -> Tuple [ List [ str ], Dict [ str , str ]] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the variable names used for orbit setup in the (HL)LHC. Returns: Type Description None A tuple with a list of all orbit variables, and a dict of additional variables, that in the default configurations have the same value as another variable. View Source def lhc_orbit_variables () -> Tuple [ List[str ] , Dict [ str, str ] ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the variable names used for orbit setup in the (HL)LHC. Returns: A tuple with a list of all orbit variables, and a dict of additional variables, that in the default configurations have the same value as another variable. \"\"\" logger . trace ( \"Returning (HL)LHC orbit variables\" ) on_variables = ( \"crab1\" , \"crab5\" , # exists only in HL - LHC \"x1\" , \"sep1\" , \"o1\" , \"oh1\" , \"ov1\" , \"x2\" , \"sep2\" , \"o2\" , \"oe2\" , \"a2\" , \"oh2\" , \"ov2\" , \"x5\" , \"sep5\" , \"o5\" , \"oh5\" , \"ov5\" , \"phi_IR5\" , \"x8\" , \"sep8\" , \"o8\" , \"a8\" , \"sep8h\" , \"x8v\" , \"oh8\" , \"ov8\" , \"alice\" , \"sol_alice\" , \"lhcb\" , \"sol_atlas\" , \"sol_cms\" , ) variables = [ f\"on_{var}\" for var in on_variables ] + [ f\"phi_IR{ir:d}\" for ir in (1, 2, 5, 8) ] special = { \"on_ssep1\" : \"on_sep1\" , \"on_xx1\" : \"on_x1\" , \"on_ssep5\" : \"on_sep5\" , \"on_xx5\" : \"on_x5\" , } return variables , special setup_lhc_orbit def setup_lhc_orbit ( madx : cpymad . madx . Madx , scheme : str = 'flat' , ** kwargs ) -> Dict [ str , float ] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Automated orbit setup for (hl)lhc runs, for some default schemes. Assumed that at least sequence and optics files have been called. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. scheme (str): the default scheme to apply, as defined in LHC_CROSSING_SCHEMES . Accepted values are keys of LHC_CROSSING_SCHEMES . Defaults to 'flat' (every orbit variable to 0). Keyword Args: All standard crossing scheme variables (on_x1, phi_IR1, etc). Values given here override the values in the default scheme configurations. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. View Source def setup_lhc_orbit ( madx : Madx , scheme : str = \"flat\" , ** kwargs ) -> Dict [ str, float ] : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Automated orbit setup for (hl)lhc runs, for some default schemes. Assumed that at least sequence and optics files have been called. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. scheme (str): the default scheme to apply, as defined in `LHC_CROSSING_SCHEMES`. Accepted values are keys of `LHC_CROSSING_SCHEMES`. Defaults to 'flat' (every orbit variable to 0). Keyword Args: All standard crossing scheme variables (on_x1, phi_IR1, etc). Values given here override the values in the default scheme configurations. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" if scheme not in LHC_CROSSING_SCHEMES . keys () : logger . error ( f \"Invalid scheme parameter, should be one of {LHC_CROSSING_SCHEMES.keys()}\" ) raise ValueError ( \"Invalid scheme parameter given\" ) logger . debug ( \"Getting orbit variables\" ) variables , special = lhc_orbit_variables () scheme_dict = LHC_CROSSING_SCHEMES [ scheme ] final_scheme = {} for orbit_variable in variables : variable_value = kwargs . get ( orbit_variable , scheme_dict . get ( orbit_variable , 0 )) logger . trace ( f \"Setting orbit variable '{orbit_variable}' to {variable_value}\" ) # Sets value in MAD - X globals & returned dict , taken from scheme dict or kwargs if provided madx . globals [ orbit_variable ] = final_scheme [ orbit_variable ] = variable_value for special_variable , copy_from in special . items () : special_variable_value = kwargs . get ( special_variable , madx . globals [ copy_from ] ) logger . trace ( f \"Setting special orbit variable '{special_variable}' to {special_variable_value}\" ) # Sets value in MAD - X globals & returned dict , taken from a given global or kwargs if provided madx . globals [ special_variable ] = final_scheme [ special_variable ] = special_variable_value return final_scheme","title":"Orbit"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#module-pyhdtoolkitcpymadtoolsorbit","text":"Module cpymadtools.orbit Created on 2020.02.03 View Source \"\"\" Module cpymadtools.orbit ------------------------ Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X orbit setup with a cpymad.madx.Madx object, mainly for LHC and HLLHC machines. \"\"\" from typing import Dict , List , Tuple from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.cpymadtools.constants import LHC_CROSSING_SCHEMES # ----- Utilities ----- # def lhc_orbit_variables () -> Tuple [ List [ str ], Dict [ str , str ]]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the variable names used for orbit setup in the (HL)LHC. Returns: A tuple with a list of all orbit variables, and a dict of additional variables, that in the default configurations have the same value as another variable. \"\"\" logger . trace ( \"Returning (HL)LHC orbit variables\" ) on_variables = ( \"crab1\" , \"crab5\" , # exists only in HL-LHC \"x1\" , \"sep1\" , \"o1\" , \"oh1\" , \"ov1\" , \"x2\" , \"sep2\" , \"o2\" , \"oe2\" , \"a2\" , \"oh2\" , \"ov2\" , \"x5\" , \"sep5\" , \"o5\" , \"oh5\" , \"ov5\" , \"phi_IR5\" , \"x8\" , \"sep8\" , \"o8\" , \"a8\" , \"sep8h\" , \"x8v\" , \"oh8\" , \"ov8\" , \"alice\" , \"sol_alice\" , \"lhcb\" , \"sol_atlas\" , \"sol_cms\" , ) variables = [ f \"on_ { var } \" for var in on_variables ] + [ f \"phi_IR { ir : d } \" for ir in ( 1 , 2 , 5 , 8 )] special = { \"on_ssep1\" : \"on_sep1\" , \"on_xx1\" : \"on_x1\" , \"on_ssep5\" : \"on_sep5\" , \"on_xx5\" : \"on_x5\" , } return variables , special def setup_lhc_orbit ( madx : Madx , scheme : str = \"flat\" , ** kwargs ) -> Dict [ str , float ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Automated orbit setup for (hl)lhc runs, for some default schemes. Assumed that at least sequence and optics files have been called. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. scheme (str): the default scheme to apply, as defined in `LHC_CROSSING_SCHEMES`. Accepted values are keys of `LHC_CROSSING_SCHEMES`. Defaults to 'flat' (every orbit variable to 0). Keyword Args: All standard crossing scheme variables (on_x1, phi_IR1, etc). Values given here override the values in the default scheme configurations. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" if scheme not in LHC_CROSSING_SCHEMES . keys (): logger . error ( f \"Invalid scheme parameter, should be one of { LHC_CROSSING_SCHEMES . keys () } \" ) raise ValueError ( \"Invalid scheme parameter given\" ) logger . debug ( \"Getting orbit variables\" ) variables , special = lhc_orbit_variables () scheme_dict = LHC_CROSSING_SCHEMES [ scheme ] final_scheme = {} for orbit_variable in variables : variable_value = kwargs . get ( orbit_variable , scheme_dict . get ( orbit_variable , 0 )) logger . trace ( f \"Setting orbit variable ' { orbit_variable } ' to { variable_value } \" ) # Sets value in MAD-X globals & returned dict, taken from scheme dict or kwargs if provided madx . globals [ orbit_variable ] = final_scheme [ orbit_variable ] = variable_value for special_variable , copy_from in special . items (): special_variable_value = kwargs . get ( special_variable , madx . globals [ copy_from ]) logger . trace ( f \"Setting special orbit variable ' { special_variable } ' to { special_variable_value } \" ) # Sets value in MAD-X globals & returned dict, taken from a given global or kwargs if provided madx . globals [ special_variable ] = final_scheme [ special_variable ] = special_variable_value return final_scheme def get_current_orbit_setup ( madx : Madx ) -> Dict [ str , float ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the current values for the (HL)LHC orbit variales. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" logger . debug ( \"Extracting orbit variables from global table\" ) variables , specials = lhc_orbit_variables () return { orbit_variable : madx . globals [ orbit_variable ] for orbit_variable in variables + list ( specials . keys ()) } def correct_lhc_orbit ( madx : Madx , orbit_tolerance : float = 1e-14 , iterations : int = 3 , mode : str = \"svd\" , ** kwargs ) -> None : \"\"\" Routine for orbit correction using 'MCB.*' elements in the LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. orbit_tolerance (float): the tolerance for the correction. Defaults to 1e-14. iterations (int): the number of iterations of the correction to perform. Defaults to 3. mode (str): the method to use for the correction. Defaults to 'svd'. Keyword Args: Any keyword argument that can be given to the MAD-X CORRECT command, such as `mode`, `ncorr`, etc. \"\"\" logger . info ( \"Starting orbit correction\" ) for default_kicker in ( \"kicker\" , \"hkicker\" , \"vkicker\" , \"virtualcorrector\" ): logger . trace ( f \"Disabling default corrector class ' { default_kicker } '\" ) madx . command . usekick ( status = \"off\" , class_ = default_kicker ) logger . debug ( \"Selecting '^MCB.*' correctors\" ) madx . command . usekick ( status = \"on\" , pattern = \"^MCB.*\" ) madx . command . usemonitor ( status = \"on\" , class_ = \"monitor\" ) for _ in range ( iterations ): logger . trace ( \"Doing orbit correction for Y then X plane\" ) madx . twiss ( chrom = True ) madx . command . correct ( plane = \"y\" , flag = \"ring\" , error = orbit_tolerance , mode = mode , ** kwargs ) madx . command . correct ( plane = \"x\" , flag = \"ring\" , error = orbit_tolerance , mode = mode , ** kwargs )","title":"Module pyhdtoolkit.cpymadtools.orbit"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#variables","text":"LHC_CROSSING_SCHEMES","title":"Variables"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#correct_lhc_orbit","text":"def correct_lhc_orbit ( madx : cpymad . madx . Madx , orbit_tolerance : float = 1e-14 , iterations : int = 3 , mode : str = 'svd' , ** kwargs ) -> None Routine for orbit correction using 'MCB.*' elements in the LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. orbit_tolerance (float): the tolerance for the correction. Defaults to 1e-14. iterations (int): the number of iterations of the correction to perform. Defaults to 3. mode (str): the method to use for the correction. Defaults to 'svd'. Keyword Args: Any keyword argument that can be given to the MAD-X CORRECT command, such as mode , ncorr , etc. View Source def correct_lhc_orbit ( madx : Madx , orbit_tolerance : float = 1e-14 , iterations : int = 3 , mode : str = \"svd\" , ** kwargs ) -> None : \" \"\" Routine for orbit correction using 'MCB.*' elements in the LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. orbit_tolerance (float): the tolerance for the correction. Defaults to 1e-14. iterations (int): the number of iterations of the correction to perform. Defaults to 3. mode (str): the method to use for the correction. Defaults to 'svd'. Keyword Args: Any keyword argument that can be given to the MAD-X CORRECT command, such as `mode`, `ncorr`, etc. \"\" \" logger . info ( \"Starting orbit correction\" ) for default_kicker in ( \"kicker\" , \"hkicker\" , \"vkicker\" , \"virtualcorrector\" ) : logger . trace ( f \"Disabling default corrector class '{default_kicker}'\" ) madx . command . usekick ( status = \"off\" , class_ = default_kicker ) logger . debug ( \"Selecting '^MCB.*' correctors\" ) madx . command . usekick ( status = \"on\" , pattern = \"^MCB.*\" ) madx . command . usemonitor ( status = \"on\" , class_ = \"monitor\" ) for _ in range ( iterations ) : logger . trace ( \"Doing orbit correction for Y then X plane\" ) madx . twiss ( chrom = True ) madx . command . correct ( plane = \"y\" , flag = \"ring\" , error = orbit_tolerance , mode = mode , ** kwargs ) madx . command . correct ( plane = \"x\" , flag = \"ring\" , error = orbit_tolerance , mode = mode , ** kwargs )","title":"correct_lhc_orbit"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#get_current_orbit_setup","text":"def get_current_orbit_setup ( madx : cpymad . madx . Madx ) -> Dict [ str , float ] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the current values for the (HL)LHC orbit variales. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None Returns: Type Description None A dictionary of all orbit variables set, and their values as set in the MAD-X globals. View Source def get_current_orbit_setup ( madx : Madx ) -> Dict [ str, float ] : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the current values for the (HL)LHC orbit variales. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" logger . debug ( \"Extracting orbit variables from global table\" ) variables , specials = lhc_orbit_variables () return { orbit_variable : madx . globals [ orbit_variable ] for orbit_variable in variables + list ( specials . keys ()) }","title":"get_current_orbit_setup"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#lhc_orbit_variables","text":"def lhc_orbit_variables ( ) -> Tuple [ List [ str ], Dict [ str , str ]] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the variable names used for orbit setup in the (HL)LHC. Returns: Type Description None A tuple with a list of all orbit variables, and a dict of additional variables, that in the default configurations have the same value as another variable. View Source def lhc_orbit_variables () -> Tuple [ List[str ] , Dict [ str, str ] ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the variable names used for orbit setup in the (HL)LHC. Returns: A tuple with a list of all orbit variables, and a dict of additional variables, that in the default configurations have the same value as another variable. \"\"\" logger . trace ( \"Returning (HL)LHC orbit variables\" ) on_variables = ( \"crab1\" , \"crab5\" , # exists only in HL - LHC \"x1\" , \"sep1\" , \"o1\" , \"oh1\" , \"ov1\" , \"x2\" , \"sep2\" , \"o2\" , \"oe2\" , \"a2\" , \"oh2\" , \"ov2\" , \"x5\" , \"sep5\" , \"o5\" , \"oh5\" , \"ov5\" , \"phi_IR5\" , \"x8\" , \"sep8\" , \"o8\" , \"a8\" , \"sep8h\" , \"x8v\" , \"oh8\" , \"ov8\" , \"alice\" , \"sol_alice\" , \"lhcb\" , \"sol_atlas\" , \"sol_cms\" , ) variables = [ f\"on_{var}\" for var in on_variables ] + [ f\"phi_IR{ir:d}\" for ir in (1, 2, 5, 8) ] special = { \"on_ssep1\" : \"on_sep1\" , \"on_xx1\" : \"on_x1\" , \"on_ssep5\" : \"on_sep5\" , \"on_xx5\" : \"on_x5\" , } return variables , special","title":"lhc_orbit_variables"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#setup_lhc_orbit","text":"def setup_lhc_orbit ( madx : cpymad . madx . Madx , scheme : str = 'flat' , ** kwargs ) -> Dict [ str , float ] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Automated orbit setup for (hl)lhc runs, for some default schemes. Assumed that at least sequence and optics files have been called. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. scheme (str): the default scheme to apply, as defined in LHC_CROSSING_SCHEMES . Accepted values are keys of LHC_CROSSING_SCHEMES . Defaults to 'flat' (every orbit variable to 0). Keyword Args: All standard crossing scheme variables (on_x1, phi_IR1, etc). Values given here override the values in the default scheme configurations. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. View Source def setup_lhc_orbit ( madx : Madx , scheme : str = \"flat\" , ** kwargs ) -> Dict [ str, float ] : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Automated orbit setup for (hl)lhc runs, for some default schemes. Assumed that at least sequence and optics files have been called. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. scheme (str): the default scheme to apply, as defined in `LHC_CROSSING_SCHEMES`. Accepted values are keys of `LHC_CROSSING_SCHEMES`. Defaults to 'flat' (every orbit variable to 0). Keyword Args: All standard crossing scheme variables (on_x1, phi_IR1, etc). Values given here override the values in the default scheme configurations. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" if scheme not in LHC_CROSSING_SCHEMES . keys () : logger . error ( f \"Invalid scheme parameter, should be one of {LHC_CROSSING_SCHEMES.keys()}\" ) raise ValueError ( \"Invalid scheme parameter given\" ) logger . debug ( \"Getting orbit variables\" ) variables , special = lhc_orbit_variables () scheme_dict = LHC_CROSSING_SCHEMES [ scheme ] final_scheme = {} for orbit_variable in variables : variable_value = kwargs . get ( orbit_variable , scheme_dict . get ( orbit_variable , 0 )) logger . trace ( f \"Setting orbit variable '{orbit_variable}' to {variable_value}\" ) # Sets value in MAD - X globals & returned dict , taken from scheme dict or kwargs if provided madx . globals [ orbit_variable ] = final_scheme [ orbit_variable ] = variable_value for special_variable , copy_from in special . items () : special_variable_value = kwargs . get ( special_variable , madx . globals [ copy_from ] ) logger . trace ( f \"Setting special orbit variable '{special_variable}' to {special_variable_value}\" ) # Sets value in MAD - X globals & returned dict , taken from a given global or kwargs if provided madx . globals [ special_variable ] = final_scheme [ special_variable ] = special_variable_value return final_scheme","title":"setup_lhc_orbit"},{"location":"reference/pyhdtoolkit/cpymadtools/parameters/","text":"Module pyhdtoolkit.cpymadtools.parameters Module cpymadtools.parameters Created on 2020.02.03 View Source \"\"\" Module cpymadtools.parameters ----------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to compute different beam and machine parameters. \"\"\" from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.models.madx import MADXBeam # ----- Utilities ----- # def query_beam_attributes ( madx : Madx ) -> MADXBeam : \"\"\" Returns all `BEAM` attributes from the `MAD-X` process based on the currently defined beam. If no beam has been defined at function call, then `MAD-X` will return all the default values. See the `MAD-X` manual for details. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A validated `MADXBeam` object. \"\"\" logger . info ( \"Retrieving BEAM attributes from the MAD-X process\" ) return MADXBeam ( ** dict ( madx . beam )) Functions query_beam_attributes def query_beam_attributes ( madx : cpymad . madx . Madx ) -> pyhdtoolkit . models . madx . MADXBeam Returns all BEAM attributes from the MAD-X process based on the currently defined beam. If no beam has been defined at function call, then MAD-X will return all the default values. See the MAD-X manual for details. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None Returns: Type Description None A validated MADXBeam object. View Source def query_beam_attributes ( madx : Madx ) -> MADXBeam : \" \"\" Returns all `BEAM` attributes from the `MAD-X` process based on the currently defined beam. If no beam has been defined at function call, then `MAD-X` will return all the default values. See the `MAD-X` manual for details. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A validated `MADXBeam` object. \"\" \" logger . info ( \"Retrieving BEAM attributes from the MAD-X process\" ) return MADXBeam ( ** dict ( madx . beam ))","title":"Parameters"},{"location":"reference/pyhdtoolkit/cpymadtools/parameters/#module-pyhdtoolkitcpymadtoolsparameters","text":"Module cpymadtools.parameters Created on 2020.02.03 View Source \"\"\" Module cpymadtools.parameters ----------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to compute different beam and machine parameters. \"\"\" from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.models.madx import MADXBeam # ----- Utilities ----- # def query_beam_attributes ( madx : Madx ) -> MADXBeam : \"\"\" Returns all `BEAM` attributes from the `MAD-X` process based on the currently defined beam. If no beam has been defined at function call, then `MAD-X` will return all the default values. See the `MAD-X` manual for details. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A validated `MADXBeam` object. \"\"\" logger . info ( \"Retrieving BEAM attributes from the MAD-X process\" ) return MADXBeam ( ** dict ( madx . beam ))","title":"Module pyhdtoolkit.cpymadtools.parameters"},{"location":"reference/pyhdtoolkit/cpymadtools/parameters/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/parameters/#query_beam_attributes","text":"def query_beam_attributes ( madx : cpymad . madx . Madx ) -> pyhdtoolkit . models . madx . MADXBeam Returns all BEAM attributes from the MAD-X process based on the currently defined beam. If no beam has been defined at function call, then MAD-X will return all the default values. See the MAD-X manual for details. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None Returns: Type Description None A validated MADXBeam object. View Source def query_beam_attributes ( madx : Madx ) -> MADXBeam : \" \"\" Returns all `BEAM` attributes from the `MAD-X` process based on the currently defined beam. If no beam has been defined at function call, then `MAD-X` will return all the default values. See the `MAD-X` manual for details. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A validated `MADXBeam` object. \"\" \" logger . info ( \"Retrieving BEAM attributes from the MAD-X process\" ) return MADXBeam ( ** dict ( madx . beam ))","title":"query_beam_attributes"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/","text":"Module pyhdtoolkit.cpymadtools.plotters Module cpymadtools.plotters Created on 2019.12.08 View Source \"\"\" Module cpymadtools.plotters --------------------------- Created on 2019.12.08 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions to plot different output results from a cpymad.madx.Madx object's simulation results. \"\"\" from pathlib import Path from typing import Tuple import matplotlib import matplotlib.pyplot as plt import numpy as np import pandas as pd from cpymad.madx import Madx from loguru import logger from matplotlib import colors as mcolors from pyhdtoolkit.models.beam import BeamParameters from pyhdtoolkit.optics.twiss import courant_snyder_transform from pyhdtoolkit.utils.defaults import PLOT_PARAMS plt . rcParams . update ( PLOT_PARAMS ) COLORS_DICT = dict ( mcolors . BASE_COLORS , ** mcolors . CSS4_COLORS ) BY_HSV = sorted ( ( tuple ( mcolors . rgb_to_hsv ( mcolors . to_rgba ( color )[: 3 ])), name ) for name , color in COLORS_DICT . items () ) SORTED_COLORS = [ name for hsv , name in BY_HSV ] class AperturePlotter : \"\"\" A class to plot the physical aperture of your machine. \"\"\" @staticmethod def plot_aperture ( madx : Madx , beam_params : BeamParameters , figsize : Tuple [ int , int ] = ( 13 , 20 ), xlimits : Tuple [ float , float ] = None , hplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. beam_params (BeamParameters): a validated BeamParameters object from `pyhdtoolkit.optics.beam.compute_beam_parameters`. figsize (str): size of the figure, defaults to (15, 15). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. hplane_ylim (Tuple[float, float]): the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). vplane_ylim (Tuple[float, float]): the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint: disable=too-many-arguments # We need to interpolate in order to get high resolution along the s direction logger . debug ( \"Running interpolation in cpymad\" ) madx . input ( \"\"\" select, flag=interpolate, class=drift, slice=4, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=8, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; select, flag=interpolate, class=rbend, slice=10, range=#s/#e; twiss; \"\"\" ) logger . debug ( \"Getting Twiss dframe from cpymad\" ) twiss_hr : pd . DataFrame = madx . table . twiss . dframe () twiss_hr [ \"betatronic_envelope_x\" ] = np . sqrt ( twiss_hr . betx . values * beam_params . eg_y_m ) twiss_hr [ \"betatronic_envelope_y\" ] = np . sqrt ( twiss_hr . bety . values * beam_params . eg_y_m ) twiss_hr [ \"dispersive_envelope_x\" ] = twiss_hr . dx . values * beam_params . deltap_p twiss_hr [ \"dispersive_envelope_y\" ] = twiss_hr . dy . values * beam_params . deltap_p twiss_hr [ \"envelope_x\" ] = np . sqrt ( twiss_hr . betatronic_envelope_x . values ** 2 + ( twiss_hr . dx . values * beam_params . deltap_p ) ** 2 ) twiss_hr [ \"envelope_y\" ] = np . sqrt ( twiss_hr . betatronic_envelope_y . values ** 2 + ( twiss_hr . dy . values * beam_params . deltap_p ) ** 2 ) machine = twiss_hr [ twiss_hr . apertype == \"ellipse\" ] figure = plt . figure ( figsize = figsize ) logger . debug ( \"Plotting the horizontal aperture\" ) axis1 = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) axis1 . plot ( twiss_hr . s , twiss_hr . envelope_x , color = \"b\" ) axis1 . plot ( twiss_hr . s , - twiss_hr . envelope_x , color = \"b\" ) axis1 . fill_between ( twiss_hr . s , twiss_hr . envelope_x , - twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_x , - 3 * twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( machine . s , machine . aper_1 , machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . fill_between ( machine . s , - machine . aper_1 , - machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . plot ( machine . s , machine . aper_1 , \"k.-\" ) axis1 . plot ( machine . s , - machine . aper_1 , \"k.-\" ) axis1 . set_xlim ( xlimits ) axis1 . set_ylim ( hplane_ylim ) axis1 . set_ylabel ( \"x [m]\" ) axis1 . set_xlabel ( \"s [m]\" ) axis1 . set_title ( f \"Horizontal aperture at { beam_params . pc_GeV } GeV/c\" ) logger . debug ( \"Plotting the vertical aperture\" ) axis2 = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis2 . plot ( twiss_hr . s , twiss_hr . envelope_y , color = \"r\" ) axis2 . plot ( twiss_hr . s , - twiss_hr . envelope_y , color = \"r\" ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( machine . s , machine . aper_2 , machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . fill_between ( machine . s , - machine . aper_2 , - machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . plot ( machine . s , machine . aper_2 , \"k.-\" ) axis2 . plot ( machine . s , - machine . aper_2 , \"k.-\" ) axis2 . set_ylim ( vplane_ylim ) axis2 . set_ylabel ( \"y [m]\" ) axis2 . set_xlabel ( \"s [m]\" ) axis2 . set_title ( f \"Vertical aperture at { beam_params . pc_GeV } GeV/c\" ) logger . debug ( \"Plotting the stay-clear envelope\" ) axis3 = plt . subplot2grid (( 3 , 3 ), ( 2 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis3 . plot ( machine . s , machine . aper_1 / machine . envelope_x , \".-b\" , label = \"Horizontal plane\" ) axis3 . plot ( machine . s , machine . aper_2 / machine . envelope_y , \".-r\" , label = \"Vertical plane\" ) axis3 . set_xlim ( xlimits ) axis3 . set_ylabel ( \"n1\" ) axis3 . set_xlabel ( \"s [m]\" ) axis3 . legend ( loc = \"best\" ) axis3 . set_title ( f \"Stay-clear envelope at { beam_params . pc_GeV } GeV/c\" ) if savefig : logger . info ( f \"Saving aperture plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure class DynamicAperturePlotter : \"\"\" A class to plot the dynamic aperture of your machine. \"\"\" @staticmethod def plot_dynamic_aperture ( vx_coords : np . ndarray , vy_coords : np . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure : \"\"\" Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Args: vx_coords (np.ndarray): numpy array of horizontal coordinates over turns. vy_coords (np.ndarray): numpy array of vertical coordinates over turns. n_particles (int): number of particles simulated. savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" figure = plt . figure ( figsize = ( 12 , 7 )) turn_lost = [] x_in_lost = [] for particle in range ( n_particles ): nb = len ( vx_coords [ particle ]) - max ( np . isnan ( vx_coords [ particle ]) . sum (), np . isnan ( vy_coords [ particle ]) . sum () ) turn_lost . append ( nb ) x_in_lost . append ( vx_coords [ particle ][ 0 ] ** 2 + vy_coords [ particle ][ 0 ] ** 2 ) turn_lost = np . array ( turn_lost ) x_in_lost = np . array ( x_in_lost ) plt . scatter ( turn_lost , x_in_lost * 1000 , linewidths = 0.7 , c = \"darkblue\" , marker = \".\" ) plt . title ( \"Amplitudes lost over turns\" , fontsize = 20 ) plt . xlabel ( \"Number of Turns Survived\" , fontsize = 17 ) plt . ylabel ( \"Initial amplitude [mm]\" , fontsize = 17 ) if savefig : logger . info ( f \"Saving dynamic aperture plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure class PhaseSpacePlotter : \"\"\" A class to plot Courant-Snyder coordinates phase space. \"\"\" @staticmethod def plot_courant_snyder_phase_space ( madx : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ): logger . error ( f \"Plane should be either Horizontal or Vertical but ' { plane } ' was given\" ) raise ValueError ( \"Invalid plane value\" ) figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) # Getting the P matrix to compute Courant-Snyder coordinates logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = madx . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . alfy [ 0 ] beta = madx . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . bety [ 0 ] logger . debug ( f \"Plotting Courant-Snyder phase space for the { plane . lower () } plane\" ) for index , _ in enumerate ( u_coordinates ): logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle { index } \" ) u = np . array ([ u_coordinates [ index ], pu_coordinates [ index ]]) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0 , :] * 1e3 , u_bar [ 1 , :] * 1e3 , s = 0.1 , c = \"k\" ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$ \\\\ bar {x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$ \\\\ bar {y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving Courant-Snyder phase space plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure @staticmethod def plot_courant_snyder_phase_space_colored ( madx : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156th color. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ): logger . error ( f \"Plane should be either Horizontal or Vertical but ' { plane } ' was given\" ) raise ValueError ( \"Invalid plane value\" ) # Getting a sufficiently long array of colors to use colors = int ( np . floor ( len ( u_coordinates ) / 100 )) * SORTED_COLORS while len ( colors ) > len ( u_coordinates ): colors . pop () figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = madx . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . alfy [ 0 ] beta = madx . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . bety [ 0 ] logger . debug ( f \"Plotting colored normalised phase space for the { plane . lower () } plane\" ) for index , _ in enumerate ( u_coordinates ): logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle { index } \" ) u = np . array ([ u_coordinates [ index ], pu_coordinates [ index ]]) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0 , :] * 1e3 , u_bar [ 1 , :] * 1e3 , s = 0.1 , c = colors [ index ]) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$ \\\\ bar {x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$ \\\\ bar {y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving colored Courant-Snyder phase space plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure class TuneDiagramPlotter : \"\"\" A class to plot a blank tune diagram with Farey sequences, as well as your working points. \"\"\" @staticmethod def farey_sequence ( order : int ) -> list : \"\"\" Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Args: order (int): the order up to which we want to calculate the sequence. Returns: The sequence as a list. \"\"\" seq = [[ 0 , 1 ]] a , b , c , d = 0 , 1 , 1 , order while c <= order : k = int (( order + b ) / d ) a , b , c , d = c , d , k * c - a , k * d - b seq . append ([ a , b ]) return seq @staticmethod def plot_blank_tune_diagram () -> matplotlib . figure . Figure : \"\"\" Plotting the tune diagram up to the 6th order. Original code from Rogelio Tom\u00e1s. Returns: The figure on which resonance lines from farey sequences are drawn. \"\"\" logger . debug ( \"Plotting resonance lines from Farey sequence, up to 5th order\" ) figure = plt . figure ( figsize = ( 13 , 13 )) plt . ylim (( 0 , 1 )) plt . xlim (( 0 , 1 )) x = np . linspace ( 0 , 1 , 1000 ) for i in range ( 1 , 6 ): farey_sequences = TuneDiagramPlotter . farey_sequence ( i ) for f in farey_sequences : h , k = f # Node h/k on the axes for sequence in farey_sequences : p , q = sequence a = float ( k * p ) # Resonance linea Qx + b*Qy = clinkedtop / q if a > 0 : b = float ( q - k * p ) c = float ( p * h ) plt . plot ( x , c / a - x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( x , c / a + x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , 1 - x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , 1 - x , \"b\" , alpha = 0.1 ) if q == k and p == 1 : # FN elements below 1/k break plt . title ( \"Tune Diagram\" , fontsize = 20 ) plt . axis ( \"square\" ) plt . xlim ([ 0 , 1 ]) plt . ylim ([ 0 , 1 ]) plt . xlabel ( \"$Q_ {x} }$\" , fontsize = 17 ) plt . ylabel ( \"$Q_ {y} $\" , fontsize = 17 ) return figure @staticmethod def plot_tune_diagram ( madx : Madx , v_qx : np . ndarray = np . array ([ 0 ]), vxgood : np . ndarray = np . array ([ False ]), v_qy : np . ndarray = np . array ([ 0 ]), vygood : np . ndarray = np . array ([ False ]), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plots the evolution of particles' tunes on a Tune Diagram. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. v_qx (np.ndarray): horizontal tune value as a numpy array. vxgood (np.ndarray): ?? v_qy (np.ndarray): vertical tune value as a numpy array. vygood (np.ndarray): ?? savefig: will save the figure if this is not None, using the string value passed. Returns: The figure on which the diagram is drawn. \"\"\" figure = TuneDiagramPlotter . plot_blank_tune_diagram () logger . debug ( \"Getting Tunes from cpymad\" ) new_q1 : float = madx . table . summ . dframe () . q1 [ 0 ] new_q2 : float = madx . table . summ . dframe () . q2 [ 0 ] if vxgood . any () and vygood . any (): plt . plot ( v_qx [ vxgood * vygood ], v_qy [ vxgood * vygood ], \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif vxgood . any () and ~ vygood . any (): tp = np . ones ( len ( vxgood )) * ( new_q2 - np . floor ( new_q2 )) plt . plot ( v_qx [ vxgood ], tp [ vxgood ], \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif ~ vxgood . any () and vygood . any (): tp = np . ones ( len ( vygood )) * ( new_q1 - np . floor ( new_q1 )) plt . plot ( tp [ vygood ], v_qy [ vygood ], \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) if savefig : logger . info ( f \"Saving Tune diagram plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure Variables BY_HSV COLORS_DICT PLOT_PARAMS SORTED_COLORS Classes AperturePlotter class AperturePlotter ( / , * args , ** kwargs ) View Source class AperturePlotter : \"\"\" A class to plot the physical aperture of your machine. \"\"\" @staticmethod def plot_aperture ( madx : Madx , beam_params : BeamParameters , figsize : Tuple [ int, int ] = ( 13 , 20 ), xlimits : Tuple [ float, float ] = None , hplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. beam_params (BeamParameters): a validated BeamParameters object from `pyhdtoolkit.optics.beam.compute_beam_parameters`. figsize (str): size of the figure, defaults to (15, 15). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. hplane_ylim (Tuple[float, float]): the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). vplane_ylim (Tuple[float, float]): the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint : disable = too - many - arguments # We need to interpolate in order to get high resolution along the s direction logger . debug ( \"Running interpolation in cpymad\" ) madx . input ( \"\"\" select, flag=interpolate, class=drift, slice=4, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=8, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; select, flag=interpolate, class=rbend, slice=10, range=#s/#e; twiss; \"\"\" ) logger . debug ( \"Getting Twiss dframe from cpymad\" ) twiss_hr : pd . DataFrame = madx . table . twiss . dframe () twiss_hr [ \"betatronic_envelope_x\" ] = np . sqrt ( twiss_hr . betx . values * beam_params . eg_y_m ) twiss_hr [ \"betatronic_envelope_y\" ] = np . sqrt ( twiss_hr . bety . values * beam_params . eg_y_m ) twiss_hr [ \"dispersive_envelope_x\" ] = twiss_hr . dx . values * beam_params . deltap_p twiss_hr [ \"dispersive_envelope_y\" ] = twiss_hr . dy . values * beam_params . deltap_p twiss_hr [ \"envelope_x\" ] = np . sqrt ( twiss_hr . betatronic_envelope_x . values ** 2 + ( twiss_hr . dx . values * beam_params . deltap_p ) ** 2 ) twiss_hr [ \"envelope_y\" ] = np . sqrt ( twiss_hr . betatronic_envelope_y . values ** 2 + ( twiss_hr . dy . values * beam_params . deltap_p ) ** 2 ) machine = twiss_hr [ twiss_hr.apertype == \"ellipse\" ] figure = plt . figure ( figsize = figsize ) logger . debug ( \"Plotting the horizontal aperture\" ) axis1 = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) axis1 . plot ( twiss_hr . s , twiss_hr . envelope_x , color = \"b\" ) axis1 . plot ( twiss_hr . s , - twiss_hr . envelope_x , color = \"b\" ) axis1 . fill_between ( twiss_hr . s , twiss_hr . envelope_x , - twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_x , - 3 * twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( machine . s , machine . aper_1 , machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . fill_between ( machine . s , - machine . aper_1 , - machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . plot ( machine . s , machine . aper_1 , \"k.-\" ) axis1 . plot ( machine . s , - machine . aper_1 , \"k.-\" ) axis1 . set_xlim ( xlimits ) axis1 . set_ylim ( hplane_ylim ) axis1 . set_ylabel ( \"x [m]\" ) axis1 . set_xlabel ( \"s [m]\" ) axis1 . set_title ( f \"Horizontal aperture at {beam_params.pc_GeV} GeV/c\" ) logger . debug ( \"Plotting the vertical aperture\" ) axis2 = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis2 . plot ( twiss_hr . s , twiss_hr . envelope_y , color = \"r\" ) axis2 . plot ( twiss_hr . s , - twiss_hr . envelope_y , color = \"r\" ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( machine . s , machine . aper_2 , machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . fill_between ( machine . s , - machine . aper_2 , - machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . plot ( machine . s , machine . aper_2 , \"k.-\" ) axis2 . plot ( machine . s , - machine . aper_2 , \"k.-\" ) axis2 . set_ylim ( vplane_ylim ) axis2 . set_ylabel ( \"y [m]\" ) axis2 . set_xlabel ( \"s [m]\" ) axis2 . set_title ( f \"Vertical aperture at {beam_params.pc_GeV} GeV/c\" ) logger . debug ( \"Plotting the stay-clear envelope\" ) axis3 = plt . subplot2grid (( 3 , 3 ), ( 2 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis3 . plot ( machine . s , machine . aper_1 / machine . envelope_x , \".-b\" , label = \"Horizontal plane\" ) axis3 . plot ( machine . s , machine . aper_2 / machine . envelope_y , \".-r\" , label = \"Vertical plane\" ) axis3 . set_xlim ( xlimits ) axis3 . set_ylabel ( \"n1\" ) axis3 . set_xlabel ( \"s [m]\" ) axis3 . legend ( loc = \"best\" ) axis3 . set_title ( f \"Stay-clear envelope at {beam_params.pc_GeV} GeV/c\" ) if savefig : logger . info ( f \"Saving aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure Static methods plot_aperture def plot_aperture ( madx : cpymad . madx . Madx , beam_params : pyhdtoolkit . models . beam . BeamParameters , figsize : Tuple [ int , int ] = ( 13 , 20 ), xlimits : Tuple [ float , float ] = None , hplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), savefig : str = None ) -> matplotlib . figure . Figure Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None beam_params BeamParameters a validated BeamParameters object from pyhdtoolkit.optics.beam.compute_beam_parameters . None figsize str size of the figure, defaults to (15, 15). None xlimits Tuple[float, float] will implement xlim (for the s coordinate) if this is not None, using the tuple passed. None hplane_ylim Tuple[float, float] the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). None vplane_ylim Tuple[float, float] the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). None savefig str will save the figure if this is not None, using the string value passed. None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_aperture ( madx : Madx , beam_params : BeamParameters , figsize : Tuple [ int, int ] = ( 13 , 20 ), xlimits : Tuple [ float, float ] = None , hplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. beam_params (BeamParameters): a validated BeamParameters object from `pyhdtoolkit.optics.beam.compute_beam_parameters`. figsize (str): size of the figure, defaults to (15, 15). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. hplane_ylim (Tuple[float, float]): the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). vplane_ylim (Tuple[float, float]): the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint : disable = too - many - arguments # We need to interpolate in order to get high resolution along the s direction logger . debug ( \"Running interpolation in cpymad\" ) madx . input ( \"\"\" select, flag=interpolate, class=drift, slice=4, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=8, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; select, flag=interpolate, class=rbend, slice=10, range=#s/#e; twiss; \"\"\" ) logger . debug ( \"Getting Twiss dframe from cpymad\" ) twiss_hr : pd . DataFrame = madx . table . twiss . dframe () twiss_hr [ \"betatronic_envelope_x\" ] = np . sqrt ( twiss_hr . betx . values * beam_params . eg_y_m ) twiss_hr [ \"betatronic_envelope_y\" ] = np . sqrt ( twiss_hr . bety . values * beam_params . eg_y_m ) twiss_hr [ \"dispersive_envelope_x\" ] = twiss_hr . dx . values * beam_params . deltap_p twiss_hr [ \"dispersive_envelope_y\" ] = twiss_hr . dy . values * beam_params . deltap_p twiss_hr [ \"envelope_x\" ] = np . sqrt ( twiss_hr . betatronic_envelope_x . values ** 2 + ( twiss_hr . dx . values * beam_params . deltap_p ) ** 2 ) twiss_hr [ \"envelope_y\" ] = np . sqrt ( twiss_hr . betatronic_envelope_y . values ** 2 + ( twiss_hr . dy . values * beam_params . deltap_p ) ** 2 ) machine = twiss_hr [ twiss_hr.apertype == \"ellipse\" ] figure = plt . figure ( figsize = figsize ) logger . debug ( \"Plotting the horizontal aperture\" ) axis1 = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) axis1 . plot ( twiss_hr . s , twiss_hr . envelope_x , color = \"b\" ) axis1 . plot ( twiss_hr . s , - twiss_hr . envelope_x , color = \"b\" ) axis1 . fill_between ( twiss_hr . s , twiss_hr . envelope_x , - twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_x , - 3 * twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( machine . s , machine . aper_1 , machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . fill_between ( machine . s , - machine . aper_1 , - machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . plot ( machine . s , machine . aper_1 , \"k.-\" ) axis1 . plot ( machine . s , - machine . aper_1 , \"k.-\" ) axis1 . set_xlim ( xlimits ) axis1 . set_ylim ( hplane_ylim ) axis1 . set_ylabel ( \"x [m]\" ) axis1 . set_xlabel ( \"s [m]\" ) axis1 . set_title ( f \"Horizontal aperture at {beam_params.pc_GeV} GeV/c\" ) logger . debug ( \"Plotting the vertical aperture\" ) axis2 = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis2 . plot ( twiss_hr . s , twiss_hr . envelope_y , color = \"r\" ) axis2 . plot ( twiss_hr . s , - twiss_hr . envelope_y , color = \"r\" ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( machine . s , machine . aper_2 , machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . fill_between ( machine . s , - machine . aper_2 , - machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . plot ( machine . s , machine . aper_2 , \"k.-\" ) axis2 . plot ( machine . s , - machine . aper_2 , \"k.-\" ) axis2 . set_ylim ( vplane_ylim ) axis2 . set_ylabel ( \"y [m]\" ) axis2 . set_xlabel ( \"s [m]\" ) axis2 . set_title ( f \"Vertical aperture at {beam_params.pc_GeV} GeV/c\" ) logger . debug ( \"Plotting the stay-clear envelope\" ) axis3 = plt . subplot2grid (( 3 , 3 ), ( 2 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis3 . plot ( machine . s , machine . aper_1 / machine . envelope_x , \".-b\" , label = \"Horizontal plane\" ) axis3 . plot ( machine . s , machine . aper_2 / machine . envelope_y , \".-r\" , label = \"Vertical plane\" ) axis3 . set_xlim ( xlimits ) axis3 . set_ylabel ( \"n1\" ) axis3 . set_xlabel ( \"s [m]\" ) axis3 . legend ( loc = \"best\" ) axis3 . set_title ( f \"Stay-clear envelope at {beam_params.pc_GeV} GeV/c\" ) if savefig : logger . info ( f \"Saving aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure DynamicAperturePlotter class DynamicAperturePlotter ( / , * args , ** kwargs ) View Source class DynamicAperturePlotter : \"\"\" A class to plot the dynamic aperture of your machine. \"\"\" @staticmethod def plot_dynamic_aperture ( vx_coords : np . ndarray , vy_coords : np . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure : \"\"\" Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Args: vx_coords (np.ndarray): numpy array of horizontal coordinates over turns. vy_coords (np.ndarray): numpy array of vertical coordinates over turns. n_particles (int): number of particles simulated. savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" figure = plt . figure ( figsize = ( 12 , 7 )) turn_lost = [] x_in_lost = [] for particle in range ( n_particles ) : nb = len ( vx_coords [ particle ] ) - max ( np . isnan ( vx_coords [ particle ] ). sum (), np . isnan ( vy_coords [ particle ] ). sum () ) turn_lost . append ( nb ) x_in_lost . append ( vx_coords [ particle ][ 0 ] ** 2 + vy_coords [ particle ][ 0 ] ** 2 ) turn_lost = np . array ( turn_lost ) x_in_lost = np . array ( x_in_lost ) plt . scatter ( turn_lost , x_in_lost * 1000 , linewidths = 0.7 , c = \"darkblue\" , marker = \".\" ) plt . title ( \"Amplitudes lost over turns\" , fontsize = 20 ) plt . xlabel ( \"Number of Turns Survived\" , fontsize = 17 ) plt . ylabel ( \"Initial amplitude [mm]\" , fontsize = 17 ) if savefig : logger . info ( f \"Saving dynamic aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure Static methods plot_dynamic_aperture def plot_dynamic_aperture ( vx_coords : numpy . ndarray , vy_coords : numpy . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Parameters: Name Type Description Default vx_coords np.ndarray numpy array of horizontal coordinates over turns. None vy_coords np.ndarray numpy array of vertical coordinates over turns. None n_particles int number of particles simulated. None savefig str will save the figure if this is not None, using the string value passed. None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_dynamic_aperture ( vx_coords : np . ndarray , vy_coords : np . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure : \"\"\" Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Args: vx_coords (np.ndarray): numpy array of horizontal coordinates over turns. vy_coords (np.ndarray): numpy array of vertical coordinates over turns. n_particles (int): number of particles simulated. savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" figure = plt . figure ( figsize = ( 12 , 7 )) turn_lost = [] x_in_lost = [] for particle in range ( n_particles ) : nb = len ( vx_coords [ particle ] ) - max ( np . isnan ( vx_coords [ particle ] ). sum (), np . isnan ( vy_coords [ particle ] ). sum () ) turn_lost . append ( nb ) x_in_lost . append ( vx_coords [ particle ][ 0 ] ** 2 + vy_coords [ particle ][ 0 ] ** 2 ) turn_lost = np . array ( turn_lost ) x_in_lost = np . array ( x_in_lost ) plt . scatter ( turn_lost , x_in_lost * 1000 , linewidths = 0.7 , c = \"darkblue\" , marker = \".\" ) plt . title ( \"Amplitudes lost over turns\" , fontsize = 20 ) plt . xlabel ( \"Number of Turns Survived\" , fontsize = 17 ) plt . ylabel ( \"Initial amplitude [mm]\" , fontsize = 17 ) if savefig : logger . info ( f \"Saving dynamic aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure PhaseSpacePlotter class PhaseSpacePlotter ( / , * args , ** kwargs ) View Source class PhaseSpacePlotter : \"\"\" A class to plot Courant-Snyder coordinates phase space. \"\"\" @staticmethod def plot_courant_snyder_phase_space ( madx : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) # Getting the P matrix to compute Courant - Snyder coordinates logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = madx . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . alfy [ 0 ] beta = madx . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . bety [ 0 ] logger . debug ( f \"Plotting Courant-Snyder phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = \"k\" ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure @staticmethod def plot_courant_snyder_phase_space_colored ( madx : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156th color. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) # Getting a sufficiently long array of colors to use colors = int ( np . floor ( len ( u_coordinates ) / 100 )) * SORTED_COLORS while len ( colors ) > len ( u_coordinates ) : colors . pop () figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = madx . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . alfy [ 0 ] beta = madx . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . bety [ 0 ] logger . debug ( f \"Plotting colored normalised phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = colors [ index ] ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving colored Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure Static methods plot_courant_snyder_phase_space def plot_courant_snyder_phase_space ( madx : cpymad . madx . Madx , u_coordinates : numpy . ndarray , pu_coordinates : numpy . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = 'Horizontal' ) -> matplotlib . figure . Figure Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None u_coordinates np.ndarray numpy array of particles's coordinates for the given plane. None pu_coordinates np.ndarray numpy array of particles's momentum coordinates for the given plane. None savefig str will save the figure if this is not None, using the string value passed. None size Tuple[int, int] the wanted matplotlib figure size. Defaults to (16, 8). (16, 8) plane str the physical plane to plot. Defaults to 'Horizontal'. 'Horizontal' Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_courant_snyder_phase_space ( madx : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) # Getting the P matrix to compute Courant - Snyder coordinates logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = madx . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . alfy [ 0 ] beta = madx . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . bety [ 0 ] logger . debug ( f \"Plotting Courant-Snyder phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = \"k\" ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure plot_courant_snyder_phase_space_colored def plot_courant_snyder_phase_space_colored ( madx : cpymad . madx . Madx , u_coordinates : numpy . ndarray , pu_coordinates : numpy . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = 'Horizontal' ) -> matplotlib . figure . Figure Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156 th color. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None u_coordinates np.ndarray numpy array of particles's coordinates for the given plane. None pu_coordinates np.ndarray numpy array of particles's momentum coordinates for the given plane. None savefig str will save the figure if this is not None, using the string value passed. None size Tuple[int, int] the wanted matplotlib figure size. Defaults to (16, 8). (16, 8) plane str the physical plane to plot. Defaults to 'Horizontal'. 'Horizontal' Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_courant_snyder_phase_space_colored ( madx : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156th color. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) # Getting a sufficiently long array of colors to use colors = int ( np . floor ( len ( u_coordinates ) / 100 )) * SORTED_COLORS while len ( colors ) > len ( u_coordinates ) : colors . pop () figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = madx . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . alfy [ 0 ] beta = madx . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . bety [ 0 ] logger . debug ( f \"Plotting colored normalised phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = colors [ index ] ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving colored Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure TuneDiagramPlotter class TuneDiagramPlotter ( / , * args , ** kwargs ) View Source class TuneDiagramPlotter : \"\"\" A class to plot a blank tune diagram with Farey sequences, as well as your working points. \"\"\" @staticmethod def farey_sequence ( order : int ) -> list : \"\"\" Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Args: order (int): the order up to which we want to calculate the sequence. Returns: The sequence as a list. \"\"\" seq = [ [0, 1 ] ] a , b , c , d = 0 , 1 , 1 , order while c <= order : k = int (( order + b ) / d ) a , b , c , d = c , d , k * c - a , k * d - b seq . append ( [ a, b ] ) return seq @staticmethod def plot_blank_tune_diagram () -> matplotlib . figure . Figure : \"\"\" Plotting the tune diagram up to the 6th order. Original code from Rogelio Tom\u00e1s. Returns: The figure on which resonance lines from farey sequences are drawn. \"\"\" logger . debug ( \"Plotting resonance lines from Farey sequence, up to 5th order\" ) figure = plt . figure ( figsize = ( 13 , 13 )) plt . ylim (( 0 , 1 )) plt . xlim (( 0 , 1 )) x = np . linspace ( 0 , 1 , 1000 ) for i in range ( 1 , 6 ) : farey_sequences = TuneDiagramPlotter . farey_sequence ( i ) for f in farey_sequences : h , k = f # Node h / k on the axes for sequence in farey_sequences : p , q = sequence a = float ( k * p ) # Resonance linea Qx + b * Qy = clinkedtop / q if a > 0 : b = float ( q - k * p ) c = float ( p * h ) plt . plot ( x , c / a - x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( x , c / a + x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , 1 - x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , 1 - x , \"b\" , alpha = 0.1 ) if q == k and p == 1 : # FN elements below 1 / k break plt . title ( \"Tune Diagram\" , fontsize = 20 ) plt . axis ( \"square\" ) plt . xlim ( [ 0, 1 ] ) plt . ylim ( [ 0, 1 ] ) plt . xlabel ( \"$Q_{x}}$\" , fontsize = 17 ) plt . ylabel ( \"$Q_{y}$\" , fontsize = 17 ) return figure @staticmethod def plot_tune_diagram ( madx : Madx , v_qx : np . ndarray = np . array ( [ 0 ] ), vxgood : np . ndarray = np . array ( [ False ] ), v_qy : np . ndarray = np . array ( [ 0 ] ), vygood : np . ndarray = np . array ( [ False ] ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plots the evolution of particles' tunes on a Tune Diagram. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. v_qx (np.ndarray): horizontal tune value as a numpy array. vxgood (np.ndarray): ?? v_qy (np.ndarray): vertical tune value as a numpy array. vygood (np.ndarray): ?? savefig: will save the figure if this is not None, using the string value passed. Returns: The figure on which the diagram is drawn. \"\"\" figure = TuneDiagramPlotter . plot_blank_tune_diagram () logger . debug ( \"Getting Tunes from cpymad\" ) new_q1 : float = madx . table . summ . dframe (). q1 [ 0 ] new_q2 : float = madx . table . summ . dframe (). q2 [ 0 ] if vxgood . any () and vygood . any () : plt . plot ( v_qx [ vxgood * vygood ] , v_qy [ vxgood * vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif vxgood . any () and ~ vygood . any () : tp = np . ones ( len ( vxgood )) * ( new_q2 - np . floor ( new_q2 )) plt . plot ( v_qx [ vxgood ] , tp [ vxgood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif ~ vxgood . any () and vygood . any () : tp = np . ones ( len ( vygood )) * ( new_q1 - np . floor ( new_q1 )) plt . plot ( tp [ vygood ] , v_qy [ vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) if savefig : logger . info ( f \"Saving Tune diagram plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure Static methods farey_sequence def farey_sequence ( order : int ) -> list Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Parameters: Name Type Description Default order int the order up to which we want to calculate the sequence. None Returns: Type Description None The sequence as a list. View Source @staticmethod def farey_sequence ( order : int ) -> list : \"\"\" Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Args: order (int): the order up to which we want to calculate the sequence. Returns: The sequence as a list. \"\"\" seq = [ [0, 1 ] ] a , b , c , d = 0 , 1 , 1 , order while c <= order : k = int (( order + b ) / d ) a , b , c , d = c , d , k * c - a , k * d - b seq . append ( [ a, b ] ) return seq plot_blank_tune_diagram def plot_blank_tune_diagram ( ) -> matplotlib . figure . Figure Plotting the tune diagram up to the 6 th order. Original code from Rogelio Tom\u00e1s. Returns: Type Description None The figure on which resonance lines from farey sequences are drawn. View Source @staticmethod def plot_blank_tune_diagram () -> matplotlib . figure . Figure : \"\"\" Plotting the tune diagram up to the 6th order. Original code from Rogelio Tom\u00e1s. Returns: The figure on which resonance lines from farey sequences are drawn. \"\"\" logger . debug ( \"Plotting resonance lines from Farey sequence, up to 5th order\" ) figure = plt . figure ( figsize = ( 13 , 13 )) plt . ylim (( 0 , 1 )) plt . xlim (( 0 , 1 )) x = np . linspace ( 0 , 1 , 1000 ) for i in range ( 1 , 6 ) : farey_sequences = TuneDiagramPlotter . farey_sequence ( i ) for f in farey_sequences : h , k = f # Node h / k on the axes for sequence in farey_sequences : p , q = sequence a = float ( k * p ) # Resonance linea Qx + b * Qy = clinkedtop / q if a > 0 : b = float ( q - k * p ) c = float ( p * h ) plt . plot ( x , c / a - x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( x , c / a + x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , 1 - x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , 1 - x , \"b\" , alpha = 0.1 ) if q == k and p == 1 : # FN elements below 1 / k break plt . title ( \"Tune Diagram\" , fontsize = 20 ) plt . axis ( \"square\" ) plt . xlim ( [ 0, 1 ] ) plt . ylim ( [ 0, 1 ] ) plt . xlabel ( \"$Q_{x}}$\" , fontsize = 17 ) plt . ylabel ( \"$Q_{y}$\" , fontsize = 17 ) return figure plot_tune_diagram def plot_tune_diagram ( madx : cpymad . madx . Madx , v_qx : numpy . ndarray = array ([ 0 ]), vxgood : numpy . ndarray = array ([ False ]), v_qy : numpy . ndarray = array ([ 0 ]), vygood : numpy . ndarray = array ([ False ]), savefig : str = None ) -> matplotlib . figure . Figure Plots the evolution of particles' tunes on a Tune Diagram. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None v_qx np.ndarray horizontal tune value as a numpy array. None vxgood np.ndarray ?? None v_qy np.ndarray vertical tune value as a numpy array. None vygood np.ndarray ?? None savefig None will save the figure if this is not None, using the string value passed. None Returns: Type Description None The figure on which the diagram is drawn. View Source @staticmethod def plot_tune_diagram ( madx : Madx , v_qx : np . ndarray = np . array ( [ 0 ] ), vxgood : np . ndarray = np . array ( [ False ] ), v_qy : np . ndarray = np . array ( [ 0 ] ), vygood : np . ndarray = np . array ( [ False ] ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plots the evolution of particles' tunes on a Tune Diagram. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. v_qx (np.ndarray): horizontal tune value as a numpy array. vxgood (np.ndarray): ?? v_qy (np.ndarray): vertical tune value as a numpy array. vygood (np.ndarray): ?? savefig: will save the figure if this is not None, using the string value passed. Returns: The figure on which the diagram is drawn. \"\"\" figure = TuneDiagramPlotter . plot_blank_tune_diagram () logger . debug ( \"Getting Tunes from cpymad\" ) new_q1 : float = madx . table . summ . dframe (). q1 [ 0 ] new_q2 : float = madx . table . summ . dframe (). q2 [ 0 ] if vxgood . any () and vygood . any () : plt . plot ( v_qx [ vxgood * vygood ] , v_qy [ vxgood * vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif vxgood . any () and ~ vygood . any () : tp = np . ones ( len ( vxgood )) * ( new_q2 - np . floor ( new_q2 )) plt . plot ( v_qx [ vxgood ] , tp [ vxgood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif ~ vxgood . any () and vygood . any () : tp = np . ones ( len ( vygood )) * ( new_q1 - np . floor ( new_q1 )) plt . plot ( tp [ vygood ] , v_qy [ vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) if savefig : logger . info ( f \"Saving Tune diagram plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"Plotters"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#module-pyhdtoolkitcpymadtoolsplotters","text":"Module cpymadtools.plotters Created on 2019.12.08 View Source \"\"\" Module cpymadtools.plotters --------------------------- Created on 2019.12.08 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions to plot different output results from a cpymad.madx.Madx object's simulation results. \"\"\" from pathlib import Path from typing import Tuple import matplotlib import matplotlib.pyplot as plt import numpy as np import pandas as pd from cpymad.madx import Madx from loguru import logger from matplotlib import colors as mcolors from pyhdtoolkit.models.beam import BeamParameters from pyhdtoolkit.optics.twiss import courant_snyder_transform from pyhdtoolkit.utils.defaults import PLOT_PARAMS plt . rcParams . update ( PLOT_PARAMS ) COLORS_DICT = dict ( mcolors . BASE_COLORS , ** mcolors . CSS4_COLORS ) BY_HSV = sorted ( ( tuple ( mcolors . rgb_to_hsv ( mcolors . to_rgba ( color )[: 3 ])), name ) for name , color in COLORS_DICT . items () ) SORTED_COLORS = [ name for hsv , name in BY_HSV ] class AperturePlotter : \"\"\" A class to plot the physical aperture of your machine. \"\"\" @staticmethod def plot_aperture ( madx : Madx , beam_params : BeamParameters , figsize : Tuple [ int , int ] = ( 13 , 20 ), xlimits : Tuple [ float , float ] = None , hplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. beam_params (BeamParameters): a validated BeamParameters object from `pyhdtoolkit.optics.beam.compute_beam_parameters`. figsize (str): size of the figure, defaults to (15, 15). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. hplane_ylim (Tuple[float, float]): the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). vplane_ylim (Tuple[float, float]): the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint: disable=too-many-arguments # We need to interpolate in order to get high resolution along the s direction logger . debug ( \"Running interpolation in cpymad\" ) madx . input ( \"\"\" select, flag=interpolate, class=drift, slice=4, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=8, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; select, flag=interpolate, class=rbend, slice=10, range=#s/#e; twiss; \"\"\" ) logger . debug ( \"Getting Twiss dframe from cpymad\" ) twiss_hr : pd . DataFrame = madx . table . twiss . dframe () twiss_hr [ \"betatronic_envelope_x\" ] = np . sqrt ( twiss_hr . betx . values * beam_params . eg_y_m ) twiss_hr [ \"betatronic_envelope_y\" ] = np . sqrt ( twiss_hr . bety . values * beam_params . eg_y_m ) twiss_hr [ \"dispersive_envelope_x\" ] = twiss_hr . dx . values * beam_params . deltap_p twiss_hr [ \"dispersive_envelope_y\" ] = twiss_hr . dy . values * beam_params . deltap_p twiss_hr [ \"envelope_x\" ] = np . sqrt ( twiss_hr . betatronic_envelope_x . values ** 2 + ( twiss_hr . dx . values * beam_params . deltap_p ) ** 2 ) twiss_hr [ \"envelope_y\" ] = np . sqrt ( twiss_hr . betatronic_envelope_y . values ** 2 + ( twiss_hr . dy . values * beam_params . deltap_p ) ** 2 ) machine = twiss_hr [ twiss_hr . apertype == \"ellipse\" ] figure = plt . figure ( figsize = figsize ) logger . debug ( \"Plotting the horizontal aperture\" ) axis1 = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) axis1 . plot ( twiss_hr . s , twiss_hr . envelope_x , color = \"b\" ) axis1 . plot ( twiss_hr . s , - twiss_hr . envelope_x , color = \"b\" ) axis1 . fill_between ( twiss_hr . s , twiss_hr . envelope_x , - twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_x , - 3 * twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( machine . s , machine . aper_1 , machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . fill_between ( machine . s , - machine . aper_1 , - machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . plot ( machine . s , machine . aper_1 , \"k.-\" ) axis1 . plot ( machine . s , - machine . aper_1 , \"k.-\" ) axis1 . set_xlim ( xlimits ) axis1 . set_ylim ( hplane_ylim ) axis1 . set_ylabel ( \"x [m]\" ) axis1 . set_xlabel ( \"s [m]\" ) axis1 . set_title ( f \"Horizontal aperture at { beam_params . pc_GeV } GeV/c\" ) logger . debug ( \"Plotting the vertical aperture\" ) axis2 = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis2 . plot ( twiss_hr . s , twiss_hr . envelope_y , color = \"r\" ) axis2 . plot ( twiss_hr . s , - twiss_hr . envelope_y , color = \"r\" ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( machine . s , machine . aper_2 , machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . fill_between ( machine . s , - machine . aper_2 , - machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . plot ( machine . s , machine . aper_2 , \"k.-\" ) axis2 . plot ( machine . s , - machine . aper_2 , \"k.-\" ) axis2 . set_ylim ( vplane_ylim ) axis2 . set_ylabel ( \"y [m]\" ) axis2 . set_xlabel ( \"s [m]\" ) axis2 . set_title ( f \"Vertical aperture at { beam_params . pc_GeV } GeV/c\" ) logger . debug ( \"Plotting the stay-clear envelope\" ) axis3 = plt . subplot2grid (( 3 , 3 ), ( 2 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis3 . plot ( machine . s , machine . aper_1 / machine . envelope_x , \".-b\" , label = \"Horizontal plane\" ) axis3 . plot ( machine . s , machine . aper_2 / machine . envelope_y , \".-r\" , label = \"Vertical plane\" ) axis3 . set_xlim ( xlimits ) axis3 . set_ylabel ( \"n1\" ) axis3 . set_xlabel ( \"s [m]\" ) axis3 . legend ( loc = \"best\" ) axis3 . set_title ( f \"Stay-clear envelope at { beam_params . pc_GeV } GeV/c\" ) if savefig : logger . info ( f \"Saving aperture plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure class DynamicAperturePlotter : \"\"\" A class to plot the dynamic aperture of your machine. \"\"\" @staticmethod def plot_dynamic_aperture ( vx_coords : np . ndarray , vy_coords : np . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure : \"\"\" Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Args: vx_coords (np.ndarray): numpy array of horizontal coordinates over turns. vy_coords (np.ndarray): numpy array of vertical coordinates over turns. n_particles (int): number of particles simulated. savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" figure = plt . figure ( figsize = ( 12 , 7 )) turn_lost = [] x_in_lost = [] for particle in range ( n_particles ): nb = len ( vx_coords [ particle ]) - max ( np . isnan ( vx_coords [ particle ]) . sum (), np . isnan ( vy_coords [ particle ]) . sum () ) turn_lost . append ( nb ) x_in_lost . append ( vx_coords [ particle ][ 0 ] ** 2 + vy_coords [ particle ][ 0 ] ** 2 ) turn_lost = np . array ( turn_lost ) x_in_lost = np . array ( x_in_lost ) plt . scatter ( turn_lost , x_in_lost * 1000 , linewidths = 0.7 , c = \"darkblue\" , marker = \".\" ) plt . title ( \"Amplitudes lost over turns\" , fontsize = 20 ) plt . xlabel ( \"Number of Turns Survived\" , fontsize = 17 ) plt . ylabel ( \"Initial amplitude [mm]\" , fontsize = 17 ) if savefig : logger . info ( f \"Saving dynamic aperture plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure class PhaseSpacePlotter : \"\"\" A class to plot Courant-Snyder coordinates phase space. \"\"\" @staticmethod def plot_courant_snyder_phase_space ( madx : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ): logger . error ( f \"Plane should be either Horizontal or Vertical but ' { plane } ' was given\" ) raise ValueError ( \"Invalid plane value\" ) figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) # Getting the P matrix to compute Courant-Snyder coordinates logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = madx . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . alfy [ 0 ] beta = madx . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . bety [ 0 ] logger . debug ( f \"Plotting Courant-Snyder phase space for the { plane . lower () } plane\" ) for index , _ in enumerate ( u_coordinates ): logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle { index } \" ) u = np . array ([ u_coordinates [ index ], pu_coordinates [ index ]]) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0 , :] * 1e3 , u_bar [ 1 , :] * 1e3 , s = 0.1 , c = \"k\" ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$ \\\\ bar {x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$ \\\\ bar {y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving Courant-Snyder phase space plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure @staticmethod def plot_courant_snyder_phase_space_colored ( madx : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156th color. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ): logger . error ( f \"Plane should be either Horizontal or Vertical but ' { plane } ' was given\" ) raise ValueError ( \"Invalid plane value\" ) # Getting a sufficiently long array of colors to use colors = int ( np . floor ( len ( u_coordinates ) / 100 )) * SORTED_COLORS while len ( colors ) > len ( u_coordinates ): colors . pop () figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = madx . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . alfy [ 0 ] beta = madx . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . bety [ 0 ] logger . debug ( f \"Plotting colored normalised phase space for the { plane . lower () } plane\" ) for index , _ in enumerate ( u_coordinates ): logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle { index } \" ) u = np . array ([ u_coordinates [ index ], pu_coordinates [ index ]]) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0 , :] * 1e3 , u_bar [ 1 , :] * 1e3 , s = 0.1 , c = colors [ index ]) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$ \\\\ bar {x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$ \\\\ bar {y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving colored Courant-Snyder phase space plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure class TuneDiagramPlotter : \"\"\" A class to plot a blank tune diagram with Farey sequences, as well as your working points. \"\"\" @staticmethod def farey_sequence ( order : int ) -> list : \"\"\" Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Args: order (int): the order up to which we want to calculate the sequence. Returns: The sequence as a list. \"\"\" seq = [[ 0 , 1 ]] a , b , c , d = 0 , 1 , 1 , order while c <= order : k = int (( order + b ) / d ) a , b , c , d = c , d , k * c - a , k * d - b seq . append ([ a , b ]) return seq @staticmethod def plot_blank_tune_diagram () -> matplotlib . figure . Figure : \"\"\" Plotting the tune diagram up to the 6th order. Original code from Rogelio Tom\u00e1s. Returns: The figure on which resonance lines from farey sequences are drawn. \"\"\" logger . debug ( \"Plotting resonance lines from Farey sequence, up to 5th order\" ) figure = plt . figure ( figsize = ( 13 , 13 )) plt . ylim (( 0 , 1 )) plt . xlim (( 0 , 1 )) x = np . linspace ( 0 , 1 , 1000 ) for i in range ( 1 , 6 ): farey_sequences = TuneDiagramPlotter . farey_sequence ( i ) for f in farey_sequences : h , k = f # Node h/k on the axes for sequence in farey_sequences : p , q = sequence a = float ( k * p ) # Resonance linea Qx + b*Qy = clinkedtop / q if a > 0 : b = float ( q - k * p ) c = float ( p * h ) plt . plot ( x , c / a - x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( x , c / a + x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , 1 - x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , 1 - x , \"b\" , alpha = 0.1 ) if q == k and p == 1 : # FN elements below 1/k break plt . title ( \"Tune Diagram\" , fontsize = 20 ) plt . axis ( \"square\" ) plt . xlim ([ 0 , 1 ]) plt . ylim ([ 0 , 1 ]) plt . xlabel ( \"$Q_ {x} }$\" , fontsize = 17 ) plt . ylabel ( \"$Q_ {y} $\" , fontsize = 17 ) return figure @staticmethod def plot_tune_diagram ( madx : Madx , v_qx : np . ndarray = np . array ([ 0 ]), vxgood : np . ndarray = np . array ([ False ]), v_qy : np . ndarray = np . array ([ 0 ]), vygood : np . ndarray = np . array ([ False ]), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plots the evolution of particles' tunes on a Tune Diagram. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. v_qx (np.ndarray): horizontal tune value as a numpy array. vxgood (np.ndarray): ?? v_qy (np.ndarray): vertical tune value as a numpy array. vygood (np.ndarray): ?? savefig: will save the figure if this is not None, using the string value passed. Returns: The figure on which the diagram is drawn. \"\"\" figure = TuneDiagramPlotter . plot_blank_tune_diagram () logger . debug ( \"Getting Tunes from cpymad\" ) new_q1 : float = madx . table . summ . dframe () . q1 [ 0 ] new_q2 : float = madx . table . summ . dframe () . q2 [ 0 ] if vxgood . any () and vygood . any (): plt . plot ( v_qx [ vxgood * vygood ], v_qy [ vxgood * vygood ], \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif vxgood . any () and ~ vygood . any (): tp = np . ones ( len ( vxgood )) * ( new_q2 - np . floor ( new_q2 )) plt . plot ( v_qx [ vxgood ], tp [ vxgood ], \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif ~ vxgood . any () and vygood . any (): tp = np . ones ( len ( vygood )) * ( new_q1 - np . floor ( new_q1 )) plt . plot ( tp [ vygood ], v_qy [ vygood ], \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) if savefig : logger . info ( f \"Saving Tune diagram plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"Module pyhdtoolkit.cpymadtools.plotters"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#variables","text":"BY_HSV COLORS_DICT PLOT_PARAMS SORTED_COLORS","title":"Variables"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#apertureplotter","text":"class AperturePlotter ( / , * args , ** kwargs ) View Source class AperturePlotter : \"\"\" A class to plot the physical aperture of your machine. \"\"\" @staticmethod def plot_aperture ( madx : Madx , beam_params : BeamParameters , figsize : Tuple [ int, int ] = ( 13 , 20 ), xlimits : Tuple [ float, float ] = None , hplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. beam_params (BeamParameters): a validated BeamParameters object from `pyhdtoolkit.optics.beam.compute_beam_parameters`. figsize (str): size of the figure, defaults to (15, 15). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. hplane_ylim (Tuple[float, float]): the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). vplane_ylim (Tuple[float, float]): the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint : disable = too - many - arguments # We need to interpolate in order to get high resolution along the s direction logger . debug ( \"Running interpolation in cpymad\" ) madx . input ( \"\"\" select, flag=interpolate, class=drift, slice=4, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=8, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; select, flag=interpolate, class=rbend, slice=10, range=#s/#e; twiss; \"\"\" ) logger . debug ( \"Getting Twiss dframe from cpymad\" ) twiss_hr : pd . DataFrame = madx . table . twiss . dframe () twiss_hr [ \"betatronic_envelope_x\" ] = np . sqrt ( twiss_hr . betx . values * beam_params . eg_y_m ) twiss_hr [ \"betatronic_envelope_y\" ] = np . sqrt ( twiss_hr . bety . values * beam_params . eg_y_m ) twiss_hr [ \"dispersive_envelope_x\" ] = twiss_hr . dx . values * beam_params . deltap_p twiss_hr [ \"dispersive_envelope_y\" ] = twiss_hr . dy . values * beam_params . deltap_p twiss_hr [ \"envelope_x\" ] = np . sqrt ( twiss_hr . betatronic_envelope_x . values ** 2 + ( twiss_hr . dx . values * beam_params . deltap_p ) ** 2 ) twiss_hr [ \"envelope_y\" ] = np . sqrt ( twiss_hr . betatronic_envelope_y . values ** 2 + ( twiss_hr . dy . values * beam_params . deltap_p ) ** 2 ) machine = twiss_hr [ twiss_hr.apertype == \"ellipse\" ] figure = plt . figure ( figsize = figsize ) logger . debug ( \"Plotting the horizontal aperture\" ) axis1 = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) axis1 . plot ( twiss_hr . s , twiss_hr . envelope_x , color = \"b\" ) axis1 . plot ( twiss_hr . s , - twiss_hr . envelope_x , color = \"b\" ) axis1 . fill_between ( twiss_hr . s , twiss_hr . envelope_x , - twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_x , - 3 * twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( machine . s , machine . aper_1 , machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . fill_between ( machine . s , - machine . aper_1 , - machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . plot ( machine . s , machine . aper_1 , \"k.-\" ) axis1 . plot ( machine . s , - machine . aper_1 , \"k.-\" ) axis1 . set_xlim ( xlimits ) axis1 . set_ylim ( hplane_ylim ) axis1 . set_ylabel ( \"x [m]\" ) axis1 . set_xlabel ( \"s [m]\" ) axis1 . set_title ( f \"Horizontal aperture at {beam_params.pc_GeV} GeV/c\" ) logger . debug ( \"Plotting the vertical aperture\" ) axis2 = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis2 . plot ( twiss_hr . s , twiss_hr . envelope_y , color = \"r\" ) axis2 . plot ( twiss_hr . s , - twiss_hr . envelope_y , color = \"r\" ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( machine . s , machine . aper_2 , machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . fill_between ( machine . s , - machine . aper_2 , - machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . plot ( machine . s , machine . aper_2 , \"k.-\" ) axis2 . plot ( machine . s , - machine . aper_2 , \"k.-\" ) axis2 . set_ylim ( vplane_ylim ) axis2 . set_ylabel ( \"y [m]\" ) axis2 . set_xlabel ( \"s [m]\" ) axis2 . set_title ( f \"Vertical aperture at {beam_params.pc_GeV} GeV/c\" ) logger . debug ( \"Plotting the stay-clear envelope\" ) axis3 = plt . subplot2grid (( 3 , 3 ), ( 2 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis3 . plot ( machine . s , machine . aper_1 / machine . envelope_x , \".-b\" , label = \"Horizontal plane\" ) axis3 . plot ( machine . s , machine . aper_2 / machine . envelope_y , \".-r\" , label = \"Vertical plane\" ) axis3 . set_xlim ( xlimits ) axis3 . set_ylabel ( \"n1\" ) axis3 . set_xlabel ( \"s [m]\" ) axis3 . legend ( loc = \"best\" ) axis3 . set_title ( f \"Stay-clear envelope at {beam_params.pc_GeV} GeV/c\" ) if savefig : logger . info ( f \"Saving aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"AperturePlotter"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#plot_aperture","text":"def plot_aperture ( madx : cpymad . madx . Madx , beam_params : pyhdtoolkit . models . beam . BeamParameters , figsize : Tuple [ int , int ] = ( 13 , 20 ), xlimits : Tuple [ float , float ] = None , hplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), savefig : str = None ) -> matplotlib . figure . Figure Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None beam_params BeamParameters a validated BeamParameters object from pyhdtoolkit.optics.beam.compute_beam_parameters . None figsize str size of the figure, defaults to (15, 15). None xlimits Tuple[float, float] will implement xlim (for the s coordinate) if this is not None, using the tuple passed. None hplane_ylim Tuple[float, float] the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). None vplane_ylim Tuple[float, float] the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). None savefig str will save the figure if this is not None, using the string value passed. None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_aperture ( madx : Madx , beam_params : BeamParameters , figsize : Tuple [ int, int ] = ( 13 , 20 ), xlimits : Tuple [ float, float ] = None , hplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. beam_params (BeamParameters): a validated BeamParameters object from `pyhdtoolkit.optics.beam.compute_beam_parameters`. figsize (str): size of the figure, defaults to (15, 15). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. hplane_ylim (Tuple[float, float]): the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). vplane_ylim (Tuple[float, float]): the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint : disable = too - many - arguments # We need to interpolate in order to get high resolution along the s direction logger . debug ( \"Running interpolation in cpymad\" ) madx . input ( \"\"\" select, flag=interpolate, class=drift, slice=4, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=8, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; select, flag=interpolate, class=rbend, slice=10, range=#s/#e; twiss; \"\"\" ) logger . debug ( \"Getting Twiss dframe from cpymad\" ) twiss_hr : pd . DataFrame = madx . table . twiss . dframe () twiss_hr [ \"betatronic_envelope_x\" ] = np . sqrt ( twiss_hr . betx . values * beam_params . eg_y_m ) twiss_hr [ \"betatronic_envelope_y\" ] = np . sqrt ( twiss_hr . bety . values * beam_params . eg_y_m ) twiss_hr [ \"dispersive_envelope_x\" ] = twiss_hr . dx . values * beam_params . deltap_p twiss_hr [ \"dispersive_envelope_y\" ] = twiss_hr . dy . values * beam_params . deltap_p twiss_hr [ \"envelope_x\" ] = np . sqrt ( twiss_hr . betatronic_envelope_x . values ** 2 + ( twiss_hr . dx . values * beam_params . deltap_p ) ** 2 ) twiss_hr [ \"envelope_y\" ] = np . sqrt ( twiss_hr . betatronic_envelope_y . values ** 2 + ( twiss_hr . dy . values * beam_params . deltap_p ) ** 2 ) machine = twiss_hr [ twiss_hr.apertype == \"ellipse\" ] figure = plt . figure ( figsize = figsize ) logger . debug ( \"Plotting the horizontal aperture\" ) axis1 = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) axis1 . plot ( twiss_hr . s , twiss_hr . envelope_x , color = \"b\" ) axis1 . plot ( twiss_hr . s , - twiss_hr . envelope_x , color = \"b\" ) axis1 . fill_between ( twiss_hr . s , twiss_hr . envelope_x , - twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_x , - 3 * twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( machine . s , machine . aper_1 , machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . fill_between ( machine . s , - machine . aper_1 , - machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . plot ( machine . s , machine . aper_1 , \"k.-\" ) axis1 . plot ( machine . s , - machine . aper_1 , \"k.-\" ) axis1 . set_xlim ( xlimits ) axis1 . set_ylim ( hplane_ylim ) axis1 . set_ylabel ( \"x [m]\" ) axis1 . set_xlabel ( \"s [m]\" ) axis1 . set_title ( f \"Horizontal aperture at {beam_params.pc_GeV} GeV/c\" ) logger . debug ( \"Plotting the vertical aperture\" ) axis2 = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis2 . plot ( twiss_hr . s , twiss_hr . envelope_y , color = \"r\" ) axis2 . plot ( twiss_hr . s , - twiss_hr . envelope_y , color = \"r\" ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( machine . s , machine . aper_2 , machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . fill_between ( machine . s , - machine . aper_2 , - machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . plot ( machine . s , machine . aper_2 , \"k.-\" ) axis2 . plot ( machine . s , - machine . aper_2 , \"k.-\" ) axis2 . set_ylim ( vplane_ylim ) axis2 . set_ylabel ( \"y [m]\" ) axis2 . set_xlabel ( \"s [m]\" ) axis2 . set_title ( f \"Vertical aperture at {beam_params.pc_GeV} GeV/c\" ) logger . debug ( \"Plotting the stay-clear envelope\" ) axis3 = plt . subplot2grid (( 3 , 3 ), ( 2 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis3 . plot ( machine . s , machine . aper_1 / machine . envelope_x , \".-b\" , label = \"Horizontal plane\" ) axis3 . plot ( machine . s , machine . aper_2 / machine . envelope_y , \".-r\" , label = \"Vertical plane\" ) axis3 . set_xlim ( xlimits ) axis3 . set_ylabel ( \"n1\" ) axis3 . set_xlabel ( \"s [m]\" ) axis3 . legend ( loc = \"best\" ) axis3 . set_title ( f \"Stay-clear envelope at {beam_params.pc_GeV} GeV/c\" ) if savefig : logger . info ( f \"Saving aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"plot_aperture"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#dynamicapertureplotter","text":"class DynamicAperturePlotter ( / , * args , ** kwargs ) View Source class DynamicAperturePlotter : \"\"\" A class to plot the dynamic aperture of your machine. \"\"\" @staticmethod def plot_dynamic_aperture ( vx_coords : np . ndarray , vy_coords : np . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure : \"\"\" Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Args: vx_coords (np.ndarray): numpy array of horizontal coordinates over turns. vy_coords (np.ndarray): numpy array of vertical coordinates over turns. n_particles (int): number of particles simulated. savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" figure = plt . figure ( figsize = ( 12 , 7 )) turn_lost = [] x_in_lost = [] for particle in range ( n_particles ) : nb = len ( vx_coords [ particle ] ) - max ( np . isnan ( vx_coords [ particle ] ). sum (), np . isnan ( vy_coords [ particle ] ). sum () ) turn_lost . append ( nb ) x_in_lost . append ( vx_coords [ particle ][ 0 ] ** 2 + vy_coords [ particle ][ 0 ] ** 2 ) turn_lost = np . array ( turn_lost ) x_in_lost = np . array ( x_in_lost ) plt . scatter ( turn_lost , x_in_lost * 1000 , linewidths = 0.7 , c = \"darkblue\" , marker = \".\" ) plt . title ( \"Amplitudes lost over turns\" , fontsize = 20 ) plt . xlabel ( \"Number of Turns Survived\" , fontsize = 17 ) plt . ylabel ( \"Initial amplitude [mm]\" , fontsize = 17 ) if savefig : logger . info ( f \"Saving dynamic aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"DynamicAperturePlotter"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#plot_dynamic_aperture","text":"def plot_dynamic_aperture ( vx_coords : numpy . ndarray , vy_coords : numpy . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Parameters: Name Type Description Default vx_coords np.ndarray numpy array of horizontal coordinates over turns. None vy_coords np.ndarray numpy array of vertical coordinates over turns. None n_particles int number of particles simulated. None savefig str will save the figure if this is not None, using the string value passed. None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_dynamic_aperture ( vx_coords : np . ndarray , vy_coords : np . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure : \"\"\" Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Args: vx_coords (np.ndarray): numpy array of horizontal coordinates over turns. vy_coords (np.ndarray): numpy array of vertical coordinates over turns. n_particles (int): number of particles simulated. savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" figure = plt . figure ( figsize = ( 12 , 7 )) turn_lost = [] x_in_lost = [] for particle in range ( n_particles ) : nb = len ( vx_coords [ particle ] ) - max ( np . isnan ( vx_coords [ particle ] ). sum (), np . isnan ( vy_coords [ particle ] ). sum () ) turn_lost . append ( nb ) x_in_lost . append ( vx_coords [ particle ][ 0 ] ** 2 + vy_coords [ particle ][ 0 ] ** 2 ) turn_lost = np . array ( turn_lost ) x_in_lost = np . array ( x_in_lost ) plt . scatter ( turn_lost , x_in_lost * 1000 , linewidths = 0.7 , c = \"darkblue\" , marker = \".\" ) plt . title ( \"Amplitudes lost over turns\" , fontsize = 20 ) plt . xlabel ( \"Number of Turns Survived\" , fontsize = 17 ) plt . ylabel ( \"Initial amplitude [mm]\" , fontsize = 17 ) if savefig : logger . info ( f \"Saving dynamic aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"plot_dynamic_aperture"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#phasespaceplotter","text":"class PhaseSpacePlotter ( / , * args , ** kwargs ) View Source class PhaseSpacePlotter : \"\"\" A class to plot Courant-Snyder coordinates phase space. \"\"\" @staticmethod def plot_courant_snyder_phase_space ( madx : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) # Getting the P matrix to compute Courant - Snyder coordinates logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = madx . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . alfy [ 0 ] beta = madx . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . bety [ 0 ] logger . debug ( f \"Plotting Courant-Snyder phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = \"k\" ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure @staticmethod def plot_courant_snyder_phase_space_colored ( madx : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156th color. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) # Getting a sufficiently long array of colors to use colors = int ( np . floor ( len ( u_coordinates ) / 100 )) * SORTED_COLORS while len ( colors ) > len ( u_coordinates ) : colors . pop () figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = madx . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . alfy [ 0 ] beta = madx . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . bety [ 0 ] logger . debug ( f \"Plotting colored normalised phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = colors [ index ] ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving colored Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"PhaseSpacePlotter"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#plot_courant_snyder_phase_space","text":"def plot_courant_snyder_phase_space ( madx : cpymad . madx . Madx , u_coordinates : numpy . ndarray , pu_coordinates : numpy . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = 'Horizontal' ) -> matplotlib . figure . Figure Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None u_coordinates np.ndarray numpy array of particles's coordinates for the given plane. None pu_coordinates np.ndarray numpy array of particles's momentum coordinates for the given plane. None savefig str will save the figure if this is not None, using the string value passed. None size Tuple[int, int] the wanted matplotlib figure size. Defaults to (16, 8). (16, 8) plane str the physical plane to plot. Defaults to 'Horizontal'. 'Horizontal' Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_courant_snyder_phase_space ( madx : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) # Getting the P matrix to compute Courant - Snyder coordinates logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = madx . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . alfy [ 0 ] beta = madx . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . bety [ 0 ] logger . debug ( f \"Plotting Courant-Snyder phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = \"k\" ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"plot_courant_snyder_phase_space"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#plot_courant_snyder_phase_space_colored","text":"def plot_courant_snyder_phase_space_colored ( madx : cpymad . madx . Madx , u_coordinates : numpy . ndarray , pu_coordinates : numpy . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = 'Horizontal' ) -> matplotlib . figure . Figure Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156 th color. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None u_coordinates np.ndarray numpy array of particles's coordinates for the given plane. None pu_coordinates np.ndarray numpy array of particles's momentum coordinates for the given plane. None savefig str will save the figure if this is not None, using the string value passed. None size Tuple[int, int] the wanted matplotlib figure size. Defaults to (16, 8). (16, 8) plane str the physical plane to plot. Defaults to 'Horizontal'. 'Horizontal' Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_courant_snyder_phase_space_colored ( madx : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156th color. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) # Getting a sufficiently long array of colors to use colors = int ( np . floor ( len ( u_coordinates ) / 100 )) * SORTED_COLORS while len ( colors ) > len ( u_coordinates ) : colors . pop () figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = madx . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . alfy [ 0 ] beta = madx . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else madx . table . twiss . bety [ 0 ] logger . debug ( f \"Plotting colored normalised phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = colors [ index ] ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving colored Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"plot_courant_snyder_phase_space_colored"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#tunediagramplotter","text":"class TuneDiagramPlotter ( / , * args , ** kwargs ) View Source class TuneDiagramPlotter : \"\"\" A class to plot a blank tune diagram with Farey sequences, as well as your working points. \"\"\" @staticmethod def farey_sequence ( order : int ) -> list : \"\"\" Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Args: order (int): the order up to which we want to calculate the sequence. Returns: The sequence as a list. \"\"\" seq = [ [0, 1 ] ] a , b , c , d = 0 , 1 , 1 , order while c <= order : k = int (( order + b ) / d ) a , b , c , d = c , d , k * c - a , k * d - b seq . append ( [ a, b ] ) return seq @staticmethod def plot_blank_tune_diagram () -> matplotlib . figure . Figure : \"\"\" Plotting the tune diagram up to the 6th order. Original code from Rogelio Tom\u00e1s. Returns: The figure on which resonance lines from farey sequences are drawn. \"\"\" logger . debug ( \"Plotting resonance lines from Farey sequence, up to 5th order\" ) figure = plt . figure ( figsize = ( 13 , 13 )) plt . ylim (( 0 , 1 )) plt . xlim (( 0 , 1 )) x = np . linspace ( 0 , 1 , 1000 ) for i in range ( 1 , 6 ) : farey_sequences = TuneDiagramPlotter . farey_sequence ( i ) for f in farey_sequences : h , k = f # Node h / k on the axes for sequence in farey_sequences : p , q = sequence a = float ( k * p ) # Resonance linea Qx + b * Qy = clinkedtop / q if a > 0 : b = float ( q - k * p ) c = float ( p * h ) plt . plot ( x , c / a - x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( x , c / a + x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , 1 - x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , 1 - x , \"b\" , alpha = 0.1 ) if q == k and p == 1 : # FN elements below 1 / k break plt . title ( \"Tune Diagram\" , fontsize = 20 ) plt . axis ( \"square\" ) plt . xlim ( [ 0, 1 ] ) plt . ylim ( [ 0, 1 ] ) plt . xlabel ( \"$Q_{x}}$\" , fontsize = 17 ) plt . ylabel ( \"$Q_{y}$\" , fontsize = 17 ) return figure @staticmethod def plot_tune_diagram ( madx : Madx , v_qx : np . ndarray = np . array ( [ 0 ] ), vxgood : np . ndarray = np . array ( [ False ] ), v_qy : np . ndarray = np . array ( [ 0 ] ), vygood : np . ndarray = np . array ( [ False ] ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plots the evolution of particles' tunes on a Tune Diagram. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. v_qx (np.ndarray): horizontal tune value as a numpy array. vxgood (np.ndarray): ?? v_qy (np.ndarray): vertical tune value as a numpy array. vygood (np.ndarray): ?? savefig: will save the figure if this is not None, using the string value passed. Returns: The figure on which the diagram is drawn. \"\"\" figure = TuneDiagramPlotter . plot_blank_tune_diagram () logger . debug ( \"Getting Tunes from cpymad\" ) new_q1 : float = madx . table . summ . dframe (). q1 [ 0 ] new_q2 : float = madx . table . summ . dframe (). q2 [ 0 ] if vxgood . any () and vygood . any () : plt . plot ( v_qx [ vxgood * vygood ] , v_qy [ vxgood * vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif vxgood . any () and ~ vygood . any () : tp = np . ones ( len ( vxgood )) * ( new_q2 - np . floor ( new_q2 )) plt . plot ( v_qx [ vxgood ] , tp [ vxgood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif ~ vxgood . any () and vygood . any () : tp = np . ones ( len ( vygood )) * ( new_q1 - np . floor ( new_q1 )) plt . plot ( tp [ vygood ] , v_qy [ vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) if savefig : logger . info ( f \"Saving Tune diagram plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"TuneDiagramPlotter"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#farey_sequence","text":"def farey_sequence ( order : int ) -> list Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Parameters: Name Type Description Default order int the order up to which we want to calculate the sequence. None Returns: Type Description None The sequence as a list. View Source @staticmethod def farey_sequence ( order : int ) -> list : \"\"\" Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Args: order (int): the order up to which we want to calculate the sequence. Returns: The sequence as a list. \"\"\" seq = [ [0, 1 ] ] a , b , c , d = 0 , 1 , 1 , order while c <= order : k = int (( order + b ) / d ) a , b , c , d = c , d , k * c - a , k * d - b seq . append ( [ a, b ] ) return seq","title":"farey_sequence"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#plot_blank_tune_diagram","text":"def plot_blank_tune_diagram ( ) -> matplotlib . figure . Figure Plotting the tune diagram up to the 6 th order. Original code from Rogelio Tom\u00e1s. Returns: Type Description None The figure on which resonance lines from farey sequences are drawn. View Source @staticmethod def plot_blank_tune_diagram () -> matplotlib . figure . Figure : \"\"\" Plotting the tune diagram up to the 6th order. Original code from Rogelio Tom\u00e1s. Returns: The figure on which resonance lines from farey sequences are drawn. \"\"\" logger . debug ( \"Plotting resonance lines from Farey sequence, up to 5th order\" ) figure = plt . figure ( figsize = ( 13 , 13 )) plt . ylim (( 0 , 1 )) plt . xlim (( 0 , 1 )) x = np . linspace ( 0 , 1 , 1000 ) for i in range ( 1 , 6 ) : farey_sequences = TuneDiagramPlotter . farey_sequence ( i ) for f in farey_sequences : h , k = f # Node h / k on the axes for sequence in farey_sequences : p , q = sequence a = float ( k * p ) # Resonance linea Qx + b * Qy = clinkedtop / q if a > 0 : b = float ( q - k * p ) c = float ( p * h ) plt . plot ( x , c / a - x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( x , c / a + x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , 1 - x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , 1 - x , \"b\" , alpha = 0.1 ) if q == k and p == 1 : # FN elements below 1 / k break plt . title ( \"Tune Diagram\" , fontsize = 20 ) plt . axis ( \"square\" ) plt . xlim ( [ 0, 1 ] ) plt . ylim ( [ 0, 1 ] ) plt . xlabel ( \"$Q_{x}}$\" , fontsize = 17 ) plt . ylabel ( \"$Q_{y}$\" , fontsize = 17 ) return figure","title":"plot_blank_tune_diagram"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#plot_tune_diagram","text":"def plot_tune_diagram ( madx : cpymad . madx . Madx , v_qx : numpy . ndarray = array ([ 0 ]), vxgood : numpy . ndarray = array ([ False ]), v_qy : numpy . ndarray = array ([ 0 ]), vygood : numpy . ndarray = array ([ False ]), savefig : str = None ) -> matplotlib . figure . Figure Plots the evolution of particles' tunes on a Tune Diagram. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None v_qx np.ndarray horizontal tune value as a numpy array. None vxgood np.ndarray ?? None v_qy np.ndarray vertical tune value as a numpy array. None vygood np.ndarray ?? None savefig None will save the figure if this is not None, using the string value passed. None Returns: Type Description None The figure on which the diagram is drawn. View Source @staticmethod def plot_tune_diagram ( madx : Madx , v_qx : np . ndarray = np . array ( [ 0 ] ), vxgood : np . ndarray = np . array ( [ False ] ), v_qy : np . ndarray = np . array ( [ 0 ] ), vygood : np . ndarray = np . array ( [ False ] ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plots the evolution of particles' tunes on a Tune Diagram. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. v_qx (np.ndarray): horizontal tune value as a numpy array. vxgood (np.ndarray): ?? v_qy (np.ndarray): vertical tune value as a numpy array. vygood (np.ndarray): ?? savefig: will save the figure if this is not None, using the string value passed. Returns: The figure on which the diagram is drawn. \"\"\" figure = TuneDiagramPlotter . plot_blank_tune_diagram () logger . debug ( \"Getting Tunes from cpymad\" ) new_q1 : float = madx . table . summ . dframe (). q1 [ 0 ] new_q2 : float = madx . table . summ . dframe (). q2 [ 0 ] if vxgood . any () and vygood . any () : plt . plot ( v_qx [ vxgood * vygood ] , v_qy [ vxgood * vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif vxgood . any () and ~ vygood . any () : tp = np . ones ( len ( vxgood )) * ( new_q2 - np . floor ( new_q2 )) plt . plot ( v_qx [ vxgood ] , tp [ vxgood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif ~ vxgood . any () and vygood . any () : tp = np . ones ( len ( vygood )) * ( new_q1 - np . floor ( new_q1 )) plt . plot ( tp [ vygood ] , v_qy [ vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) if savefig : logger . info ( f \"Saving Tune diagram plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"plot_tune_diagram"},{"location":"reference/pyhdtoolkit/cpymadtools/ptc/","text":"Module pyhdtoolkit.cpymadtools.ptc Module cpymadtools.ptc Created on 2020.02.03 View Source \"\"\" Module cpymadtools.ptc ---------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to harness MAD-X PTC functionality with a cpymad.madx.Madx object. \"\"\" from pathlib import Path from typing import Dict , Sequence , Tuple , Union import pandas as pd import tfs from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.cpymadtools.utils import get_table_tfs def get_amplitude_detuning ( madx : Madx , order : int = 2 , file : Union [ Path , str ] = None , fringe : bool = False , ** kwargs ) -> tfs . TfsDataFrame : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD), but this has been heavily refactored. Calculate amplitude detuning via `PTC_NORMAL`, with sensible defaults set for other relevant `PTC` commands. The result table is returned as a `TfsDataFrame`, the headers of which are the contents of the internal `SUMM` table. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_NORMAL` command is explicitely given `icase=6` to enforce 6D calculations (see the `MAD-X` manual for details), `no=5` (map order for derivative evaluation of Twiss parameters) and `closedorbit=True` (triggers closed orbit calculation) and `normal=True` (activate calculation of the Normal Form). Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order coefficient (only 0, 1 or 2 implemented in `PTC`). Defaults to `2`. file (Union[Path, str]): path to output file. Default `None` fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. Keyword Args: Any keyword argument is transmitted to the `PTC_NORMAL` command. See above which arguments are already set for `PTC_NORMAL` to avoid trying to override them. Returns: A `TfsDataframe` with results. \"\"\" if order >= 3 : logger . error ( f \"Maximum amplitude detuning order in PTC is 2, but { order : d } was requested\" ) raise NotImplementedError ( \"PTC amplitude detuning is not implemented for order > 2\" ) logger . info ( \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . trace ( \"Selecting tune orders\" ) madx . select_ptc_normal ( q1 = \"0\" , q2 = \"0\" ) for ii in range ( 1 , order + 1 ): # These are d^iQ/ddp^i madx . select_ptc_normal ( dq1 = f \" { ii : d } \" , dq2 = f \" { ii : d } \" ) # ANH = anharmonicities (ex, ey, deltap), works only with parameters as full strings # could be done nicer with permutations ... logger . trace ( \"Selecting anharmonicities\" ) if order >= 1 : # madx.select_ptc_normal('anhx=0, 0, 1') # dQx/ddp # madx.select_ptc_normal('anhy=0, 0, 1') # dQy/ddp madx . select_ptc_normal ( \"anhx=1, 0, 0\" ) # dQx/dex madx . select_ptc_normal ( \"anhx=0, 1, 0\" ) # dQx/dey madx . select_ptc_normal ( \"anhy=1, 0, 0\" ) # dQy/dex madx . select_ptc_normal ( \"anhy=0, 1, 0\" ) # dQy/dey if order >= 2 : # madx.select_ptc_normal('anhx=0, 0, 2') # d^2Qx/ddp^2 # madx.select_ptc_normal('anhy=0, 0, 2') # d^2Qy/ddp^2 madx . select_ptc_normal ( \"anhx=2, 0, 0\" ) # d^2Qx/dex^2 madx . select_ptc_normal ( \"anhx=1, 1, 0\" ) # d^2Qx/dexdey madx . select_ptc_normal ( \"anhx=0, 2, 0\" ) # d^2Qx/dey^2 madx . select_ptc_normal ( \"anhy=2, 0, 0\" ) # d^2Qy/dex^2 madx . select_ptc_normal ( \"anhy=1, 1, 0\" ) # d^2Qy/dexdey madx . select_ptc_normal ( \"anhy=0, 2, 0\" ) # d^2Qy/dey^2 logger . debug ( \"Executing PTC Normal\" ) madx . ptc_normal ( icase = 6 , no = 5 , closed_orbit = True , normal = True , ** kwargs ) madx . ptc_end () dframe = get_table_tfs ( madx , table_name = \"normal_results\" ) dframe . index = range ( len ( dframe . NAME )) # table has a weird index if file : logger . debug ( f \"Exporting results to disk at ' { Path ( file ) . absolute () } '\" ) tfs . write ( file , dframe ) return dframe def get_rdts ( madx : Madx , order : int = 4 , file : Union [ Path , str ] = None , fringe : bool = False , ** kwargs ) -> tfs . TfsDataFrame : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD), but this has been heavily refactored. Calculate the `RDTs` via `PTC_TWISS`, with sensible defaults set for other relevant `PTC` commands. The result table is returned as a `TfsDataFrame`, the headers of which are the contents of the internal `SUMM` table. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_TWISS` command is explicitely given `icase=6` to enforce 6D calculations (see the `MAD-X` manual for details), `trackrdts=True` (for this function to fullfill its purpose) and `normal=True` to trigger saving the normal form analysis results in a table called `NONLIN` which will then be available through the provided `Madx` instance. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): map order for derivative evaluation of Twiss parameters. Defaults to `4`. file (Union[Path, str]): path to output file. Default `None` fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. Keyword Args: Any keyword argument is transmitted to the `PTC_TWISS` command. See above which arguments are already set for `PTC_TWISS` to avoid trying to override them. Returns: A `TfsDataframe` with results. \"\"\" logger . info ( \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . debug ( \"Executing PTC Twiss\" ) madx . ptc_twiss ( icase = 6 , no = order , normal = True , trackrdts = True , ** kwargs ) madx . ptc_end () dframe = get_table_tfs ( madx , table_name = \"twissrdt\" , headers_table = \"ptc_twiss_summary\" ) if file : logger . debug ( f \"Exporting results to disk at ' { Path ( file ) . absolute () } '\" ) tfs . write ( file , dframe ) return dframe def ptc_twiss ( madx : Madx , order : int = 4 , file : Union [ Path , str ] = None , fringe : bool = False , table : str = \"ptc_twiss\" , ** kwargs , ) -> tfs . TfsDataFrame : \"\"\" Calculate the TWISS parameters according to G. Ripken's formalism via `PTC_TWISS`, with sensible defaults set for other relevant `PTC` commands. The result table is returned as a `TfsDataFrame`, the headers of which are the contents of the internal `SUMM` table. This is very similar to the `get_rdts` function as both use `ptc_twiss` internally, however this function does not track RDTs which makes the calculations significantly faster. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_TWISS` command is explicitely given `icase=6` to enforce 6D calculations (see the `MAD-X` manual for details) `normal=True` to trigger saving the normal form analysis results in a table called `NONLIN` which will then be available through the provided `Madx` instance. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): map order for derivative evaluation of Twiss parameters. Defaults to `4`. file (Union[Path, str]): path to output file. Default `None` fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. table (str): the name of the internal table in which to save the results. Defaults to 'ptc_twiss'. Keyword Args: Any keyword argument is transmitted to the `PTC_TWISS` command. See above which arguments are already set for `PTC_TWISS` to avoid trying to override them. Returns: A `TfsDataframe` with results. \"\"\" logger . info ( f \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . debug ( \"Executing PTC Twiss\" ) madx . ptc_twiss ( icase = 6 , no = order , normal = True , table = table , ** kwargs ) madx . ptc_end () dframe = get_table_tfs ( madx , table_name = table , headers_table = \"ptc_twiss_summary\" ) if file : logger . debug ( f \"Exporting results to disk at ' { Path ( file ) . absolute () } '\" ) tfs . write ( file , dframe ) return dframe def ptc_track_particle ( madx : Madx , initial_coordinates : Tuple [ float , float , float , float , float , float ], nturns : int , sequence : str = None , observation_points : Sequence [ str ] = None , onetable : bool = False , fringe : bool = False , ** kwargs , ) -> Dict [ str , pd . DataFrame ]: \"\"\" Tracks a single particle for nturns through PTC_TRACK, based on its initial coordinates. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_TRACK` command is explicitely given `ELEMENT_BY_ELEMENT=True` to force element by element tracking mode. Args: madx (Madx): an instantiated cpymad.madx.Madx object. initial_coordinates (Tuple[float, float, float, float, float, float]): a tuple with the X, PX, Y, PY, T, PT starting coordinates the particle to track. Defaults to all 0 if none given. nturns (int): the number of turns to track for. sequence (str): the sequence to use for tracking. If no value is provided, it is assumed that a sequence is already defined and in use, and this one will be picked up by MAD-X. observation_points (Sequence[str]): sequence of all element names at which to OBSERVE during the tracking. onetable (bool): flag to combine all observation points data into a single table. Defaults to `False`. fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. Keyword Args: Any keyword argument to be given to the `PTC_TRACK` command, such as the `CLOSED_ORBIT` flag to activate closed orbit calculation before tracking. Refer to the `MAD-X` manual for options. Returns: A dictionary with a copy of the track table's dataframe for each defined observation point, with as columns the coordinates x, px, y, py, t, pt, s and e (energy). The keys of the dictionary are simply numbered 'observation_point_1', 'observation_point_2' etc. The first observation point always corresponds to the start of machine, the others correspond to the ones manually defined, in the order they are defined in. If the user has set `onetable` to `True`, only one entry is in the dictionary under the key 'trackone' and it has the combined table as a pandas DataFrame for value. \"\"\" start = initial_coordinates if initial_coordinates else [ 0 , 0 , 0 , 0 , 0 , 0 ] observation_points = observation_points if observation_points else [] if isinstance ( sequence , str ): logger . debug ( f \"Using sequence ' { sequence } ' for tracking\" ) madx . use ( sequence = sequence ) logger . info ( f \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . debug ( f \"Tracking coordinates with initial X, PX, Y, PY, T, PT of ' { initial_coordinates } '\" ) madx . command . ptc_start ( X = start [ 0 ], PX = start [ 1 ], Y = start [ 2 ], PY = start [ 3 ], T = start [ 4 ], PT = start [ 5 ], ) for element in observation_points : logger . trace ( f \"Setting observation point for tracking with OBSERVE at element ' { element } '\" ) madx . command . ptc_observe ( place = element ) madx . command . ptc_track ( turns = nturns , element_by_element = True , onetable = onetable , ** kwargs ) madx . ptc_end () if onetable : # user asked for ONETABLE, there will only be one table 'trackone' given back by MAD-X logger . debug ( \"Because of option ONETABLE only one table 'TRACKONE' exists to be returned.\" ) return { \"trackone\" : madx . table . trackone . dframe () . copy ()} return { f \"observation_point_ { point : d } \" : madx . table [ f \"track.obs { point : 04d } .p0001\" ] . dframe () . copy () for point in range ( 1 , len ( observation_points ) + 2 ) # len(observation_points) + 1 for start of # machine + 1 because MAD-X starts indexing these at 1 } Functions get_amplitude_detuning def get_amplitude_detuning ( madx : cpymad . madx . Madx , order : int = 2 , file : Union [ pathlib . Path , str ] = None , fringe : bool = False , ** kwargs ) -> tfs . handler . TfsDataFrame INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD), but this has been heavily refactored. Calculate amplitude detuning via PTC_NORMAL , with sensible defaults set for other relevant PTC commands. The result table is returned as a TfsDataFrame , the headers of which are the contents of the internal SUMM table. The PTC_CREATE_LAYOUT command is issued with model=3 ( SixTrack model), method=4 (integration order), nst=3 (number of integratioin steps, aka body slices for elements) and exact=True (use exact Hamiltonian, not an approximated one). The PTC_NORMAL command is explicitely given icase=6 to enforce 6D calculations (see the MAD-X manual for details), no=5 (map order for derivative evaluation of Twiss parameters) and closedorbit=True (triggers closed orbit calculation) and normal=True (activate calculation of the Normal Form). Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order coefficient (only 0, 1 or 2 implemented in PTC ). Defaults to 2 . file (Union[Path, str]): path to output file. Default None fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to False . Keyword Args: Any keyword argument is transmitted to the PTC_NORMAL command. See above which arguments are already set for PTC_NORMAL to avoid trying to override them. Returns: A TfsDataframe with results. View Source def get_amplitude_detuning ( madx : Madx , order : int = 2 , file : Union [ Path , str ] = None , fringe : bool = False , ** kwargs ) -> tfs . TfsDataFrame : \" \"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD), but this has been heavily refactored. Calculate amplitude detuning via `PTC_NORMAL`, with sensible defaults set for other relevant `PTC` commands. The result table is returned as a `TfsDataFrame`, the headers of which are the contents of the internal `SUMM` table. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_NORMAL` command is explicitely given `icase=6` to enforce 6D calculations (see the `MAD-X` manual for details), `no=5` (map order for derivative evaluation of Twiss parameters) and `closedorbit=True` (triggers closed orbit calculation) and `normal=True` (activate calculation of the Normal Form). Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order coefficient (only 0, 1 or 2 implemented in `PTC`). Defaults to `2`. file (Union[Path, str]): path to output file. Default `None` fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. Keyword Args: Any keyword argument is transmitted to the `PTC_NORMAL` command. See above which arguments are already set for `PTC_NORMAL` to avoid trying to override them. Returns: A `TfsDataframe` with results. \"\" \" if order >= 3 : logger . error ( f \"Maximum amplitude detuning order in PTC is 2, but {order:d} was requested\" ) raise NotImplementedError ( \"PTC amplitude detuning is not implemented for order > 2\" ) logger . info ( \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . trace ( \"Selecting tune orders\" ) madx . select_ptc_normal ( q1 = \"0\" , q2 = \"0\" ) for ii in range ( 1 , order + 1 ) : # These are d^iQ/ddp^i madx . select_ptc_normal ( dq1 = f \"{ii:d}\" , dq2 = f \"{ii:d}\" ) # ANH = anharmonicities (ex, ey, deltap), works only with parameters as full strings # could be done nicer with permutations ... logger . trace ( \"Selecting anharmonicities\" ) if order >= 1 : # madx.select_ptc_normal('anhx=0, 0, 1') # dQx/ddp # madx.select_ptc_normal('anhy=0, 0, 1') # dQy/ddp madx . select_ptc_normal ( \"anhx=1, 0, 0\" ) # dQx/dex madx . select_ptc_normal ( \"anhx=0, 1, 0\" ) # dQx/dey madx . select_ptc_normal ( \"anhy=1, 0, 0\" ) # dQy/dex madx . select_ptc_normal ( \"anhy=0, 1, 0\" ) # dQy/dey if order >= 2 : # madx.select_ptc_normal('anhx=0, 0, 2') # d^2Qx/ddp^2 # madx.select_ptc_normal('anhy=0, 0, 2') # d^2Qy/ddp^2 madx . select_ptc_normal ( \"anhx=2, 0, 0\" ) # d^2Qx/dex^2 madx . select_ptc_normal ( \"anhx=1, 1, 0\" ) # d^2Qx/dexdey madx . select_ptc_normal ( \"anhx=0, 2, 0\" ) # d^2Qx/dey^2 madx . select_ptc_normal ( \"anhy=2, 0, 0\" ) # d^2Qy/dex^2 madx . select_ptc_normal ( \"anhy=1, 1, 0\" ) # d^2Qy/dexdey madx . select_ptc_normal ( \"anhy=0, 2, 0\" ) # d^2Qy/dey^2 logger . debug ( \"Executing PTC Normal\" ) madx . ptc_normal ( icase = 6 , no = 5 , closed_orbit = True , normal = True , ** kwargs ) madx . ptc_end () dframe = get_table_tfs ( madx , table_name = \"normal_results\" ) dframe . index = range ( len ( dframe . NAME )) # table has a weird index if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe get_rdts def get_rdts ( madx : cpymad . madx . Madx , order : int = 4 , file : Union [ pathlib . Path , str ] = None , fringe : bool = False , ** kwargs ) -> tfs . handler . TfsDataFrame INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD), but this has been heavily refactored. Calculate the RDTs via PTC_TWISS , with sensible defaults set for other relevant PTC commands. The result table is returned as a TfsDataFrame , the headers of which are the contents of the internal SUMM table. The PTC_CREATE_LAYOUT command is issued with model=3 ( SixTrack model), method=4 (integration order), nst=3 (number of integratioin steps, aka body slices for elements) and exact=True (use exact Hamiltonian, not an approximated one). The PTC_TWISS command is explicitely given icase=6 to enforce 6D calculations (see the MAD-X manual for details), trackrdts=True (for this function to fullfill its purpose) and normal=True to trigger saving the normal form analysis results in a table called NONLIN which will then be available through the provided Madx instance. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): map order for derivative evaluation of Twiss parameters. Defaults to 4 . file (Union[Path, str]): path to output file. Default None fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to False . Keyword Args: Any keyword argument is transmitted to the PTC_TWISS command. See above which arguments are already set for PTC_TWISS to avoid trying to override them. Returns: A TfsDataframe with results. View Source def get_rdts ( madx : Madx , order : int = 4 , file : Union [ Path , str ] = None , fringe : bool = False , ** kwargs ) -> tfs . TfsDataFrame : \" \"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD), but this has been heavily refactored. Calculate the `RDTs` via `PTC_TWISS`, with sensible defaults set for other relevant `PTC` commands. The result table is returned as a `TfsDataFrame`, the headers of which are the contents of the internal `SUMM` table. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_TWISS` command is explicitely given `icase=6` to enforce 6D calculations (see the `MAD-X` manual for details), `trackrdts=True` (for this function to fullfill its purpose) and `normal=True` to trigger saving the normal form analysis results in a table called `NONLIN` which will then be available through the provided `Madx` instance. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): map order for derivative evaluation of Twiss parameters. Defaults to `4`. file (Union[Path, str]): path to output file. Default `None` fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. Keyword Args: Any keyword argument is transmitted to the `PTC_TWISS` command. See above which arguments are already set for `PTC_TWISS` to avoid trying to override them. Returns: A `TfsDataframe` with results. \"\" \" logger . info ( \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . debug ( \"Executing PTC Twiss\" ) madx . ptc_twiss ( icase = 6 , no = order , normal = True , trackrdts = True , ** kwargs ) madx . ptc_end () dframe = get_table_tfs ( madx , table_name = \"twissrdt\" , headers_table = \"ptc_twiss_summary\" ) if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe ptc_track_particle def ptc_track_particle ( madx : cpymad . madx . Madx , initial_coordinates : Tuple [ float , float , float , float , float , float ], nturns : int , sequence : str = None , observation_points : Sequence [ str ] = None , onetable : bool = False , fringe : bool = False , ** kwargs ) -> Dict [ str , pandas . core . frame . DataFrame ] Tracks a single particle for nturns through PTC_TRACK, based on its initial coordinates. The PTC_CREATE_LAYOUT command is issued with model=3 ( SixTrack model), method=4 (integration order), nst=3 (number of integratioin steps, aka body slices for elements) and exact=True (use exact Hamiltonian, not an approximated one). The PTC_TRACK command is explicitely given ELEMENT_BY_ELEMENT=True to force element by element tracking mode. Args: madx (Madx): an instantiated cpymad.madx.Madx object. initial_coordinates (Tuple[float, float, float, float, float, float]): a tuple with the X, PX, Y, PY, T, PT starting coordinates the particle to track. Defaults to all 0 if none given. nturns (int): the number of turns to track for. sequence (str): the sequence to use for tracking. If no value is provided, it is assumed that a sequence is already defined and in use, and this one will be picked up by MAD-X. observation_points (Sequence[str]): sequence of all element names at which to OBSERVE during the tracking. onetable (bool): flag to combine all observation points data into a single table. Defaults to False . fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to False . Keyword Args: Any keyword argument to be given to the PTC_TRACK command, such as the CLOSED_ORBIT flag to activate closed orbit calculation before tracking. Refer to the MAD-X manual for options. Returns: A dictionary with a copy of the track table's dataframe for each defined observation point, with as columns the coordinates x, px, y, py, t, pt, s and e (energy). The keys of the dictionary are simply numbered 'observation_point_1', 'observation_point_2' etc. The first observation point always corresponds to the start of machine, the others correspond to the ones manually defined, in the order they are defined in. If the user has set `onetable` to `True` , only one entry is in the dictionary under the key 'trackone' and it has the combined table as a pandas DataFrame for value . View Source def ptc_track_particle ( madx : Madx , initial_coordinates : Tuple [ float , float , float , float , float , float ] , nturns : int , sequence : str = None , observation_points : Sequence [ str ] = None , onetable : bool = False , fringe : bool = False , ** kwargs , ) -> Dict [ str , pd . DataFrame ] : \" \"\" Tracks a single particle for nturns through PTC_TRACK, based on its initial coordinates. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_TRACK` command is explicitely given `ELEMENT_BY_ELEMENT=True` to force element by element tracking mode. Args: madx (Madx): an instantiated cpymad.madx.Madx object. initial_coordinates (Tuple[float, float, float, float, float, float]): a tuple with the X, PX, Y, PY, T, PT starting coordinates the particle to track. Defaults to all 0 if none given. nturns (int): the number of turns to track for. sequence (str): the sequence to use for tracking. If no value is provided, it is assumed that a sequence is already defined and in use, and this one will be picked up by MAD-X. observation_points (Sequence[str]): sequence of all element names at which to OBSERVE during the tracking. onetable (bool): flag to combine all observation points data into a single table. Defaults to `False`. fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. Keyword Args: Any keyword argument to be given to the `PTC_TRACK` command, such as the `CLOSED_ORBIT` flag to activate closed orbit calculation before tracking. Refer to the `MAD-X` manual for options. Returns: A dictionary with a copy of the track table's dataframe for each defined observation point, with as columns the coordinates x, px, y, py, t, pt, s and e (energy). The keys of the dictionary are simply numbered 'observation_point_1', 'observation_point_2' etc. The first observation point always corresponds to the start of machine, the others correspond to the ones manually defined, in the order they are defined in. If the user has set `onetable` to `True`, only one entry is in the dictionary under the key 'trackone' and it has the combined table as a pandas DataFrame for value. \"\" \" start = initial_coordinates if initial_coordinates else [ 0 , 0 , 0 , 0 , 0 , 0 ] observation_points = observation_points if observation_points else [] if isinstance ( sequence , str ) : logger . debug ( f \"Using sequence '{sequence}' for tracking\" ) madx . use ( sequence = sequence ) logger . info ( f \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . debug ( f \"Tracking coordinates with initial X, PX, Y, PY, T, PT of '{initial_coordinates}'\" ) madx . command . ptc_start ( X = start [ 0 ] , PX = start [ 1 ] , Y = start [ 2 ] , PY = start [ 3 ] , T = start [ 4 ] , PT = start [ 5 ] , ) for element in observation_points : logger . trace ( f \"Setting observation point for tracking with OBSERVE at element '{element}'\" ) madx . command . ptc_observe ( place = element ) madx . command . ptc_track ( turns = nturns , element_by_element = True , onetable = onetable , ** kwargs ) madx . ptc_end () if onetable : # user asked for ONETABLE, there will only be one table 'trackone' given back by MAD-X logger . debug ( \"Because of option ONETABLE only one table 'TRACKONE' exists to be returned.\" ) return { \"trackone\" : madx . table . trackone . dframe (). copy () } return { f \"observation_point_{point:d}\" : madx . table [ f \"track.obs{point:04d}.p0001\" ] . dframe (). copy () for point in range ( 1 , len ( observation_points ) + 2 ) # len(observation_points) + 1 for start of # machine + 1 because MAD-X starts indexing these at 1 } ptc_twiss def ptc_twiss ( madx : cpymad . madx . Madx , order : int = 4 , file : Union [ pathlib . Path , str ] = None , fringe : bool = False , table : str = 'ptc_twiss' , ** kwargs ) -> tfs . handler . TfsDataFrame Calculate the TWISS parameters according to G. Ripken's formalism via PTC_TWISS , with sensible defaults set for other relevant PTC commands. The result table is returned as a TfsDataFrame , the headers of which are the contents of the internal SUMM table. This is very similar to the get_rdts function as both use ptc_twiss internally, however this function does not track RDTs which makes the calculations significantly faster. The PTC_CREATE_LAYOUT command is issued with model=3 ( SixTrack model), method=4 (integration order), nst=3 (number of integratioin steps, aka body slices for elements) and exact=True (use exact Hamiltonian, not an approximated one). The PTC_TWISS command is explicitely given icase=6 to enforce 6D calculations (see the MAD-X manual for details) normal=True to trigger saving the normal form analysis results in a table called NONLIN which will then be available through the provided Madx instance. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): map order for derivative evaluation of Twiss parameters. Defaults to 4 . file (Union[Path, str]): path to output file. Default None fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to False . table (str): the name of the internal table in which to save the results. Defaults to 'ptc_twiss'. Keyword Args: Any keyword argument is transmitted to the PTC_TWISS command. See above which arguments are already set for PTC_TWISS to avoid trying to override them. Returns: A TfsDataframe with results. View Source def ptc_twiss ( madx : Madx , order : int = 4 , file : Union [ Path , str ] = None , fringe : bool = False , table : str = \"ptc_twiss\" , ** kwargs , ) -> tfs . TfsDataFrame : \" \"\" Calculate the TWISS parameters according to G. Ripken's formalism via `PTC_TWISS`, with sensible defaults set for other relevant `PTC` commands. The result table is returned as a `TfsDataFrame`, the headers of which are the contents of the internal `SUMM` table. This is very similar to the `get_rdts` function as both use `ptc_twiss` internally, however this function does not track RDTs which makes the calculations significantly faster. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_TWISS` command is explicitely given `icase=6` to enforce 6D calculations (see the `MAD-X` manual for details) `normal=True` to trigger saving the normal form analysis results in a table called `NONLIN` which will then be available through the provided `Madx` instance. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): map order for derivative evaluation of Twiss parameters. Defaults to `4`. file (Union[Path, str]): path to output file. Default `None` fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. table (str): the name of the internal table in which to save the results. Defaults to 'ptc_twiss'. Keyword Args: Any keyword argument is transmitted to the `PTC_TWISS` command. See above which arguments are already set for `PTC_TWISS` to avoid trying to override them. Returns: A `TfsDataframe` with results. \"\" \" logger . info ( f \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . debug ( \"Executing PTC Twiss\" ) madx . ptc_twiss ( icase = 6 , no = order , normal = True , table = table , ** kwargs ) madx . ptc_end () dframe = get_table_tfs ( madx , table_name = table , headers_table = \"ptc_twiss_summary\" ) if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe","title":"Ptc"},{"location":"reference/pyhdtoolkit/cpymadtools/ptc/#module-pyhdtoolkitcpymadtoolsptc","text":"Module cpymadtools.ptc Created on 2020.02.03 View Source \"\"\" Module cpymadtools.ptc ---------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to harness MAD-X PTC functionality with a cpymad.madx.Madx object. \"\"\" from pathlib import Path from typing import Dict , Sequence , Tuple , Union import pandas as pd import tfs from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.cpymadtools.utils import get_table_tfs def get_amplitude_detuning ( madx : Madx , order : int = 2 , file : Union [ Path , str ] = None , fringe : bool = False , ** kwargs ) -> tfs . TfsDataFrame : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD), but this has been heavily refactored. Calculate amplitude detuning via `PTC_NORMAL`, with sensible defaults set for other relevant `PTC` commands. The result table is returned as a `TfsDataFrame`, the headers of which are the contents of the internal `SUMM` table. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_NORMAL` command is explicitely given `icase=6` to enforce 6D calculations (see the `MAD-X` manual for details), `no=5` (map order for derivative evaluation of Twiss parameters) and `closedorbit=True` (triggers closed orbit calculation) and `normal=True` (activate calculation of the Normal Form). Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order coefficient (only 0, 1 or 2 implemented in `PTC`). Defaults to `2`. file (Union[Path, str]): path to output file. Default `None` fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. Keyword Args: Any keyword argument is transmitted to the `PTC_NORMAL` command. See above which arguments are already set for `PTC_NORMAL` to avoid trying to override them. Returns: A `TfsDataframe` with results. \"\"\" if order >= 3 : logger . error ( f \"Maximum amplitude detuning order in PTC is 2, but { order : d } was requested\" ) raise NotImplementedError ( \"PTC amplitude detuning is not implemented for order > 2\" ) logger . info ( \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . trace ( \"Selecting tune orders\" ) madx . select_ptc_normal ( q1 = \"0\" , q2 = \"0\" ) for ii in range ( 1 , order + 1 ): # These are d^iQ/ddp^i madx . select_ptc_normal ( dq1 = f \" { ii : d } \" , dq2 = f \" { ii : d } \" ) # ANH = anharmonicities (ex, ey, deltap), works only with parameters as full strings # could be done nicer with permutations ... logger . trace ( \"Selecting anharmonicities\" ) if order >= 1 : # madx.select_ptc_normal('anhx=0, 0, 1') # dQx/ddp # madx.select_ptc_normal('anhy=0, 0, 1') # dQy/ddp madx . select_ptc_normal ( \"anhx=1, 0, 0\" ) # dQx/dex madx . select_ptc_normal ( \"anhx=0, 1, 0\" ) # dQx/dey madx . select_ptc_normal ( \"anhy=1, 0, 0\" ) # dQy/dex madx . select_ptc_normal ( \"anhy=0, 1, 0\" ) # dQy/dey if order >= 2 : # madx.select_ptc_normal('anhx=0, 0, 2') # d^2Qx/ddp^2 # madx.select_ptc_normal('anhy=0, 0, 2') # d^2Qy/ddp^2 madx . select_ptc_normal ( \"anhx=2, 0, 0\" ) # d^2Qx/dex^2 madx . select_ptc_normal ( \"anhx=1, 1, 0\" ) # d^2Qx/dexdey madx . select_ptc_normal ( \"anhx=0, 2, 0\" ) # d^2Qx/dey^2 madx . select_ptc_normal ( \"anhy=2, 0, 0\" ) # d^2Qy/dex^2 madx . select_ptc_normal ( \"anhy=1, 1, 0\" ) # d^2Qy/dexdey madx . select_ptc_normal ( \"anhy=0, 2, 0\" ) # d^2Qy/dey^2 logger . debug ( \"Executing PTC Normal\" ) madx . ptc_normal ( icase = 6 , no = 5 , closed_orbit = True , normal = True , ** kwargs ) madx . ptc_end () dframe = get_table_tfs ( madx , table_name = \"normal_results\" ) dframe . index = range ( len ( dframe . NAME )) # table has a weird index if file : logger . debug ( f \"Exporting results to disk at ' { Path ( file ) . absolute () } '\" ) tfs . write ( file , dframe ) return dframe def get_rdts ( madx : Madx , order : int = 4 , file : Union [ Path , str ] = None , fringe : bool = False , ** kwargs ) -> tfs . TfsDataFrame : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD), but this has been heavily refactored. Calculate the `RDTs` via `PTC_TWISS`, with sensible defaults set for other relevant `PTC` commands. The result table is returned as a `TfsDataFrame`, the headers of which are the contents of the internal `SUMM` table. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_TWISS` command is explicitely given `icase=6` to enforce 6D calculations (see the `MAD-X` manual for details), `trackrdts=True` (for this function to fullfill its purpose) and `normal=True` to trigger saving the normal form analysis results in a table called `NONLIN` which will then be available through the provided `Madx` instance. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): map order for derivative evaluation of Twiss parameters. Defaults to `4`. file (Union[Path, str]): path to output file. Default `None` fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. Keyword Args: Any keyword argument is transmitted to the `PTC_TWISS` command. See above which arguments are already set for `PTC_TWISS` to avoid trying to override them. Returns: A `TfsDataframe` with results. \"\"\" logger . info ( \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . debug ( \"Executing PTC Twiss\" ) madx . ptc_twiss ( icase = 6 , no = order , normal = True , trackrdts = True , ** kwargs ) madx . ptc_end () dframe = get_table_tfs ( madx , table_name = \"twissrdt\" , headers_table = \"ptc_twiss_summary\" ) if file : logger . debug ( f \"Exporting results to disk at ' { Path ( file ) . absolute () } '\" ) tfs . write ( file , dframe ) return dframe def ptc_twiss ( madx : Madx , order : int = 4 , file : Union [ Path , str ] = None , fringe : bool = False , table : str = \"ptc_twiss\" , ** kwargs , ) -> tfs . TfsDataFrame : \"\"\" Calculate the TWISS parameters according to G. Ripken's formalism via `PTC_TWISS`, with sensible defaults set for other relevant `PTC` commands. The result table is returned as a `TfsDataFrame`, the headers of which are the contents of the internal `SUMM` table. This is very similar to the `get_rdts` function as both use `ptc_twiss` internally, however this function does not track RDTs which makes the calculations significantly faster. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_TWISS` command is explicitely given `icase=6` to enforce 6D calculations (see the `MAD-X` manual for details) `normal=True` to trigger saving the normal form analysis results in a table called `NONLIN` which will then be available through the provided `Madx` instance. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): map order for derivative evaluation of Twiss parameters. Defaults to `4`. file (Union[Path, str]): path to output file. Default `None` fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. table (str): the name of the internal table in which to save the results. Defaults to 'ptc_twiss'. Keyword Args: Any keyword argument is transmitted to the `PTC_TWISS` command. See above which arguments are already set for `PTC_TWISS` to avoid trying to override them. Returns: A `TfsDataframe` with results. \"\"\" logger . info ( f \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . debug ( \"Executing PTC Twiss\" ) madx . ptc_twiss ( icase = 6 , no = order , normal = True , table = table , ** kwargs ) madx . ptc_end () dframe = get_table_tfs ( madx , table_name = table , headers_table = \"ptc_twiss_summary\" ) if file : logger . debug ( f \"Exporting results to disk at ' { Path ( file ) . absolute () } '\" ) tfs . write ( file , dframe ) return dframe def ptc_track_particle ( madx : Madx , initial_coordinates : Tuple [ float , float , float , float , float , float ], nturns : int , sequence : str = None , observation_points : Sequence [ str ] = None , onetable : bool = False , fringe : bool = False , ** kwargs , ) -> Dict [ str , pd . DataFrame ]: \"\"\" Tracks a single particle for nturns through PTC_TRACK, based on its initial coordinates. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_TRACK` command is explicitely given `ELEMENT_BY_ELEMENT=True` to force element by element tracking mode. Args: madx (Madx): an instantiated cpymad.madx.Madx object. initial_coordinates (Tuple[float, float, float, float, float, float]): a tuple with the X, PX, Y, PY, T, PT starting coordinates the particle to track. Defaults to all 0 if none given. nturns (int): the number of turns to track for. sequence (str): the sequence to use for tracking. If no value is provided, it is assumed that a sequence is already defined and in use, and this one will be picked up by MAD-X. observation_points (Sequence[str]): sequence of all element names at which to OBSERVE during the tracking. onetable (bool): flag to combine all observation points data into a single table. Defaults to `False`. fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. Keyword Args: Any keyword argument to be given to the `PTC_TRACK` command, such as the `CLOSED_ORBIT` flag to activate closed orbit calculation before tracking. Refer to the `MAD-X` manual for options. Returns: A dictionary with a copy of the track table's dataframe for each defined observation point, with as columns the coordinates x, px, y, py, t, pt, s and e (energy). The keys of the dictionary are simply numbered 'observation_point_1', 'observation_point_2' etc. The first observation point always corresponds to the start of machine, the others correspond to the ones manually defined, in the order they are defined in. If the user has set `onetable` to `True`, only one entry is in the dictionary under the key 'trackone' and it has the combined table as a pandas DataFrame for value. \"\"\" start = initial_coordinates if initial_coordinates else [ 0 , 0 , 0 , 0 , 0 , 0 ] observation_points = observation_points if observation_points else [] if isinstance ( sequence , str ): logger . debug ( f \"Using sequence ' { sequence } ' for tracking\" ) madx . use ( sequence = sequence ) logger . info ( f \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . debug ( f \"Tracking coordinates with initial X, PX, Y, PY, T, PT of ' { initial_coordinates } '\" ) madx . command . ptc_start ( X = start [ 0 ], PX = start [ 1 ], Y = start [ 2 ], PY = start [ 3 ], T = start [ 4 ], PT = start [ 5 ], ) for element in observation_points : logger . trace ( f \"Setting observation point for tracking with OBSERVE at element ' { element } '\" ) madx . command . ptc_observe ( place = element ) madx . command . ptc_track ( turns = nturns , element_by_element = True , onetable = onetable , ** kwargs ) madx . ptc_end () if onetable : # user asked for ONETABLE, there will only be one table 'trackone' given back by MAD-X logger . debug ( \"Because of option ONETABLE only one table 'TRACKONE' exists to be returned.\" ) return { \"trackone\" : madx . table . trackone . dframe () . copy ()} return { f \"observation_point_ { point : d } \" : madx . table [ f \"track.obs { point : 04d } .p0001\" ] . dframe () . copy () for point in range ( 1 , len ( observation_points ) + 2 ) # len(observation_points) + 1 for start of # machine + 1 because MAD-X starts indexing these at 1 }","title":"Module pyhdtoolkit.cpymadtools.ptc"},{"location":"reference/pyhdtoolkit/cpymadtools/ptc/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/ptc/#get_amplitude_detuning","text":"def get_amplitude_detuning ( madx : cpymad . madx . Madx , order : int = 2 , file : Union [ pathlib . Path , str ] = None , fringe : bool = False , ** kwargs ) -> tfs . handler . TfsDataFrame INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD), but this has been heavily refactored. Calculate amplitude detuning via PTC_NORMAL , with sensible defaults set for other relevant PTC commands. The result table is returned as a TfsDataFrame , the headers of which are the contents of the internal SUMM table. The PTC_CREATE_LAYOUT command is issued with model=3 ( SixTrack model), method=4 (integration order), nst=3 (number of integratioin steps, aka body slices for elements) and exact=True (use exact Hamiltonian, not an approximated one). The PTC_NORMAL command is explicitely given icase=6 to enforce 6D calculations (see the MAD-X manual for details), no=5 (map order for derivative evaluation of Twiss parameters) and closedorbit=True (triggers closed orbit calculation) and normal=True (activate calculation of the Normal Form). Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order coefficient (only 0, 1 or 2 implemented in PTC ). Defaults to 2 . file (Union[Path, str]): path to output file. Default None fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to False . Keyword Args: Any keyword argument is transmitted to the PTC_NORMAL command. See above which arguments are already set for PTC_NORMAL to avoid trying to override them. Returns: A TfsDataframe with results. View Source def get_amplitude_detuning ( madx : Madx , order : int = 2 , file : Union [ Path , str ] = None , fringe : bool = False , ** kwargs ) -> tfs . TfsDataFrame : \" \"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD), but this has been heavily refactored. Calculate amplitude detuning via `PTC_NORMAL`, with sensible defaults set for other relevant `PTC` commands. The result table is returned as a `TfsDataFrame`, the headers of which are the contents of the internal `SUMM` table. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_NORMAL` command is explicitely given `icase=6` to enforce 6D calculations (see the `MAD-X` manual for details), `no=5` (map order for derivative evaluation of Twiss parameters) and `closedorbit=True` (triggers closed orbit calculation) and `normal=True` (activate calculation of the Normal Form). Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order coefficient (only 0, 1 or 2 implemented in `PTC`). Defaults to `2`. file (Union[Path, str]): path to output file. Default `None` fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. Keyword Args: Any keyword argument is transmitted to the `PTC_NORMAL` command. See above which arguments are already set for `PTC_NORMAL` to avoid trying to override them. Returns: A `TfsDataframe` with results. \"\" \" if order >= 3 : logger . error ( f \"Maximum amplitude detuning order in PTC is 2, but {order:d} was requested\" ) raise NotImplementedError ( \"PTC amplitude detuning is not implemented for order > 2\" ) logger . info ( \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . trace ( \"Selecting tune orders\" ) madx . select_ptc_normal ( q1 = \"0\" , q2 = \"0\" ) for ii in range ( 1 , order + 1 ) : # These are d^iQ/ddp^i madx . select_ptc_normal ( dq1 = f \"{ii:d}\" , dq2 = f \"{ii:d}\" ) # ANH = anharmonicities (ex, ey, deltap), works only with parameters as full strings # could be done nicer with permutations ... logger . trace ( \"Selecting anharmonicities\" ) if order >= 1 : # madx.select_ptc_normal('anhx=0, 0, 1') # dQx/ddp # madx.select_ptc_normal('anhy=0, 0, 1') # dQy/ddp madx . select_ptc_normal ( \"anhx=1, 0, 0\" ) # dQx/dex madx . select_ptc_normal ( \"anhx=0, 1, 0\" ) # dQx/dey madx . select_ptc_normal ( \"anhy=1, 0, 0\" ) # dQy/dex madx . select_ptc_normal ( \"anhy=0, 1, 0\" ) # dQy/dey if order >= 2 : # madx.select_ptc_normal('anhx=0, 0, 2') # d^2Qx/ddp^2 # madx.select_ptc_normal('anhy=0, 0, 2') # d^2Qy/ddp^2 madx . select_ptc_normal ( \"anhx=2, 0, 0\" ) # d^2Qx/dex^2 madx . select_ptc_normal ( \"anhx=1, 1, 0\" ) # d^2Qx/dexdey madx . select_ptc_normal ( \"anhx=0, 2, 0\" ) # d^2Qx/dey^2 madx . select_ptc_normal ( \"anhy=2, 0, 0\" ) # d^2Qy/dex^2 madx . select_ptc_normal ( \"anhy=1, 1, 0\" ) # d^2Qy/dexdey madx . select_ptc_normal ( \"anhy=0, 2, 0\" ) # d^2Qy/dey^2 logger . debug ( \"Executing PTC Normal\" ) madx . ptc_normal ( icase = 6 , no = 5 , closed_orbit = True , normal = True , ** kwargs ) madx . ptc_end () dframe = get_table_tfs ( madx , table_name = \"normal_results\" ) dframe . index = range ( len ( dframe . NAME )) # table has a weird index if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe","title":"get_amplitude_detuning"},{"location":"reference/pyhdtoolkit/cpymadtools/ptc/#get_rdts","text":"def get_rdts ( madx : cpymad . madx . Madx , order : int = 4 , file : Union [ pathlib . Path , str ] = None , fringe : bool = False , ** kwargs ) -> tfs . handler . TfsDataFrame INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD), but this has been heavily refactored. Calculate the RDTs via PTC_TWISS , with sensible defaults set for other relevant PTC commands. The result table is returned as a TfsDataFrame , the headers of which are the contents of the internal SUMM table. The PTC_CREATE_LAYOUT command is issued with model=3 ( SixTrack model), method=4 (integration order), nst=3 (number of integratioin steps, aka body slices for elements) and exact=True (use exact Hamiltonian, not an approximated one). The PTC_TWISS command is explicitely given icase=6 to enforce 6D calculations (see the MAD-X manual for details), trackrdts=True (for this function to fullfill its purpose) and normal=True to trigger saving the normal form analysis results in a table called NONLIN which will then be available through the provided Madx instance. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): map order for derivative evaluation of Twiss parameters. Defaults to 4 . file (Union[Path, str]): path to output file. Default None fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to False . Keyword Args: Any keyword argument is transmitted to the PTC_TWISS command. See above which arguments are already set for PTC_TWISS to avoid trying to override them. Returns: A TfsDataframe with results. View Source def get_rdts ( madx : Madx , order : int = 4 , file : Union [ Path , str ] = None , fringe : bool = False , ** kwargs ) -> tfs . TfsDataFrame : \" \"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD), but this has been heavily refactored. Calculate the `RDTs` via `PTC_TWISS`, with sensible defaults set for other relevant `PTC` commands. The result table is returned as a `TfsDataFrame`, the headers of which are the contents of the internal `SUMM` table. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_TWISS` command is explicitely given `icase=6` to enforce 6D calculations (see the `MAD-X` manual for details), `trackrdts=True` (for this function to fullfill its purpose) and `normal=True` to trigger saving the normal form analysis results in a table called `NONLIN` which will then be available through the provided `Madx` instance. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): map order for derivative evaluation of Twiss parameters. Defaults to `4`. file (Union[Path, str]): path to output file. Default `None` fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. Keyword Args: Any keyword argument is transmitted to the `PTC_TWISS` command. See above which arguments are already set for `PTC_TWISS` to avoid trying to override them. Returns: A `TfsDataframe` with results. \"\" \" logger . info ( \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . debug ( \"Executing PTC Twiss\" ) madx . ptc_twiss ( icase = 6 , no = order , normal = True , trackrdts = True , ** kwargs ) madx . ptc_end () dframe = get_table_tfs ( madx , table_name = \"twissrdt\" , headers_table = \"ptc_twiss_summary\" ) if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe","title":"get_rdts"},{"location":"reference/pyhdtoolkit/cpymadtools/ptc/#ptc_track_particle","text":"def ptc_track_particle ( madx : cpymad . madx . Madx , initial_coordinates : Tuple [ float , float , float , float , float , float ], nturns : int , sequence : str = None , observation_points : Sequence [ str ] = None , onetable : bool = False , fringe : bool = False , ** kwargs ) -> Dict [ str , pandas . core . frame . DataFrame ] Tracks a single particle for nturns through PTC_TRACK, based on its initial coordinates. The PTC_CREATE_LAYOUT command is issued with model=3 ( SixTrack model), method=4 (integration order), nst=3 (number of integratioin steps, aka body slices for elements) and exact=True (use exact Hamiltonian, not an approximated one). The PTC_TRACK command is explicitely given ELEMENT_BY_ELEMENT=True to force element by element tracking mode. Args: madx (Madx): an instantiated cpymad.madx.Madx object. initial_coordinates (Tuple[float, float, float, float, float, float]): a tuple with the X, PX, Y, PY, T, PT starting coordinates the particle to track. Defaults to all 0 if none given. nturns (int): the number of turns to track for. sequence (str): the sequence to use for tracking. If no value is provided, it is assumed that a sequence is already defined and in use, and this one will be picked up by MAD-X. observation_points (Sequence[str]): sequence of all element names at which to OBSERVE during the tracking. onetable (bool): flag to combine all observation points data into a single table. Defaults to False . fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to False . Keyword Args: Any keyword argument to be given to the PTC_TRACK command, such as the CLOSED_ORBIT flag to activate closed orbit calculation before tracking. Refer to the MAD-X manual for options. Returns: A dictionary with a copy of the track table's dataframe for each defined observation point, with as columns the coordinates x, px, y, py, t, pt, s and e (energy). The keys of the dictionary are simply numbered 'observation_point_1', 'observation_point_2' etc. The first observation point always corresponds to the start of machine, the others correspond to the ones manually defined, in the order they are defined in. If the user has set `onetable` to `True` , only one entry is in the dictionary under the key 'trackone' and it has the combined table as a pandas DataFrame for value . View Source def ptc_track_particle ( madx : Madx , initial_coordinates : Tuple [ float , float , float , float , float , float ] , nturns : int , sequence : str = None , observation_points : Sequence [ str ] = None , onetable : bool = False , fringe : bool = False , ** kwargs , ) -> Dict [ str , pd . DataFrame ] : \" \"\" Tracks a single particle for nturns through PTC_TRACK, based on its initial coordinates. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_TRACK` command is explicitely given `ELEMENT_BY_ELEMENT=True` to force element by element tracking mode. Args: madx (Madx): an instantiated cpymad.madx.Madx object. initial_coordinates (Tuple[float, float, float, float, float, float]): a tuple with the X, PX, Y, PY, T, PT starting coordinates the particle to track. Defaults to all 0 if none given. nturns (int): the number of turns to track for. sequence (str): the sequence to use for tracking. If no value is provided, it is assumed that a sequence is already defined and in use, and this one will be picked up by MAD-X. observation_points (Sequence[str]): sequence of all element names at which to OBSERVE during the tracking. onetable (bool): flag to combine all observation points data into a single table. Defaults to `False`. fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. Keyword Args: Any keyword argument to be given to the `PTC_TRACK` command, such as the `CLOSED_ORBIT` flag to activate closed orbit calculation before tracking. Refer to the `MAD-X` manual for options. Returns: A dictionary with a copy of the track table's dataframe for each defined observation point, with as columns the coordinates x, px, y, py, t, pt, s and e (energy). The keys of the dictionary are simply numbered 'observation_point_1', 'observation_point_2' etc. The first observation point always corresponds to the start of machine, the others correspond to the ones manually defined, in the order they are defined in. If the user has set `onetable` to `True`, only one entry is in the dictionary under the key 'trackone' and it has the combined table as a pandas DataFrame for value. \"\" \" start = initial_coordinates if initial_coordinates else [ 0 , 0 , 0 , 0 , 0 , 0 ] observation_points = observation_points if observation_points else [] if isinstance ( sequence , str ) : logger . debug ( f \"Using sequence '{sequence}' for tracking\" ) madx . use ( sequence = sequence ) logger . info ( f \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . debug ( f \"Tracking coordinates with initial X, PX, Y, PY, T, PT of '{initial_coordinates}'\" ) madx . command . ptc_start ( X = start [ 0 ] , PX = start [ 1 ] , Y = start [ 2 ] , PY = start [ 3 ] , T = start [ 4 ] , PT = start [ 5 ] , ) for element in observation_points : logger . trace ( f \"Setting observation point for tracking with OBSERVE at element '{element}'\" ) madx . command . ptc_observe ( place = element ) madx . command . ptc_track ( turns = nturns , element_by_element = True , onetable = onetable , ** kwargs ) madx . ptc_end () if onetable : # user asked for ONETABLE, there will only be one table 'trackone' given back by MAD-X logger . debug ( \"Because of option ONETABLE only one table 'TRACKONE' exists to be returned.\" ) return { \"trackone\" : madx . table . trackone . dframe (). copy () } return { f \"observation_point_{point:d}\" : madx . table [ f \"track.obs{point:04d}.p0001\" ] . dframe (). copy () for point in range ( 1 , len ( observation_points ) + 2 ) # len(observation_points) + 1 for start of # machine + 1 because MAD-X starts indexing these at 1 }","title":"ptc_track_particle"},{"location":"reference/pyhdtoolkit/cpymadtools/ptc/#ptc_twiss","text":"def ptc_twiss ( madx : cpymad . madx . Madx , order : int = 4 , file : Union [ pathlib . Path , str ] = None , fringe : bool = False , table : str = 'ptc_twiss' , ** kwargs ) -> tfs . handler . TfsDataFrame Calculate the TWISS parameters according to G. Ripken's formalism via PTC_TWISS , with sensible defaults set for other relevant PTC commands. The result table is returned as a TfsDataFrame , the headers of which are the contents of the internal SUMM table. This is very similar to the get_rdts function as both use ptc_twiss internally, however this function does not track RDTs which makes the calculations significantly faster. The PTC_CREATE_LAYOUT command is issued with model=3 ( SixTrack model), method=4 (integration order), nst=3 (number of integratioin steps, aka body slices for elements) and exact=True (use exact Hamiltonian, not an approximated one). The PTC_TWISS command is explicitely given icase=6 to enforce 6D calculations (see the MAD-X manual for details) normal=True to trigger saving the normal form analysis results in a table called NONLIN which will then be available through the provided Madx instance. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): map order for derivative evaluation of Twiss parameters. Defaults to 4 . file (Union[Path, str]): path to output file. Default None fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to False . table (str): the name of the internal table in which to save the results. Defaults to 'ptc_twiss'. Keyword Args: Any keyword argument is transmitted to the PTC_TWISS command. See above which arguments are already set for PTC_TWISS to avoid trying to override them. Returns: A TfsDataframe with results. View Source def ptc_twiss ( madx : Madx , order : int = 4 , file : Union [ Path , str ] = None , fringe : bool = False , table : str = \"ptc_twiss\" , ** kwargs , ) -> tfs . TfsDataFrame : \" \"\" Calculate the TWISS parameters according to G. Ripken's formalism via `PTC_TWISS`, with sensible defaults set for other relevant `PTC` commands. The result table is returned as a `TfsDataFrame`, the headers of which are the contents of the internal `SUMM` table. This is very similar to the `get_rdts` function as both use `ptc_twiss` internally, however this function does not track RDTs which makes the calculations significantly faster. The `PTC_CREATE_LAYOUT` command is issued with `model=3` (`SixTrack` model), `method=4` (integration order), `nst=3` (number of integratioin steps, aka body slices for elements) and `exact=True` (use exact Hamiltonian, not an approximated one). The `PTC_TWISS` command is explicitely given `icase=6` to enforce 6D calculations (see the `MAD-X` manual for details) `normal=True` to trigger saving the normal form analysis results in a table called `NONLIN` which will then be available through the provided `Madx` instance. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): map order for derivative evaluation of Twiss parameters. Defaults to `4`. file (Union[Path, str]): path to output file. Default `None` fringe (bool): boolean flag to include fringe field effects in the calculation. Defaults to `False`. table (str): the name of the internal table in which to save the results. Defaults to 'ptc_twiss'. Keyword Args: Any keyword argument is transmitted to the `PTC_TWISS` command. See above which arguments are already set for `PTC_TWISS` to avoid trying to override them. Returns: A `TfsDataframe` with results. \"\" \" logger . info ( f \"Creating PTC universe\" ) madx . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) madx . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) logger . trace ( \"Incorporating MAD-X alignment errors\" ) madx . ptc_align () # use madx alignment errors madx . ptc_setswitch ( fringe = fringe ) logger . debug ( \"Executing PTC Twiss\" ) madx . ptc_twiss ( icase = 6 , no = order , normal = True , table = table , ** kwargs ) madx . ptc_end () dframe = get_table_tfs ( madx , table_name = table , headers_table = \"ptc_twiss_summary\" ) if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe","title":"ptc_twiss"},{"location":"reference/pyhdtoolkit/cpymadtools/special/","text":"Module pyhdtoolkit.cpymadtools.special Module cpymadtools.special Created on 2020.02.03 View Source \"\"\" Module cpymadtools.special -------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X actions with a cpymad.madx.Madx object, that are very specific to what I do in LHC and HLLHC use cases. \"\"\" from typing import List , Sequence import numpy as np from cpymad.madx import Madx from loguru import logger # ----- Setup Utlites ----- # def make_lhc_beams ( madx : Madx , energy : float = 7000 , emittance : float = 3.75e-6 , ** kwargs ) -> None : \"\"\" Define beams with default configuratons for `LHCB1` and `LHCB2` sequences. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. Defaults to 6500. emittance (float): emittance in meters, which will be used to calculate geometric emittance, then fed to the BEAM command. Keyword Args: Any keyword argument that can be given to the MAD-X BEAM command. \"\"\" logger . info ( \"Making default beams for 'lhcb1' and 'lhbc2' sequences\" ) madx . globals [ \"NRJ\" ] = energy madx . globals [ \"brho\" ] = energy * 1e9 / madx . globals . clight geometric_emit = madx . globals [ \"geometric_emit\" ] = emittance / ( energy / 0.938 ) for beam in ( 1 , 2 ): logger . trace ( f \"Defining beam for sequence 'lhcb { beam : d } '\" ) madx . command . beam ( sequence = f \"lhcb { beam : d } \" , particle = \"proton\" , bv = 1 if beam == 1 else - 1 , energy = energy , npart = 1.15e11 , ex = geometric_emit , ey = geometric_emit , sige = 4.5e-4 , ** kwargs , ) def power_landau_octupoles ( madx : Madx , mo_current : float , beam : int , defective_arc : bool = False ) -> None : \"\"\" Power the Landau octupoles in the (HL)LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. mo_current (float): MO powering in Amps. beam (int): beam to use. defective_arc: If set to `True`, the KOD in Arc 56 are powered for less Imax. \"\"\" try : brho = madx . globals . nrj * 1e9 / madx . globals . clight # clight is MAD-X constant except AttributeError as madx_error : logger . error ( \"The global MAD-X variable 'NRJ' should have been set in the optics files but is not defined.\" ) raise EnvironmentError ( \"No 'NRJ' variable found in scripts\" ) from madx_error logger . info ( f \"Powering Landau Octupoles, beam { beam } @ { madx . globals . nrj } GeV with { mo_current } A.\" ) strength = mo_current / madx . globals . Imax_MO * madx . globals . Kmax_MO / brho beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ): for fd in \"FD\" : octupole = f \"KO { fd } . { arc } \" logger . trace ( f \"Powering element ' { octupole } ' at { strength } Amps\" ) madx . globals [ octupole ] = strength if defective_arc and ( beam == 1 ): madx . globals [ \"KOD.A56B1\" ] = strength * 4.65 / 6 # defective MO group def deactivate_lhc_arc_sextupoles ( madx : Madx , beam : int ) -> None : \"\"\" Deactivate all arc sextupoles in the (HL)LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. beam (int): beam to use. \"\"\" # KSF1 and KSD2 - Strong sextupoles of sectors 81/12/45/56 # KSF2 and KSD1 - Weak sextupoles of sectors 81/12/45/56 # Rest: Weak sextupoles in sectors 78/23/34/67 logger . info ( f \"Deactivating all arc sextupoles for beam { beam } .\" ) beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ): for fd in \"FD\" : for i in ( 1 , 2 ): sextupole = f \"KS { fd }{ i : d } . { arc } \" logger . trace ( f \"De-powering element ' { sextupole } '\" ) madx . globals [ sextupole ] = 0.0 def apply_lhc_colinearity_knob ( madx : Madx , colinearity_knob_value : float = 0 , ir : int = None ) -> None : \"\"\" Applies the LHC colinearity knob. If you don't know what this is, you should not be using this function. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. colinearity_knob_value (float): Units of the colinearity knob to apply. Defaults to 0 so users don't mess up local coupling by mistake. This should be a positive integer, normally between 1 and 10. ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. \"\"\" logger . info ( f \"Applying Colinearity knob with a unit setting of { colinearity_knob_value } \" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) knob_variables = ( f \"KQSX3.R { ir : d } \" , f \"KQSX3.L { ir : d } \" ) # MQSX IP coupling correctors right_knob , left_knob = knob_variables madx . globals [ right_knob ] = colinearity_knob_value * 1e-4 logger . trace ( f \"Set ' { right_knob } ' to { madx . globals [ right_knob ] } \" ) madx . globals [ left_knob ] = - 1 * colinearity_knob_value * 1e-4 logger . trace ( f \"Set ' { left_knob } ' to { madx . globals [ left_knob ] } \" ) def apply_lhc_rigidity_waist_shift_knob ( madx : Madx , rigidty_waist_shift_value : float = 0 , ir : int = None , side : str = \"left\" ) -> None : \"\"\" Applies the LHC rigidity waist shift knob, moving the waist left or right of IP. If you don't know what this is, you should not be using this function. The waist shift is done by unbalancing the triplet powering knob of the left and right-hand sides of the IP. Warning: Applying the shift will modify your tunes and most likely flip them, making a subsequent matching impossible if your lattice has coupling. To avoid this, match to tunes split further apart before applying the waist shift knob, and then match to the desired working point. For instance for the LHC, matching to (62.27, 60.36) before applying and afterwards rematching to (62.31, 60.32) usually works well. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. rigidty_waist_shift_value (float): Units of the rigidity waist shift knob (positive values only). ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. side (str): Which side of the IP to move the waist to, determines a sign in the calculation. Defaults to 'left', which means s_waist < s_ip (and setting it to 'right' would move the waist to s_waist > s_ip). \"\"\" logger . info ( f \"Applying Rigidity Waist Shift knob with a unit setting of { rigidty_waist_shift_value } \" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) right_knob , left_knob = f \"kqx.r { ir : d } \" , f \"kqx.l { ir : d } \" # IP triplet default knob (no trims) current_right_knob = madx . globals [ right_knob ] current_left_knob = madx . globals [ left_knob ] if side . lower () == \"left\" : madx . globals [ right_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_right_knob madx . globals [ left_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_left_knob elif side . lower () == \"right\" : madx . globals [ right_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_right_knob madx . globals [ left_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_left_knob else : logger . error ( f \"Given side ' { side } ' invalid, only 'left' and 'right' are accepted values.\" ) raise ValueError ( \"Invalid value for parameter 'side'.\" ) logger . trace ( f \"Set ' { right_knob } ' to { madx . globals [ right_knob ] } \" ) logger . trace ( f \"Set ' { left_knob } ' to { madx . globals [ left_knob ] } \" ) def apply_lhc_coupling_knob ( madx : Madx , coupling_knob : float = 0 , beam : int = 1 , telescopic_squeeze : bool = False ) -> None : \"\"\" Applies the LHC coupling knob to reach the desired C- value. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. coupling_knob (float): Desired value for the Cminus, typically a few units of 1E-3. Defaults to 0 so users don't mess up coupling by mistake beam (int): beam to apply the knob to, defaults to beam 1. telescopic_squeeze (bool): if set to True, uses the knobs for Telescopic Squeeze configuration. Defaults to False. \"\"\" logger . info ( \"Applying coupling knob\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) suffix = \"_sq\" if telescopic_squeeze else \"\" knob_name = f \"CMRS.b { beam : d }{ suffix } \" logger . trace ( f \"Knob ' { knob_name } ' is { madx . globals [ knob_name ] } before implementation\" ) madx . globals [ knob_name ] = coupling_knob logger . trace ( f \"Set ' { knob_name } ' to { madx . globals [ knob_name ] } \" ) def install_ac_dipole ( madx : Madx , deltaqx : float , deltaqy : float , sigma_x : float , sigma_y : float , geometric_emit : float = None , start_turn : int = 100 , ramp_turns : int = 2000 , top_turns : int = 6600 , ) -> None : \"\"\" Installs an AC dipole for BEAM 1 ONLY. This function assumes that you have already defined lhcb1, made a beam for it (BEAM command or `make_lhc_beams` function) and matched to your desired working point. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. deltaqx (float): the deltaQx (horizontal tune excitation) used by the AC dipole. deltaqy (float): the deltaQy (vertical tune excitation) used by the AC dipole. sigma_x (float): the horizontal amplitude to drive the beam to, in bunch sigma. sigma_y (float): the vertical amplitude to drive the beam to, in bunch sigma. geometric_emit (float): the geometric emittance that was used when defining the beam. If not provided, it is assumed that 'geometric_emit' is a defined global in MAD-X, and the value will be directly queried from the internal tables. start_turn (int): the turn at which to start ramping up the AC dipole. Defaults to 100. ramp_turns (int): the number of turns to use for the ramp-up and the ramp-down of the AC dipole. This number is important in order to preserve the adiabaticity of the cycle. Defaults to 2000 as in the LHC. top_turns (int): the number of turns to drive the beam for. Defaults to 6600 as in the LHC. \"\"\" if top_turns > 6600 : logger . warning ( f \"Configuring the AC Dipole for { top_turns } of driving is fine for MAD-X but is \" \"higher than what the device can do in the (HL)LHC! Beware.\" ) ramp1 , ramp2 = start_turn , start_turn + ramp_turns ramp3 = ramp2 + top_turns ramp4 = ramp3 + ramp_turns logger . debug ( \"Retrieving tunes from internal tables\" ) q1 , q2 = madx . table . summ . q1 [ 0 ], madx . table . summ . q2 [ 0 ] logger . trace ( f \"Retrieved values are q1 = { q1 } , q2 = { q2 } \" ) q1_dipole , q2_dipole = q1 + deltaqx , q2 + deltaqy if not geometric_emit : logger . debug ( \"No value provided for the geometric emittance used when creating the beam, the value will be \" \"queried from MAD-X's global 'geometric_emit'\" ) geometric_emit = madx . globals [ \"geometric_emit\" ] logger . info ( f \"Installing AC Dipole to drive the tunes to Qx_D = { q1_dipole } | Qy_D = { q2_dipole } \" ) madx . input ( f \"MKACH.6L4.B1: hacdipole, l=0, freq:= { q1_dipole } , lag=0, volt:=voltx, ramp1= { ramp1 } , \" f \"ramp2= { ramp2 } , ramp3= { ramp3 } , ramp4= { ramp4 } ;\" ) madx . input ( f \"MKACV.6L4.B1: vacdipole, l=0, freq:= { q2_dipole } , lag=0, volt:=volty, ramp1= { ramp1 } , \" f \"ramp2= { ramp2 } , ramp3= { ramp3 } , ramp4= { ramp4 } ;\" ) madx . command . seqedit ( sequence = \"lhcb1\" ) madx . command . flatten () madx . command . install ( element = \"MKACH.6L4.B1\" , at = \"0.0\" , from_ = \"MKQA.6L4.B1\" ) madx . command . install ( element = \"MKACV.6L4.B1\" , at = \"0.0\" , from_ = \"MKQA.6L4.B1\" ) madx . command . endedit () logger . trace ( \"Querying BETX and BETY at AC Dipole location\" ) madx . input ( \"betax_acd = table(twiss, MKQA.6L4.B1, betx);\" ) madx . input ( \"betay_acd = table(twiss, MKQA.6L4.B1, bety);\" ) betax_acd = madx . globals [ \"betax_acd\" ] betay_acd = madx . globals [ \"betay_acd\" ] brho = madx . sequence . lhcb1 . beam . brho voltx = sigma_x * np . sqrt ( geometric_emit ) * brho * np . abs ( deltaqx ) * 4 * np . pi / np . sqrt ( betax_acd ) volty = sigma_y * np . sqrt ( geometric_emit ) * brho * np . abs ( deltaqy ) * 4 * np . pi / np . sqrt ( betay_acd ) madx . globals [ \"voltx\" ] = voltx madx . globals [ \"volty\" ] = volty def vary_independent_ir_quadrupoles ( madx : Madx , quad_numbers : Sequence [ int ], ip : int , sides : Sequence [ str ] = ( \"r\" , \"l\" ), beam : int = 1 ) -> None : \"\"\" Send the `vary` commands for the desired quadrupoles in the IRs. The independent quadrupoles for which this is implemented are Q4 to Q13 included. This is useful to setup some specific matching involving these elements. It is necessary to have defined a 'brho' variable when creating your beams. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. quad_numbers (Sequence[int]): quadrupoles to be varied, by number (aka position from IP). ip (int): the IP around which to apply the instructions. Defaults to 1. sides (Sequence[str]): the sides of IP to act on. Should be `R` for right and `L` for left. Defaults to both sides of the IP. beam (int): the beam for which to apply the instructions. Defaults to 1. \"\"\" if ( ip not in ( 1 , 2 , 5 , 8 ) or any ( side . upper () not in ( \"R\" , \"L\" ) for side in sides ) or any ( quad not in ( 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 ) for quad in quad_numbers ) ): logger . error ( \"Either the IP number of the side provided are invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'quad_numbers', 'ip', 'sides' argument\" ) logger . debug ( f \"Preparing a knob involving quadrupoles { quad_numbers } \" ) # Each quad has a specific power circuit used for their k1 boundaries power_circuits : Dict [ int , str ] = { 4 : \"mqy\" , 5 : \"mqml\" , 6 : \"mqml\" , 7 : \"mqm\" , 8 : \"mqml\" , 9 : \"mqm\" , 10 : \"mqml\" , 11 : \"mqtli\" , 12 : \"mqt\" , 13 : \"mqt\" , } for quad in quad_numbers : circuit = power_circuits [ quad ] for side in sides : logger . trace ( f \"Sending vary command for Q { quad }{ side . upper () }{ ip } \" ) madx . command . vary ( name = f \"kq { 't' if quad >= 11 else '' }{ 'l' if quad == 11 else '' }{ quad } . { side }{ ip } b { beam } \" , step = 1e-7 , lower = f \"- { circuit } . { 'b' if quad == 7 else '' }{ quad }{ side }{ ip } .b { beam } ->kmax/brho\" , upper = f \"+ { circuit } . { 'b' if quad == 7 else '' }{ quad }{ side }{ ip } .b { beam } ->kmax/brho\" , ) # ----- Output Utilities ----- # def make_sixtrack_output ( madx : Madx , energy : int ) -> None : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Prepare output for sixtrack run. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. \"\"\" logger . info ( \"Preparing outputs for SixTrack\" ) logger . debug ( \"Powering RF cavities\" ) madx . globals [ \"VRF400\" ] = 8 if energy < 5000 else 16 # is 6 at injection for protons iirc? madx . globals [ \"LAGRF400.B1\" ] = 0.5 # cavity phase difference in units of 2pi madx . globals [ \"LAGRF400.B2\" ] = 0.0 logger . debug ( \"Executing TWISS and SIXTRACK commands\" ) madx . twiss () # used by sixtrack madx . sixtrack ( cavall = True , radius = 0.017 ) # this value is only ok for HL(LHC) magnet radius # ----- Miscellaneous Utilities ----- # def make_lhc_thin ( madx : Madx , sequence : str , slicefactor : int = 1 , ** kwargs ) -> None : \"\"\" Makethin for the LHC sequence as previously done in MAD-X macros. This will use the `teapot` style and will enforce `makedipedge`. Args: madx (Madx): an instantiated cpymad.madx.Madx object. sequence (str): the sequence to use for the MAKETHIN command. slicefactor (int): the slice factor to apply in makethin. Defaults to 1. Keyword Args: The keyword arguments for the MAD-X MAKETHN commands, namely `style` (will default to `teapot`) and the `makedipedge` flag (will default to True). \"\"\" logger . info ( f \"Slicing sequence ' { sequence } '\" ) madx . select ( flag = \"makethin\" , clear = True ) four_slices_patterns = [ r \"mbx\\.\" , r \"mbrb\\.\" , r \"mbrc\\.\" , r \"mbrs\\.\" ] four_slicefactor_patterns = [ r \"mqwa\\.\" , r \"mqwb\\.\" , r \"mqy\\.\" , r \"mqm\\.\" , r \"mqmc\\.\" , r \"mqml\\.\" , r \"mqtlh\\.\" , r \"mqtli\\.\" , r \"mqt\\.\" , ] logger . trace ( \"Defining slices for general MB and MQ elements\" ) madx . select ( flag = \"makethin\" , class_ = \"MB\" , slice = 2 ) madx . select ( flag = \"makethin\" , class_ = \"MQ\" , slice = 2 * slicefactor ) logger . trace ( \"Defining slices for triplets\" ) madx . select ( flag = \"makethin\" , class_ = \"mqxa\" , slice = 16 * slicefactor ) madx . select ( flag = \"makethin\" , class_ = \"mqxb\" , slice = 16 * slicefactor ) logger . trace ( \"Defining slices for various specifc mb elements\" ) for pattern in four_slices_patterns : madx . select ( flag = \"makethin\" , pattern = pattern , slice = 4 ) logger . trace ( \"Defining slices for varous specifc mq elements\" ) for pattern in four_slicefactor_patterns : madx . select ( flag = \"makethin\" , pattern = pattern , slice = 4 * slicefactor ) madx . use ( sequence = sequence ) style = kwargs . get ( \"style\" , \"teapot\" ) makedipedge = kwargs . get ( \"makedipedge\" , False ) # defaults to False to compensate default TEAPOT style madx . command . makethin ( sequence = sequence , style = style , makedipedge = makedipedge ) def re_cycle_sequence ( madx : Madx , sequence : str = \"lhcb1\" , start : str = \"IP3\" ) -> None : \"\"\" Re-cycle the provided sequence from a different starting point. Args: madx (Madx): an instantiated cpymad.madx.Madx object. sequence (str): the sequence to re cycle. start (str): element to start the new cycle from. \"\"\" logger . debug ( f \"Re-cycling sequence ' { sequence } ' from { start } \" ) madx . command . seqedit ( sequence = sequence ) madx . command . flatten () madx . command . cycle ( start = start ) madx . command . endedit () def match_no_coupling_through_ripkens ( madx : Madx , sequence : str = None , location : str = None , vary_knobs : Sequence [ str ] = None ) -> None : \"\"\" Matching routine to get cross-term Ripken parameters beta_12 and beta_21 to be 0 at a given location. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. sequence (str): name of the sequence to activate for the matching. location (str): the name of the element at which one wants the cross-term Ripkens to be 0. vary_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. \"\"\" logger . info ( f \"Matching Ripken parameters for no coupling at location { location } \" ) logger . debug ( \"Creating macro tu update Ripkens\" ) madx . input ( \"do_ripken: macro = {twiss, ripken=True;}\" ) # cpymad needs .input for macros logger . debug ( \"Matching Parameters\" ) madx . command . match ( sequence = sequence , use_macro = True , chrom = True ) for knob in vary_knobs : madx . command . vary ( name = knob ) madx . command . use_macro ( name = \"do_ripken\" ) madx . input ( f \"constraint, expr=table(twiss, { location } , beta12)=0\" ) # need input else includes \" and fails madx . input ( f \"constraint, expr=table(twiss, { location } , beta21)=0\" ) # need input else includes \" and fails madx . command . lmdif ( calls = 500 , tolerance = 1e-21 ) madx . command . endmatch () # ----- Helpers ----- # def _all_lhc_arcs ( beam : int ) -> List [ str ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Names of all LHC arcs for a given beam. Args: beam (int): beam to get names for. Returns: The list of names. \"\"\" return [ f \"A { i + 1 }{ ( i + 1 ) % 8 + 1 } B { beam : d } \" for i in range ( 8 )] def _get_k_strings ( start : int = 0 , stop : int = 8 , orientation : str = \"both\" ) -> List [ str ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Returns the list of K-strings for various magnets and orders (K1L, K2SL etc strings). Args: start (int): the starting order, defaults to 0. stop (int): the order to go up to, defaults to 8. orientation (str): magnet orientation, can be 'straight', 'skew' or 'both'. Defaults to 'both'. Returns: The list of names as strings. \"\"\" if orientation not in ( \"straight\" , \"skew\" , \"both\" ,): logger . error ( f \"Orientation ' { orientation } ' is not accepted, should be one of 'straight', 'skew', 'both'.\" ) raise ValueError ( \"Invalid 'orientation' parameter\" ) if orientation == \"straight\" : orientation = ( \"\" ,) elif orientation == \"skew\" : orientation = ( \"S\" ,) else : # both orientation = ( \"\" , \"S\" ) return [ f \"K { i : d }{ s : s } L\" for i in range ( start , stop ) for s in orientation ] Functions apply_lhc_colinearity_knob def apply_lhc_colinearity_knob ( madx : cpymad . madx . Madx , colinearity_knob_value : float = 0 , ir : int = None ) -> None Applies the LHC colinearity knob. If you don't know what this is, you should not be using this function. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None colinearity_knob_value float Units of the colinearity knob to apply. Defaults to 0 so users don't mess up local coupling by mistake. This should be a positive integer, normally between 1 and 10. None ir int The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. None View Source def apply_lhc_colinearity_knob ( madx : Madx , colinearity_knob_value : float = 0 , ir : int = None ) -> None : \"\"\" Applies the LHC colinearity knob. If you don't know what this is, you should not be using this function. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. colinearity_knob_value (float): Units of the colinearity knob to apply. Defaults to 0 so users don't mess up local coupling by mistake. This should be a positive integer, normally between 1 and 10. ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. \"\"\" logger . info ( f \"Applying Colinearity knob with a unit setting of {colinearity_knob_value}\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) knob_variables = ( f \"KQSX3.R{ir:d}\" , f \"KQSX3.L{ir:d}\" ) # MQSX IP coupling correctors right_knob , left_knob = knob_variables madx . globals [ right_knob ] = colinearity_knob_value * 1e-4 logger . trace ( f \"Set '{right_knob}' to {madx.globals[right_knob]}\" ) madx . globals [ left_knob ] = - 1 * colinearity_knob_value * 1e-4 logger . trace ( f \"Set '{left_knob}' to {madx.globals[left_knob]}\" ) apply_lhc_coupling_knob def apply_lhc_coupling_knob ( madx : cpymad . madx . Madx , coupling_knob : float = 0 , beam : int = 1 , telescopic_squeeze : bool = False ) -> None Applies the LHC coupling knob to reach the desired C- value. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. coupling_knob (float): Desired value for the Cminus, typically a few units of 1E-3. Defaults to 0 so users don't mess up coupling by mistake beam (int): beam to apply the knob to, defaults to beam 1. telescopic_squeeze (bool): if set to True, uses the knobs for Telescopic Squeeze configuration. Defaults to False. View Source def apply_lhc_coupling_knob ( madx : Madx , coupling_knob : float = 0 , beam : int = 1 , telescopic_squeeze : bool = False ) -> None : \"\"\" Applies the LHC coupling knob to reach the desired C- value. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. coupling_knob (float): Desired value for the Cminus, typically a few units of 1E-3. Defaults to 0 so users don't mess up coupling by mistake beam (int): beam to apply the knob to, defaults to beam 1. telescopic_squeeze (bool): if set to True, uses the knobs for Telescopic Squeeze configuration. Defaults to False. \"\"\" logger . info ( \"Applying coupling knob\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) suffix = \"_sq\" if telescopic_squeeze else \"\" knob_name = f \"CMRS.b{beam:d}{suffix}\" logger . trace ( f \"Knob '{knob_name}' is {madx.globals[knob_name]} before implementation\" ) madx . globals [ knob_name ] = coupling_knob logger . trace ( f \"Set '{knob_name}' to {madx.globals[knob_name]}\" ) apply_lhc_rigidity_waist_shift_knob def apply_lhc_rigidity_waist_shift_knob ( madx : cpymad . madx . Madx , rigidty_waist_shift_value : float = 0 , ir : int = None , side : str = 'left' ) -> None Applies the LHC rigidity waist shift knob, moving the waist left or right of IP. If you don't know what this is, you should not be using this function. The waist shift is done by unbalancing the triplet powering knob of the left and right-hand sides of the IP. Warning: Applying the shift will modify your tunes and most likely flip them, making a subsequent matching impossible if your lattice has coupling. To avoid this, match to tunes split further apart before applying the waist shift knob, and then match to the desired working point. For instance for the LHC, matching to (62.27, 60.36) before applying and afterwards rematching to (62.31, 60.32) usually works well. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None rigidty_waist_shift_value float Units of the rigidity waist shift knob (positive values only). None ir int The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. None side str Which side of the IP to move the waist to, determines a sign in the calculation. Defaults to 'left', which means s_waist < s_ip (and setting it to 'right' would move the waist to s_waist > s_ip). None View Source def apply_lhc_rigidity_waist_shift_knob ( madx : Madx , rigidty_waist_shift_value : float = 0 , ir : int = None , side : str = \"left\" ) -> None : \"\"\" Applies the LHC rigidity waist shift knob, moving the waist left or right of IP. If you don't know what this is, you should not be using this function. The waist shift is done by unbalancing the triplet powering knob of the left and right-hand sides of the IP. Warning: Applying the shift will modify your tunes and most likely flip them, making a subsequent matching impossible if your lattice has coupling. To avoid this, match to tunes split further apart before applying the waist shift knob, and then match to the desired working point. For instance for the LHC, matching to (62.27, 60.36) before applying and afterwards rematching to (62.31, 60.32) usually works well. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. rigidty_waist_shift_value (float): Units of the rigidity waist shift knob (positive values only). ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. side (str): Which side of the IP to move the waist to, determines a sign in the calculation. Defaults to 'left', which means s_waist < s_ip (and setting it to 'right' would move the waist to s_waist > s_ip). \"\"\" logger . info ( f \"Applying Rigidity Waist Shift knob with a unit setting of {rigidty_waist_shift_value}\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) right_knob , left_knob = f \"kqx.r{ir:d}\" , f \"kqx.l{ir:d}\" # IP triplet default knob ( no trims ) current_right_knob = madx . globals [ right_knob ] current_left_knob = madx . globals [ left_knob ] if side . lower () == \"left\" : madx . globals [ right_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_right_knob madx . globals [ left_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_left_knob elif side . lower () == \"right\" : madx . globals [ right_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_right_knob madx . globals [ left_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_left_knob else : logger . error ( f \"Given side '{side}' invalid, only 'left' and 'right' are accepted values.\" ) raise ValueError ( \"Invalid value for parameter 'side'.\" ) logger . trace ( f \"Set '{right_knob}' to {madx.globals[right_knob]}\" ) logger . trace ( f \"Set '{left_knob}' to {madx.globals[left_knob]}\" ) deactivate_lhc_arc_sextupoles def deactivate_lhc_arc_sextupoles ( madx : cpymad . madx . Madx , beam : int ) -> None Deactivate all arc sextupoles in the (HL)LHC. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None beam int beam to use. None View Source def deactivate_lhc_arc_sextupoles ( madx : Madx , beam : int ) -> None : \"\"\" Deactivate all arc sextupoles in the (HL)LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. beam (int): beam to use. \"\"\" # KSF1 and KSD2 - Strong sextupoles of sectors 81 / 12 / 45 / 56 # KSF2 and KSD1 - Weak sextupoles of sectors 81 / 12 / 45 / 56 # Rest : Weak sextupoles in sectors 78 / 23 / 34 / 67 logger . info ( f \"Deactivating all arc sextupoles for beam {beam}.\" ) beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ) : for fd in \"FD\" : for i in ( 1 , 2 ) : sextupole = f \"KS{fd}{i:d}.{arc}\" logger . trace ( f \"De-powering element '{sextupole}'\" ) madx . globals [ sextupole ] = 0.0 install_ac_dipole def install_ac_dipole ( madx : cpymad . madx . Madx , deltaqx : float , deltaqy : float , sigma_x : float , sigma_y : float , geometric_emit : float = None , start_turn : int = 100 , ramp_turns : int = 2000 , top_turns : int = 6600 ) -> None Installs an AC dipole for BEAM 1 ONLY. This function assumes that you have already defined lhcb1, made a beam for it (BEAM command or make_lhc_beams function) and matched to your desired working point. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None deltaqx float the deltaQx (horizontal tune excitation) used by the AC dipole. None deltaqy float the deltaQy (vertical tune excitation) used by the AC dipole. None sigma_x float the horizontal amplitude to drive the beam to, in bunch sigma. None sigma_y float the vertical amplitude to drive the beam to, in bunch sigma. None geometric_emit float the geometric emittance that was used when defining the beam. If not provided, it is assumed that 'geometric_emit' is a defined global in MAD-X, and the value will be directly queried from the internal tables. None start_turn int the turn at which to start ramping up the AC dipole. Defaults to 100. 100 ramp_turns int the number of turns to use for the ramp-up and the ramp-down of the AC dipole. This number is important in order to preserve the adiabaticity of the cycle. Defaults to 2000 as in the LHC. None top_turns int the number of turns to drive the beam for. Defaults to 6600 as in the LHC. 6600 as in the LHC View Source def install_ac_dipole ( madx : Madx , deltaqx : float , deltaqy : float , sigma_x : float , sigma_y : float , geometric_emit : float = None , start_turn : int = 100 , ramp_turns : int = 2000 , top_turns : int = 6600 , ) -> None : \" \"\" Installs an AC dipole for BEAM 1 ONLY. This function assumes that you have already defined lhcb1, made a beam for it (BEAM command or `make_lhc_beams` function) and matched to your desired working point. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. deltaqx (float): the deltaQx (horizontal tune excitation) used by the AC dipole. deltaqy (float): the deltaQy (vertical tune excitation) used by the AC dipole. sigma_x (float): the horizontal amplitude to drive the beam to, in bunch sigma. sigma_y (float): the vertical amplitude to drive the beam to, in bunch sigma. geometric_emit (float): the geometric emittance that was used when defining the beam. If not provided, it is assumed that 'geometric_emit' is a defined global in MAD-X, and the value will be directly queried from the internal tables. start_turn (int): the turn at which to start ramping up the AC dipole. Defaults to 100. ramp_turns (int): the number of turns to use for the ramp-up and the ramp-down of the AC dipole. This number is important in order to preserve the adiabaticity of the cycle. Defaults to 2000 as in the LHC. top_turns (int): the number of turns to drive the beam for. Defaults to 6600 as in the LHC. \"\" \" if top_turns > 6600 : logger . warning ( f \"Configuring the AC Dipole for {top_turns} of driving is fine for MAD-X but is \" \"higher than what the device can do in the (HL)LHC! Beware.\" ) ramp1 , ramp2 = start_turn , start_turn + ramp_turns ramp3 = ramp2 + top_turns ramp4 = ramp3 + ramp_turns logger . debug ( \"Retrieving tunes from internal tables\" ) q1 , q2 = madx . table . summ . q1 [ 0 ] , madx . table . summ . q2 [ 0 ] logger . trace ( f \"Retrieved values are q1 = {q1}, q2 = {q2}\" ) q1_dipole , q2_dipole = q1 + deltaqx , q2 + deltaqy if not geometric_emit : logger . debug ( \"No value provided for the geometric emittance used when creating the beam, the value will be \" \"queried from MAD-X's global 'geometric_emit'\" ) geometric_emit = madx . globals [ \"geometric_emit\" ] logger . info ( f \"Installing AC Dipole to drive the tunes to Qx_D = {q1_dipole} | Qy_D = {q2_dipole}\" ) madx . input ( f \"MKACH.6L4.B1: hacdipole, l=0, freq:={q1_dipole}, lag=0, volt:=voltx, ramp1={ramp1}, \" f \"ramp2={ramp2}, ramp3={ramp3}, ramp4={ramp4};\" ) madx . input ( f \"MKACV.6L4.B1: vacdipole, l=0, freq:={q2_dipole}, lag=0, volt:=volty, ramp1={ramp1}, \" f \"ramp2={ramp2}, ramp3={ramp3}, ramp4={ramp4};\" ) madx . command . seqedit ( sequence = \"lhcb1\" ) madx . command . flatten () madx . command . install ( element = \"MKACH.6L4.B1\" , at = \"0.0\" , from_ = \"MKQA.6L4.B1\" ) madx . command . install ( element = \"MKACV.6L4.B1\" , at = \"0.0\" , from_ = \"MKQA.6L4.B1\" ) madx . command . endedit () logger . trace ( \"Querying BETX and BETY at AC Dipole location\" ) madx . input ( \"betax_acd = table(twiss, MKQA.6L4.B1, betx);\" ) madx . input ( \"betay_acd = table(twiss, MKQA.6L4.B1, bety);\" ) betax_acd = madx . globals [ \"betax_acd\" ] betay_acd = madx . globals [ \"betay_acd\" ] brho = madx . sequence . lhcb1 . beam . brho voltx = sigma_x * np . sqrt ( geometric_emit ) * brho * np . abs ( deltaqx ) * 4 * np . pi / np . sqrt ( betax_acd ) volty = sigma_y * np . sqrt ( geometric_emit ) * brho * np . abs ( deltaqy ) * 4 * np . pi / np . sqrt ( betay_acd ) madx . globals [ \"voltx\" ] = voltx madx . globals [ \"volty\" ] = volty make_lhc_beams def make_lhc_beams ( madx : cpymad . madx . Madx , energy : float = 7000 , emittance : float = 3.75e-06 , ** kwargs ) -> None Define beams with default configuratons for LHCB1 and LHCB2 sequences. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. Defaults to 6500. emittance (float): emittance in meters, which will be used to calculate geometric emittance, then fed to the BEAM command. Keyword Args: Any keyword argument that can be given to the MAD-X BEAM command. View Source def make_lhc_beams ( madx : Madx , energy : float = 7000 , emittance : float = 3.75e-6 , ** kwargs ) -> None : \" \"\" Define beams with default configuratons for `LHCB1` and `LHCB2` sequences. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. Defaults to 6500. emittance (float): emittance in meters, which will be used to calculate geometric emittance, then fed to the BEAM command. Keyword Args: Any keyword argument that can be given to the MAD-X BEAM command. \"\" \" logger . info ( \"Making default beams for 'lhcb1' and 'lhbc2' sequences\" ) madx . globals [ \"NRJ\" ] = energy madx . globals [ \"brho\" ] = energy * 1e9 / madx . globals . clight geometric_emit = madx . globals [ \"geometric_emit\" ] = emittance / ( energy / 0.938 ) for beam in ( 1 , 2 ) : logger . trace ( f \"Defining beam for sequence 'lhcb{beam:d}'\" ) madx . command . beam ( sequence = f \"lhcb{beam:d}\" , particle = \"proton\" , bv = 1 if beam == 1 else - 1 , energy = energy , npart = 1.15e11 , ex = geometric_emit , ey = geometric_emit , sige = 4.5e-4 , ** kwargs , ) make_lhc_thin def make_lhc_thin ( madx : cpymad . madx . Madx , sequence : str , slicefactor : int = 1 , ** kwargs ) -> None Makethin for the LHC sequence as previously done in MAD-X macros. This will use the teapot style and will enforce makedipedge . Args: madx (Madx): an instantiated cpymad.madx.Madx object. sequence (str): the sequence to use for the MAKETHIN command. slicefactor (int): the slice factor to apply in makethin. Defaults to 1. Keyword Args: The keyword arguments for the MAD-X MAKETHN commands, namely style (will default to teapot ) and the makedipedge flag (will default to True). View Source def make_lhc_thin ( madx : Madx , sequence : str , slicefactor : int = 1 , ** kwargs ) -> None : \" \"\" Makethin for the LHC sequence as previously done in MAD-X macros. This will use the `teapot` style and will enforce `makedipedge`. Args: madx (Madx): an instantiated cpymad.madx.Madx object. sequence (str): the sequence to use for the MAKETHIN command. slicefactor (int): the slice factor to apply in makethin. Defaults to 1. Keyword Args: The keyword arguments for the MAD-X MAKETHN commands, namely `style` (will default to `teapot`) and the `makedipedge` flag (will default to True). \"\" \" logger . info ( f \"Slicing sequence '{sequence}'\" ) madx . select ( flag = \"makethin\" , clear = True ) four_slices_patterns = [ r \"mbx \\ .\" , r \"mbrb \\ .\" , r \"mbrc \\ .\" , r \"mbrs \\ .\" ] four_slicefactor_patterns = [ r \"mqwa \\ .\" , r \"mqwb \\ .\" , r \"mqy \\ .\" , r \"mqm \\ .\" , r \"mqmc \\ .\" , r \"mqml \\ .\" , r \"mqtlh \\ .\" , r \"mqtli \\ .\" , r \"mqt \\ .\" , ] logger . trace ( \"Defining slices for general MB and MQ elements\" ) madx . select ( flag = \"makethin\" , class_ = \"MB\" , slice = 2 ) madx . select ( flag = \"makethin\" , class_ = \"MQ\" , slice = 2 * slicefactor ) logger . trace ( \"Defining slices for triplets\" ) madx . select ( flag = \"makethin\" , class_ = \"mqxa\" , slice = 16 * slicefactor ) madx . select ( flag = \"makethin\" , class_ = \"mqxb\" , slice = 16 * slicefactor ) logger . trace ( \"Defining slices for various specifc mb elements\" ) for pattern in four_slices_patterns : madx . select ( flag = \"makethin\" , pattern = pattern , slice = 4 ) logger . trace ( \"Defining slices for varous specifc mq elements\" ) for pattern in four_slicefactor_patterns : madx . select ( flag = \"makethin\" , pattern = pattern , slice = 4 * slicefactor ) madx . use ( sequence = sequence ) style = kwargs . get ( \"style\" , \"teapot\" ) makedipedge = kwargs . get ( \"makedipedge\" , False ) # defaults to False to compensate default TEAPOT style madx . command . makethin ( sequence = sequence , style = style , makedipedge = makedipedge ) make_sixtrack_output def make_sixtrack_output ( madx : cpymad . madx . Madx , energy : int ) -> None INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Prepare output for sixtrack run. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None energy float beam energy in GeV. None View Source def make_sixtrack_output ( madx : Madx , energy : int ) -> None : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Prepare output for sixtrack run. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. \"\"\" logger . info ( \"Preparing outputs for SixTrack\" ) logger . debug ( \"Powering RF cavities\" ) madx . globals [ \"VRF400\" ] = 8 if energy < 5000 else 16 # is 6 at injection for protons iirc ? madx . globals [ \"LAGRF400.B1\" ] = 0.5 # cavity phase difference in units of 2 pi madx . globals [ \"LAGRF400.B2\" ] = 0.0 logger . debug ( \"Executing TWISS and SIXTRACK commands\" ) madx . twiss () # used by sixtrack madx . sixtrack ( cavall = True , radius = 0.017 ) # this value is only ok for HL ( LHC ) magnet radius match_no_coupling_through_ripkens def match_no_coupling_through_ripkens ( madx : cpymad . madx . Madx , sequence : str = None , location : str = None , vary_knobs : Sequence [ str ] = None ) -> None Matching routine to get cross-term Ripken parameters beta_12 and beta_21 to be 0 at a given location. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None sequence str name of the sequence to activate for the matching. None location str the name of the element at which one wants the cross-term Ripkens to be 0. None vary_knobs Sequence[str] the variables names to 'vary' in the MADX routine. None View Source def match_no_coupling_through_ripkens ( madx : Madx , sequence : str = None , location : str = None , vary_knobs : Sequence [ str ] = None ) -> None : \"\"\" Matching routine to get cross-term Ripken parameters beta_12 and beta_21 to be 0 at a given location. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. sequence (str): name of the sequence to activate for the matching. location (str): the name of the element at which one wants the cross-term Ripkens to be 0. vary_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. \"\"\" logger . info ( f \"Matching Ripken parameters for no coupling at location {location}\" ) logger . debug ( \"Creating macro tu update Ripkens\" ) madx . input ( \"do_ripken: macro = {twiss, ripken=True;}\" ) # cpymad needs . input for macros logger . debug ( \"Matching Parameters\" ) madx . command . match ( sequence = sequence , use_macro = True , chrom = True ) for knob in vary_knobs : madx . command . vary ( name = knob ) madx . command . use_macro ( name = \"do_ripken\" ) madx . input ( f \"constraint, expr=table(twiss, {location}, beta12)=0\" ) # need input else includes \" and fails madx.input(f\" constraint , expr = table ( twiss , { location } , beta21 ) = 0 \") # need input else includes \" and fails madx . command . lmdif ( calls = 500 , tolerance = 1e-21 ) madx . command . endmatch () power_landau_octupoles def power_landau_octupoles ( madx : cpymad . madx . Madx , mo_current : float , beam : int , defective_arc : bool = False ) -> None Power the Landau octupoles in the (HL)LHC. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None mo_current float MO powering in Amps. None beam int beam to use. None defective_arc None If set to True , the KOD in Arc 56 are powered for less Imax. None View Source def power_landau_octupoles ( madx : Madx , mo_current : float , beam : int , defective_arc : bool = False ) -> None : \"\"\" Power the Landau octupoles in the (HL)LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. mo_current (float): MO powering in Amps. beam (int): beam to use. defective_arc: If set to `True`, the KOD in Arc 56 are powered for less Imax. \"\"\" try : brho = madx . globals . nrj * 1e9 / madx . globals . clight # clight is MAD-X constant except AttributeError as madx_error : logger . error ( \"The global MAD-X variable 'NRJ' should have been set in the optics files but is not defined.\" ) raise EnvironmentError ( \"No 'NRJ' variable found in scripts\" ) from madx_error logger . info ( f \"Powering Landau Octupoles, beam {beam} @ {madx.globals.nrj} GeV with {mo_current} A.\" ) strength = mo_current / madx . globals . Imax_MO * madx . globals . Kmax_MO / brho beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ): for fd in \"FD\" : octupole = f \"KO{fd}.{arc}\" logger . trace ( f \"Powering element '{octupole}' at {strength} Amps\" ) madx . globals [ octupole ] = strength if defective_arc and ( beam == 1 ): madx . globals [ \"KOD.A56B1\" ] = strength * 4.65 / 6 # defective MO group re_cycle_sequence def re_cycle_sequence ( madx : cpymad . madx . Madx , sequence : str = 'lhcb1' , start : str = 'IP3' ) -> None Re-cycle the provided sequence from a different starting point. Parameters: Name Type Description Default madx Madx an instantiated cpymad.madx.Madx object. None sequence str the sequence to re cycle. None start str element to start the new cycle from. None View Source def re_cycle_sequence ( madx : Madx , sequence : str = \"lhcb1\" , start : str = \"IP3\" ) -> None : \"\"\" Re-cycle the provided sequence from a different starting point. Args: madx (Madx): an instantiated cpymad.madx.Madx object. sequence (str): the sequence to re cycle. start (str): element to start the new cycle from. \"\"\" logger . debug ( f \"Re-cycling sequence ' { sequence } ' from {start}\" ) madx . command . seqedit ( sequence = sequence ) madx . command . flatten () madx . command . cycle ( start = start ) madx . command . ende dit () vary_independent_ir_quadrupoles def vary_independent_ir_quadrupoles ( madx : cpymad . madx . Madx , quad_numbers : Sequence [ int ], ip : int , sides : Sequence [ str ] = ( 'r' , 'l' ), beam : int = 1 ) -> None Send the vary commands for the desired quadrupoles in the IRs. The independent quadrupoles for which this is implemented are Q4 to Q13 included. This is useful to setup some specific matching involving these elements. It is necessary to have defined a 'brho' variable when creating your beams. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None quad_numbers Sequence[int] quadrupoles to be varied, by number (aka position from IP). None ip int the IP around which to apply the instructions. Defaults to 1. 1 sides Sequence[str] the sides of IP to act on. Should be R for right and L for left. Defaults to both sides of the IP. None beam int the beam for which to apply the instructions. Defaults to 1. 1 View Source def vary_independent_ir_quadrupoles ( madx : Madx , quad_numbers : Sequence [ int ], ip : int , sides : Sequence [ str ] = ( \"r\" , \"l\" ), beam : int = 1 ) -> None : \"\"\" Send the `vary` commands for the desired quadrupoles in the IRs. The independent quadrupoles for which this is implemented are Q4 to Q13 included. This is useful to setup some specific matching involving these elements. It is necessary to have defined a 'brho' variable when creating your beams. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. quad_numbers (Sequence[int]): quadrupoles to be varied, by number (aka position from IP). ip (int): the IP around which to apply the instructions. Defaults to 1. sides (Sequence[str]): the sides of IP to act on. Should be `R` for right and `L` for left. Defaults to both sides of the IP. beam (int): the beam for which to apply the instructions. Defaults to 1. \"\"\" if ( ip not in ( 1 , 2 , 5 , 8 ) or any ( side . upper () not in ( \"R\" , \"L\" ) for side in sides ) or any ( quad not in ( 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 ) for quad in quad_numbers ) ): logger . error ( \"Either the IP number of the side provided are invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'quad_numbers', 'ip', 'sides' argument\" ) logger . debug ( f \"Preparing a knob involving quadrupoles {quad_numbers}\" ) # Each quad has a specific power circuit used for their k1 boundaries power_circuits : Dict [ int , str ] = { 4 : \"mqy\" , 5 : \"mqml\" , 6 : \"mqml\" , 7 : \"mqm\" , 8 : \"mqml\" , 9 : \"mqm\" , 10 : \"mqml\" , 11 : \"mqtli\" , 12 : \"mqt\" , 13 : \"mqt\" , } for quad in quad_numbers : circuit = power_circuits [ quad ] for side in sides : logger . trace ( f \"Sending vary command for Q{quad}{side.upper()}{ip}\" ) madx . command . vary ( name = f \"kq{'t' if quad >= 11 else ''}{'l' if quad == 11 else ''}{quad}.{side}{ip}b{beam}\" , step = 1e-7 , lower = f \"-{circuit}.{'b' if quad == 7 else ''}{quad}{side}{ip}.b{beam}->kmax/brho\" , upper = f \"+{circuit}.{'b' if quad == 7 else ''}{quad}{side}{ip}.b{beam}->kmax/brho\" , )","title":"Special"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#module-pyhdtoolkitcpymadtoolsspecial","text":"Module cpymadtools.special Created on 2020.02.03 View Source \"\"\" Module cpymadtools.special -------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X actions with a cpymad.madx.Madx object, that are very specific to what I do in LHC and HLLHC use cases. \"\"\" from typing import List , Sequence import numpy as np from cpymad.madx import Madx from loguru import logger # ----- Setup Utlites ----- # def make_lhc_beams ( madx : Madx , energy : float = 7000 , emittance : float = 3.75e-6 , ** kwargs ) -> None : \"\"\" Define beams with default configuratons for `LHCB1` and `LHCB2` sequences. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. Defaults to 6500. emittance (float): emittance in meters, which will be used to calculate geometric emittance, then fed to the BEAM command. Keyword Args: Any keyword argument that can be given to the MAD-X BEAM command. \"\"\" logger . info ( \"Making default beams for 'lhcb1' and 'lhbc2' sequences\" ) madx . globals [ \"NRJ\" ] = energy madx . globals [ \"brho\" ] = energy * 1e9 / madx . globals . clight geometric_emit = madx . globals [ \"geometric_emit\" ] = emittance / ( energy / 0.938 ) for beam in ( 1 , 2 ): logger . trace ( f \"Defining beam for sequence 'lhcb { beam : d } '\" ) madx . command . beam ( sequence = f \"lhcb { beam : d } \" , particle = \"proton\" , bv = 1 if beam == 1 else - 1 , energy = energy , npart = 1.15e11 , ex = geometric_emit , ey = geometric_emit , sige = 4.5e-4 , ** kwargs , ) def power_landau_octupoles ( madx : Madx , mo_current : float , beam : int , defective_arc : bool = False ) -> None : \"\"\" Power the Landau octupoles in the (HL)LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. mo_current (float): MO powering in Amps. beam (int): beam to use. defective_arc: If set to `True`, the KOD in Arc 56 are powered for less Imax. \"\"\" try : brho = madx . globals . nrj * 1e9 / madx . globals . clight # clight is MAD-X constant except AttributeError as madx_error : logger . error ( \"The global MAD-X variable 'NRJ' should have been set in the optics files but is not defined.\" ) raise EnvironmentError ( \"No 'NRJ' variable found in scripts\" ) from madx_error logger . info ( f \"Powering Landau Octupoles, beam { beam } @ { madx . globals . nrj } GeV with { mo_current } A.\" ) strength = mo_current / madx . globals . Imax_MO * madx . globals . Kmax_MO / brho beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ): for fd in \"FD\" : octupole = f \"KO { fd } . { arc } \" logger . trace ( f \"Powering element ' { octupole } ' at { strength } Amps\" ) madx . globals [ octupole ] = strength if defective_arc and ( beam == 1 ): madx . globals [ \"KOD.A56B1\" ] = strength * 4.65 / 6 # defective MO group def deactivate_lhc_arc_sextupoles ( madx : Madx , beam : int ) -> None : \"\"\" Deactivate all arc sextupoles in the (HL)LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. beam (int): beam to use. \"\"\" # KSF1 and KSD2 - Strong sextupoles of sectors 81/12/45/56 # KSF2 and KSD1 - Weak sextupoles of sectors 81/12/45/56 # Rest: Weak sextupoles in sectors 78/23/34/67 logger . info ( f \"Deactivating all arc sextupoles for beam { beam } .\" ) beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ): for fd in \"FD\" : for i in ( 1 , 2 ): sextupole = f \"KS { fd }{ i : d } . { arc } \" logger . trace ( f \"De-powering element ' { sextupole } '\" ) madx . globals [ sextupole ] = 0.0 def apply_lhc_colinearity_knob ( madx : Madx , colinearity_knob_value : float = 0 , ir : int = None ) -> None : \"\"\" Applies the LHC colinearity knob. If you don't know what this is, you should not be using this function. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. colinearity_knob_value (float): Units of the colinearity knob to apply. Defaults to 0 so users don't mess up local coupling by mistake. This should be a positive integer, normally between 1 and 10. ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. \"\"\" logger . info ( f \"Applying Colinearity knob with a unit setting of { colinearity_knob_value } \" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) knob_variables = ( f \"KQSX3.R { ir : d } \" , f \"KQSX3.L { ir : d } \" ) # MQSX IP coupling correctors right_knob , left_knob = knob_variables madx . globals [ right_knob ] = colinearity_knob_value * 1e-4 logger . trace ( f \"Set ' { right_knob } ' to { madx . globals [ right_knob ] } \" ) madx . globals [ left_knob ] = - 1 * colinearity_knob_value * 1e-4 logger . trace ( f \"Set ' { left_knob } ' to { madx . globals [ left_knob ] } \" ) def apply_lhc_rigidity_waist_shift_knob ( madx : Madx , rigidty_waist_shift_value : float = 0 , ir : int = None , side : str = \"left\" ) -> None : \"\"\" Applies the LHC rigidity waist shift knob, moving the waist left or right of IP. If you don't know what this is, you should not be using this function. The waist shift is done by unbalancing the triplet powering knob of the left and right-hand sides of the IP. Warning: Applying the shift will modify your tunes and most likely flip them, making a subsequent matching impossible if your lattice has coupling. To avoid this, match to tunes split further apart before applying the waist shift knob, and then match to the desired working point. For instance for the LHC, matching to (62.27, 60.36) before applying and afterwards rematching to (62.31, 60.32) usually works well. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. rigidty_waist_shift_value (float): Units of the rigidity waist shift knob (positive values only). ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. side (str): Which side of the IP to move the waist to, determines a sign in the calculation. Defaults to 'left', which means s_waist < s_ip (and setting it to 'right' would move the waist to s_waist > s_ip). \"\"\" logger . info ( f \"Applying Rigidity Waist Shift knob with a unit setting of { rigidty_waist_shift_value } \" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) right_knob , left_knob = f \"kqx.r { ir : d } \" , f \"kqx.l { ir : d } \" # IP triplet default knob (no trims) current_right_knob = madx . globals [ right_knob ] current_left_knob = madx . globals [ left_knob ] if side . lower () == \"left\" : madx . globals [ right_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_right_knob madx . globals [ left_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_left_knob elif side . lower () == \"right\" : madx . globals [ right_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_right_knob madx . globals [ left_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_left_knob else : logger . error ( f \"Given side ' { side } ' invalid, only 'left' and 'right' are accepted values.\" ) raise ValueError ( \"Invalid value for parameter 'side'.\" ) logger . trace ( f \"Set ' { right_knob } ' to { madx . globals [ right_knob ] } \" ) logger . trace ( f \"Set ' { left_knob } ' to { madx . globals [ left_knob ] } \" ) def apply_lhc_coupling_knob ( madx : Madx , coupling_knob : float = 0 , beam : int = 1 , telescopic_squeeze : bool = False ) -> None : \"\"\" Applies the LHC coupling knob to reach the desired C- value. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. coupling_knob (float): Desired value for the Cminus, typically a few units of 1E-3. Defaults to 0 so users don't mess up coupling by mistake beam (int): beam to apply the knob to, defaults to beam 1. telescopic_squeeze (bool): if set to True, uses the knobs for Telescopic Squeeze configuration. Defaults to False. \"\"\" logger . info ( \"Applying coupling knob\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) suffix = \"_sq\" if telescopic_squeeze else \"\" knob_name = f \"CMRS.b { beam : d }{ suffix } \" logger . trace ( f \"Knob ' { knob_name } ' is { madx . globals [ knob_name ] } before implementation\" ) madx . globals [ knob_name ] = coupling_knob logger . trace ( f \"Set ' { knob_name } ' to { madx . globals [ knob_name ] } \" ) def install_ac_dipole ( madx : Madx , deltaqx : float , deltaqy : float , sigma_x : float , sigma_y : float , geometric_emit : float = None , start_turn : int = 100 , ramp_turns : int = 2000 , top_turns : int = 6600 , ) -> None : \"\"\" Installs an AC dipole for BEAM 1 ONLY. This function assumes that you have already defined lhcb1, made a beam for it (BEAM command or `make_lhc_beams` function) and matched to your desired working point. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. deltaqx (float): the deltaQx (horizontal tune excitation) used by the AC dipole. deltaqy (float): the deltaQy (vertical tune excitation) used by the AC dipole. sigma_x (float): the horizontal amplitude to drive the beam to, in bunch sigma. sigma_y (float): the vertical amplitude to drive the beam to, in bunch sigma. geometric_emit (float): the geometric emittance that was used when defining the beam. If not provided, it is assumed that 'geometric_emit' is a defined global in MAD-X, and the value will be directly queried from the internal tables. start_turn (int): the turn at which to start ramping up the AC dipole. Defaults to 100. ramp_turns (int): the number of turns to use for the ramp-up and the ramp-down of the AC dipole. This number is important in order to preserve the adiabaticity of the cycle. Defaults to 2000 as in the LHC. top_turns (int): the number of turns to drive the beam for. Defaults to 6600 as in the LHC. \"\"\" if top_turns > 6600 : logger . warning ( f \"Configuring the AC Dipole for { top_turns } of driving is fine for MAD-X but is \" \"higher than what the device can do in the (HL)LHC! Beware.\" ) ramp1 , ramp2 = start_turn , start_turn + ramp_turns ramp3 = ramp2 + top_turns ramp4 = ramp3 + ramp_turns logger . debug ( \"Retrieving tunes from internal tables\" ) q1 , q2 = madx . table . summ . q1 [ 0 ], madx . table . summ . q2 [ 0 ] logger . trace ( f \"Retrieved values are q1 = { q1 } , q2 = { q2 } \" ) q1_dipole , q2_dipole = q1 + deltaqx , q2 + deltaqy if not geometric_emit : logger . debug ( \"No value provided for the geometric emittance used when creating the beam, the value will be \" \"queried from MAD-X's global 'geometric_emit'\" ) geometric_emit = madx . globals [ \"geometric_emit\" ] logger . info ( f \"Installing AC Dipole to drive the tunes to Qx_D = { q1_dipole } | Qy_D = { q2_dipole } \" ) madx . input ( f \"MKACH.6L4.B1: hacdipole, l=0, freq:= { q1_dipole } , lag=0, volt:=voltx, ramp1= { ramp1 } , \" f \"ramp2= { ramp2 } , ramp3= { ramp3 } , ramp4= { ramp4 } ;\" ) madx . input ( f \"MKACV.6L4.B1: vacdipole, l=0, freq:= { q2_dipole } , lag=0, volt:=volty, ramp1= { ramp1 } , \" f \"ramp2= { ramp2 } , ramp3= { ramp3 } , ramp4= { ramp4 } ;\" ) madx . command . seqedit ( sequence = \"lhcb1\" ) madx . command . flatten () madx . command . install ( element = \"MKACH.6L4.B1\" , at = \"0.0\" , from_ = \"MKQA.6L4.B1\" ) madx . command . install ( element = \"MKACV.6L4.B1\" , at = \"0.0\" , from_ = \"MKQA.6L4.B1\" ) madx . command . endedit () logger . trace ( \"Querying BETX and BETY at AC Dipole location\" ) madx . input ( \"betax_acd = table(twiss, MKQA.6L4.B1, betx);\" ) madx . input ( \"betay_acd = table(twiss, MKQA.6L4.B1, bety);\" ) betax_acd = madx . globals [ \"betax_acd\" ] betay_acd = madx . globals [ \"betay_acd\" ] brho = madx . sequence . lhcb1 . beam . brho voltx = sigma_x * np . sqrt ( geometric_emit ) * brho * np . abs ( deltaqx ) * 4 * np . pi / np . sqrt ( betax_acd ) volty = sigma_y * np . sqrt ( geometric_emit ) * brho * np . abs ( deltaqy ) * 4 * np . pi / np . sqrt ( betay_acd ) madx . globals [ \"voltx\" ] = voltx madx . globals [ \"volty\" ] = volty def vary_independent_ir_quadrupoles ( madx : Madx , quad_numbers : Sequence [ int ], ip : int , sides : Sequence [ str ] = ( \"r\" , \"l\" ), beam : int = 1 ) -> None : \"\"\" Send the `vary` commands for the desired quadrupoles in the IRs. The independent quadrupoles for which this is implemented are Q4 to Q13 included. This is useful to setup some specific matching involving these elements. It is necessary to have defined a 'brho' variable when creating your beams. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. quad_numbers (Sequence[int]): quadrupoles to be varied, by number (aka position from IP). ip (int): the IP around which to apply the instructions. Defaults to 1. sides (Sequence[str]): the sides of IP to act on. Should be `R` for right and `L` for left. Defaults to both sides of the IP. beam (int): the beam for which to apply the instructions. Defaults to 1. \"\"\" if ( ip not in ( 1 , 2 , 5 , 8 ) or any ( side . upper () not in ( \"R\" , \"L\" ) for side in sides ) or any ( quad not in ( 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 ) for quad in quad_numbers ) ): logger . error ( \"Either the IP number of the side provided are invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'quad_numbers', 'ip', 'sides' argument\" ) logger . debug ( f \"Preparing a knob involving quadrupoles { quad_numbers } \" ) # Each quad has a specific power circuit used for their k1 boundaries power_circuits : Dict [ int , str ] = { 4 : \"mqy\" , 5 : \"mqml\" , 6 : \"mqml\" , 7 : \"mqm\" , 8 : \"mqml\" , 9 : \"mqm\" , 10 : \"mqml\" , 11 : \"mqtli\" , 12 : \"mqt\" , 13 : \"mqt\" , } for quad in quad_numbers : circuit = power_circuits [ quad ] for side in sides : logger . trace ( f \"Sending vary command for Q { quad }{ side . upper () }{ ip } \" ) madx . command . vary ( name = f \"kq { 't' if quad >= 11 else '' }{ 'l' if quad == 11 else '' }{ quad } . { side }{ ip } b { beam } \" , step = 1e-7 , lower = f \"- { circuit } . { 'b' if quad == 7 else '' }{ quad }{ side }{ ip } .b { beam } ->kmax/brho\" , upper = f \"+ { circuit } . { 'b' if quad == 7 else '' }{ quad }{ side }{ ip } .b { beam } ->kmax/brho\" , ) # ----- Output Utilities ----- # def make_sixtrack_output ( madx : Madx , energy : int ) -> None : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Prepare output for sixtrack run. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. \"\"\" logger . info ( \"Preparing outputs for SixTrack\" ) logger . debug ( \"Powering RF cavities\" ) madx . globals [ \"VRF400\" ] = 8 if energy < 5000 else 16 # is 6 at injection for protons iirc? madx . globals [ \"LAGRF400.B1\" ] = 0.5 # cavity phase difference in units of 2pi madx . globals [ \"LAGRF400.B2\" ] = 0.0 logger . debug ( \"Executing TWISS and SIXTRACK commands\" ) madx . twiss () # used by sixtrack madx . sixtrack ( cavall = True , radius = 0.017 ) # this value is only ok for HL(LHC) magnet radius # ----- Miscellaneous Utilities ----- # def make_lhc_thin ( madx : Madx , sequence : str , slicefactor : int = 1 , ** kwargs ) -> None : \"\"\" Makethin for the LHC sequence as previously done in MAD-X macros. This will use the `teapot` style and will enforce `makedipedge`. Args: madx (Madx): an instantiated cpymad.madx.Madx object. sequence (str): the sequence to use for the MAKETHIN command. slicefactor (int): the slice factor to apply in makethin. Defaults to 1. Keyword Args: The keyword arguments for the MAD-X MAKETHN commands, namely `style` (will default to `teapot`) and the `makedipedge` flag (will default to True). \"\"\" logger . info ( f \"Slicing sequence ' { sequence } '\" ) madx . select ( flag = \"makethin\" , clear = True ) four_slices_patterns = [ r \"mbx\\.\" , r \"mbrb\\.\" , r \"mbrc\\.\" , r \"mbrs\\.\" ] four_slicefactor_patterns = [ r \"mqwa\\.\" , r \"mqwb\\.\" , r \"mqy\\.\" , r \"mqm\\.\" , r \"mqmc\\.\" , r \"mqml\\.\" , r \"mqtlh\\.\" , r \"mqtli\\.\" , r \"mqt\\.\" , ] logger . trace ( \"Defining slices for general MB and MQ elements\" ) madx . select ( flag = \"makethin\" , class_ = \"MB\" , slice = 2 ) madx . select ( flag = \"makethin\" , class_ = \"MQ\" , slice = 2 * slicefactor ) logger . trace ( \"Defining slices for triplets\" ) madx . select ( flag = \"makethin\" , class_ = \"mqxa\" , slice = 16 * slicefactor ) madx . select ( flag = \"makethin\" , class_ = \"mqxb\" , slice = 16 * slicefactor ) logger . trace ( \"Defining slices for various specifc mb elements\" ) for pattern in four_slices_patterns : madx . select ( flag = \"makethin\" , pattern = pattern , slice = 4 ) logger . trace ( \"Defining slices for varous specifc mq elements\" ) for pattern in four_slicefactor_patterns : madx . select ( flag = \"makethin\" , pattern = pattern , slice = 4 * slicefactor ) madx . use ( sequence = sequence ) style = kwargs . get ( \"style\" , \"teapot\" ) makedipedge = kwargs . get ( \"makedipedge\" , False ) # defaults to False to compensate default TEAPOT style madx . command . makethin ( sequence = sequence , style = style , makedipedge = makedipedge ) def re_cycle_sequence ( madx : Madx , sequence : str = \"lhcb1\" , start : str = \"IP3\" ) -> None : \"\"\" Re-cycle the provided sequence from a different starting point. Args: madx (Madx): an instantiated cpymad.madx.Madx object. sequence (str): the sequence to re cycle. start (str): element to start the new cycle from. \"\"\" logger . debug ( f \"Re-cycling sequence ' { sequence } ' from { start } \" ) madx . command . seqedit ( sequence = sequence ) madx . command . flatten () madx . command . cycle ( start = start ) madx . command . endedit () def match_no_coupling_through_ripkens ( madx : Madx , sequence : str = None , location : str = None , vary_knobs : Sequence [ str ] = None ) -> None : \"\"\" Matching routine to get cross-term Ripken parameters beta_12 and beta_21 to be 0 at a given location. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. sequence (str): name of the sequence to activate for the matching. location (str): the name of the element at which one wants the cross-term Ripkens to be 0. vary_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. \"\"\" logger . info ( f \"Matching Ripken parameters for no coupling at location { location } \" ) logger . debug ( \"Creating macro tu update Ripkens\" ) madx . input ( \"do_ripken: macro = {twiss, ripken=True;}\" ) # cpymad needs .input for macros logger . debug ( \"Matching Parameters\" ) madx . command . match ( sequence = sequence , use_macro = True , chrom = True ) for knob in vary_knobs : madx . command . vary ( name = knob ) madx . command . use_macro ( name = \"do_ripken\" ) madx . input ( f \"constraint, expr=table(twiss, { location } , beta12)=0\" ) # need input else includes \" and fails madx . input ( f \"constraint, expr=table(twiss, { location } , beta21)=0\" ) # need input else includes \" and fails madx . command . lmdif ( calls = 500 , tolerance = 1e-21 ) madx . command . endmatch () # ----- Helpers ----- # def _all_lhc_arcs ( beam : int ) -> List [ str ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Names of all LHC arcs for a given beam. Args: beam (int): beam to get names for. Returns: The list of names. \"\"\" return [ f \"A { i + 1 }{ ( i + 1 ) % 8 + 1 } B { beam : d } \" for i in range ( 8 )] def _get_k_strings ( start : int = 0 , stop : int = 8 , orientation : str = \"both\" ) -> List [ str ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Returns the list of K-strings for various magnets and orders (K1L, K2SL etc strings). Args: start (int): the starting order, defaults to 0. stop (int): the order to go up to, defaults to 8. orientation (str): magnet orientation, can be 'straight', 'skew' or 'both'. Defaults to 'both'. Returns: The list of names as strings. \"\"\" if orientation not in ( \"straight\" , \"skew\" , \"both\" ,): logger . error ( f \"Orientation ' { orientation } ' is not accepted, should be one of 'straight', 'skew', 'both'.\" ) raise ValueError ( \"Invalid 'orientation' parameter\" ) if orientation == \"straight\" : orientation = ( \"\" ,) elif orientation == \"skew\" : orientation = ( \"S\" ,) else : # both orientation = ( \"\" , \"S\" ) return [ f \"K { i : d }{ s : s } L\" for i in range ( start , stop ) for s in orientation ]","title":"Module pyhdtoolkit.cpymadtools.special"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#apply_lhc_colinearity_knob","text":"def apply_lhc_colinearity_knob ( madx : cpymad . madx . Madx , colinearity_knob_value : float = 0 , ir : int = None ) -> None Applies the LHC colinearity knob. If you don't know what this is, you should not be using this function. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None colinearity_knob_value float Units of the colinearity knob to apply. Defaults to 0 so users don't mess up local coupling by mistake. This should be a positive integer, normally between 1 and 10. None ir int The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. None View Source def apply_lhc_colinearity_knob ( madx : Madx , colinearity_knob_value : float = 0 , ir : int = None ) -> None : \"\"\" Applies the LHC colinearity knob. If you don't know what this is, you should not be using this function. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. colinearity_knob_value (float): Units of the colinearity knob to apply. Defaults to 0 so users don't mess up local coupling by mistake. This should be a positive integer, normally between 1 and 10. ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. \"\"\" logger . info ( f \"Applying Colinearity knob with a unit setting of {colinearity_knob_value}\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) knob_variables = ( f \"KQSX3.R{ir:d}\" , f \"KQSX3.L{ir:d}\" ) # MQSX IP coupling correctors right_knob , left_knob = knob_variables madx . globals [ right_knob ] = colinearity_knob_value * 1e-4 logger . trace ( f \"Set '{right_knob}' to {madx.globals[right_knob]}\" ) madx . globals [ left_knob ] = - 1 * colinearity_knob_value * 1e-4 logger . trace ( f \"Set '{left_knob}' to {madx.globals[left_knob]}\" )","title":"apply_lhc_colinearity_knob"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#apply_lhc_coupling_knob","text":"def apply_lhc_coupling_knob ( madx : cpymad . madx . Madx , coupling_knob : float = 0 , beam : int = 1 , telescopic_squeeze : bool = False ) -> None Applies the LHC coupling knob to reach the desired C- value. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. coupling_knob (float): Desired value for the Cminus, typically a few units of 1E-3. Defaults to 0 so users don't mess up coupling by mistake beam (int): beam to apply the knob to, defaults to beam 1. telescopic_squeeze (bool): if set to True, uses the knobs for Telescopic Squeeze configuration. Defaults to False. View Source def apply_lhc_coupling_knob ( madx : Madx , coupling_knob : float = 0 , beam : int = 1 , telescopic_squeeze : bool = False ) -> None : \"\"\" Applies the LHC coupling knob to reach the desired C- value. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. coupling_knob (float): Desired value for the Cminus, typically a few units of 1E-3. Defaults to 0 so users don't mess up coupling by mistake beam (int): beam to apply the knob to, defaults to beam 1. telescopic_squeeze (bool): if set to True, uses the knobs for Telescopic Squeeze configuration. Defaults to False. \"\"\" logger . info ( \"Applying coupling knob\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) suffix = \"_sq\" if telescopic_squeeze else \"\" knob_name = f \"CMRS.b{beam:d}{suffix}\" logger . trace ( f \"Knob '{knob_name}' is {madx.globals[knob_name]} before implementation\" ) madx . globals [ knob_name ] = coupling_knob logger . trace ( f \"Set '{knob_name}' to {madx.globals[knob_name]}\" )","title":"apply_lhc_coupling_knob"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#apply_lhc_rigidity_waist_shift_knob","text":"def apply_lhc_rigidity_waist_shift_knob ( madx : cpymad . madx . Madx , rigidty_waist_shift_value : float = 0 , ir : int = None , side : str = 'left' ) -> None Applies the LHC rigidity waist shift knob, moving the waist left or right of IP. If you don't know what this is, you should not be using this function. The waist shift is done by unbalancing the triplet powering knob of the left and right-hand sides of the IP. Warning: Applying the shift will modify your tunes and most likely flip them, making a subsequent matching impossible if your lattice has coupling. To avoid this, match to tunes split further apart before applying the waist shift knob, and then match to the desired working point. For instance for the LHC, matching to (62.27, 60.36) before applying and afterwards rematching to (62.31, 60.32) usually works well. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None rigidty_waist_shift_value float Units of the rigidity waist shift knob (positive values only). None ir int The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. None side str Which side of the IP to move the waist to, determines a sign in the calculation. Defaults to 'left', which means s_waist < s_ip (and setting it to 'right' would move the waist to s_waist > s_ip). None View Source def apply_lhc_rigidity_waist_shift_knob ( madx : Madx , rigidty_waist_shift_value : float = 0 , ir : int = None , side : str = \"left\" ) -> None : \"\"\" Applies the LHC rigidity waist shift knob, moving the waist left or right of IP. If you don't know what this is, you should not be using this function. The waist shift is done by unbalancing the triplet powering knob of the left and right-hand sides of the IP. Warning: Applying the shift will modify your tunes and most likely flip them, making a subsequent matching impossible if your lattice has coupling. To avoid this, match to tunes split further apart before applying the waist shift knob, and then match to the desired working point. For instance for the LHC, matching to (62.27, 60.36) before applying and afterwards rematching to (62.31, 60.32) usually works well. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. rigidty_waist_shift_value (float): Units of the rigidity waist shift knob (positive values only). ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. side (str): Which side of the IP to move the waist to, determines a sign in the calculation. Defaults to 'left', which means s_waist < s_ip (and setting it to 'right' would move the waist to s_waist > s_ip). \"\"\" logger . info ( f \"Applying Rigidity Waist Shift knob with a unit setting of {rigidty_waist_shift_value}\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) right_knob , left_knob = f \"kqx.r{ir:d}\" , f \"kqx.l{ir:d}\" # IP triplet default knob ( no trims ) current_right_knob = madx . globals [ right_knob ] current_left_knob = madx . globals [ left_knob ] if side . lower () == \"left\" : madx . globals [ right_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_right_knob madx . globals [ left_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_left_knob elif side . lower () == \"right\" : madx . globals [ right_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_right_knob madx . globals [ left_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_left_knob else : logger . error ( f \"Given side '{side}' invalid, only 'left' and 'right' are accepted values.\" ) raise ValueError ( \"Invalid value for parameter 'side'.\" ) logger . trace ( f \"Set '{right_knob}' to {madx.globals[right_knob]}\" ) logger . trace ( f \"Set '{left_knob}' to {madx.globals[left_knob]}\" )","title":"apply_lhc_rigidity_waist_shift_knob"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#deactivate_lhc_arc_sextupoles","text":"def deactivate_lhc_arc_sextupoles ( madx : cpymad . madx . Madx , beam : int ) -> None Deactivate all arc sextupoles in the (HL)LHC. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None beam int beam to use. None View Source def deactivate_lhc_arc_sextupoles ( madx : Madx , beam : int ) -> None : \"\"\" Deactivate all arc sextupoles in the (HL)LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. beam (int): beam to use. \"\"\" # KSF1 and KSD2 - Strong sextupoles of sectors 81 / 12 / 45 / 56 # KSF2 and KSD1 - Weak sextupoles of sectors 81 / 12 / 45 / 56 # Rest : Weak sextupoles in sectors 78 / 23 / 34 / 67 logger . info ( f \"Deactivating all arc sextupoles for beam {beam}.\" ) beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ) : for fd in \"FD\" : for i in ( 1 , 2 ) : sextupole = f \"KS{fd}{i:d}.{arc}\" logger . trace ( f \"De-powering element '{sextupole}'\" ) madx . globals [ sextupole ] = 0.0","title":"deactivate_lhc_arc_sextupoles"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#install_ac_dipole","text":"def install_ac_dipole ( madx : cpymad . madx . Madx , deltaqx : float , deltaqy : float , sigma_x : float , sigma_y : float , geometric_emit : float = None , start_turn : int = 100 , ramp_turns : int = 2000 , top_turns : int = 6600 ) -> None Installs an AC dipole for BEAM 1 ONLY. This function assumes that you have already defined lhcb1, made a beam for it (BEAM command or make_lhc_beams function) and matched to your desired working point. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None deltaqx float the deltaQx (horizontal tune excitation) used by the AC dipole. None deltaqy float the deltaQy (vertical tune excitation) used by the AC dipole. None sigma_x float the horizontal amplitude to drive the beam to, in bunch sigma. None sigma_y float the vertical amplitude to drive the beam to, in bunch sigma. None geometric_emit float the geometric emittance that was used when defining the beam. If not provided, it is assumed that 'geometric_emit' is a defined global in MAD-X, and the value will be directly queried from the internal tables. None start_turn int the turn at which to start ramping up the AC dipole. Defaults to 100. 100 ramp_turns int the number of turns to use for the ramp-up and the ramp-down of the AC dipole. This number is important in order to preserve the adiabaticity of the cycle. Defaults to 2000 as in the LHC. None top_turns int the number of turns to drive the beam for. Defaults to 6600 as in the LHC. 6600 as in the LHC View Source def install_ac_dipole ( madx : Madx , deltaqx : float , deltaqy : float , sigma_x : float , sigma_y : float , geometric_emit : float = None , start_turn : int = 100 , ramp_turns : int = 2000 , top_turns : int = 6600 , ) -> None : \" \"\" Installs an AC dipole for BEAM 1 ONLY. This function assumes that you have already defined lhcb1, made a beam for it (BEAM command or `make_lhc_beams` function) and matched to your desired working point. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. deltaqx (float): the deltaQx (horizontal tune excitation) used by the AC dipole. deltaqy (float): the deltaQy (vertical tune excitation) used by the AC dipole. sigma_x (float): the horizontal amplitude to drive the beam to, in bunch sigma. sigma_y (float): the vertical amplitude to drive the beam to, in bunch sigma. geometric_emit (float): the geometric emittance that was used when defining the beam. If not provided, it is assumed that 'geometric_emit' is a defined global in MAD-X, and the value will be directly queried from the internal tables. start_turn (int): the turn at which to start ramping up the AC dipole. Defaults to 100. ramp_turns (int): the number of turns to use for the ramp-up and the ramp-down of the AC dipole. This number is important in order to preserve the adiabaticity of the cycle. Defaults to 2000 as in the LHC. top_turns (int): the number of turns to drive the beam for. Defaults to 6600 as in the LHC. \"\" \" if top_turns > 6600 : logger . warning ( f \"Configuring the AC Dipole for {top_turns} of driving is fine for MAD-X but is \" \"higher than what the device can do in the (HL)LHC! Beware.\" ) ramp1 , ramp2 = start_turn , start_turn + ramp_turns ramp3 = ramp2 + top_turns ramp4 = ramp3 + ramp_turns logger . debug ( \"Retrieving tunes from internal tables\" ) q1 , q2 = madx . table . summ . q1 [ 0 ] , madx . table . summ . q2 [ 0 ] logger . trace ( f \"Retrieved values are q1 = {q1}, q2 = {q2}\" ) q1_dipole , q2_dipole = q1 + deltaqx , q2 + deltaqy if not geometric_emit : logger . debug ( \"No value provided for the geometric emittance used when creating the beam, the value will be \" \"queried from MAD-X's global 'geometric_emit'\" ) geometric_emit = madx . globals [ \"geometric_emit\" ] logger . info ( f \"Installing AC Dipole to drive the tunes to Qx_D = {q1_dipole} | Qy_D = {q2_dipole}\" ) madx . input ( f \"MKACH.6L4.B1: hacdipole, l=0, freq:={q1_dipole}, lag=0, volt:=voltx, ramp1={ramp1}, \" f \"ramp2={ramp2}, ramp3={ramp3}, ramp4={ramp4};\" ) madx . input ( f \"MKACV.6L4.B1: vacdipole, l=0, freq:={q2_dipole}, lag=0, volt:=volty, ramp1={ramp1}, \" f \"ramp2={ramp2}, ramp3={ramp3}, ramp4={ramp4};\" ) madx . command . seqedit ( sequence = \"lhcb1\" ) madx . command . flatten () madx . command . install ( element = \"MKACH.6L4.B1\" , at = \"0.0\" , from_ = \"MKQA.6L4.B1\" ) madx . command . install ( element = \"MKACV.6L4.B1\" , at = \"0.0\" , from_ = \"MKQA.6L4.B1\" ) madx . command . endedit () logger . trace ( \"Querying BETX and BETY at AC Dipole location\" ) madx . input ( \"betax_acd = table(twiss, MKQA.6L4.B1, betx);\" ) madx . input ( \"betay_acd = table(twiss, MKQA.6L4.B1, bety);\" ) betax_acd = madx . globals [ \"betax_acd\" ] betay_acd = madx . globals [ \"betay_acd\" ] brho = madx . sequence . lhcb1 . beam . brho voltx = sigma_x * np . sqrt ( geometric_emit ) * brho * np . abs ( deltaqx ) * 4 * np . pi / np . sqrt ( betax_acd ) volty = sigma_y * np . sqrt ( geometric_emit ) * brho * np . abs ( deltaqy ) * 4 * np . pi / np . sqrt ( betay_acd ) madx . globals [ \"voltx\" ] = voltx madx . globals [ \"volty\" ] = volty","title":"install_ac_dipole"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#make_lhc_beams","text":"def make_lhc_beams ( madx : cpymad . madx . Madx , energy : float = 7000 , emittance : float = 3.75e-06 , ** kwargs ) -> None Define beams with default configuratons for LHCB1 and LHCB2 sequences. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. Defaults to 6500. emittance (float): emittance in meters, which will be used to calculate geometric emittance, then fed to the BEAM command. Keyword Args: Any keyword argument that can be given to the MAD-X BEAM command. View Source def make_lhc_beams ( madx : Madx , energy : float = 7000 , emittance : float = 3.75e-6 , ** kwargs ) -> None : \" \"\" Define beams with default configuratons for `LHCB1` and `LHCB2` sequences. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. Defaults to 6500. emittance (float): emittance in meters, which will be used to calculate geometric emittance, then fed to the BEAM command. Keyword Args: Any keyword argument that can be given to the MAD-X BEAM command. \"\" \" logger . info ( \"Making default beams for 'lhcb1' and 'lhbc2' sequences\" ) madx . globals [ \"NRJ\" ] = energy madx . globals [ \"brho\" ] = energy * 1e9 / madx . globals . clight geometric_emit = madx . globals [ \"geometric_emit\" ] = emittance / ( energy / 0.938 ) for beam in ( 1 , 2 ) : logger . trace ( f \"Defining beam for sequence 'lhcb{beam:d}'\" ) madx . command . beam ( sequence = f \"lhcb{beam:d}\" , particle = \"proton\" , bv = 1 if beam == 1 else - 1 , energy = energy , npart = 1.15e11 , ex = geometric_emit , ey = geometric_emit , sige = 4.5e-4 , ** kwargs , )","title":"make_lhc_beams"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#make_lhc_thin","text":"def make_lhc_thin ( madx : cpymad . madx . Madx , sequence : str , slicefactor : int = 1 , ** kwargs ) -> None Makethin for the LHC sequence as previously done in MAD-X macros. This will use the teapot style and will enforce makedipedge . Args: madx (Madx): an instantiated cpymad.madx.Madx object. sequence (str): the sequence to use for the MAKETHIN command. slicefactor (int): the slice factor to apply in makethin. Defaults to 1. Keyword Args: The keyword arguments for the MAD-X MAKETHN commands, namely style (will default to teapot ) and the makedipedge flag (will default to True). View Source def make_lhc_thin ( madx : Madx , sequence : str , slicefactor : int = 1 , ** kwargs ) -> None : \" \"\" Makethin for the LHC sequence as previously done in MAD-X macros. This will use the `teapot` style and will enforce `makedipedge`. Args: madx (Madx): an instantiated cpymad.madx.Madx object. sequence (str): the sequence to use for the MAKETHIN command. slicefactor (int): the slice factor to apply in makethin. Defaults to 1. Keyword Args: The keyword arguments for the MAD-X MAKETHN commands, namely `style` (will default to `teapot`) and the `makedipedge` flag (will default to True). \"\" \" logger . info ( f \"Slicing sequence '{sequence}'\" ) madx . select ( flag = \"makethin\" , clear = True ) four_slices_patterns = [ r \"mbx \\ .\" , r \"mbrb \\ .\" , r \"mbrc \\ .\" , r \"mbrs \\ .\" ] four_slicefactor_patterns = [ r \"mqwa \\ .\" , r \"mqwb \\ .\" , r \"mqy \\ .\" , r \"mqm \\ .\" , r \"mqmc \\ .\" , r \"mqml \\ .\" , r \"mqtlh \\ .\" , r \"mqtli \\ .\" , r \"mqt \\ .\" , ] logger . trace ( \"Defining slices for general MB and MQ elements\" ) madx . select ( flag = \"makethin\" , class_ = \"MB\" , slice = 2 ) madx . select ( flag = \"makethin\" , class_ = \"MQ\" , slice = 2 * slicefactor ) logger . trace ( \"Defining slices for triplets\" ) madx . select ( flag = \"makethin\" , class_ = \"mqxa\" , slice = 16 * slicefactor ) madx . select ( flag = \"makethin\" , class_ = \"mqxb\" , slice = 16 * slicefactor ) logger . trace ( \"Defining slices for various specifc mb elements\" ) for pattern in four_slices_patterns : madx . select ( flag = \"makethin\" , pattern = pattern , slice = 4 ) logger . trace ( \"Defining slices for varous specifc mq elements\" ) for pattern in four_slicefactor_patterns : madx . select ( flag = \"makethin\" , pattern = pattern , slice = 4 * slicefactor ) madx . use ( sequence = sequence ) style = kwargs . get ( \"style\" , \"teapot\" ) makedipedge = kwargs . get ( \"makedipedge\" , False ) # defaults to False to compensate default TEAPOT style madx . command . makethin ( sequence = sequence , style = style , makedipedge = makedipedge )","title":"make_lhc_thin"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#make_sixtrack_output","text":"def make_sixtrack_output ( madx : cpymad . madx . Madx , energy : int ) -> None INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Prepare output for sixtrack run. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None energy float beam energy in GeV. None View Source def make_sixtrack_output ( madx : Madx , energy : int ) -> None : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Prepare output for sixtrack run. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. \"\"\" logger . info ( \"Preparing outputs for SixTrack\" ) logger . debug ( \"Powering RF cavities\" ) madx . globals [ \"VRF400\" ] = 8 if energy < 5000 else 16 # is 6 at injection for protons iirc ? madx . globals [ \"LAGRF400.B1\" ] = 0.5 # cavity phase difference in units of 2 pi madx . globals [ \"LAGRF400.B2\" ] = 0.0 logger . debug ( \"Executing TWISS and SIXTRACK commands\" ) madx . twiss () # used by sixtrack madx . sixtrack ( cavall = True , radius = 0.017 ) # this value is only ok for HL ( LHC ) magnet radius","title":"make_sixtrack_output"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#match_no_coupling_through_ripkens","text":"def match_no_coupling_through_ripkens ( madx : cpymad . madx . Madx , sequence : str = None , location : str = None , vary_knobs : Sequence [ str ] = None ) -> None Matching routine to get cross-term Ripken parameters beta_12 and beta_21 to be 0 at a given location. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None sequence str name of the sequence to activate for the matching. None location str the name of the element at which one wants the cross-term Ripkens to be 0. None vary_knobs Sequence[str] the variables names to 'vary' in the MADX routine. None View Source def match_no_coupling_through_ripkens ( madx : Madx , sequence : str = None , location : str = None , vary_knobs : Sequence [ str ] = None ) -> None : \"\"\" Matching routine to get cross-term Ripken parameters beta_12 and beta_21 to be 0 at a given location. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. sequence (str): name of the sequence to activate for the matching. location (str): the name of the element at which one wants the cross-term Ripkens to be 0. vary_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. \"\"\" logger . info ( f \"Matching Ripken parameters for no coupling at location {location}\" ) logger . debug ( \"Creating macro tu update Ripkens\" ) madx . input ( \"do_ripken: macro = {twiss, ripken=True;}\" ) # cpymad needs . input for macros logger . debug ( \"Matching Parameters\" ) madx . command . match ( sequence = sequence , use_macro = True , chrom = True ) for knob in vary_knobs : madx . command . vary ( name = knob ) madx . command . use_macro ( name = \"do_ripken\" ) madx . input ( f \"constraint, expr=table(twiss, {location}, beta12)=0\" ) # need input else includes \" and fails madx.input(f\" constraint , expr = table ( twiss , { location } , beta21 ) = 0 \") # need input else includes \" and fails madx . command . lmdif ( calls = 500 , tolerance = 1e-21 ) madx . command . endmatch ()","title":"match_no_coupling_through_ripkens"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#power_landau_octupoles","text":"def power_landau_octupoles ( madx : cpymad . madx . Madx , mo_current : float , beam : int , defective_arc : bool = False ) -> None Power the Landau octupoles in the (HL)LHC. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None mo_current float MO powering in Amps. None beam int beam to use. None defective_arc None If set to True , the KOD in Arc 56 are powered for less Imax. None View Source def power_landau_octupoles ( madx : Madx , mo_current : float , beam : int , defective_arc : bool = False ) -> None : \"\"\" Power the Landau octupoles in the (HL)LHC. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. mo_current (float): MO powering in Amps. beam (int): beam to use. defective_arc: If set to `True`, the KOD in Arc 56 are powered for less Imax. \"\"\" try : brho = madx . globals . nrj * 1e9 / madx . globals . clight # clight is MAD-X constant except AttributeError as madx_error : logger . error ( \"The global MAD-X variable 'NRJ' should have been set in the optics files but is not defined.\" ) raise EnvironmentError ( \"No 'NRJ' variable found in scripts\" ) from madx_error logger . info ( f \"Powering Landau Octupoles, beam {beam} @ {madx.globals.nrj} GeV with {mo_current} A.\" ) strength = mo_current / madx . globals . Imax_MO * madx . globals . Kmax_MO / brho beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ): for fd in \"FD\" : octupole = f \"KO{fd}.{arc}\" logger . trace ( f \"Powering element '{octupole}' at {strength} Amps\" ) madx . globals [ octupole ] = strength if defective_arc and ( beam == 1 ): madx . globals [ \"KOD.A56B1\" ] = strength * 4.65 / 6 # defective MO group","title":"power_landau_octupoles"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#re_cycle_sequence","text":"def re_cycle_sequence ( madx : cpymad . madx . Madx , sequence : str = 'lhcb1' , start : str = 'IP3' ) -> None Re-cycle the provided sequence from a different starting point. Parameters: Name Type Description Default madx Madx an instantiated cpymad.madx.Madx object. None sequence str the sequence to re cycle. None start str element to start the new cycle from. None View Source def re_cycle_sequence ( madx : Madx , sequence : str = \"lhcb1\" , start : str = \"IP3\" ) -> None : \"\"\" Re-cycle the provided sequence from a different starting point. Args: madx (Madx): an instantiated cpymad.madx.Madx object. sequence (str): the sequence to re cycle. start (str): element to start the new cycle from. \"\"\" logger . debug ( f \"Re-cycling sequence ' { sequence } ' from {start}\" ) madx . command . seqedit ( sequence = sequence ) madx . command . flatten () madx . command . cycle ( start = start ) madx . command . ende dit ()","title":"re_cycle_sequence"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#vary_independent_ir_quadrupoles","text":"def vary_independent_ir_quadrupoles ( madx : cpymad . madx . Madx , quad_numbers : Sequence [ int ], ip : int , sides : Sequence [ str ] = ( 'r' , 'l' ), beam : int = 1 ) -> None Send the vary commands for the desired quadrupoles in the IRs. The independent quadrupoles for which this is implemented are Q4 to Q13 included. This is useful to setup some specific matching involving these elements. It is necessary to have defined a 'brho' variable when creating your beams. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None quad_numbers Sequence[int] quadrupoles to be varied, by number (aka position from IP). None ip int the IP around which to apply the instructions. Defaults to 1. 1 sides Sequence[str] the sides of IP to act on. Should be R for right and L for left. Defaults to both sides of the IP. None beam int the beam for which to apply the instructions. Defaults to 1. 1 View Source def vary_independent_ir_quadrupoles ( madx : Madx , quad_numbers : Sequence [ int ], ip : int , sides : Sequence [ str ] = ( \"r\" , \"l\" ), beam : int = 1 ) -> None : \"\"\" Send the `vary` commands for the desired quadrupoles in the IRs. The independent quadrupoles for which this is implemented are Q4 to Q13 included. This is useful to setup some specific matching involving these elements. It is necessary to have defined a 'brho' variable when creating your beams. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. quad_numbers (Sequence[int]): quadrupoles to be varied, by number (aka position from IP). ip (int): the IP around which to apply the instructions. Defaults to 1. sides (Sequence[str]): the sides of IP to act on. Should be `R` for right and `L` for left. Defaults to both sides of the IP. beam (int): the beam for which to apply the instructions. Defaults to 1. \"\"\" if ( ip not in ( 1 , 2 , 5 , 8 ) or any ( side . upper () not in ( \"R\" , \"L\" ) for side in sides ) or any ( quad not in ( 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 ) for quad in quad_numbers ) ): logger . error ( \"Either the IP number of the side provided are invalid, not applying any error.\" ) raise ValueError ( \"Invalid 'quad_numbers', 'ip', 'sides' argument\" ) logger . debug ( f \"Preparing a knob involving quadrupoles {quad_numbers}\" ) # Each quad has a specific power circuit used for their k1 boundaries power_circuits : Dict [ int , str ] = { 4 : \"mqy\" , 5 : \"mqml\" , 6 : \"mqml\" , 7 : \"mqm\" , 8 : \"mqml\" , 9 : \"mqm\" , 10 : \"mqml\" , 11 : \"mqtli\" , 12 : \"mqt\" , 13 : \"mqt\" , } for quad in quad_numbers : circuit = power_circuits [ quad ] for side in sides : logger . trace ( f \"Sending vary command for Q{quad}{side.upper()}{ip}\" ) madx . command . vary ( name = f \"kq{'t' if quad >= 11 else ''}{'l' if quad == 11 else ''}{quad}.{side}{ip}b{beam}\" , step = 1e-7 , lower = f \"-{circuit}.{'b' if quad == 7 else ''}{quad}{side}{ip}.b{beam}->kmax/brho\" , upper = f \"+{circuit}.{'b' if quad == 7 else ''}{quad}{side}{ip}.b{beam}->kmax/brho\" , )","title":"vary_independent_ir_quadrupoles"},{"location":"reference/pyhdtoolkit/cpymadtools/track/","text":"Module pyhdtoolkit.cpymadtools.track Module cpymadtools.track Created on 2020.02.03 View Source \"\"\" Module cpymadtools.track ------------------------ Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to manipulate MAD-X TRACK functionality through a cpymad.madx.Madx object. \"\"\" from typing import Dict , Sequence , Tuple import pandas as pd from cpymad.madx import Madx from loguru import logger # ----- Utlites ----- # def track_single_particle ( madx : Madx , initial_coordinates : Tuple [ float , float , float , float , float , float ], nturns : int , sequence : str = None , observation_points : Sequence [ str ] = None , ** kwargs , ) -> Dict [ str , pd . DataFrame ]: \"\"\" Tracks a single particle for nturns, based on its initial coordinates. Args: madx (Madx): an instantiated cpymad.madx.Madx object. initial_coordinates (Tuple[float, float, float, float, float, float]): a tuple with the X, PX, Y, PY, T, PT starting coordinates the particle to track. Defaults to all 0 if none given. nturns (int): the number of turns to track for. sequence (str): the sequence to use for tracking. If no value is provided, it is assumed that a sequence is already defined and in use, and this one will be picked up by MAD-X. observation_points (Sequence[str]): sequence of all element names at which to OBSERVE during the tracking. Keyword Args: Any keyword argument to be given to the `TRACK` command like it would be given directly into `MAD-X`, for instance `ONETABLE` etc. Refer to the `MAD-X` manual for options. Returns: A dictionary with a copy of the track table's dataframe for each defined observation point, with as columns the coordinates x, px, y, py, t, pt, s and e (energy). The keys of the dictionary are simply numbered 'observation_point_1', 'observation_point_2' etc. The first observation point always corresponds to the start of machine, the others correspond to the ones manually defined, in the order they are defined in. If the user has set `onetable` to `True`, only one entry is in the dictionary under the key 'trackone' and it has the combined table as a pandas DataFrame for value. \"\"\" onetable = kwargs . get ( \"onetable\" , False ) if \"onetable\" in kwargs else kwargs . get ( \"ONETABLE\" , False ) start = initial_coordinates if initial_coordinates else [ 0 , 0 , 0 , 0 , 0 , 0 ] observation_points = observation_points if observation_points else [] if isinstance ( sequence , str ): logger . debug ( f \"Using sequence ' { sequence } ' for tracking\" ) madx . use ( sequence = sequence ) logger . debug ( f \"Tracking coordinates with initial X, PX, Y, PY, T, PT of ' { initial_coordinates } '\" ) madx . command . track ( ** kwargs ) for element in observation_points : logger . trace ( f \"Setting observation point for tracking with OBSERVE at element ' { element } '\" ) madx . command . observe ( place = element ) madx . command . start ( X = start [ 0 ], PX = start [ 1 ], Y = start [ 2 ], PY = start [ 3 ], T = start [ 4 ], PT = start [ 5 ]) madx . command . run ( turns = nturns ) madx . command . endtrack () if onetable : # user asked for ONETABLE, there will only be one table 'trackone' given back by MAD-X logger . debug ( \"Because of option ONETABLE only one table 'TRACKONE' exists to be returned.\" ) return { \"trackone\" : madx . table . trackone . dframe () . copy ()} return { f \"observation_point_ { point : d } \" : madx . table [ f \"track.obs { point : 04d } .p0001\" ] . dframe () . copy () for point in range ( 1 , len ( observation_points ) + 2 ) # len(observation_points) + 1 for start of # machine + 1 because MAD-X starts indexing these at 1 } Functions track_single_particle def track_single_particle ( madx : cpymad . madx . Madx , initial_coordinates : Tuple [ float , float , float , float , float , float ], nturns : int , sequence : str = None , observation_points : Sequence [ str ] = None , ** kwargs ) -> Dict [ str , pandas . core . frame . DataFrame ] Tracks a single particle for nturns, based on its initial coordinates. Args: madx (Madx): an instantiated cpymad.madx.Madx object. initial_coordinates (Tuple[float, float, float, float, float, float]): a tuple with the X, PX, Y, PY, T, PT starting coordinates the particle to track. Defaults to all 0 if none given. nturns (int): the number of turns to track for. sequence (str): the sequence to use for tracking. If no value is provided, it is assumed that a sequence is already defined and in use, and this one will be picked up by MAD-X. observation_points (Sequence[str]): sequence of all element names at which to OBSERVE during the tracking. Keyword Args: Any keyword argument to be given to the TRACK command like it would be given directly into MAD-X , for instance ONETABLE etc. Refer to the MAD-X manual for options. Returns: A dictionary with a copy of the track table's dataframe for each defined observation point, with as columns the coordinates x, px, y, py, t, pt, s and e (energy). The keys of the dictionary are simply numbered 'observation_point_1', 'observation_point_2' etc. The first observation point always corresponds to the start of machine, the others correspond to the ones manually defined, in the order they are defined in. If the user has set `onetable` to `True` , only one entry is in the dictionary under the key 'trackone' and it has the combined table as a pandas DataFrame for value . View Source def track_single_particle ( madx : Madx , initial_coordinates : Tuple [ float , float , float , float , float , float ] , nturns : int , sequence : str = None , observation_points : Sequence [ str ] = None , ** kwargs , ) -> Dict [ str , pd . DataFrame ] : \" \"\" Tracks a single particle for nturns, based on its initial coordinates. Args: madx (Madx): an instantiated cpymad.madx.Madx object. initial_coordinates (Tuple[float, float, float, float, float, float]): a tuple with the X, PX, Y, PY, T, PT starting coordinates the particle to track. Defaults to all 0 if none given. nturns (int): the number of turns to track for. sequence (str): the sequence to use for tracking. If no value is provided, it is assumed that a sequence is already defined and in use, and this one will be picked up by MAD-X. observation_points (Sequence[str]): sequence of all element names at which to OBSERVE during the tracking. Keyword Args: Any keyword argument to be given to the `TRACK` command like it would be given directly into `MAD-X`, for instance `ONETABLE` etc. Refer to the `MAD-X` manual for options. Returns: A dictionary with a copy of the track table's dataframe for each defined observation point, with as columns the coordinates x, px, y, py, t, pt, s and e (energy). The keys of the dictionary are simply numbered 'observation_point_1', 'observation_point_2' etc. The first observation point always corresponds to the start of machine, the others correspond to the ones manually defined, in the order they are defined in. If the user has set `onetable` to `True`, only one entry is in the dictionary under the key 'trackone' and it has the combined table as a pandas DataFrame for value. \"\" \" onetable = kwargs . get ( \"onetable\" , False ) if \"onetable\" in kwargs else kwargs . get ( \"ONETABLE\" , False ) start = initial_coordinates if initial_coordinates else [ 0 , 0 , 0 , 0 , 0 , 0 ] observation_points = observation_points if observation_points else [] if isinstance ( sequence , str ) : logger . debug ( f \"Using sequence '{sequence}' for tracking\" ) madx . use ( sequence = sequence ) logger . debug ( f \"Tracking coordinates with initial X, PX, Y, PY, T, PT of '{initial_coordinates}'\" ) madx . command . track ( ** kwargs ) for element in observation_points : logger . trace ( f \"Setting observation point for tracking with OBSERVE at element '{element}'\" ) madx . command . observe ( place = element ) madx . command . start ( X = start [ 0 ] , PX = start [ 1 ] , Y = start [ 2 ] , PY = start [ 3 ] , T = start [ 4 ] , PT = start [ 5 ] ) madx . command . run ( turns = nturns ) madx . command . endtrack () if onetable : # user asked for ONETABLE, there will only be one table 'trackone' given back by MAD-X logger . debug ( \"Because of option ONETABLE only one table 'TRACKONE' exists to be returned.\" ) return { \"trackone\" : madx . table . trackone . dframe (). copy () } return { f \"observation_point_{point:d}\" : madx . table [ f \"track.obs{point:04d}.p0001\" ] . dframe (). copy () for point in range ( 1 , len ( observation_points ) + 2 ) # len(observation_points) + 1 for start of # machine + 1 because MAD-X starts indexing these at 1 }","title":"Track"},{"location":"reference/pyhdtoolkit/cpymadtools/track/#module-pyhdtoolkitcpymadtoolstrack","text":"Module cpymadtools.track Created on 2020.02.03 View Source \"\"\" Module cpymadtools.track ------------------------ Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to manipulate MAD-X TRACK functionality through a cpymad.madx.Madx object. \"\"\" from typing import Dict , Sequence , Tuple import pandas as pd from cpymad.madx import Madx from loguru import logger # ----- Utlites ----- # def track_single_particle ( madx : Madx , initial_coordinates : Tuple [ float , float , float , float , float , float ], nturns : int , sequence : str = None , observation_points : Sequence [ str ] = None , ** kwargs , ) -> Dict [ str , pd . DataFrame ]: \"\"\" Tracks a single particle for nturns, based on its initial coordinates. Args: madx (Madx): an instantiated cpymad.madx.Madx object. initial_coordinates (Tuple[float, float, float, float, float, float]): a tuple with the X, PX, Y, PY, T, PT starting coordinates the particle to track. Defaults to all 0 if none given. nturns (int): the number of turns to track for. sequence (str): the sequence to use for tracking. If no value is provided, it is assumed that a sequence is already defined and in use, and this one will be picked up by MAD-X. observation_points (Sequence[str]): sequence of all element names at which to OBSERVE during the tracking. Keyword Args: Any keyword argument to be given to the `TRACK` command like it would be given directly into `MAD-X`, for instance `ONETABLE` etc. Refer to the `MAD-X` manual for options. Returns: A dictionary with a copy of the track table's dataframe for each defined observation point, with as columns the coordinates x, px, y, py, t, pt, s and e (energy). The keys of the dictionary are simply numbered 'observation_point_1', 'observation_point_2' etc. The first observation point always corresponds to the start of machine, the others correspond to the ones manually defined, in the order they are defined in. If the user has set `onetable` to `True`, only one entry is in the dictionary under the key 'trackone' and it has the combined table as a pandas DataFrame for value. \"\"\" onetable = kwargs . get ( \"onetable\" , False ) if \"onetable\" in kwargs else kwargs . get ( \"ONETABLE\" , False ) start = initial_coordinates if initial_coordinates else [ 0 , 0 , 0 , 0 , 0 , 0 ] observation_points = observation_points if observation_points else [] if isinstance ( sequence , str ): logger . debug ( f \"Using sequence ' { sequence } ' for tracking\" ) madx . use ( sequence = sequence ) logger . debug ( f \"Tracking coordinates with initial X, PX, Y, PY, T, PT of ' { initial_coordinates } '\" ) madx . command . track ( ** kwargs ) for element in observation_points : logger . trace ( f \"Setting observation point for tracking with OBSERVE at element ' { element } '\" ) madx . command . observe ( place = element ) madx . command . start ( X = start [ 0 ], PX = start [ 1 ], Y = start [ 2 ], PY = start [ 3 ], T = start [ 4 ], PT = start [ 5 ]) madx . command . run ( turns = nturns ) madx . command . endtrack () if onetable : # user asked for ONETABLE, there will only be one table 'trackone' given back by MAD-X logger . debug ( \"Because of option ONETABLE only one table 'TRACKONE' exists to be returned.\" ) return { \"trackone\" : madx . table . trackone . dframe () . copy ()} return { f \"observation_point_ { point : d } \" : madx . table [ f \"track.obs { point : 04d } .p0001\" ] . dframe () . copy () for point in range ( 1 , len ( observation_points ) + 2 ) # len(observation_points) + 1 for start of # machine + 1 because MAD-X starts indexing these at 1 }","title":"Module pyhdtoolkit.cpymadtools.track"},{"location":"reference/pyhdtoolkit/cpymadtools/track/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/track/#track_single_particle","text":"def track_single_particle ( madx : cpymad . madx . Madx , initial_coordinates : Tuple [ float , float , float , float , float , float ], nturns : int , sequence : str = None , observation_points : Sequence [ str ] = None , ** kwargs ) -> Dict [ str , pandas . core . frame . DataFrame ] Tracks a single particle for nturns, based on its initial coordinates. Args: madx (Madx): an instantiated cpymad.madx.Madx object. initial_coordinates (Tuple[float, float, float, float, float, float]): a tuple with the X, PX, Y, PY, T, PT starting coordinates the particle to track. Defaults to all 0 if none given. nturns (int): the number of turns to track for. sequence (str): the sequence to use for tracking. If no value is provided, it is assumed that a sequence is already defined and in use, and this one will be picked up by MAD-X. observation_points (Sequence[str]): sequence of all element names at which to OBSERVE during the tracking. Keyword Args: Any keyword argument to be given to the TRACK command like it would be given directly into MAD-X , for instance ONETABLE etc. Refer to the MAD-X manual for options. Returns: A dictionary with a copy of the track table's dataframe for each defined observation point, with as columns the coordinates x, px, y, py, t, pt, s and e (energy). The keys of the dictionary are simply numbered 'observation_point_1', 'observation_point_2' etc. The first observation point always corresponds to the start of machine, the others correspond to the ones manually defined, in the order they are defined in. If the user has set `onetable` to `True` , only one entry is in the dictionary under the key 'trackone' and it has the combined table as a pandas DataFrame for value . View Source def track_single_particle ( madx : Madx , initial_coordinates : Tuple [ float , float , float , float , float , float ] , nturns : int , sequence : str = None , observation_points : Sequence [ str ] = None , ** kwargs , ) -> Dict [ str , pd . DataFrame ] : \" \"\" Tracks a single particle for nturns, based on its initial coordinates. Args: madx (Madx): an instantiated cpymad.madx.Madx object. initial_coordinates (Tuple[float, float, float, float, float, float]): a tuple with the X, PX, Y, PY, T, PT starting coordinates the particle to track. Defaults to all 0 if none given. nturns (int): the number of turns to track for. sequence (str): the sequence to use for tracking. If no value is provided, it is assumed that a sequence is already defined and in use, and this one will be picked up by MAD-X. observation_points (Sequence[str]): sequence of all element names at which to OBSERVE during the tracking. Keyword Args: Any keyword argument to be given to the `TRACK` command like it would be given directly into `MAD-X`, for instance `ONETABLE` etc. Refer to the `MAD-X` manual for options. Returns: A dictionary with a copy of the track table's dataframe for each defined observation point, with as columns the coordinates x, px, y, py, t, pt, s and e (energy). The keys of the dictionary are simply numbered 'observation_point_1', 'observation_point_2' etc. The first observation point always corresponds to the start of machine, the others correspond to the ones manually defined, in the order they are defined in. If the user has set `onetable` to `True`, only one entry is in the dictionary under the key 'trackone' and it has the combined table as a pandas DataFrame for value. \"\" \" onetable = kwargs . get ( \"onetable\" , False ) if \"onetable\" in kwargs else kwargs . get ( \"ONETABLE\" , False ) start = initial_coordinates if initial_coordinates else [ 0 , 0 , 0 , 0 , 0 , 0 ] observation_points = observation_points if observation_points else [] if isinstance ( sequence , str ) : logger . debug ( f \"Using sequence '{sequence}' for tracking\" ) madx . use ( sequence = sequence ) logger . debug ( f \"Tracking coordinates with initial X, PX, Y, PY, T, PT of '{initial_coordinates}'\" ) madx . command . track ( ** kwargs ) for element in observation_points : logger . trace ( f \"Setting observation point for tracking with OBSERVE at element '{element}'\" ) madx . command . observe ( place = element ) madx . command . start ( X = start [ 0 ] , PX = start [ 1 ] , Y = start [ 2 ] , PY = start [ 3 ] , T = start [ 4 ] , PT = start [ 5 ] ) madx . command . run ( turns = nturns ) madx . command . endtrack () if onetable : # user asked for ONETABLE, there will only be one table 'trackone' given back by MAD-X logger . debug ( \"Because of option ONETABLE only one table 'TRACKONE' exists to be returned.\" ) return { \"trackone\" : madx . table . trackone . dframe (). copy () } return { f \"observation_point_{point:d}\" : madx . table [ f \"track.obs{point:04d}.p0001\" ] . dframe (). copy () for point in range ( 1 , len ( observation_points ) + 2 ) # len(observation_points) + 1 for start of # machine + 1 because MAD-X starts indexing these at 1 }","title":"track_single_particle"},{"location":"reference/pyhdtoolkit/cpymadtools/tune/","text":"Module pyhdtoolkit.cpymadtools.tune Module cpymadtools.tune Created on 2021.04.01 View Source \"\"\" Module cpymadtools.tune ----------------------- Created on 2021.04.01 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions to manipulate MAD-X functionality around the tune through a cpymad.madx.Madx object. \"\"\" import math import sys from pathlib import Path from typing import Dict , List , Tuple import matplotlib.collections import matplotlib.patches import numpy as np import tfs from cpymad.madx import Madx from loguru import logger def make_footprint_table ( madx : Madx , sigma : float = 5 , dense : bool = False , file : str = None , cleanup : bool = True , ** kwargs , ) -> tfs . TfsDataFrame : \"\"\" Instantiates an ensemble of particles up to the desired bunch sigma amplitude to be tracked for the DYNAP command, letting MAD-X infer their tunes. Particules are instantiated for different angle variables for each amplitude, creating an ensemble able to represent the tune footprint. Beware: since the `DYNAP` command makes use of tracking, your sequence needs to be sliced before calling this function. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. sigma (float): the maximum amplitude of the tracked particles, in bunch sigma. Defaults to 5. dense (bool): if set to True, an increased number of particles will be tracked. Defaults to False. file (str): If given, the `dynaptune` table will be exported as a TFS file with the provided name. cleanup (bool): If True, the `fort.69` and `lyapunov.data` files are cleared before returning the dynap table. Defaults to True. Keyword Args: Any keyword argument that will be transmitted to the DYNAP command in MAD-X. Returns: The resulting `dynaptune` table, as a pandas DataFrame. \"\"\" logger . info ( f \"Initiating particules up to { sigma : d } bunch sigma to create a tune footprint table\" ) small , big = 0.05 , math . sqrt ( 1 - 0.05 ** 2 ) sigma_multiplier , angle_multiplier = 0.1 , 0 logger . debug ( \"Initializing particles\" ) madx . command . track () madx . command . start ( fx = small , fy = small ) while sigma_multiplier <= sigma + 1 : angle = 15 * angle_multiplier * math . pi / 180 if angle_multiplier == 0 : madx . command . start ( fx = sigma_multiplier * big , fy = sigma_multiplier * small ) elif angle_multiplier == 6 : madx . command . start ( fx = sigma_multiplier * small , fy = sigma_multiplier * big ) else : madx . command . start ( fx = sigma_multiplier * math . cos ( angle ), fy = sigma_multiplier * math . sin ( angle )) angle_multiplier += 0.5 if int ( angle_multiplier ) == 7 : angle_multiplier = 0 sigma_multiplier += 1 if not dense else 0.5 logger . debug ( \"Starting DYNAP tracking with initialized particles\" ) try : madx . command . dynap ( fastune = True , turns = 1024 , ** kwargs ) madx . command . endtrack () except RuntimeError as madx_crash : logger . error ( \"Remote MAD-X process crashed, most likely because you did not slice the sequence \" \"before running DYNAP. Restart and slice before calling this function.\" ) raise RuntimeError ( \"DYNAP command crashed the MAD-X process\" ) from madx_crash if cleanup and sys . platform not in ( \"win32\" , \"cygwin\" ): # fails on Windows due to its I/O system, since MAD-X still has \"control\" of the files try : logger . debug ( \"Cleaning up DYNAP output files `fort.69` and `lyapunov.data`\" ) Path ( \"fort.69\" ) . unlink () Path ( \"lyapunov.data\" ) . unlink () except FileNotFoundError : logger . error ( \"Could not cleanup DYNAP output files, they might have not been created\" ) tfs_dframe = tfs . TfsDataFrame ( data = madx . table . dynaptune . dframe (), headers = dict ( NAME = \"DYNAPTUNE\" , TYPE = \"DYNAPTUNE\" , TITLE = \"FOOTPRINT TABLE\" , MADX_VERSION = str ( madx . version ) . upper (), ORIGIN = \"pyhdtoolkit.cpymadtools.tune.make_footprint_table() function\" , ANGLE = 7 , # default of the function AMPLITUDE = sigma , DSIGMA = 1 if not dense else 0.5 , ANGLE_MEANING = \"Number of different starting angles used for each starting amplitude\" , AMPLITUDE_MEANING = \"Up to which bunch sigma the starting amplitudes were ramped up\" , DSIGMA_MEANING = \"Increment value of AMPLITUDE at each new starting amplitude\" , ), ) tfs_dframe = tfs_dframe . reset_index ( drop = True ) if file : tfs . write ( Path ( file ) . absolute (), tfs_dframe ) return tfs_dframe def get_footprint_lines ( dynap_dframe : tfs . TfsDataFrame ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Provided with the `TfsDataFrame` returned by `make_footprint_table()`, determines the various (Qx, Qy) points needed to plot the footprint data with lines representing the different amplitudes and angles from starting particles, and returns these in immediately plottable numpy arrays. WARNING: This function is some DARK MAGIC stuff I have taken out of very dusty drawers, and I cannot explain exactly how it works. I also do not know who wrote this initially. Results are not guaranteed to be correct and should be checked with a quick plot. Usage: ```python dynap_tfs = make_footprint_table(madx) qxs, qys = get_footprint_lines(dynap_tfs) plt.plot(qxs, qys, \"o--\", label=\"Tune Footprint from DYNAP Table\") ``` Args: dynap_dframe (tfs.TfsDataFrame): the dynap data frame returned by `make_footprint_table()`. Returns: The Qx and Qy data points to plot directly, both as numpy.ndarrays. \"\"\" logger . info ( \"Determining footprint plottable\" ) logger . debug ( \"Retrieving AMPLITUDE, ANGLE and DSIGMA data from TfsDataFrame headers\" ) amplitude = dynap_dframe . headers [ \"AMPLITUDE\" ] angle = dynap_dframe . headers [ \"ANGLE\" ] dsigma = dynap_dframe . headers [ \"DSIGMA\" ] tune_groups = _make_tune_groups ( dynap_string_rep = _get_dynap_string_rep ( dynap_dframe ), dsigma = dsigma ) footprint = _Footprint ( tune_groups , amplitude , angle , dsigma ) qxs , qys = footprint . get_plottable () return np . array ( qxs , dtype = float ), np . array ( qys , dtype = float ) def get_footprint_patches ( dynap_dframe : tfs . TfsDataFrame ,) -> matplotlib . collections . PatchCollection : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO KONSTANTINOS PARASCHOU (@kparasch). Provided with the `TfsDataFrame` returned by `make_footprint_table()`, computes the polygon patches needed to plot the footprint data, with lines representing the different amplitudes and angles from starting particles, and returns the `PatchCollection` with the computed polygons. The polygons will have blue edges, except the ones corresponding to the last starting angle particles ( in red) and the last starting amplitude particles (in green). WARNING: The internal construction of polygons can be tricky, and you might need to change the `ANGLE` or `AMPLITUDE` values in `dynap_dframe`'s headers. Usage: ```python fig, axis = plt.subplots() dynap_tfs = make_footprint_table(madx) footprint_polygons = get_footprint_patches(dynap_tfs) axis.add_collection(footprint_polygons) ``` Args: dynap_dframe (tfs.TfsDataFrame): the dynap data frame returned by `make_footprint_table()`. Returns: The `matplotlib.collections.PatchCollection` with the created polygons. \"\"\" logger . info ( \"Determining footprint polygons\" ) angle = dynap_dframe . headers [ \"ANGLE\" ] amplitude = dynap_dframe . headers [ \"AMPLITUDE\" ] logger . debug ( \"Grouping tune points according to starting angles and amplitudes\" ) try : A = np . zeros ([ amplitude , angle , 2 ]) A [ 0 , :, 0 ] = dynap_dframe [ \"tunx\" ] . to_numpy ()[ 0 ] A [ 0 , :, 1 ] = dynap_dframe [ \"tuny\" ] . to_numpy ()[ 0 ] A [ 1 :, :, 0 ] = dynap_dframe [ \"tunx\" ] . to_numpy ()[ 1 :] . reshape ( - 1 , angle ) A [ 1 :, :, 1 ] = dynap_dframe [ \"tuny\" ] . to_numpy ()[ 1 :] . reshape ( - 1 , angle ) except ValueError as tune_grouping_error : logger . error ( \"Cannot group tune points according to starting angles and amplitudes. Try changing \" \"the 'AMPLITUDE' value in the provided TfsDataFrame's headers.\" ) raise tune_grouping_error logger . debug ( \"Determining polygon vertices\" ) sx = A . shape [ 0 ] - 1 sy = A . shape [ 1 ] - 1 p1 = A [: - 1 , : - 1 , :] . reshape ( sx * sy , 2 )[:, :] p2 = A [ 1 :, : - 1 , :] . reshape ( sx * sy , 2 )[:] p3 = A [ 1 :, 1 :, :] . reshape ( sx * sy , 2 )[:] p4 = A [: - 1 , 1 :, :] . reshape ( sx * sy , 2 )[:] polygons = np . stack (( p1 , p2 , p3 , p4 )) # Stack endpoints to form polygons polygons = np . transpose ( polygons , ( 1 , 0 , 2 )) # transpose polygons logger . debug ( \"Creating PatchCollection of Polygons\" ) patches = list ( map ( matplotlib . patches . Polygon , polygons )) patch_colors = [( 0 , 0 , 1 ) for _ in polygons ] patch_colors [( sx - 1 ) * sy :] = [( 0 , 1 , 0 )] * sy # differentiate first angle in green patch_colors [( sy - 1 ) :: sy ] = [( 1 , 0 , 0 )] * sx # differentiate last amplitude in red return matplotlib . collections . PatchCollection ( patches , facecolors = [], edgecolor = patch_colors ) # ----- Arcane Private Utilities ----- # def _get_dynap_string_rep ( dynap_dframe : tfs . TfsDataFrame ) -> str : \"\"\" This is a weird dusty function to get a specific useful string representation from the `TfsDataFrame` returned by `make_footprint_table()`. This specific dataframe contains important information. Args: dynap_dframe (tfs.TfsDataFrame): the dynap data frame returned by `make_footprint_table()`. Returns: A weird string representation gathering tune points split according to the number of angles and amplitudes used in `make_footprint_table()`. \"\"\" logger . trace ( \"Retrieving AMPLITUDE and ANGLE data from TfsDataFrame headers\" ) amplitude = dynap_dframe . headers [ \"AMPLITUDE\" ] angle = dynap_dframe . headers [ \"ANGLE\" ] string_rep = f \"TMPNAME, { amplitude } ,1,< { dynap_dframe . tunx [ 0 ] } ; { dynap_dframe . tuny [ 0 ] } >\" for n in range ( 1 , amplitude ): string_rep += f \", { angle } \" for m in range ( angle ): string_rep += f \",< { dynap_dframe . tunx [ 1 + ( n - 1 ) * angle + m ] } ; { dynap_dframe . tuny [ 1 + ( n - 1 ) * angle + m ] } >\" return string_rep def _make_tune_groups ( dynap_string_rep : str , dsigma : float = 1.0 ) -> List [ List [ Dict [ str , float ]]]: \"\"\" Creates appropriate tune points groups from the arcane string representation returned by `_get_dynap_string_rep` based on starting amplitude and angle for each particle. Args: dynap_string_rep (str): weird string representation of the `TfsDataFrame` returned by `make_footprint_table()` and as given by `_get_dynap_string_rep()`. dsigma (float): The increment in amplitude between different starting amplitudes when starting particles for the `DYNAP` command in `MAD-X`. This information is in the headers of the `TfsDataFrame` returned by `make_footprint_table()`. Returns: A list of list of dictionaries containing horizontal and vertical tune points. The data is constructed such that one can access the data of a particle starting with 'amplitude' and 'angle' with data[amplitude][angle][\"H\"/\"V\"]. This function is only meant to be used internally by `get_footprint_lines()`. \"\"\" logger . debug ( \"Constructing tune points groups based on starting amplitudes and angles\" ) tune_groups = [] items = dynap_string_rep . strip () . split ( \",\" ) amplitude = int ( items [ 1 ]) current = 2 for i in np . arange ( amplitude ): tune_groups . append ([]) angle = int ( items [ current ]) current = current + 1 for j in np . arange ( angle ): tune_groups [ i ] . append ([]) tune_string = items [ current ] . lstrip ( \"<\" ) . rstrip ( \">\" ) . split ( \";\" ) tune_groups [ i ][ j ] = { \"H\" : float ( tune_string [ 0 ]), \"V\" : float ( tune_string [ 1 ])} current = current + 1 return tune_groups class _Footprint : \"\"\"More dark magic from the past here, close your eyes my friends.\"\"\" def __init__ ( self , tune_groups : List [ List [ Dict [ str , float ]]], amplitude : int , angle : int , dsigma : float ): self . _tunes = tune_groups self . _maxnangl = angle self . _nampl = amplitude self . _dSigma = dsigma def get_h_tune ( self , ampl , angl ): if len ( self . _tunes [ ampl ]) <= angl < self . _maxnangl : return self . _tunes [ ampl ][ len ( self . _tunes [ ampl ]) - 1 ][ \"H\" ] else : return self . _tunes [ ampl ][ angl ][ \"H\" ] def get_v_tune ( self , ampl , angl ): if len ( self . _tunes [ ampl ]) <= angl < self . _maxnangl : return self . _tunes [ ampl ][ len ( self . _tunes [ ampl ]) - 1 ][ \"V\" ] else : return self . _tunes [ ampl ][ angl ][ \"V\" ] def get_plottable ( self ) -> Tuple [ List [ float ], List [ float ]]: qxs , qys = [], [] for i in np . arange ( 0 , self . _nampl - 1 , 2 ): for j in np . arange ( self . _maxnangl ): qxs . append ( self . get_h_tune ( i , j )) qys . append ( self . get_v_tune ( i , j )) for j in np . arange ( self . _maxnangl - 1 , - 1 , - 1 ): qxs . append ( self . get_h_tune ( i + 1 , j )) qys . append ( self . get_v_tune ( i + 1 , j )) if self . _nampl % 2 == 0 : for j in np . arange ( 0 , self . _maxnangl - 1 , 2 ): for i in np . arange ( self . _nampl - 1 , - 1 , - 1 ): qxs . append ( self . get_h_tune ( i , j )) qys . append ( self . get_v_tune ( i , j )) for i in np . arange ( 0 , self . _nampl , 1 ): qxs . append ( self . get_h_tune ( i , j + 1 )) qys . append ( self . get_v_tune ( i , j + 1 )) if self . _maxnangl % 2 != 0 : for i in np . arange ( self . _nampl - 1 , - 1 , - 1 ): qxs . append ( self . get_h_tune ( i , self . _maxnangl - 1 )) qys . append ( self . get_v_tune ( i , self . _maxnangl - 1 )) qxs . append ( self . get_h_tune ( 0 , self . _maxnangl - 2 )) qys . append ( self . get_v_tune ( 0 , self . _maxnangl - 2 )) else : for j in np . arange ( self . _maxnangl ): qxs . append ( self . get_h_tune ( self . _nampl - 1 , j )) qys . append ( self . get_v_tune ( self . _nampl - 1 , j )) for j in np . arange ( self . _maxnangl - 1 , - 1 , - 2 ): for i in np . arange ( self . _nampl - 1 , - 1 , - 1 ): qxs . append ( self . get_h_tune ( i , j )) qys . append ( self . get_v_tune ( i , j )) for i in np . arange ( 0 , self . _nampl , 1 ): qxs . append ( self . get_h_tune ( i , j - 1 )) qys . append ( self . get_v_tune ( i , j - 1 )) return qxs , qys Functions get_footprint_lines def get_footprint_lines ( dynap_dframe : tfs . handler . TfsDataFrame ) -> Tuple [ numpy . ndarray , numpy . ndarray ] Provided with the TfsDataFrame returned by make_footprint_table() , determines the various (Qx, Qy) points needed to plot the footprint data with lines representing the different amplitudes and angles from starting particles, and returns these in immediately plottable numpy arrays. WARNING : This function is some DARK MAGIC stuff I have taken out of very dusty drawers , and I cannot explain exactly how it works . I also do not know who wrote this initially . Results are not guaranteed to be correct and should be checked with a quick plot . Usage: dynap_tfs = make_footprint_table ( madx ) qxs , qys = get_footprint_lines ( dynap_tfs ) plt . plot ( qxs , qys , \"o--\" , label = \"Tune Footprint from DYNAP Table\" ) Parameters: Name Type Description Default dynap_dframe tfs.TfsDataFrame the dynap data frame returned by make_footprint_table() . None Returns: Type Description None The Qx and Qy data points to plot directly, both as numpy.ndarrays. View Source def get_footprint_lines ( dynap_dframe : tfs . TfsDataFrame ) -> Tuple [ np . ndarray , np . ndarray ] : \" \"\" Provided with the `TfsDataFrame` returned by `make_footprint_table()`, determines the various (Qx, Qy) points needed to plot the footprint data with lines representing the different amplitudes and angles from starting particles, and returns these in immediately plottable numpy arrays. WARNING: This function is some DARK MAGIC stuff I have taken out of very dusty drawers, and I cannot explain exactly how it works. I also do not know who wrote this initially. Results are not guaranteed to be correct and should be checked with a quick plot. Usage: ```python dynap_tfs = make_footprint_table(madx) qxs, qys = get_footprint_lines(dynap_tfs) plt.plot(qxs, qys, \" o -- \", label=\" Tune Footprint from DYNAP Table \") ``` Args: dynap_dframe (tfs.TfsDataFrame): the dynap data frame returned by `make_footprint_table()`. Returns: The Qx and Qy data points to plot directly, both as numpy.ndarrays. \"\" \" logger . info ( \"Determining footprint plottable\" ) logger . debug ( \"Retrieving AMPLITUDE, ANGLE and DSIGMA data from TfsDataFrame headers\" ) amplitude = dynap_dframe . headers [ \"AMPLITUDE\" ] angle = dynap_dframe . headers [ \"ANGLE\" ] dsigma = dynap_dframe . headers [ \"DSIGMA\" ] tune_groups = _make_tune_groups ( dynap_string_rep = _get_dynap_string_rep ( dynap_dframe ), dsigma = dsigma ) footprint = _Footprint ( tune_groups , amplitude , angle , dsigma ) qxs , qys = footprint . get_plottable () return np . array ( qxs , dtype = float ), np . array ( qys , dtype = float ) get_footprint_patches def get_footprint_patches ( dynap_dframe : tfs . handler . TfsDataFrame ) -> matplotlib . collections . PatchCollection INITIAL IMPLEMENTATION CREDITS GO TO KONSTANTINOS PARASCHOU (@kparasch). Provided with the TfsDataFrame returned by make_footprint_table() , computes the polygon patches needed to plot the footprint data, with lines representing the different amplitudes and angles from starting particles, and returns the PatchCollection with the computed polygons. The polygons will have blue edges, except the ones corresponding to the last starting angle particles ( in red) and the last starting amplitude particles (in green). WARNING : The internal construction of polygons can be tricky , and you might need to change the `ANGLE` or `AMPLITUDE` values in `dynap_dframe` 's headers. Usage: ```python fig, axis = plt.subplots() dynap_tfs = make_footprint_table(madx) footprint_polygons = get_footprint_patches(dynap_tfs) axis.add_collection(footprint_polygons) ``` Parameters: Name Type Description Default dynap_dframe tfs.TfsDataFrame the dynap data frame returned by make_footprint_table() . None Returns: Type Description None The matplotlib.collections.PatchCollection with the created polygons. View Source def get_footprint_patches ( dynap_dframe: tfs . TfsDataFrame ,) -> matplotlib . collections . PatchCollection : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO KONSTANTINOS PARASCHOU (@kparasch). Provided with the `TfsDataFrame` returned by `make_footprint_table()`, computes the polygon patches needed to plot the footprint data, with lines representing the different amplitudes and angles from starting particles, and returns the `PatchCollection` with the computed polygons. The polygons will have blue edges, except the ones corresponding to the last starting angle particles ( in red) and the last starting amplitude particles (in green). WARNING: The internal construction of polygons can be tricky, and you might need to change the `ANGLE` or `AMPLITUDE` values in `dynap_dframe`'s headers. Usage: ```python fig, axis = plt.subplots() dynap_tfs = make_footprint_table(madx) footprint_polygons = get_footprint_patches(dynap_tfs) axis.add_collection(footprint_polygons) ``` Args: dynap_dframe (tfs.TfsDataFrame): the dynap data frame returned by `make_footprint_table()`. Returns: The `matplotlib.collections.PatchCollection` with the created polygons. \"\"\" logger . info ( \"Determining footprint polygons\" ) angle = dynap_dframe . headers [ \"ANGLE\" ] amplitude = dynap_dframe . headers [ \"AMPLITUDE\" ] logger . debug ( \"Grouping tune points according to starting angles and amplitudes\" ) try : A = np . zeros ([ amplitude , angle , 2 ]) A [ 0 , : , 0 ] = dynap_dframe [ \"tunx\" ]. to_numpy ()[ 0 ] A [ 0 , : , 1 ] = dynap_dframe [ \"tuny\" ]. to_numpy ()[ 0 ] A [ 1 : , : , 0 ] = dynap_dframe [ \"tunx\" ]. to_numpy ()[ 1 : ]. reshape ( - 1 , angle ) A [ 1 : , : , 1 ] = dynap_dframe [ \"tuny\" ]. to_numpy ()[ 1 : ]. reshape ( - 1 , angle ) except ValueError as tune_grouping_error: logger . error ( \"Cannot group tune points according to starting angles and amplitudes. Try changing \" \"the 'AMPLITUDE' value in the provided TfsDataFrame's headers.\" ) raise tune_grouping_error logger . debug ( \"Determining polygon vertices\" ) sx = A . shape [ 0 ] - 1 sy = A . shape [ 1 ] - 1 p1 = A [:- 1 , :- 1 , : ]. reshape ( sx * sy , 2 )[ : , : ] p2 = A [ 1 : , :- 1 , : ]. reshape ( sx * sy , 2 )[ : ] p3 = A [ 1 : , 1 : , : ]. reshape ( sx * sy , 2 )[ : ] p4 = A [:- 1 , 1 : , : ]. reshape ( sx * sy , 2 )[ : ] polygons = np . stack (( p1 , p2 , p3 , p4 )) # Stack endpoints to form polygons polygons = np . transpose ( polygons , ( 1 , 0 , 2 )) # transpose polygons logger . debug ( \"Creating PatchCollection of Polygons\" ) patches = list ( map ( matplotlib . patches . Polygon , polygons )) patch_colors = [( 0 , 0 , 1 ) for _ in polygons ] patch_colors [( sx - 1 ) * sy : ] = [( 0 , 1 , 0 )] * sy # differentiate first angle in green patch_colors [( sy - 1 ) :: sy ] = [( 1 , 0 , 0 )] * sx # differentiate last amplitude in red return matplotlib . collections . PatchCollection ( patches , facecolors = [], edgecolor = patch_colors ) make_footprint_table def make_footprint_table ( madx : cpymad . madx . Madx , sigma : float = 5 , dense : bool = False , file : str = None , cleanup : bool = True , ** kwargs ) -> tfs . handler . TfsDataFrame Instantiates an ensemble of particles up to the desired bunch sigma amplitude to be tracked for the DYNAP command, letting MAD-X infer their tunes. Particules are instantiated for different angle variables for each amplitude, creating an ensemble able to represent the tune footprint. Beware: since the DYNAP command makes use of tracking, your sequence needs to be sliced before calling this function. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None sigma float the maximum amplitude of the tracked particles, in bunch sigma. Defaults to 5. 5 dense bool if set to True, an increased number of particles will be tracked. Defaults to False. False file str If given, the dynaptune table will be exported as a TFS file with the provided name. None cleanup bool If True, the fort.69 and lyapunov.data files are cleared before returning the dynap table. Defaults to True. None Keyword Args None Any keyword argument that will be transmitted to the DYNAP command in MAD-X. None Returns: Type Description None The resulting dynaptune table, as a pandas DataFrame. View Source def make_footprint_table ( madx : Madx , sigma : float = 5 , dense : bool = False , file : str = None , cleanup : bool = True , ** kwargs , ) -> tfs . TfsDataFrame : \" \"\" Instantiates an ensemble of particles up to the desired bunch sigma amplitude to be tracked for the DYNAP command, letting MAD-X infer their tunes. Particules are instantiated for different angle variables for each amplitude, creating an ensemble able to represent the tune footprint. Beware: since the `DYNAP` command makes use of tracking, your sequence needs to be sliced before calling this function. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. sigma (float): the maximum amplitude of the tracked particles, in bunch sigma. Defaults to 5. dense (bool): if set to True, an increased number of particles will be tracked. Defaults to False. file (str): If given, the `dynaptune` table will be exported as a TFS file with the provided name. cleanup (bool): If True, the `fort.69` and `lyapunov.data` files are cleared before returning the dynap table. Defaults to True. Keyword Args: Any keyword argument that will be transmitted to the DYNAP command in MAD-X. Returns: The resulting `dynaptune` table, as a pandas DataFrame. \"\" \" logger . info ( f \"Initiating particules up to {sigma:d} bunch sigma to create a tune footprint table\" ) small , big = 0.05 , math . sqrt ( 1 - 0.05 ** 2 ) sigma_multiplier , angle_multiplier = 0.1 , 0 logger . debug ( \"Initializing particles\" ) madx . command . track () madx . command . start ( fx = small , fy = small ) while sigma_multiplier <= sigma + 1 : angle = 15 * angle_multiplier * math . pi / 180 if angle_multiplier == 0 : madx . command . start ( fx = sigma_multiplier * big , fy = sigma_multiplier * small ) elif angle_multiplier == 6 : madx . command . start ( fx = sigma_multiplier * small , fy = sigma_multiplier * big ) else : madx . command . start ( fx = sigma_multiplier * math . cos ( angle ), fy = sigma_multiplier * math . sin ( angle )) angle_multiplier += 0.5 if int ( angle_multiplier ) == 7 : angle_multiplier = 0 sigma_multiplier += 1 if not dense else 0.5 logger . debug ( \"Starting DYNAP tracking with initialized particles\" ) try : madx . command . dynap ( fastune = True , turns = 1024 , ** kwargs ) madx . command . endtrack () except RuntimeError as madx_crash : logger . error ( \"Remote MAD-X process crashed, most likely because you did not slice the sequence \" \"before running DYNAP. Restart and slice before calling this function.\" ) raise RuntimeError ( \"DYNAP command crashed the MAD-X process\" ) from madx_crash if cleanup and sys . platform not in ( \"win32\" , \"cygwin\" ) : # fails on Windows due to its I/O system, since MAD-X still has \"control\" of the files try : logger . debug ( \"Cleaning up DYNAP output files `fort.69` and `lyapunov.data`\" ) Path ( \"fort.69\" ). unlink () Path ( \"lyapunov.data\" ). unlink () except FileNotFoundError : logger . error ( \"Could not cleanup DYNAP output files, they might have not been created\" ) tfs_dframe = tfs . TfsDataFrame ( data = madx . table . dynaptune . dframe (), headers = dict ( NAME = \"DYNAPTUNE\" , TYPE = \"DYNAPTUNE\" , TITLE = \"FOOTPRINT TABLE\" , MADX_VERSION = str ( madx . version ). upper (), ORIGIN = \"pyhdtoolkit.cpymadtools.tune.make_footprint_table() function\" , ANGLE = 7 , # default of the function AMPLITUDE = sigma , DSIGMA = 1 if not dense else 0.5 , ANGLE_MEANING = \"Number of different starting angles used for each starting amplitude\" , AMPLITUDE_MEANING = \"Up to which bunch sigma the starting amplitudes were ramped up\" , DSIGMA_MEANING = \"Increment value of AMPLITUDE at each new starting amplitude\" , ), ) tfs_dframe = tfs_dframe . reset_index ( drop = True ) if file : tfs . write ( Path ( file ). absolute (), tfs_dframe ) return tfs_dframe","title":"Tune"},{"location":"reference/pyhdtoolkit/cpymadtools/tune/#module-pyhdtoolkitcpymadtoolstune","text":"Module cpymadtools.tune Created on 2021.04.01 View Source \"\"\" Module cpymadtools.tune ----------------------- Created on 2021.04.01 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions to manipulate MAD-X functionality around the tune through a cpymad.madx.Madx object. \"\"\" import math import sys from pathlib import Path from typing import Dict , List , Tuple import matplotlib.collections import matplotlib.patches import numpy as np import tfs from cpymad.madx import Madx from loguru import logger def make_footprint_table ( madx : Madx , sigma : float = 5 , dense : bool = False , file : str = None , cleanup : bool = True , ** kwargs , ) -> tfs . TfsDataFrame : \"\"\" Instantiates an ensemble of particles up to the desired bunch sigma amplitude to be tracked for the DYNAP command, letting MAD-X infer their tunes. Particules are instantiated for different angle variables for each amplitude, creating an ensemble able to represent the tune footprint. Beware: since the `DYNAP` command makes use of tracking, your sequence needs to be sliced before calling this function. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. sigma (float): the maximum amplitude of the tracked particles, in bunch sigma. Defaults to 5. dense (bool): if set to True, an increased number of particles will be tracked. Defaults to False. file (str): If given, the `dynaptune` table will be exported as a TFS file with the provided name. cleanup (bool): If True, the `fort.69` and `lyapunov.data` files are cleared before returning the dynap table. Defaults to True. Keyword Args: Any keyword argument that will be transmitted to the DYNAP command in MAD-X. Returns: The resulting `dynaptune` table, as a pandas DataFrame. \"\"\" logger . info ( f \"Initiating particules up to { sigma : d } bunch sigma to create a tune footprint table\" ) small , big = 0.05 , math . sqrt ( 1 - 0.05 ** 2 ) sigma_multiplier , angle_multiplier = 0.1 , 0 logger . debug ( \"Initializing particles\" ) madx . command . track () madx . command . start ( fx = small , fy = small ) while sigma_multiplier <= sigma + 1 : angle = 15 * angle_multiplier * math . pi / 180 if angle_multiplier == 0 : madx . command . start ( fx = sigma_multiplier * big , fy = sigma_multiplier * small ) elif angle_multiplier == 6 : madx . command . start ( fx = sigma_multiplier * small , fy = sigma_multiplier * big ) else : madx . command . start ( fx = sigma_multiplier * math . cos ( angle ), fy = sigma_multiplier * math . sin ( angle )) angle_multiplier += 0.5 if int ( angle_multiplier ) == 7 : angle_multiplier = 0 sigma_multiplier += 1 if not dense else 0.5 logger . debug ( \"Starting DYNAP tracking with initialized particles\" ) try : madx . command . dynap ( fastune = True , turns = 1024 , ** kwargs ) madx . command . endtrack () except RuntimeError as madx_crash : logger . error ( \"Remote MAD-X process crashed, most likely because you did not slice the sequence \" \"before running DYNAP. Restart and slice before calling this function.\" ) raise RuntimeError ( \"DYNAP command crashed the MAD-X process\" ) from madx_crash if cleanup and sys . platform not in ( \"win32\" , \"cygwin\" ): # fails on Windows due to its I/O system, since MAD-X still has \"control\" of the files try : logger . debug ( \"Cleaning up DYNAP output files `fort.69` and `lyapunov.data`\" ) Path ( \"fort.69\" ) . unlink () Path ( \"lyapunov.data\" ) . unlink () except FileNotFoundError : logger . error ( \"Could not cleanup DYNAP output files, they might have not been created\" ) tfs_dframe = tfs . TfsDataFrame ( data = madx . table . dynaptune . dframe (), headers = dict ( NAME = \"DYNAPTUNE\" , TYPE = \"DYNAPTUNE\" , TITLE = \"FOOTPRINT TABLE\" , MADX_VERSION = str ( madx . version ) . upper (), ORIGIN = \"pyhdtoolkit.cpymadtools.tune.make_footprint_table() function\" , ANGLE = 7 , # default of the function AMPLITUDE = sigma , DSIGMA = 1 if not dense else 0.5 , ANGLE_MEANING = \"Number of different starting angles used for each starting amplitude\" , AMPLITUDE_MEANING = \"Up to which bunch sigma the starting amplitudes were ramped up\" , DSIGMA_MEANING = \"Increment value of AMPLITUDE at each new starting amplitude\" , ), ) tfs_dframe = tfs_dframe . reset_index ( drop = True ) if file : tfs . write ( Path ( file ) . absolute (), tfs_dframe ) return tfs_dframe def get_footprint_lines ( dynap_dframe : tfs . TfsDataFrame ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Provided with the `TfsDataFrame` returned by `make_footprint_table()`, determines the various (Qx, Qy) points needed to plot the footprint data with lines representing the different amplitudes and angles from starting particles, and returns these in immediately plottable numpy arrays. WARNING: This function is some DARK MAGIC stuff I have taken out of very dusty drawers, and I cannot explain exactly how it works. I also do not know who wrote this initially. Results are not guaranteed to be correct and should be checked with a quick plot. Usage: ```python dynap_tfs = make_footprint_table(madx) qxs, qys = get_footprint_lines(dynap_tfs) plt.plot(qxs, qys, \"o--\", label=\"Tune Footprint from DYNAP Table\") ``` Args: dynap_dframe (tfs.TfsDataFrame): the dynap data frame returned by `make_footprint_table()`. Returns: The Qx and Qy data points to plot directly, both as numpy.ndarrays. \"\"\" logger . info ( \"Determining footprint plottable\" ) logger . debug ( \"Retrieving AMPLITUDE, ANGLE and DSIGMA data from TfsDataFrame headers\" ) amplitude = dynap_dframe . headers [ \"AMPLITUDE\" ] angle = dynap_dframe . headers [ \"ANGLE\" ] dsigma = dynap_dframe . headers [ \"DSIGMA\" ] tune_groups = _make_tune_groups ( dynap_string_rep = _get_dynap_string_rep ( dynap_dframe ), dsigma = dsigma ) footprint = _Footprint ( tune_groups , amplitude , angle , dsigma ) qxs , qys = footprint . get_plottable () return np . array ( qxs , dtype = float ), np . array ( qys , dtype = float ) def get_footprint_patches ( dynap_dframe : tfs . TfsDataFrame ,) -> matplotlib . collections . PatchCollection : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO KONSTANTINOS PARASCHOU (@kparasch). Provided with the `TfsDataFrame` returned by `make_footprint_table()`, computes the polygon patches needed to plot the footprint data, with lines representing the different amplitudes and angles from starting particles, and returns the `PatchCollection` with the computed polygons. The polygons will have blue edges, except the ones corresponding to the last starting angle particles ( in red) and the last starting amplitude particles (in green). WARNING: The internal construction of polygons can be tricky, and you might need to change the `ANGLE` or `AMPLITUDE` values in `dynap_dframe`'s headers. Usage: ```python fig, axis = plt.subplots() dynap_tfs = make_footprint_table(madx) footprint_polygons = get_footprint_patches(dynap_tfs) axis.add_collection(footprint_polygons) ``` Args: dynap_dframe (tfs.TfsDataFrame): the dynap data frame returned by `make_footprint_table()`. Returns: The `matplotlib.collections.PatchCollection` with the created polygons. \"\"\" logger . info ( \"Determining footprint polygons\" ) angle = dynap_dframe . headers [ \"ANGLE\" ] amplitude = dynap_dframe . headers [ \"AMPLITUDE\" ] logger . debug ( \"Grouping tune points according to starting angles and amplitudes\" ) try : A = np . zeros ([ amplitude , angle , 2 ]) A [ 0 , :, 0 ] = dynap_dframe [ \"tunx\" ] . to_numpy ()[ 0 ] A [ 0 , :, 1 ] = dynap_dframe [ \"tuny\" ] . to_numpy ()[ 0 ] A [ 1 :, :, 0 ] = dynap_dframe [ \"tunx\" ] . to_numpy ()[ 1 :] . reshape ( - 1 , angle ) A [ 1 :, :, 1 ] = dynap_dframe [ \"tuny\" ] . to_numpy ()[ 1 :] . reshape ( - 1 , angle ) except ValueError as tune_grouping_error : logger . error ( \"Cannot group tune points according to starting angles and amplitudes. Try changing \" \"the 'AMPLITUDE' value in the provided TfsDataFrame's headers.\" ) raise tune_grouping_error logger . debug ( \"Determining polygon vertices\" ) sx = A . shape [ 0 ] - 1 sy = A . shape [ 1 ] - 1 p1 = A [: - 1 , : - 1 , :] . reshape ( sx * sy , 2 )[:, :] p2 = A [ 1 :, : - 1 , :] . reshape ( sx * sy , 2 )[:] p3 = A [ 1 :, 1 :, :] . reshape ( sx * sy , 2 )[:] p4 = A [: - 1 , 1 :, :] . reshape ( sx * sy , 2 )[:] polygons = np . stack (( p1 , p2 , p3 , p4 )) # Stack endpoints to form polygons polygons = np . transpose ( polygons , ( 1 , 0 , 2 )) # transpose polygons logger . debug ( \"Creating PatchCollection of Polygons\" ) patches = list ( map ( matplotlib . patches . Polygon , polygons )) patch_colors = [( 0 , 0 , 1 ) for _ in polygons ] patch_colors [( sx - 1 ) * sy :] = [( 0 , 1 , 0 )] * sy # differentiate first angle in green patch_colors [( sy - 1 ) :: sy ] = [( 1 , 0 , 0 )] * sx # differentiate last amplitude in red return matplotlib . collections . PatchCollection ( patches , facecolors = [], edgecolor = patch_colors ) # ----- Arcane Private Utilities ----- # def _get_dynap_string_rep ( dynap_dframe : tfs . TfsDataFrame ) -> str : \"\"\" This is a weird dusty function to get a specific useful string representation from the `TfsDataFrame` returned by `make_footprint_table()`. This specific dataframe contains important information. Args: dynap_dframe (tfs.TfsDataFrame): the dynap data frame returned by `make_footprint_table()`. Returns: A weird string representation gathering tune points split according to the number of angles and amplitudes used in `make_footprint_table()`. \"\"\" logger . trace ( \"Retrieving AMPLITUDE and ANGLE data from TfsDataFrame headers\" ) amplitude = dynap_dframe . headers [ \"AMPLITUDE\" ] angle = dynap_dframe . headers [ \"ANGLE\" ] string_rep = f \"TMPNAME, { amplitude } ,1,< { dynap_dframe . tunx [ 0 ] } ; { dynap_dframe . tuny [ 0 ] } >\" for n in range ( 1 , amplitude ): string_rep += f \", { angle } \" for m in range ( angle ): string_rep += f \",< { dynap_dframe . tunx [ 1 + ( n - 1 ) * angle + m ] } ; { dynap_dframe . tuny [ 1 + ( n - 1 ) * angle + m ] } >\" return string_rep def _make_tune_groups ( dynap_string_rep : str , dsigma : float = 1.0 ) -> List [ List [ Dict [ str , float ]]]: \"\"\" Creates appropriate tune points groups from the arcane string representation returned by `_get_dynap_string_rep` based on starting amplitude and angle for each particle. Args: dynap_string_rep (str): weird string representation of the `TfsDataFrame` returned by `make_footprint_table()` and as given by `_get_dynap_string_rep()`. dsigma (float): The increment in amplitude between different starting amplitudes when starting particles for the `DYNAP` command in `MAD-X`. This information is in the headers of the `TfsDataFrame` returned by `make_footprint_table()`. Returns: A list of list of dictionaries containing horizontal and vertical tune points. The data is constructed such that one can access the data of a particle starting with 'amplitude' and 'angle' with data[amplitude][angle][\"H\"/\"V\"]. This function is only meant to be used internally by `get_footprint_lines()`. \"\"\" logger . debug ( \"Constructing tune points groups based on starting amplitudes and angles\" ) tune_groups = [] items = dynap_string_rep . strip () . split ( \",\" ) amplitude = int ( items [ 1 ]) current = 2 for i in np . arange ( amplitude ): tune_groups . append ([]) angle = int ( items [ current ]) current = current + 1 for j in np . arange ( angle ): tune_groups [ i ] . append ([]) tune_string = items [ current ] . lstrip ( \"<\" ) . rstrip ( \">\" ) . split ( \";\" ) tune_groups [ i ][ j ] = { \"H\" : float ( tune_string [ 0 ]), \"V\" : float ( tune_string [ 1 ])} current = current + 1 return tune_groups class _Footprint : \"\"\"More dark magic from the past here, close your eyes my friends.\"\"\" def __init__ ( self , tune_groups : List [ List [ Dict [ str , float ]]], amplitude : int , angle : int , dsigma : float ): self . _tunes = tune_groups self . _maxnangl = angle self . _nampl = amplitude self . _dSigma = dsigma def get_h_tune ( self , ampl , angl ): if len ( self . _tunes [ ampl ]) <= angl < self . _maxnangl : return self . _tunes [ ampl ][ len ( self . _tunes [ ampl ]) - 1 ][ \"H\" ] else : return self . _tunes [ ampl ][ angl ][ \"H\" ] def get_v_tune ( self , ampl , angl ): if len ( self . _tunes [ ampl ]) <= angl < self . _maxnangl : return self . _tunes [ ampl ][ len ( self . _tunes [ ampl ]) - 1 ][ \"V\" ] else : return self . _tunes [ ampl ][ angl ][ \"V\" ] def get_plottable ( self ) -> Tuple [ List [ float ], List [ float ]]: qxs , qys = [], [] for i in np . arange ( 0 , self . _nampl - 1 , 2 ): for j in np . arange ( self . _maxnangl ): qxs . append ( self . get_h_tune ( i , j )) qys . append ( self . get_v_tune ( i , j )) for j in np . arange ( self . _maxnangl - 1 , - 1 , - 1 ): qxs . append ( self . get_h_tune ( i + 1 , j )) qys . append ( self . get_v_tune ( i + 1 , j )) if self . _nampl % 2 == 0 : for j in np . arange ( 0 , self . _maxnangl - 1 , 2 ): for i in np . arange ( self . _nampl - 1 , - 1 , - 1 ): qxs . append ( self . get_h_tune ( i , j )) qys . append ( self . get_v_tune ( i , j )) for i in np . arange ( 0 , self . _nampl , 1 ): qxs . append ( self . get_h_tune ( i , j + 1 )) qys . append ( self . get_v_tune ( i , j + 1 )) if self . _maxnangl % 2 != 0 : for i in np . arange ( self . _nampl - 1 , - 1 , - 1 ): qxs . append ( self . get_h_tune ( i , self . _maxnangl - 1 )) qys . append ( self . get_v_tune ( i , self . _maxnangl - 1 )) qxs . append ( self . get_h_tune ( 0 , self . _maxnangl - 2 )) qys . append ( self . get_v_tune ( 0 , self . _maxnangl - 2 )) else : for j in np . arange ( self . _maxnangl ): qxs . append ( self . get_h_tune ( self . _nampl - 1 , j )) qys . append ( self . get_v_tune ( self . _nampl - 1 , j )) for j in np . arange ( self . _maxnangl - 1 , - 1 , - 2 ): for i in np . arange ( self . _nampl - 1 , - 1 , - 1 ): qxs . append ( self . get_h_tune ( i , j )) qys . append ( self . get_v_tune ( i , j )) for i in np . arange ( 0 , self . _nampl , 1 ): qxs . append ( self . get_h_tune ( i , j - 1 )) qys . append ( self . get_v_tune ( i , j - 1 )) return qxs , qys","title":"Module pyhdtoolkit.cpymadtools.tune"},{"location":"reference/pyhdtoolkit/cpymadtools/tune/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/tune/#get_footprint_lines","text":"def get_footprint_lines ( dynap_dframe : tfs . handler . TfsDataFrame ) -> Tuple [ numpy . ndarray , numpy . ndarray ] Provided with the TfsDataFrame returned by make_footprint_table() , determines the various (Qx, Qy) points needed to plot the footprint data with lines representing the different amplitudes and angles from starting particles, and returns these in immediately plottable numpy arrays. WARNING : This function is some DARK MAGIC stuff I have taken out of very dusty drawers , and I cannot explain exactly how it works . I also do not know who wrote this initially . Results are not guaranteed to be correct and should be checked with a quick plot . Usage: dynap_tfs = make_footprint_table ( madx ) qxs , qys = get_footprint_lines ( dynap_tfs ) plt . plot ( qxs , qys , \"o--\" , label = \"Tune Footprint from DYNAP Table\" ) Parameters: Name Type Description Default dynap_dframe tfs.TfsDataFrame the dynap data frame returned by make_footprint_table() . None Returns: Type Description None The Qx and Qy data points to plot directly, both as numpy.ndarrays. View Source def get_footprint_lines ( dynap_dframe : tfs . TfsDataFrame ) -> Tuple [ np . ndarray , np . ndarray ] : \" \"\" Provided with the `TfsDataFrame` returned by `make_footprint_table()`, determines the various (Qx, Qy) points needed to plot the footprint data with lines representing the different amplitudes and angles from starting particles, and returns these in immediately plottable numpy arrays. WARNING: This function is some DARK MAGIC stuff I have taken out of very dusty drawers, and I cannot explain exactly how it works. I also do not know who wrote this initially. Results are not guaranteed to be correct and should be checked with a quick plot. Usage: ```python dynap_tfs = make_footprint_table(madx) qxs, qys = get_footprint_lines(dynap_tfs) plt.plot(qxs, qys, \" o -- \", label=\" Tune Footprint from DYNAP Table \") ``` Args: dynap_dframe (tfs.TfsDataFrame): the dynap data frame returned by `make_footprint_table()`. Returns: The Qx and Qy data points to plot directly, both as numpy.ndarrays. \"\" \" logger . info ( \"Determining footprint plottable\" ) logger . debug ( \"Retrieving AMPLITUDE, ANGLE and DSIGMA data from TfsDataFrame headers\" ) amplitude = dynap_dframe . headers [ \"AMPLITUDE\" ] angle = dynap_dframe . headers [ \"ANGLE\" ] dsigma = dynap_dframe . headers [ \"DSIGMA\" ] tune_groups = _make_tune_groups ( dynap_string_rep = _get_dynap_string_rep ( dynap_dframe ), dsigma = dsigma ) footprint = _Footprint ( tune_groups , amplitude , angle , dsigma ) qxs , qys = footprint . get_plottable () return np . array ( qxs , dtype = float ), np . array ( qys , dtype = float )","title":"get_footprint_lines"},{"location":"reference/pyhdtoolkit/cpymadtools/tune/#get_footprint_patches","text":"def get_footprint_patches ( dynap_dframe : tfs . handler . TfsDataFrame ) -> matplotlib . collections . PatchCollection INITIAL IMPLEMENTATION CREDITS GO TO KONSTANTINOS PARASCHOU (@kparasch). Provided with the TfsDataFrame returned by make_footprint_table() , computes the polygon patches needed to plot the footprint data, with lines representing the different amplitudes and angles from starting particles, and returns the PatchCollection with the computed polygons. The polygons will have blue edges, except the ones corresponding to the last starting angle particles ( in red) and the last starting amplitude particles (in green). WARNING : The internal construction of polygons can be tricky , and you might need to change the `ANGLE` or `AMPLITUDE` values in `dynap_dframe` 's headers. Usage: ```python fig, axis = plt.subplots() dynap_tfs = make_footprint_table(madx) footprint_polygons = get_footprint_patches(dynap_tfs) axis.add_collection(footprint_polygons) ``` Parameters: Name Type Description Default dynap_dframe tfs.TfsDataFrame the dynap data frame returned by make_footprint_table() . None Returns: Type Description None The matplotlib.collections.PatchCollection with the created polygons. View Source def get_footprint_patches ( dynap_dframe: tfs . TfsDataFrame ,) -> matplotlib . collections . PatchCollection : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO KONSTANTINOS PARASCHOU (@kparasch). Provided with the `TfsDataFrame` returned by `make_footprint_table()`, computes the polygon patches needed to plot the footprint data, with lines representing the different amplitudes and angles from starting particles, and returns the `PatchCollection` with the computed polygons. The polygons will have blue edges, except the ones corresponding to the last starting angle particles ( in red) and the last starting amplitude particles (in green). WARNING: The internal construction of polygons can be tricky, and you might need to change the `ANGLE` or `AMPLITUDE` values in `dynap_dframe`'s headers. Usage: ```python fig, axis = plt.subplots() dynap_tfs = make_footprint_table(madx) footprint_polygons = get_footprint_patches(dynap_tfs) axis.add_collection(footprint_polygons) ``` Args: dynap_dframe (tfs.TfsDataFrame): the dynap data frame returned by `make_footprint_table()`. Returns: The `matplotlib.collections.PatchCollection` with the created polygons. \"\"\" logger . info ( \"Determining footprint polygons\" ) angle = dynap_dframe . headers [ \"ANGLE\" ] amplitude = dynap_dframe . headers [ \"AMPLITUDE\" ] logger . debug ( \"Grouping tune points according to starting angles and amplitudes\" ) try : A = np . zeros ([ amplitude , angle , 2 ]) A [ 0 , : , 0 ] = dynap_dframe [ \"tunx\" ]. to_numpy ()[ 0 ] A [ 0 , : , 1 ] = dynap_dframe [ \"tuny\" ]. to_numpy ()[ 0 ] A [ 1 : , : , 0 ] = dynap_dframe [ \"tunx\" ]. to_numpy ()[ 1 : ]. reshape ( - 1 , angle ) A [ 1 : , : , 1 ] = dynap_dframe [ \"tuny\" ]. to_numpy ()[ 1 : ]. reshape ( - 1 , angle ) except ValueError as tune_grouping_error: logger . error ( \"Cannot group tune points according to starting angles and amplitudes. Try changing \" \"the 'AMPLITUDE' value in the provided TfsDataFrame's headers.\" ) raise tune_grouping_error logger . debug ( \"Determining polygon vertices\" ) sx = A . shape [ 0 ] - 1 sy = A . shape [ 1 ] - 1 p1 = A [:- 1 , :- 1 , : ]. reshape ( sx * sy , 2 )[ : , : ] p2 = A [ 1 : , :- 1 , : ]. reshape ( sx * sy , 2 )[ : ] p3 = A [ 1 : , 1 : , : ]. reshape ( sx * sy , 2 )[ : ] p4 = A [:- 1 , 1 : , : ]. reshape ( sx * sy , 2 )[ : ] polygons = np . stack (( p1 , p2 , p3 , p4 )) # Stack endpoints to form polygons polygons = np . transpose ( polygons , ( 1 , 0 , 2 )) # transpose polygons logger . debug ( \"Creating PatchCollection of Polygons\" ) patches = list ( map ( matplotlib . patches . Polygon , polygons )) patch_colors = [( 0 , 0 , 1 ) for _ in polygons ] patch_colors [( sx - 1 ) * sy : ] = [( 0 , 1 , 0 )] * sy # differentiate first angle in green patch_colors [( sy - 1 ) :: sy ] = [( 1 , 0 , 0 )] * sx # differentiate last amplitude in red return matplotlib . collections . PatchCollection ( patches , facecolors = [], edgecolor = patch_colors )","title":"get_footprint_patches"},{"location":"reference/pyhdtoolkit/cpymadtools/tune/#make_footprint_table","text":"def make_footprint_table ( madx : cpymad . madx . Madx , sigma : float = 5 , dense : bool = False , file : str = None , cleanup : bool = True , ** kwargs ) -> tfs . handler . TfsDataFrame Instantiates an ensemble of particles up to the desired bunch sigma amplitude to be tracked for the DYNAP command, letting MAD-X infer their tunes. Particules are instantiated for different angle variables for each amplitude, creating an ensemble able to represent the tune footprint. Beware: since the DYNAP command makes use of tracking, your sequence needs to be sliced before calling this function. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None sigma float the maximum amplitude of the tracked particles, in bunch sigma. Defaults to 5. 5 dense bool if set to True, an increased number of particles will be tracked. Defaults to False. False file str If given, the dynaptune table will be exported as a TFS file with the provided name. None cleanup bool If True, the fort.69 and lyapunov.data files are cleared before returning the dynap table. Defaults to True. None Keyword Args None Any keyword argument that will be transmitted to the DYNAP command in MAD-X. None Returns: Type Description None The resulting dynaptune table, as a pandas DataFrame. View Source def make_footprint_table ( madx : Madx , sigma : float = 5 , dense : bool = False , file : str = None , cleanup : bool = True , ** kwargs , ) -> tfs . TfsDataFrame : \" \"\" Instantiates an ensemble of particles up to the desired bunch sigma amplitude to be tracked for the DYNAP command, letting MAD-X infer their tunes. Particules are instantiated for different angle variables for each amplitude, creating an ensemble able to represent the tune footprint. Beware: since the `DYNAP` command makes use of tracking, your sequence needs to be sliced before calling this function. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. sigma (float): the maximum amplitude of the tracked particles, in bunch sigma. Defaults to 5. dense (bool): if set to True, an increased number of particles will be tracked. Defaults to False. file (str): If given, the `dynaptune` table will be exported as a TFS file with the provided name. cleanup (bool): If True, the `fort.69` and `lyapunov.data` files are cleared before returning the dynap table. Defaults to True. Keyword Args: Any keyword argument that will be transmitted to the DYNAP command in MAD-X. Returns: The resulting `dynaptune` table, as a pandas DataFrame. \"\" \" logger . info ( f \"Initiating particules up to {sigma:d} bunch sigma to create a tune footprint table\" ) small , big = 0.05 , math . sqrt ( 1 - 0.05 ** 2 ) sigma_multiplier , angle_multiplier = 0.1 , 0 logger . debug ( \"Initializing particles\" ) madx . command . track () madx . command . start ( fx = small , fy = small ) while sigma_multiplier <= sigma + 1 : angle = 15 * angle_multiplier * math . pi / 180 if angle_multiplier == 0 : madx . command . start ( fx = sigma_multiplier * big , fy = sigma_multiplier * small ) elif angle_multiplier == 6 : madx . command . start ( fx = sigma_multiplier * small , fy = sigma_multiplier * big ) else : madx . command . start ( fx = sigma_multiplier * math . cos ( angle ), fy = sigma_multiplier * math . sin ( angle )) angle_multiplier += 0.5 if int ( angle_multiplier ) == 7 : angle_multiplier = 0 sigma_multiplier += 1 if not dense else 0.5 logger . debug ( \"Starting DYNAP tracking with initialized particles\" ) try : madx . command . dynap ( fastune = True , turns = 1024 , ** kwargs ) madx . command . endtrack () except RuntimeError as madx_crash : logger . error ( \"Remote MAD-X process crashed, most likely because you did not slice the sequence \" \"before running DYNAP. Restart and slice before calling this function.\" ) raise RuntimeError ( \"DYNAP command crashed the MAD-X process\" ) from madx_crash if cleanup and sys . platform not in ( \"win32\" , \"cygwin\" ) : # fails on Windows due to its I/O system, since MAD-X still has \"control\" of the files try : logger . debug ( \"Cleaning up DYNAP output files `fort.69` and `lyapunov.data`\" ) Path ( \"fort.69\" ). unlink () Path ( \"lyapunov.data\" ). unlink () except FileNotFoundError : logger . error ( \"Could not cleanup DYNAP output files, they might have not been created\" ) tfs_dframe = tfs . TfsDataFrame ( data = madx . table . dynaptune . dframe (), headers = dict ( NAME = \"DYNAPTUNE\" , TYPE = \"DYNAPTUNE\" , TITLE = \"FOOTPRINT TABLE\" , MADX_VERSION = str ( madx . version ). upper (), ORIGIN = \"pyhdtoolkit.cpymadtools.tune.make_footprint_table() function\" , ANGLE = 7 , # default of the function AMPLITUDE = sigma , DSIGMA = 1 if not dense else 0.5 , ANGLE_MEANING = \"Number of different starting angles used for each starting amplitude\" , AMPLITUDE_MEANING = \"Up to which bunch sigma the starting amplitudes were ramped up\" , DSIGMA_MEANING = \"Increment value of AMPLITUDE at each new starting amplitude\" , ), ) tfs_dframe = tfs_dframe . reset_index ( drop = True ) if file : tfs . write ( Path ( file ). absolute (), tfs_dframe ) return tfs_dframe","title":"make_footprint_table"},{"location":"reference/pyhdtoolkit/cpymadtools/twiss/","text":"Module pyhdtoolkit.cpymadtools.twiss Module cpymadtools.twiss Created on 2020.02.03 View Source \"\"\" Module cpymadtools.twiss ------------------------ Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to manipulate MAD-X TWISS functionality through a cpymad.madx.Madx object. \"\"\" from typing import Sequence import numpy as np import tfs from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.cpymadtools.constants import DEFAULT_TWISS_COLUMNS # ----- Utlites ----- # def get_pattern_twiss ( madx : Madx , patterns : Sequence [ str ] = [ \"\" ], columns : Sequence [ str ] = None , ** kwargs , ) -> tfs . TfsDataFrame : \"\"\" Extract the `TWISS` table for desired variables, and for certain elements matching a pattern. Additionally, the `SUMM` table is also returned in the form of the TfsDataFrame's headers dictionary. Warning: Although the `pattern` parameter should accept a regex, MAD-X does not implement actual regexes. Please refer to the MAD-X manual, section `Regular Expressions` for details on what is implemented in MAD-X itself. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. patterns (Sequence[str]): the different element patterns (such as `MQX` or `BPM`) to be applied to the command, which will determine the rows in the returned DataFrame. Defaults to [\"\"] which will select all elements. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Defaults to None, which will return all available columns. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as `chrom`, `ripken`, `centre` or starting coordinates with `betax`, 'betay` etc. Returns: A TfsDataFrame with the selected columns for all elements matching the provided patterns, and the internal `summ` table as header dict. \"\"\" logger . trace ( \"Clearing 'TWISS' flag\" ) madx . select ( flag = \"twiss\" , clear = True ) for pattern in patterns : logger . trace ( f \"Adding pattern { pattern } to 'TWISS' flag\" ) madx . select ( flag = \"twiss\" , pattern = pattern , column = columns ) madx . twiss ( ** kwargs ) logger . trace ( \"Extracting relevant parts of the TWISS table\" ) twiss_df = tfs . TfsDataFrame ( madx . table . twiss . dframe () . copy ()) twiss_df . headers = { var . upper (): madx . table . summ [ var ][ 0 ] for var in madx . table . summ } twiss_df = twiss_df [ madx . table . twiss . selected_columns ()] . iloc [ np . array ( madx . table . twiss . selected_rows ()) . astype ( bool ) ] logger . trace ( \"Clearing 'TWISS' flag\" ) madx . select ( flag = \"twiss\" , clear = True ) return twiss_df def get_twiss_tfs ( madx : Madx ) -> tfs . TfsDataFrame : \"\"\" Returns a tfs.TfsDataFrame from the Madx instance's twiss dframe, typically in the way we're used to getting it from MAD-X outputting the TWISS (uppercase names, colnames, summ table in headers). Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A tfs.TfsDataFrame. \"\"\" logger . info ( \"Exporting internal TWISS and SUMM tables to TfsDataFrame\" ) twiss_tfs = tfs . TfsDataFrame ( madx . table . twiss . dframe ()) twiss_tfs . name = [ element [: - 2 ] for element in twiss_tfs . name ] twiss_tfs . columns = twiss_tfs . columns . str . upper () twiss_tfs = twiss_tfs . set_index ( \"NAME\" ) twiss_tfs . index = twiss_tfs . index . str . upper () twiss_tfs . headers = { var . upper (): madx . table . summ [ var ][ 0 ] for var in madx . table . summ } return twiss_tfs def get_ips_twiss ( madx : Madx , columns : Sequence [ str ] = DEFAULT_TWISS_COLUMNS , ** kwargs ) -> tfs . TfsDataFrame : \"\"\" Quickly get the `TWISS` table for certain variables at IP locations only. The `SUMM` table will be included as the TfsDataFrame's header dictionary. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as `chrom`, `ripken`, `centre` or starting coordinates with `betax`, 'betay` etc. Returns: A TfsDataFrame of the twiss output. \"\"\" logger . info ( \"Getting Twiss at IPs\" ) return get_pattern_twiss ( madx = madx , patterns = [ \"IP\" ], columns = columns , ** kwargs ) def get_ir_twiss ( madx : Madx , ir : int , columns : Sequence [ str ] = DEFAULT_TWISS_COLUMNS , ** kwargs ) -> tfs . TfsDataFrame : \"\"\" Quickly get the `TWISS` table for certain variables for one IR, meaning at the IP and Q1 to Q3 both left and right of the IP. The `SUMM` table will be included as the TfsDataFrame's header dictionary. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ir (int): which interaction region to get the TWISS for. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as `chrom`, `ripken`, `centre` or starting coordinates with `betax`, 'betay` etc. Returns: A TfsDataFrame of the twiss output. \"\"\" logger . info ( f \"Getting Twiss for IR { ir : d } \" ) return get_pattern_twiss ( madx = madx , patterns = [ f \"IP { ir : d } \" , f \"MQXA.[12345][RL] { ir : d } \" , # Q1 and Q3 LHC f \"MQXB.[AB][12345][RL] { ir : d } \" , # Q2A and Q2B LHC f \"MQXF[AB].[AB][12345][RL] { ir : d } \" , # Q1 to Q3 A and B HL-LHC ], columns = columns , ** kwargs , ) Variables DEFAULT_TWISS_COLUMNS Functions get_ips_twiss def get_ips_twiss ( madx : cpymad . madx . Madx , columns : Sequence [ str ] = [ 'name' , 's' , 'x' , 'y' , 'px' , 'py' , 'betx' , 'bety' , 'alfx' , 'alfy' , 'dx' , 'dy' , 'mux' , 'muy' , 'r11' , 'r12' , 'r21' , 'r22' , 'beta11' , 'beta12' , 'beta21' , 'beta22' ], ** kwargs ) -> tfs . handler . TfsDataFrame Quickly get the TWISS table for certain variables at IP locations only. The SUMM table will be included as the TfsDataFrame's header dictionary. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as chrom , ripken , centre or starting coordinates with betax , 'betay` etc. Returns: A TfsDataFrame of the twiss output. View Source def get_ips_twiss ( madx : Madx , columns : Sequence [ str ] = DEFAULT_TWISS_COLUMNS , ** kwargs ) -> tfs . TfsDataFrame : \" \"\" Quickly get the `TWISS` table for certain variables at IP locations only. The `SUMM` table will be included as the TfsDataFrame's header dictionary. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as `chrom`, `ripken`, `centre` or starting coordinates with `betax`, 'betay` etc. Returns: A TfsDataFrame of the twiss output. \"\" \" logger . info ( \"Getting Twiss at IPs\" ) return get_pattern_twiss ( madx = madx , patterns = [ \"IP\" ] , columns = columns , ** kwargs ) get_ir_twiss def get_ir_twiss ( madx : cpymad . madx . Madx , ir : int , columns : Sequence [ str ] = [ 'name' , 's' , 'x' , 'y' , 'px' , 'py' , 'betx' , 'bety' , 'alfx' , 'alfy' , 'dx' , 'dy' , 'mux' , 'muy' , 'r11' , 'r12' , 'r21' , 'r22' , 'beta11' , 'beta12' , 'beta21' , 'beta22' ], ** kwargs ) -> tfs . handler . TfsDataFrame Quickly get the TWISS table for certain variables for one IR, meaning at the IP and Q1 to Q3 both left and right of the IP. The SUMM table will be included as the TfsDataFrame's header dictionary. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ir (int): which interaction region to get the TWISS for. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as chrom , ripken , centre or starting coordinates with betax , 'betay` etc. Returns: A TfsDataFrame of the twiss output. View Source def get_ir_twiss ( madx : Madx , ir : int , columns : Sequence [ str ] = DEFAULT_TWISS_COLUMNS , ** kwargs ) -> tfs . TfsDataFrame : \"\"\" Quickly get the `TWISS` table for certain variables for one IR, meaning at the IP and Q1 to Q3 both left and right of the IP. The `SUMM` table will be included as the TfsDataFrame's header dictionary. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ir (int): which interaction region to get the TWISS for. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as `chrom`, `ripken`, `centre` or starting coordinates with `betax`, 'betay` etc. Returns: A TfsDataFrame of the twiss output. \"\"\" logger . info ( f \"Getting Twiss for IR {ir:d}\" ) return get_pattern_twiss ( madx = madx , patterns = [ f \"IP{ir:d}\" , f \"MQXA.[12345][RL]{ir:d}\" , # Q1 and Q3 LHC f \"MQXB.[AB][12345][RL]{ir:d}\" , # Q2A and Q2B LHC f \"MQXF[AB].[AB][12345][RL]{ir:d}\" , # Q1 to Q3 A and B HL-LHC ], columns = columns , ** kwargs , ) get_pattern_twiss def get_pattern_twiss ( madx : cpymad . madx . Madx , patterns : Sequence [ str ] = [ '' ], columns : Sequence [ str ] = None , ** kwargs ) -> tfs . handler . TfsDataFrame Extract the TWISS table for desired variables, and for certain elements matching a pattern. Additionally, the SUMM table is also returned in the form of the TfsDataFrame's headers dictionary. Warning: Although the pattern parameter should accept a regex, MAD-X does not implement actual regexes. Please refer to the MAD-X manual, section Regular Expressions for details on what is implemented in MAD-X itself. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. patterns (Sequence[str]): the different element patterns (such as MQX or BPM ) to be applied to the command, which will determine the rows in the returned DataFrame. Defaults to [\"\"] which will select all elements. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Defaults to None, which will return all available columns. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as chrom , ripken , centre or starting coordinates with betax , 'betay` etc. Returns: A TfsDataFrame with the selected columns for all elements matching the provided patterns, and the internal summ table as header dict. View Source def get_pattern_twiss ( madx : Madx , patterns : Sequence [ str ] = [ \"\" ] , columns : Sequence [ str ] = None , ** kwargs , ) -> tfs . TfsDataFrame : \" \"\" Extract the `TWISS` table for desired variables, and for certain elements matching a pattern. Additionally, the `SUMM` table is also returned in the form of the TfsDataFrame's headers dictionary. Warning: Although the `pattern` parameter should accept a regex, MAD-X does not implement actual regexes. Please refer to the MAD-X manual, section `Regular Expressions` for details on what is implemented in MAD-X itself. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. patterns (Sequence[str]): the different element patterns (such as `MQX` or `BPM`) to be applied to the command, which will determine the rows in the returned DataFrame. Defaults to [ \"\" ] which will select all elements. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Defaults to None, which will return all available columns. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as `chrom`, `ripken`, `centre` or starting coordinates with `betax`, 'betay` etc. Returns: A TfsDataFrame with the selected columns for all elements matching the provided patterns, and the internal `summ` table as header dict. \"\" \" logger . trace ( \"Clearing 'TWISS' flag\" ) madx . select ( flag = \"twiss\" , clear = True ) for pattern in patterns : logger . trace ( f \"Adding pattern {pattern} to 'TWISS' flag\" ) madx . select ( flag = \"twiss\" , pattern = pattern , column = columns ) madx . twiss ( ** kwargs ) logger . trace ( \"Extracting relevant parts of the TWISS table\" ) twiss_df = tfs . TfsDataFrame ( madx . table . twiss . dframe (). copy ()) twiss_df . headers = { var . upper () : madx . table . summ [ var ][ 0 ] for var in madx . table . summ } twiss_df = twiss_df [ madx . table . twiss . selected_columns () ] . iloc [ np . array ( madx . table . twiss . selected_rows ()). astype ( bool ) ] logger . trace ( \"Clearing 'TWISS' flag\" ) madx . select ( flag = \"twiss\" , clear = True ) return twiss_df get_twiss_tfs def get_twiss_tfs ( madx : cpymad . madx . Madx ) -> tfs . handler . TfsDataFrame Returns a tfs.TfsDataFrame from the Madx instance's twiss dframe, typically in the way we're used to getting it from MAD-X outputting the TWISS (uppercase names, colnames, summ table in headers). Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None Returns: Type Description None A tfs.TfsDataFrame. View Source def get_twiss_tfs ( madx : Madx ) -> tfs . TfsDataFrame : \"\"\" Returns a tfs.TfsDataFrame from the Madx instance's twiss dframe, typically in the way we're used to getting it from MAD-X outputting the TWISS (uppercase names, colnames, summ table in headers). Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A tfs.TfsDataFrame. \"\"\" logger . info ( \"Exporting internal TWISS and SUMM tables to TfsDataFrame\" ) twiss_tfs = tfs . TfsDataFrame ( madx . table . twiss . dframe ()) twiss_tfs . name = [ element [:- 2 ] for element in twiss_tfs . name ] twiss_tfs . columns = twiss_tfs . columns . str . upper () twiss_tfs = twiss_tfs . set_index ( \"NAME\" ) twiss_tfs . index = twiss_tfs . index . str . upper () twiss_tfs . headers = { var . upper () : madx . table . summ [ var ][ 0 ] for var in madx . table . summ } return twiss_tfs","title":"Twiss"},{"location":"reference/pyhdtoolkit/cpymadtools/twiss/#module-pyhdtoolkitcpymadtoolstwiss","text":"Module cpymadtools.twiss Created on 2020.02.03 View Source \"\"\" Module cpymadtools.twiss ------------------------ Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to manipulate MAD-X TWISS functionality through a cpymad.madx.Madx object. \"\"\" from typing import Sequence import numpy as np import tfs from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.cpymadtools.constants import DEFAULT_TWISS_COLUMNS # ----- Utlites ----- # def get_pattern_twiss ( madx : Madx , patterns : Sequence [ str ] = [ \"\" ], columns : Sequence [ str ] = None , ** kwargs , ) -> tfs . TfsDataFrame : \"\"\" Extract the `TWISS` table for desired variables, and for certain elements matching a pattern. Additionally, the `SUMM` table is also returned in the form of the TfsDataFrame's headers dictionary. Warning: Although the `pattern` parameter should accept a regex, MAD-X does not implement actual regexes. Please refer to the MAD-X manual, section `Regular Expressions` for details on what is implemented in MAD-X itself. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. patterns (Sequence[str]): the different element patterns (such as `MQX` or `BPM`) to be applied to the command, which will determine the rows in the returned DataFrame. Defaults to [\"\"] which will select all elements. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Defaults to None, which will return all available columns. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as `chrom`, `ripken`, `centre` or starting coordinates with `betax`, 'betay` etc. Returns: A TfsDataFrame with the selected columns for all elements matching the provided patterns, and the internal `summ` table as header dict. \"\"\" logger . trace ( \"Clearing 'TWISS' flag\" ) madx . select ( flag = \"twiss\" , clear = True ) for pattern in patterns : logger . trace ( f \"Adding pattern { pattern } to 'TWISS' flag\" ) madx . select ( flag = \"twiss\" , pattern = pattern , column = columns ) madx . twiss ( ** kwargs ) logger . trace ( \"Extracting relevant parts of the TWISS table\" ) twiss_df = tfs . TfsDataFrame ( madx . table . twiss . dframe () . copy ()) twiss_df . headers = { var . upper (): madx . table . summ [ var ][ 0 ] for var in madx . table . summ } twiss_df = twiss_df [ madx . table . twiss . selected_columns ()] . iloc [ np . array ( madx . table . twiss . selected_rows ()) . astype ( bool ) ] logger . trace ( \"Clearing 'TWISS' flag\" ) madx . select ( flag = \"twiss\" , clear = True ) return twiss_df def get_twiss_tfs ( madx : Madx ) -> tfs . TfsDataFrame : \"\"\" Returns a tfs.TfsDataFrame from the Madx instance's twiss dframe, typically in the way we're used to getting it from MAD-X outputting the TWISS (uppercase names, colnames, summ table in headers). Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A tfs.TfsDataFrame. \"\"\" logger . info ( \"Exporting internal TWISS and SUMM tables to TfsDataFrame\" ) twiss_tfs = tfs . TfsDataFrame ( madx . table . twiss . dframe ()) twiss_tfs . name = [ element [: - 2 ] for element in twiss_tfs . name ] twiss_tfs . columns = twiss_tfs . columns . str . upper () twiss_tfs = twiss_tfs . set_index ( \"NAME\" ) twiss_tfs . index = twiss_tfs . index . str . upper () twiss_tfs . headers = { var . upper (): madx . table . summ [ var ][ 0 ] for var in madx . table . summ } return twiss_tfs def get_ips_twiss ( madx : Madx , columns : Sequence [ str ] = DEFAULT_TWISS_COLUMNS , ** kwargs ) -> tfs . TfsDataFrame : \"\"\" Quickly get the `TWISS` table for certain variables at IP locations only. The `SUMM` table will be included as the TfsDataFrame's header dictionary. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as `chrom`, `ripken`, `centre` or starting coordinates with `betax`, 'betay` etc. Returns: A TfsDataFrame of the twiss output. \"\"\" logger . info ( \"Getting Twiss at IPs\" ) return get_pattern_twiss ( madx = madx , patterns = [ \"IP\" ], columns = columns , ** kwargs ) def get_ir_twiss ( madx : Madx , ir : int , columns : Sequence [ str ] = DEFAULT_TWISS_COLUMNS , ** kwargs ) -> tfs . TfsDataFrame : \"\"\" Quickly get the `TWISS` table for certain variables for one IR, meaning at the IP and Q1 to Q3 both left and right of the IP. The `SUMM` table will be included as the TfsDataFrame's header dictionary. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ir (int): which interaction region to get the TWISS for. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as `chrom`, `ripken`, `centre` or starting coordinates with `betax`, 'betay` etc. Returns: A TfsDataFrame of the twiss output. \"\"\" logger . info ( f \"Getting Twiss for IR { ir : d } \" ) return get_pattern_twiss ( madx = madx , patterns = [ f \"IP { ir : d } \" , f \"MQXA.[12345][RL] { ir : d } \" , # Q1 and Q3 LHC f \"MQXB.[AB][12345][RL] { ir : d } \" , # Q2A and Q2B LHC f \"MQXF[AB].[AB][12345][RL] { ir : d } \" , # Q1 to Q3 A and B HL-LHC ], columns = columns , ** kwargs , )","title":"Module pyhdtoolkit.cpymadtools.twiss"},{"location":"reference/pyhdtoolkit/cpymadtools/twiss/#variables","text":"DEFAULT_TWISS_COLUMNS","title":"Variables"},{"location":"reference/pyhdtoolkit/cpymadtools/twiss/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/twiss/#get_ips_twiss","text":"def get_ips_twiss ( madx : cpymad . madx . Madx , columns : Sequence [ str ] = [ 'name' , 's' , 'x' , 'y' , 'px' , 'py' , 'betx' , 'bety' , 'alfx' , 'alfy' , 'dx' , 'dy' , 'mux' , 'muy' , 'r11' , 'r12' , 'r21' , 'r22' , 'beta11' , 'beta12' , 'beta21' , 'beta22' ], ** kwargs ) -> tfs . handler . TfsDataFrame Quickly get the TWISS table for certain variables at IP locations only. The SUMM table will be included as the TfsDataFrame's header dictionary. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as chrom , ripken , centre or starting coordinates with betax , 'betay` etc. Returns: A TfsDataFrame of the twiss output. View Source def get_ips_twiss ( madx : Madx , columns : Sequence [ str ] = DEFAULT_TWISS_COLUMNS , ** kwargs ) -> tfs . TfsDataFrame : \" \"\" Quickly get the `TWISS` table for certain variables at IP locations only. The `SUMM` table will be included as the TfsDataFrame's header dictionary. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as `chrom`, `ripken`, `centre` or starting coordinates with `betax`, 'betay` etc. Returns: A TfsDataFrame of the twiss output. \"\" \" logger . info ( \"Getting Twiss at IPs\" ) return get_pattern_twiss ( madx = madx , patterns = [ \"IP\" ] , columns = columns , ** kwargs )","title":"get_ips_twiss"},{"location":"reference/pyhdtoolkit/cpymadtools/twiss/#get_ir_twiss","text":"def get_ir_twiss ( madx : cpymad . madx . Madx , ir : int , columns : Sequence [ str ] = [ 'name' , 's' , 'x' , 'y' , 'px' , 'py' , 'betx' , 'bety' , 'alfx' , 'alfy' , 'dx' , 'dy' , 'mux' , 'muy' , 'r11' , 'r12' , 'r21' , 'r22' , 'beta11' , 'beta12' , 'beta21' , 'beta22' ], ** kwargs ) -> tfs . handler . TfsDataFrame Quickly get the TWISS table for certain variables for one IR, meaning at the IP and Q1 to Q3 both left and right of the IP. The SUMM table will be included as the TfsDataFrame's header dictionary. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ir (int): which interaction region to get the TWISS for. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as chrom , ripken , centre or starting coordinates with betax , 'betay` etc. Returns: A TfsDataFrame of the twiss output. View Source def get_ir_twiss ( madx : Madx , ir : int , columns : Sequence [ str ] = DEFAULT_TWISS_COLUMNS , ** kwargs ) -> tfs . TfsDataFrame : \"\"\" Quickly get the `TWISS` table for certain variables for one IR, meaning at the IP and Q1 to Q3 both left and right of the IP. The `SUMM` table will be included as the TfsDataFrame's header dictionary. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. ir (int): which interaction region to get the TWISS for. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as `chrom`, `ripken`, `centre` or starting coordinates with `betax`, 'betay` etc. Returns: A TfsDataFrame of the twiss output. \"\"\" logger . info ( f \"Getting Twiss for IR {ir:d}\" ) return get_pattern_twiss ( madx = madx , patterns = [ f \"IP{ir:d}\" , f \"MQXA.[12345][RL]{ir:d}\" , # Q1 and Q3 LHC f \"MQXB.[AB][12345][RL]{ir:d}\" , # Q2A and Q2B LHC f \"MQXF[AB].[AB][12345][RL]{ir:d}\" , # Q1 to Q3 A and B HL-LHC ], columns = columns , ** kwargs , )","title":"get_ir_twiss"},{"location":"reference/pyhdtoolkit/cpymadtools/twiss/#get_pattern_twiss","text":"def get_pattern_twiss ( madx : cpymad . madx . Madx , patterns : Sequence [ str ] = [ '' ], columns : Sequence [ str ] = None , ** kwargs ) -> tfs . handler . TfsDataFrame Extract the TWISS table for desired variables, and for certain elements matching a pattern. Additionally, the SUMM table is also returned in the form of the TfsDataFrame's headers dictionary. Warning: Although the pattern parameter should accept a regex, MAD-X does not implement actual regexes. Please refer to the MAD-X manual, section Regular Expressions for details on what is implemented in MAD-X itself. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. patterns (Sequence[str]): the different element patterns (such as MQX or BPM ) to be applied to the command, which will determine the rows in the returned DataFrame. Defaults to [\"\"] which will select all elements. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Defaults to None, which will return all available columns. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as chrom , ripken , centre or starting coordinates with betax , 'betay` etc. Returns: A TfsDataFrame with the selected columns for all elements matching the provided patterns, and the internal summ table as header dict. View Source def get_pattern_twiss ( madx : Madx , patterns : Sequence [ str ] = [ \"\" ] , columns : Sequence [ str ] = None , ** kwargs , ) -> tfs . TfsDataFrame : \" \"\" Extract the `TWISS` table for desired variables, and for certain elements matching a pattern. Additionally, the `SUMM` table is also returned in the form of the TfsDataFrame's headers dictionary. Warning: Although the `pattern` parameter should accept a regex, MAD-X does not implement actual regexes. Please refer to the MAD-X manual, section `Regular Expressions` for details on what is implemented in MAD-X itself. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. patterns (Sequence[str]): the different element patterns (such as `MQX` or `BPM`) to be applied to the command, which will determine the rows in the returned DataFrame. Defaults to [ \"\" ] which will select all elements. columns (Sequence[str]): the variables to be returned, as columns in the DataFrame. Defaults to None, which will return all available columns. Keyword Args: Any keyword argument that can be given to the MAD-X TWISS command, such as `chrom`, `ripken`, `centre` or starting coordinates with `betax`, 'betay` etc. Returns: A TfsDataFrame with the selected columns for all elements matching the provided patterns, and the internal `summ` table as header dict. \"\" \" logger . trace ( \"Clearing 'TWISS' flag\" ) madx . select ( flag = \"twiss\" , clear = True ) for pattern in patterns : logger . trace ( f \"Adding pattern {pattern} to 'TWISS' flag\" ) madx . select ( flag = \"twiss\" , pattern = pattern , column = columns ) madx . twiss ( ** kwargs ) logger . trace ( \"Extracting relevant parts of the TWISS table\" ) twiss_df = tfs . TfsDataFrame ( madx . table . twiss . dframe (). copy ()) twiss_df . headers = { var . upper () : madx . table . summ [ var ][ 0 ] for var in madx . table . summ } twiss_df = twiss_df [ madx . table . twiss . selected_columns () ] . iloc [ np . array ( madx . table . twiss . selected_rows ()). astype ( bool ) ] logger . trace ( \"Clearing 'TWISS' flag\" ) madx . select ( flag = \"twiss\" , clear = True ) return twiss_df","title":"get_pattern_twiss"},{"location":"reference/pyhdtoolkit/cpymadtools/twiss/#get_twiss_tfs","text":"def get_twiss_tfs ( madx : cpymad . madx . Madx ) -> tfs . handler . TfsDataFrame Returns a tfs.TfsDataFrame from the Madx instance's twiss dframe, typically in the way we're used to getting it from MAD-X outputting the TWISS (uppercase names, colnames, summ table in headers). Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None Returns: Type Description None A tfs.TfsDataFrame. View Source def get_twiss_tfs ( madx : Madx ) -> tfs . TfsDataFrame : \"\"\" Returns a tfs.TfsDataFrame from the Madx instance's twiss dframe, typically in the way we're used to getting it from MAD-X outputting the TWISS (uppercase names, colnames, summ table in headers). Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A tfs.TfsDataFrame. \"\"\" logger . info ( \"Exporting internal TWISS and SUMM tables to TfsDataFrame\" ) twiss_tfs = tfs . TfsDataFrame ( madx . table . twiss . dframe ()) twiss_tfs . name = [ element [:- 2 ] for element in twiss_tfs . name ] twiss_tfs . columns = twiss_tfs . columns . str . upper () twiss_tfs = twiss_tfs . set_index ( \"NAME\" ) twiss_tfs . index = twiss_tfs . index . str . upper () twiss_tfs . headers = { var . upper () : madx . table . summ [ var ][ 0 ] for var in madx . table . summ } return twiss_tfs","title":"get_twiss_tfs"},{"location":"reference/pyhdtoolkit/cpymadtools/utils/","text":"Module pyhdtoolkit.cpymadtools.utils Module cpymadtools.utils Created on 2021.07.22 View Source \"\"\" Module cpymadtools.utils ------------------------ Created on 2021.07.22 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with utility functions to do mundane operatiions with `cpymad.madx.Madx` objects. \"\"\" import tfs from cpymad.madx import Madx from loguru import logger def get_table_tfs ( madx : Madx , table_name : str , headers_table : str = \"SUMM\" ) -> tfs . TfsDataFrame : \"\"\" Turns an internal table from the `MAD-X` process into a `TfsDataFrame` object. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. table_name (str): the name of the internal table. headers_table (str): the name of the internal table to use for headers. Defaults to `SUMM`. Returns: A `TfsDataFrame` object with the table data, and the desired table (usually `SUMM`) as headers. \"\"\" logger . debug ( f \"Extracting table { table_name } into a TfsDataFrame\" ) dframe = tfs . TfsDataFrame ( madx . table [ table_name ] . dframe ()) dframe . columns = dframe . columns . str . upper () if \"NAME\" in dframe . columns : logger . trace ( \"Uppercasing 'NAME' column contents\" ) dframe . NAME = dframe . NAME . str . upper () logger . trace ( f \"Turning { headers_table } table into headers\" ) dframe . headers = { var . upper (): madx . table [ headers_table ][ var ][ 0 ] for var in madx . table [ headers_table ]} return dframe Functions get_table_tfs def get_table_tfs ( madx : cpymad . madx . Madx , table_name : str , headers_table : str = 'SUMM' ) -> tfs . handler . TfsDataFrame Turns an internal table from the MAD-X process into a TfsDataFrame object. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None table_name str the name of the internal table. None headers_table str the name of the internal table to use for headers. Defaults to SUMM . SUMM Returns: Type Description None A TfsDataFrame object with the table data, and the desired table (usually SUMM ) as headers. View Source def get_table_tfs ( madx : Madx , table_name : str , headers_table : str = \"SUMM\" ) -> tfs . TfsDataFrame : \"\"\" Turns an internal table from the `MAD-X` process into a `TfsDataFrame` object. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. table_name (str): the name of the internal table. headers_table (str): the name of the internal table to use for headers. Defaults to `SUMM`. Returns: A `TfsDataFrame` object with the table data, and the desired table (usually `SUMM`) as headers. \"\"\" logger . debug ( f \"Extracting table {table_name} into a TfsDataFrame\" ) dframe = tfs . TfsDataFrame ( madx . table [ table_name ] . dframe ()) dframe . columns = dframe . columns . str . upper () if \"NAME\" in dframe . columns : logger . trace ( \"Uppercasing 'NAME' column contents\" ) dframe . NAME = dframe . NAME . str . upper () logger . trace ( f \"Turning {headers_table} table into headers\" ) dframe . headers = { var . upper (): madx . table [ headers_table ][ var ][ 0 ] for var in madx . table [ headers_table ]} return dframe","title":"Utils"},{"location":"reference/pyhdtoolkit/cpymadtools/utils/#module-pyhdtoolkitcpymadtoolsutils","text":"Module cpymadtools.utils Created on 2021.07.22 View Source \"\"\" Module cpymadtools.utils ------------------------ Created on 2021.07.22 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with utility functions to do mundane operatiions with `cpymad.madx.Madx` objects. \"\"\" import tfs from cpymad.madx import Madx from loguru import logger def get_table_tfs ( madx : Madx , table_name : str , headers_table : str = \"SUMM\" ) -> tfs . TfsDataFrame : \"\"\" Turns an internal table from the `MAD-X` process into a `TfsDataFrame` object. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. table_name (str): the name of the internal table. headers_table (str): the name of the internal table to use for headers. Defaults to `SUMM`. Returns: A `TfsDataFrame` object with the table data, and the desired table (usually `SUMM`) as headers. \"\"\" logger . debug ( f \"Extracting table { table_name } into a TfsDataFrame\" ) dframe = tfs . TfsDataFrame ( madx . table [ table_name ] . dframe ()) dframe . columns = dframe . columns . str . upper () if \"NAME\" in dframe . columns : logger . trace ( \"Uppercasing 'NAME' column contents\" ) dframe . NAME = dframe . NAME . str . upper () logger . trace ( f \"Turning { headers_table } table into headers\" ) dframe . headers = { var . upper (): madx . table [ headers_table ][ var ][ 0 ] for var in madx . table [ headers_table ]} return dframe","title":"Module pyhdtoolkit.cpymadtools.utils"},{"location":"reference/pyhdtoolkit/cpymadtools/utils/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/utils/#get_table_tfs","text":"def get_table_tfs ( madx : cpymad . madx . Madx , table_name : str , headers_table : str = 'SUMM' ) -> tfs . handler . TfsDataFrame Turns an internal table from the MAD-X process into a TfsDataFrame object. Parameters: Name Type Description Default madx cpymad.madx.Madx an instanciated cpymad Madx object. None table_name str the name of the internal table. None headers_table str the name of the internal table to use for headers. Defaults to SUMM . SUMM Returns: Type Description None A TfsDataFrame object with the table data, and the desired table (usually SUMM ) as headers. View Source def get_table_tfs ( madx : Madx , table_name : str , headers_table : str = \"SUMM\" ) -> tfs . TfsDataFrame : \"\"\" Turns an internal table from the `MAD-X` process into a `TfsDataFrame` object. Args: madx (cpymad.madx.Madx): an instanciated cpymad Madx object. table_name (str): the name of the internal table. headers_table (str): the name of the internal table to use for headers. Defaults to `SUMM`. Returns: A `TfsDataFrame` object with the table data, and the desired table (usually `SUMM`) as headers. \"\"\" logger . debug ( f \"Extracting table {table_name} into a TfsDataFrame\" ) dframe = tfs . TfsDataFrame ( madx . table [ table_name ] . dframe ()) dframe . columns = dframe . columns . str . upper () if \"NAME\" in dframe . columns : logger . trace ( \"Uppercasing 'NAME' column contents\" ) dframe . NAME = dframe . NAME . str . upper () logger . trace ( f \"Turning {headers_table} table into headers\" ) dframe . headers = { var . upper (): madx . table [ headers_table ][ var ][ 0 ] for var in madx . table [ headers_table ]} return dframe","title":"get_table_tfs"},{"location":"reference/pyhdtoolkit/maths/","text":"Module pyhdtoolkit.maths None None Sub-modules pyhdtoolkit.maths.nonconvex_phase_sync pyhdtoolkit.maths.stats_fitting pyhdtoolkit.maths.utils","title":"Index"},{"location":"reference/pyhdtoolkit/maths/#module-pyhdtoolkitmaths","text":"None None","title":"Module pyhdtoolkit.maths"},{"location":"reference/pyhdtoolkit/maths/#sub-modules","text":"pyhdtoolkit.maths.nonconvex_phase_sync pyhdtoolkit.maths.stats_fitting pyhdtoolkit.maths.utils","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/","text":"Module pyhdtoolkit.maths.nonconvex_phase_sync Module maths.nonconvex_phase_sync Created on 2020.01.13 View Source \"\"\" Module maths.nonconvex_phase_sync --------------------------------- Created on 2020.01.13 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 implementation of the Nonconvex Phase Synchronisation method found in the following paper (DOI: 10.1137/16M105808X, the algorithm reproduced is page 8). Methodology and Use Case ======================== We consider that from measurements, we can only obtain noisy relative phase advances mu_{i} - mu_{j} and want a converging solution to reconstruct the different individual mu_{1}, ...., mu_{n} values. From measurements, we construct a hermitian matrix C in the shape of: C_{ij} = z_{i} * bar(z_{j}) = exp(i * (mu_{i} - mu_{j})) A mock one with random values (500 by 500 as we have 500 BPMs per plane in the LHC) would be: c_matrix = np.exp(1j * np.random.rand(500, 500)) Considering 4 BPMs, the measurement matrix would be: M_matrix = [[mu_{1 -> 1}, mu_{1 -> 2}, mu_{1 -> 3}, mu_{1 -> 4}], [mu_{2 -> 1}, mu_{2 -> 2}, mu_{2 -> 3}, mu_{2 -> 4}], [mu_{3 -> 1}, mu_{3 -> 2}, mu_{3 -> 3}, mu_{3 -> 4}], [mu_{4 -> 1}, mu_{4 -> 2}, mu_{4 -> 3}, mu_{4 -> 4}]] Note two particular properties here: - Because our measurements are phase differences, the M_matrix will necessarily have zeros on its diagonal (mu_{k -> k} = 0). - By definition, since mu_{a -> b} = - mu_{b -> a}, M_matrix is symmetric. - Also note that for all computations, M_matrix needs to be initialised in radians! We can very simply get our C_matrix (see page 1 of referenced paper) with `numpy.exp` which, applied to a `numpy.ndarray` applies the exponential function element-wise. See reference at https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html Then follows: C_matrix = np.exp(1j * M_matrix) Note that M_matrix being symmetric, then c_matrix will be Hermitian. Note that M_matrix having zeros in its diagonal, c_matrix will have (1 + 0j) on its diagonal. With added noise to those values (noise should be included in M_matrix in the case of measurements), we can reconstruct a good estimator of the original values through the EVM method, provided in the class below. \"\"\" import numpy as np from loguru import logger class PhaseReconstructor: \"\"\" Class algorithm to reconstruct your phase values. Make sure to provide vectors as `numpy.ndarray` with shape (1, N), N being the dimension. \"\"\" __slots__ = { \"c_matrix\" : \"Hermitian square matrix from your measurements\" , \"c_matrix_eigenvalues\" : \"Eigenvalues of c_matrix\" , \"c_matrix_eigenvectors\" : \"Eigenvectors of c_matrix\" , \"space_dimension\" : \"Dimension of your measurement space\" , } def __init__ ( self , measurements_hermitian_matrix: np . ndarray ) -> None: \"\"\" Initialize your reconstructor object from measurements. Args: measurements_hermitian_matrix: a `numpy.ndarray` object built from measurements, see module docstring on how to build this matrix. \"\"\" logger . debug ( \"Checking that the provided matrix is Hermitian\" ) if np . allclose ( measurements_hermitian_matrix , np . conj ( measurements_hermitian_matrix ). T ): self . c_matrix: np . ndarray = measurements_hermitian_matrix self . c_matrix_eigenvalues: np . ndarray = np . linalg . eigvalsh ( self . c_matrix ) # Numpy gives the eigenvectors in column form, so transpose is needed there! self . c_matrix_eigenvectors: np . ndarray = np . linalg . eigh ( self . c_matrix )[- 1 ]. T self . space_dimension: int = self . c_matrix . shape [ 0 ] else: logger . exception ( \"Instantiating a PhaseReconstructor with a non hermitian matrix is \" \"not possible\" ) raise ValueError ( \"Provided matrix should be Hermitian\" ) @property def alpha ( self ) -> np . float64: \"\"\" This is a factor used to define the new reconstruction matrix. It is taken either as the operator norm of the hermitian noise matrix, or as the max value between 0 and the opposite of the min eigenvalue of c_matrix (chosen for implementation, since our noise is included in the measurements). See page 8 of the paper for reference. Returns: A real scalar value, because c_matrix is Hermitian and the eigenvalues of real symmetric or complex Hermitian matrices are always real (see G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pg. 222.) \"\"\" return np . float64 ( max ( 0 , np . amin ( self . c_matrix_eigenvalues ))) @property def leading_eigenvector ( self ) -> np . ndarray: \"\"\" Returns the leading eigenvector of `self.c_matrix`, which is the eigenvector corresponding to the max eigenvalue (in absolute value). Returns: A `numpy.ndarray` object, corresponding to said eigenvector. \"\"\" return self . c_matrix_eigenvectors [ np . where ( self . c_matrix_eigenvalues == np . amax ( np . absolute ( self . c_matrix_eigenvalues ))) ]. reshape (( 1 , self . space_dimension )) @property def reconstructor_matrix ( self ) -> np . ndarray: \"\"\" This is the reconstructor matrix built from `self.c_matrix` and the `alpha` property. It is the matrix denoted as \\\\widetilde{C} on page 8 of the reference paper. Returns: A `numpy.ndarray`, with same dimension as `self.c_matrix`. \"\"\" return self . c_matrix + self . alpha * np . identity ( self . c_matrix . shape [ 0 ]) def get_eigenvector_estimator ( self , eigenvector: np . ndarray ) -> np . ndarray: \"\"\" Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Args: eigenvector (np.ndarray): a numpy array representing the vector. Returns: A `numpy.ndarray` object of the same dimension as param `eigenvector`. \"\"\" try: return eigenvector / np . absolute ( eigenvector ) except RuntimeWarning: # In case of 0-division, we don't want `inf` values from numpy # Generate a random complex vector with same dimension (since `eigenvector` is of # dimension N * 1, `numpy.ndarray.size` method will give us N). # Remember to initialize a random real and imaginary part. logger . exception ( \"Encountered 0-division, trying normalization\" ) e_vect = np . random . randn ( eigenvector . size ) + 1 j * np . random . randn ( eigenvector . size ) while ( np . absolute ( e_vect @ eigenvector ) == 0 ): # Guarantee that we don't fall back to this edge case. e_vect = np . random . randn ( eigenvector . size ) + 1 j * np . random . randn ( eigenvector . size ) return ( e_vect @ eigenvector / np . absolute ( e_vect @ eigenvector )). reshape ( ( 1 , self . space_dimension ) ) def reconstruct_complex_phases_evm ( self ) -> np . ndarray: \"\"\" Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: The complex form of the result as a 'numpy.ndarray' instance. \"\"\" logger . debug ( \"Getting complex phase results\" ) return self . get_eigenvector_estimator ( self . leading_eigenvector ) @staticmethod def convert_complex_result_to_phase_values ( complex_estimator: np . ndarray , deg: bool = False ) -> np . ndarray: \"\"\" Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A `numpy.ndarray` with the real phase values of the result. \"\"\" logger . debug ( \"Casting complex phases to real values\" ) return np . apply_along_axis ( np . angle , axis = 0 , arr = complex_estimator , deg = deg ) Classes PhaseReconstructor class PhaseReconstructor ( measurements_hermitian_matrix : numpy . ndarray ) View Source class PhaseReconstructor : \" \"\" Class algorithm to reconstruct your phase values. Make sure to provide vectors as `numpy.ndarray` with shape (1, N), N being the dimension. \"\" \" __slots__ = { \"c_matrix\" : \"Hermitian square matrix from your measurements\" , \"c_matrix_eigenvalues\" : \"Eigenvalues of c_matrix\" , \"c_matrix_eigenvectors\" : \"Eigenvectors of c_matrix\" , \"space_dimension\" : \"Dimension of your measurement space\" , } def __init__ ( self , measurements_hermitian_matrix : np . ndarray ) -> None : \" \"\" Initialize your reconstructor object from measurements. Args: measurements_hermitian_matrix: a `numpy.ndarray` object built from measurements, see module docstring on how to build this matrix. \"\" \" logger . debug ( \"Checking that the provided matrix is Hermitian\" ) if np . allclose ( measurements_hermitian_matrix , np . conj ( measurements_hermitian_matrix ). T ) : self . c_matrix : np . ndarray = measurements_hermitian_matrix self . c_matrix_eigenvalues : np . ndarray = np . linalg . eigvalsh ( self . c_matrix ) # Numpy gives the eigenvectors in column form, so transpose is needed there! self . c_matrix_eigenvectors : np . ndarray = np . linalg . eigh ( self . c_matrix ) [ - 1 ] . T self . space_dimension : int = self . c_matrix . shape [ 0 ] else : logger . exception ( \"Instantiating a PhaseReconstructor with a non hermitian matrix is \" \"not possible\" ) raise ValueError ( \"Provided matrix should be Hermitian\" ) @property def alpha ( self ) -> np . float64 : \" \"\" This is a factor used to define the new reconstruction matrix. It is taken either as the operator norm of the hermitian noise matrix, or as the max value between 0 and the opposite of the min eigenvalue of c_matrix (chosen for implementation, since our noise is included in the measurements). See page 8 of the paper for reference. Returns: A real scalar value, because c_matrix is Hermitian and the eigenvalues of real symmetric or complex Hermitian matrices are always real (see G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pg. 222.) \"\" \" return np . float64 ( max ( 0 , np . amin ( self . c_matrix_eigenvalues ))) @property def leading_eigenvector ( self ) -> np . ndarray : \" \"\" Returns the leading eigenvector of `self.c_matrix`, which is the eigenvector corresponding to the max eigenvalue (in absolute value). Returns: A `numpy.ndarray` object, corresponding to said eigenvector. \"\" \" return self . c_matrix_eigenvectors [ np . where ( self . c_matrix_eigenvalues == np . amax ( np . absolute ( self . c_matrix_eigenvalues ))) ] . reshape (( 1 , self . space_dimension )) @property def reconstructor_matrix ( self ) -> np . ndarray : \" \"\" This is the reconstructor matrix built from `self.c_matrix` and the `alpha` property. It is the matrix denoted as \\\\ widetilde{C} on page 8 of the reference paper. Returns: A `numpy.ndarray`, with same dimension as `self.c_matrix`. \"\" \" return self . c_matrix + self . alpha * np . identity ( self . c_matrix . shape [ 0 ] ) def get_eigenvector_estimator ( self , eigenvector : np . ndarray ) -> np . ndarray : \" \"\" Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Args: eigenvector (np.ndarray): a numpy array representing the vector. Returns: A `numpy.ndarray` object of the same dimension as param `eigenvector`. \"\" \" try : return eigenvector / np . absolute ( eigenvector ) except RuntimeWarning : # In case of 0-division, we don't want `inf` values from numpy # Generate a random complex vector with same dimension (since `eigenvector` is of # dimension N * 1, `numpy.ndarray.size` method will give us N). # Remember to initialize a random real and imaginary part. logger . exception ( \"Encountered 0-division, trying normalization\" ) e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) while ( np . absolute ( e_vect @ eigenvector ) == 0 ) : # Guarantee that we don't fall back to this edge case. e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) return ( e_vect @ eigenvector / np . absolute ( e_vect @ eigenvector )). reshape ( ( 1 , self . space_dimension ) ) def reconstruct_complex_phases_evm ( self ) -> np . ndarray : \" \"\" Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: The complex form of the result as a 'numpy.ndarray' instance. \"\" \" logger . debug ( \"Getting complex phase results\" ) return self . get_eigenvector_estimator ( self . leading_eigenvector ) @staticmethod def convert_complex_result_to_phase_values ( complex_estimator : np . ndarray , deg : bool = False ) -> np . ndarray : \" \"\" Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A `numpy.ndarray` with the real phase values of the result. \"\" \" logger . debug ( \"Casting complex phases to real values\" ) return np . apply_along_axis ( np . angle , axis = 0 , arr = complex_estimator , deg = deg ) Static methods convert_complex_result_to_phase_values def convert_complex_result_to_phase_values ( complex_estimator : numpy . ndarray , deg : bool = False ) -> numpy . ndarray Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A numpy.ndarray with the real phase values of the result. View Source @staticmethod def convert_complex_result_to_phase_values ( complex_estimator : np . ndarray , deg : bool = False ) -> np . ndarray : \"\"\" Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A `numpy.ndarray` with the real phase values of the result. \"\"\" logger . debug ( \"Casting complex phases to real values\" ) return np . apply_along_axis ( np . angle , axis = 0 , arr = complex_estimator , deg = deg ) Instance variables alpha This is a factor used to define the new reconstruction matrix. It is taken either as the operator norm of the hermitian noise matrix, or as the max value between 0 and the opposite of the min eigenvalue of c_matrix (chosen for implementation, since our noise is included in the measurements). See page 8 of the paper for reference. c_matrix c_matrix_eigenvalues c_matrix_eigenvectors leading_eigenvector Returns the leading eigenvector of self.c_matrix , which is the eigenvector corresponding to the max eigenvalue (in absolute value). reconstructor_matrix This is the reconstructor matrix built from self.c_matrix and the alpha property. It is the matrix denoted as \\widetilde{C} on page 8 of the reference paper. space_dimension Methods get_eigenvector_estimator def get_eigenvector_estimator ( self , eigenvector : numpy . ndarray ) -> numpy . ndarray Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Parameters: Name Type Description Default eigenvector np.ndarray a numpy array representing the vector. None Returns: Type Description None A numpy.ndarray object of the same dimension as param eigenvector . View Source def get_eigenvector_estimator ( self , eigenvector : np . ndarray ) -> np . ndarray : \" \"\" Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Args: eigenvector (np.ndarray): a numpy array representing the vector. Returns: A `numpy.ndarray` object of the same dimension as param `eigenvector`. \"\" \" try : return eigenvector / np . absolute ( eigenvector ) except RuntimeWarning : # In case of 0-division, we don't want `inf` values from numpy # Generate a random complex vector with same dimension (since `eigenvector` is of # dimension N * 1, `numpy.ndarray.size` method will give us N). # Remember to initialize a random real and imaginary part. logger . exception ( \"Encountered 0-division, trying normalization\" ) e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) while ( np . absolute ( e_vect @ eigenvector ) == 0 ) : # Guarantee that we don't fall back to this edge case. e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) return ( e_vect @ eigenvector / np . absolute ( e_vect @ eigenvector )). reshape ( ( 1 , self . space_dimension ) ) reconstruct_complex_phases_evm def reconstruct_complex_phases_evm ( self ) -> numpy . ndarray Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: Type Description None The complex form of the result as a 'numpy.ndarray' instance. View Source def reconstruct_complex_phases_evm ( self ) -> np . ndarray : \"\"\" Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: The complex form of the result as a 'numpy.ndarray' instance. \"\"\" logger . debug ( \"Getting complex phase results\" ) return self . get_eigenvector_estimator ( self . leading_eigenvector )","title":"Nonconvex Phase Sync"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#module-pyhdtoolkitmathsnonconvex_phase_sync","text":"Module maths.nonconvex_phase_sync Created on 2020.01.13 View Source \"\"\" Module maths.nonconvex_phase_sync --------------------------------- Created on 2020.01.13 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 implementation of the Nonconvex Phase Synchronisation method found in the following paper (DOI: 10.1137/16M105808X, the algorithm reproduced is page 8). Methodology and Use Case ======================== We consider that from measurements, we can only obtain noisy relative phase advances mu_{i} - mu_{j} and want a converging solution to reconstruct the different individual mu_{1}, ...., mu_{n} values. From measurements, we construct a hermitian matrix C in the shape of: C_{ij} = z_{i} * bar(z_{j}) = exp(i * (mu_{i} - mu_{j})) A mock one with random values (500 by 500 as we have 500 BPMs per plane in the LHC) would be: c_matrix = np.exp(1j * np.random.rand(500, 500)) Considering 4 BPMs, the measurement matrix would be: M_matrix = [[mu_{1 -> 1}, mu_{1 -> 2}, mu_{1 -> 3}, mu_{1 -> 4}], [mu_{2 -> 1}, mu_{2 -> 2}, mu_{2 -> 3}, mu_{2 -> 4}], [mu_{3 -> 1}, mu_{3 -> 2}, mu_{3 -> 3}, mu_{3 -> 4}], [mu_{4 -> 1}, mu_{4 -> 2}, mu_{4 -> 3}, mu_{4 -> 4}]] Note two particular properties here: - Because our measurements are phase differences, the M_matrix will necessarily have zeros on its diagonal (mu_{k -> k} = 0). - By definition, since mu_{a -> b} = - mu_{b -> a}, M_matrix is symmetric. - Also note that for all computations, M_matrix needs to be initialised in radians! We can very simply get our C_matrix (see page 1 of referenced paper) with `numpy.exp` which, applied to a `numpy.ndarray` applies the exponential function element-wise. See reference at https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html Then follows: C_matrix = np.exp(1j * M_matrix) Note that M_matrix being symmetric, then c_matrix will be Hermitian. Note that M_matrix having zeros in its diagonal, c_matrix will have (1 + 0j) on its diagonal. With added noise to those values (noise should be included in M_matrix in the case of measurements), we can reconstruct a good estimator of the original values through the EVM method, provided in the class below. \"\"\" import numpy as np from loguru import logger class PhaseReconstructor: \"\"\" Class algorithm to reconstruct your phase values. Make sure to provide vectors as `numpy.ndarray` with shape (1, N), N being the dimension. \"\"\" __slots__ = { \"c_matrix\" : \"Hermitian square matrix from your measurements\" , \"c_matrix_eigenvalues\" : \"Eigenvalues of c_matrix\" , \"c_matrix_eigenvectors\" : \"Eigenvectors of c_matrix\" , \"space_dimension\" : \"Dimension of your measurement space\" , } def __init__ ( self , measurements_hermitian_matrix: np . ndarray ) -> None: \"\"\" Initialize your reconstructor object from measurements. Args: measurements_hermitian_matrix: a `numpy.ndarray` object built from measurements, see module docstring on how to build this matrix. \"\"\" logger . debug ( \"Checking that the provided matrix is Hermitian\" ) if np . allclose ( measurements_hermitian_matrix , np . conj ( measurements_hermitian_matrix ). T ): self . c_matrix: np . ndarray = measurements_hermitian_matrix self . c_matrix_eigenvalues: np . ndarray = np . linalg . eigvalsh ( self . c_matrix ) # Numpy gives the eigenvectors in column form, so transpose is needed there! self . c_matrix_eigenvectors: np . ndarray = np . linalg . eigh ( self . c_matrix )[- 1 ]. T self . space_dimension: int = self . c_matrix . shape [ 0 ] else: logger . exception ( \"Instantiating a PhaseReconstructor with a non hermitian matrix is \" \"not possible\" ) raise ValueError ( \"Provided matrix should be Hermitian\" ) @property def alpha ( self ) -> np . float64: \"\"\" This is a factor used to define the new reconstruction matrix. It is taken either as the operator norm of the hermitian noise matrix, or as the max value between 0 and the opposite of the min eigenvalue of c_matrix (chosen for implementation, since our noise is included in the measurements). See page 8 of the paper for reference. Returns: A real scalar value, because c_matrix is Hermitian and the eigenvalues of real symmetric or complex Hermitian matrices are always real (see G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pg. 222.) \"\"\" return np . float64 ( max ( 0 , np . amin ( self . c_matrix_eigenvalues ))) @property def leading_eigenvector ( self ) -> np . ndarray: \"\"\" Returns the leading eigenvector of `self.c_matrix`, which is the eigenvector corresponding to the max eigenvalue (in absolute value). Returns: A `numpy.ndarray` object, corresponding to said eigenvector. \"\"\" return self . c_matrix_eigenvectors [ np . where ( self . c_matrix_eigenvalues == np . amax ( np . absolute ( self . c_matrix_eigenvalues ))) ]. reshape (( 1 , self . space_dimension )) @property def reconstructor_matrix ( self ) -> np . ndarray: \"\"\" This is the reconstructor matrix built from `self.c_matrix` and the `alpha` property. It is the matrix denoted as \\\\widetilde{C} on page 8 of the reference paper. Returns: A `numpy.ndarray`, with same dimension as `self.c_matrix`. \"\"\" return self . c_matrix + self . alpha * np . identity ( self . c_matrix . shape [ 0 ]) def get_eigenvector_estimator ( self , eigenvector: np . ndarray ) -> np . ndarray: \"\"\" Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Args: eigenvector (np.ndarray): a numpy array representing the vector. Returns: A `numpy.ndarray` object of the same dimension as param `eigenvector`. \"\"\" try: return eigenvector / np . absolute ( eigenvector ) except RuntimeWarning: # In case of 0-division, we don't want `inf` values from numpy # Generate a random complex vector with same dimension (since `eigenvector` is of # dimension N * 1, `numpy.ndarray.size` method will give us N). # Remember to initialize a random real and imaginary part. logger . exception ( \"Encountered 0-division, trying normalization\" ) e_vect = np . random . randn ( eigenvector . size ) + 1 j * np . random . randn ( eigenvector . size ) while ( np . absolute ( e_vect @ eigenvector ) == 0 ): # Guarantee that we don't fall back to this edge case. e_vect = np . random . randn ( eigenvector . size ) + 1 j * np . random . randn ( eigenvector . size ) return ( e_vect @ eigenvector / np . absolute ( e_vect @ eigenvector )). reshape ( ( 1 , self . space_dimension ) ) def reconstruct_complex_phases_evm ( self ) -> np . ndarray: \"\"\" Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: The complex form of the result as a 'numpy.ndarray' instance. \"\"\" logger . debug ( \"Getting complex phase results\" ) return self . get_eigenvector_estimator ( self . leading_eigenvector ) @staticmethod def convert_complex_result_to_phase_values ( complex_estimator: np . ndarray , deg: bool = False ) -> np . ndarray: \"\"\" Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A `numpy.ndarray` with the real phase values of the result. \"\"\" logger . debug ( \"Casting complex phases to real values\" ) return np . apply_along_axis ( np . angle , axis = 0 , arr = complex_estimator , deg = deg )","title":"Module pyhdtoolkit.maths.nonconvex_phase_sync"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#phasereconstructor","text":"class PhaseReconstructor ( measurements_hermitian_matrix : numpy . ndarray ) View Source class PhaseReconstructor : \" \"\" Class algorithm to reconstruct your phase values. Make sure to provide vectors as `numpy.ndarray` with shape (1, N), N being the dimension. \"\" \" __slots__ = { \"c_matrix\" : \"Hermitian square matrix from your measurements\" , \"c_matrix_eigenvalues\" : \"Eigenvalues of c_matrix\" , \"c_matrix_eigenvectors\" : \"Eigenvectors of c_matrix\" , \"space_dimension\" : \"Dimension of your measurement space\" , } def __init__ ( self , measurements_hermitian_matrix : np . ndarray ) -> None : \" \"\" Initialize your reconstructor object from measurements. Args: measurements_hermitian_matrix: a `numpy.ndarray` object built from measurements, see module docstring on how to build this matrix. \"\" \" logger . debug ( \"Checking that the provided matrix is Hermitian\" ) if np . allclose ( measurements_hermitian_matrix , np . conj ( measurements_hermitian_matrix ). T ) : self . c_matrix : np . ndarray = measurements_hermitian_matrix self . c_matrix_eigenvalues : np . ndarray = np . linalg . eigvalsh ( self . c_matrix ) # Numpy gives the eigenvectors in column form, so transpose is needed there! self . c_matrix_eigenvectors : np . ndarray = np . linalg . eigh ( self . c_matrix ) [ - 1 ] . T self . space_dimension : int = self . c_matrix . shape [ 0 ] else : logger . exception ( \"Instantiating a PhaseReconstructor with a non hermitian matrix is \" \"not possible\" ) raise ValueError ( \"Provided matrix should be Hermitian\" ) @property def alpha ( self ) -> np . float64 : \" \"\" This is a factor used to define the new reconstruction matrix. It is taken either as the operator norm of the hermitian noise matrix, or as the max value between 0 and the opposite of the min eigenvalue of c_matrix (chosen for implementation, since our noise is included in the measurements). See page 8 of the paper for reference. Returns: A real scalar value, because c_matrix is Hermitian and the eigenvalues of real symmetric or complex Hermitian matrices are always real (see G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pg. 222.) \"\" \" return np . float64 ( max ( 0 , np . amin ( self . c_matrix_eigenvalues ))) @property def leading_eigenvector ( self ) -> np . ndarray : \" \"\" Returns the leading eigenvector of `self.c_matrix`, which is the eigenvector corresponding to the max eigenvalue (in absolute value). Returns: A `numpy.ndarray` object, corresponding to said eigenvector. \"\" \" return self . c_matrix_eigenvectors [ np . where ( self . c_matrix_eigenvalues == np . amax ( np . absolute ( self . c_matrix_eigenvalues ))) ] . reshape (( 1 , self . space_dimension )) @property def reconstructor_matrix ( self ) -> np . ndarray : \" \"\" This is the reconstructor matrix built from `self.c_matrix` and the `alpha` property. It is the matrix denoted as \\\\ widetilde{C} on page 8 of the reference paper. Returns: A `numpy.ndarray`, with same dimension as `self.c_matrix`. \"\" \" return self . c_matrix + self . alpha * np . identity ( self . c_matrix . shape [ 0 ] ) def get_eigenvector_estimator ( self , eigenvector : np . ndarray ) -> np . ndarray : \" \"\" Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Args: eigenvector (np.ndarray): a numpy array representing the vector. Returns: A `numpy.ndarray` object of the same dimension as param `eigenvector`. \"\" \" try : return eigenvector / np . absolute ( eigenvector ) except RuntimeWarning : # In case of 0-division, we don't want `inf` values from numpy # Generate a random complex vector with same dimension (since `eigenvector` is of # dimension N * 1, `numpy.ndarray.size` method will give us N). # Remember to initialize a random real and imaginary part. logger . exception ( \"Encountered 0-division, trying normalization\" ) e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) while ( np . absolute ( e_vect @ eigenvector ) == 0 ) : # Guarantee that we don't fall back to this edge case. e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) return ( e_vect @ eigenvector / np . absolute ( e_vect @ eigenvector )). reshape ( ( 1 , self . space_dimension ) ) def reconstruct_complex_phases_evm ( self ) -> np . ndarray : \" \"\" Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: The complex form of the result as a 'numpy.ndarray' instance. \"\" \" logger . debug ( \"Getting complex phase results\" ) return self . get_eigenvector_estimator ( self . leading_eigenvector ) @staticmethod def convert_complex_result_to_phase_values ( complex_estimator : np . ndarray , deg : bool = False ) -> np . ndarray : \" \"\" Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A `numpy.ndarray` with the real phase values of the result. \"\" \" logger . debug ( \"Casting complex phases to real values\" ) return np . apply_along_axis ( np . angle , axis = 0 , arr = complex_estimator , deg = deg )","title":"PhaseReconstructor"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#convert_complex_result_to_phase_values","text":"def convert_complex_result_to_phase_values ( complex_estimator : numpy . ndarray , deg : bool = False ) -> numpy . ndarray Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A numpy.ndarray with the real phase values of the result. View Source @staticmethod def convert_complex_result_to_phase_values ( complex_estimator : np . ndarray , deg : bool = False ) -> np . ndarray : \"\"\" Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A `numpy.ndarray` with the real phase values of the result. \"\"\" logger . debug ( \"Casting complex phases to real values\" ) return np . apply_along_axis ( np . angle , axis = 0 , arr = complex_estimator , deg = deg )","title":"convert_complex_result_to_phase_values"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#instance-variables","text":"alpha This is a factor used to define the new reconstruction matrix. It is taken either as the operator norm of the hermitian noise matrix, or as the max value between 0 and the opposite of the min eigenvalue of c_matrix (chosen for implementation, since our noise is included in the measurements). See page 8 of the paper for reference. c_matrix c_matrix_eigenvalues c_matrix_eigenvectors leading_eigenvector Returns the leading eigenvector of self.c_matrix , which is the eigenvector corresponding to the max eigenvalue (in absolute value). reconstructor_matrix This is the reconstructor matrix built from self.c_matrix and the alpha property. It is the matrix denoted as \\widetilde{C} on page 8 of the reference paper. space_dimension","title":"Instance variables"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#methods","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#get_eigenvector_estimator","text":"def get_eigenvector_estimator ( self , eigenvector : numpy . ndarray ) -> numpy . ndarray Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Parameters: Name Type Description Default eigenvector np.ndarray a numpy array representing the vector. None Returns: Type Description None A numpy.ndarray object of the same dimension as param eigenvector . View Source def get_eigenvector_estimator ( self , eigenvector : np . ndarray ) -> np . ndarray : \" \"\" Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Args: eigenvector (np.ndarray): a numpy array representing the vector. Returns: A `numpy.ndarray` object of the same dimension as param `eigenvector`. \"\" \" try : return eigenvector / np . absolute ( eigenvector ) except RuntimeWarning : # In case of 0-division, we don't want `inf` values from numpy # Generate a random complex vector with same dimension (since `eigenvector` is of # dimension N * 1, `numpy.ndarray.size` method will give us N). # Remember to initialize a random real and imaginary part. logger . exception ( \"Encountered 0-division, trying normalization\" ) e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) while ( np . absolute ( e_vect @ eigenvector ) == 0 ) : # Guarantee that we don't fall back to this edge case. e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) return ( e_vect @ eigenvector / np . absolute ( e_vect @ eigenvector )). reshape ( ( 1 , self . space_dimension ) )","title":"get_eigenvector_estimator"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#reconstruct_complex_phases_evm","text":"def reconstruct_complex_phases_evm ( self ) -> numpy . ndarray Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: Type Description None The complex form of the result as a 'numpy.ndarray' instance. View Source def reconstruct_complex_phases_evm ( self ) -> np . ndarray : \"\"\" Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: The complex form of the result as a 'numpy.ndarray' instance. \"\"\" logger . debug ( \"Getting complex phase results\" ) return self . get_eigenvector_estimator ( self . leading_eigenvector )","title":"reconstruct_complex_phases_evm"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/","text":"Module pyhdtoolkit.maths.stats_fitting Module maths.stats_fitting Created on 2020.02.06 View Source \"\"\" Module maths.stats_fitting -------------------------- Created on 2020.02.06 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 module implementing methods to find the best fit of statistical distributions to data. \"\"\" import warnings from typing import Dict , Tuple , Union import matplotlib import matplotlib.pyplot as plt # if omitted, get AttributeError: module 'matplotlib' has no attribute 'axes' import numpy as np import pandas as pd import scipy.stats as st from loguru import logger # Distributions to check # DISTRIBUTIONS : Dict [ st . rv_continuous , str ] = { st . chi : \"Chi\" , st . chi2 : \"Chi-Square\" , st . expon : \"Exponential\" , st . laplace : \"Laplace\" , st . lognorm : \"LogNorm\" , st . norm : \"Normal\" , } def set_distributions_dict ( dist_dict : Dict [ st . rv_continuous , str ]) -> None : \"\"\" Sets DISTRIBUTIONS as the provided dict. This is useful to define only the ones you want to try out. Args: dist_dict Dict[st.rv_continuous, str]: dictionnary with the wanted distributions, in the format of DISTRIBUTIONS, aka scipy.stats generator objects as keys, and a string representation of their name as value. Returns: Nothing, but modifies the global DISTRIBUTIONS dict called by other functions. \"\"\" # pylint: disable=global-statement logger . debug ( \"Setting tested distributions\" ) global DISTRIBUTIONS DISTRIBUTIONS = dist_dict def best_fit_distribution ( data : Union [ pd . Series , np . ndarray ], bins : int = 200 , ax : matplotlib . axes . Axes = None ) -> Tuple [ st . rv_continuous , Tuple [ float , ... ]]: \"\"\" Model data by finding best fit candidate distribution among those in DISTRIBUTIONS. Args: data (Union[pd.Series, np.ndarray]): a pandas Series or numpy array with your distribution data. bins (int): the number of bins to decompose your data in before performing fittings. ax (matplotlib.axes.Axes): the matplotlib axis object on which to plot the pdf of tried functions. This should be provided as the ax on which you plotted your distribution. Returns: A tuple containing the scipy.stats generator corresponding to the best fit candidate, and the parameters to give said generator to get this fit. \"\"\" # pylint: disable=too-many-locals logger . debug ( f \"Getting histogram of original data, in { bins } bins\" ) y , x = np . histogram ( data , bins = bins , density = True ) x = ( x + np . roll ( x , - 1 ))[: - 1 ] / 2.0 logger . debug ( \"Creating initial guess\" ) best_distribution = st . norm best_params = ( 0.0 , 1.0 ) best_sse = np . inf # Estimate distribution parameters from data for distribution , distname in DISTRIBUTIONS . items (): try : with warnings . catch_warnings (): # Ignore warnings from data that can't be fit warnings . filterwarnings ( \"ignore\" ) logger . debug ( f \"Trying to fit distribution ' { distname } '\" ) params = distribution . fit ( data ) * args , loc , scale = params logger . debug ( f \"Calculating PDF goodness of fit and error for distribution ' { distname } '\" ) pdf = distribution . pdf ( x , loc = loc , scale = scale , * args ) sse = np . sum ( np . power ( y - pdf , 2.0 )) try : if ax : logger . debug ( f \"Plotting fitted PDF for distribution ' { distname } '\" ) pd . Series ( pdf , x ) . plot ( ax = ax , label = f \" { distname } fit\" , alpha = 1 , lw = 2 ) except Exception : logger . exception ( f \"Plotting distribution ' { distname } ' failed\" ) logger . debug ( f \"Identifying if distribution ' { distname } ' is a better fit than previous tries\" ) if best_sse > sse > 0 : best_distribution = distribution best_params = params best_sse = sse except Exception : logger . exception ( f \"Trying to fit distribution ' { distname } ' failed and aborted\" ) logger . info ( f \"Found a best fit: ' { DISTRIBUTIONS [ best_distribution ] } ' distribution\" ) return best_distribution , best_params def make_pdf ( distribution : st . rv_continuous , params : Tuple [ float , ... ], size : int = 25_000 ) -> pd . Series : \"\"\" Generate a pandas Series for the distributions's Probability Distribution Function. This Series will have axis values as index, and PDF values as values. Args: distribution (st.rv_continuous): a scipy.stats generator object params (Tuple[float, ...]): the parameters for this generator given back by the fit. size (int): the number of points to evaluate. Returns: A pandas Series object with the PDF as values, corresponding axis values as index. \"\"\" # Separate parts of parameters * args , loc , scale = params logger . debug ( \"Getting sane start and end points of distribution\" ) start = ( distribution . ppf ( 0.01 , * args , loc = loc , scale = scale ) if args else distribution . ppf ( 0.01 , loc = loc , scale = scale ) ) end = ( distribution . ppf ( 0.99 , * args , loc = loc , scale = scale ) if args else distribution . ppf ( 0.99 , loc = loc , scale = scale ) ) logger . debug ( \"Building PDF\" ) x = np . linspace ( start , end , size ) y = distribution . pdf ( x , loc = loc , scale = scale , * args ) return pd . Series ( y , x ) Variables DISTRIBUTIONS Functions best_fit_distribution def best_fit_distribution ( data : Union [ pandas . core . series . Series , numpy . ndarray ], bins : int = 200 , ax : matplotlib . axes . _axes . Axes = None ) -> Tuple [ scipy . stats . _distn_infrastructure . rv_continuous , Tuple [ float , ... ]] Model data by finding best fit candidate distribution among those in DISTRIBUTIONS. Parameters: Name Type Description Default data Union[pd.Series, np.ndarray] a pandas Series or numpy array with your distribution data. None bins int the number of bins to decompose your data in before performing fittings. None ax matplotlib.axes.Axes the matplotlib axis object on which to plot the pdf of tried functions. This should be provided as the ax on which you plotted your distribution. None Returns: Type Description None A tuple containing the scipy.stats generator corresponding to the best fit candidate, and the parameters to give said generator to get this fit. View Source def best_fit_distribution ( data : Union [ pd . Series , np . ndarray ], bins : int = 200 , ax : matplotlib . axes . Axes = None ) -> Tuple [ st . rv_continuous , Tuple [ float , ...]] : \"\"\" Model data by finding best fit candidate distribution among those in DISTRIBUTIONS. Args: data (Union[pd.Series, np.ndarray]): a pandas Series or numpy array with your distribution data. bins (int): the number of bins to decompose your data in before performing fittings. ax (matplotlib.axes.Axes): the matplotlib axis object on which to plot the pdf of tried functions. This should be provided as the ax on which you plotted your distribution. Returns: A tuple containing the scipy.stats generator corresponding to the best fit candidate, and the parameters to give said generator to get this fit. \"\"\" # pylint : disable = too - many - locals logger . debug ( f \"Getting histogram of original data, in {bins} bins\" ) y , x = np . histogram ( data , bins = bins , density = True ) x = ( x + np . roll ( x , - 1 ))[:- 1 ] / 2.0 logger . debug ( \"Creating initial guess\" ) best_distribution = st . norm best_params = ( 0.0 , 1.0 ) best_sse = np . inf # Estimate distribution parameters from data for distribution , distname in DISTRIBUTIONS . items () : try : with warnings . catch_warnings () : # Ignore warnings from data that can't be fit warnings.filterwarnings(\"ignore\") logger.debug(f\"Trying to fit distribution ' { distname } '\") params = distribution.fit(data) *args, loc, scale = params logger.debug(f\"Calculating PDF goodness of fit and error for distribution ' { distname } '\") pdf = distribution.pdf(x, loc=loc, scale=scale, *args) sse = np.sum(np.power(y - pdf, 2.0)) try: if ax: logger.debug(f\"Plotting fitted PDF for distribution ' { distname } '\") pd.Series(pdf, x).plot(ax=ax, label=f\"{distname} fit\", alpha=1, lw=2) except Exception: logger.exception(f\"Plotting distribution ' { distname } ' failed\") logger.debug(f\"Identifying if distribution ' { distname } ' is a better fit than previous tries\") if best_sse > sse > 0: best_distribution = distribution best_params = params best_sse = sse except Exception: logger.exception(f\"Trying to fit distribution ' { distname } ' failed and aborted\") logger.info(f\"Found a best fit: ' { DISTRIBUTIONS [ best_distribution ]} ' distribution \" ) return best_distribution , best_params make_pdf def make_pdf ( distribution : scipy . stats . _distn_infrastructure . rv_continuous , params : Tuple [ float , ... ], size : int = 25000 ) -> pandas . core . series . Series Generate a pandas Series for the distributions's Probability Distribution Function. This Series will have axis values as index, and PDF values as values. Parameters: Name Type Description Default distribution st.rv_continuous a scipy.stats generator object None params Tuple[float, ...] the parameters for this generator given back by the fit. None size int the number of points to evaluate. None Returns: Type Description None A pandas Series object with the PDF as values, corresponding axis values as index. View Source def make_pdf ( distribution : st . rv_continuous , params : Tuple [ float , ...], size : int = 25 _000 ) -> pd . Series : \"\"\" Generate a pandas Series for the distributions' s Probability Distribution Function . This Series will have axis values as index , and PDF values as values . Args : distribution ( st . rv_continuous ) : a scipy . stats generator object params ( Tuple [ float , ...]) : the parameters for this generator given back by the fit . size ( int ) : the number of points to evaluate . Returns : A pandas Series object with the PDF as values , corresponding axis values as index . \"\"\" # Separate parts of parameters *args, loc, scale = params logger.debug(\" Getting sane start and end points of distribution \") start = ( distribution.ppf(0.01, *args, loc=loc, scale=scale) if args else distribution.ppf(0.01, loc=loc, scale=scale) ) end = ( distribution.ppf(0.99, *args, loc=loc, scale=scale) if args else distribution.ppf(0.99, loc=loc, scale=scale) ) logger.debug(\" Building PDF \") x = np.linspace(start, end, size) y = distribution.pdf(x, loc=loc, scale=scale, *args) return pd.Series(y, x) set_distributions_dict def set_distributions_dict ( dist_dict : Dict [ scipy . stats . _distn_infrastructure . rv_continuous , str ] ) -> None Sets DISTRIBUTIONS as the provided dict. This is useful to define only the ones you want to try out. Parameters: Name Type Description Default dist_dict Dict[st.rv_continuous, str] None dictionnary with the wanted distributions, in the format of DISTRIBUTIONS, aka scipy.stats generator objects as keys, and a string representation of their name as value. None Returns: Type Description None Nothing, but modifies the global DISTRIBUTIONS dict called by other functions. View Source def set_distributions_dict ( dist_dict : Dict [ st . rv_continuous , str ]) -> None : \"\"\" Sets DISTRIBUTIONS as the provided dict. This is useful to define only the ones you want to try out. Args: dist_dict Dict[st.rv_continuous, str]: dictionnary with the wanted distributions, in the format of DISTRIBUTIONS, aka scipy.stats generator objects as keys, and a string representation of their name as value. Returns: Nothing, but modifies the global DISTRIBUTIONS dict called by other functions. \"\"\" # pylint : disable = global - statement logger . debug ( \"Setting tested distributions\" ) global DISTRIBUTIONS DISTRIBUTIONS = dist_dict","title":"Stats Fitting"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/#module-pyhdtoolkitmathsstats_fitting","text":"Module maths.stats_fitting Created on 2020.02.06 View Source \"\"\" Module maths.stats_fitting -------------------------- Created on 2020.02.06 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 module implementing methods to find the best fit of statistical distributions to data. \"\"\" import warnings from typing import Dict , Tuple , Union import matplotlib import matplotlib.pyplot as plt # if omitted, get AttributeError: module 'matplotlib' has no attribute 'axes' import numpy as np import pandas as pd import scipy.stats as st from loguru import logger # Distributions to check # DISTRIBUTIONS : Dict [ st . rv_continuous , str ] = { st . chi : \"Chi\" , st . chi2 : \"Chi-Square\" , st . expon : \"Exponential\" , st . laplace : \"Laplace\" , st . lognorm : \"LogNorm\" , st . norm : \"Normal\" , } def set_distributions_dict ( dist_dict : Dict [ st . rv_continuous , str ]) -> None : \"\"\" Sets DISTRIBUTIONS as the provided dict. This is useful to define only the ones you want to try out. Args: dist_dict Dict[st.rv_continuous, str]: dictionnary with the wanted distributions, in the format of DISTRIBUTIONS, aka scipy.stats generator objects as keys, and a string representation of their name as value. Returns: Nothing, but modifies the global DISTRIBUTIONS dict called by other functions. \"\"\" # pylint: disable=global-statement logger . debug ( \"Setting tested distributions\" ) global DISTRIBUTIONS DISTRIBUTIONS = dist_dict def best_fit_distribution ( data : Union [ pd . Series , np . ndarray ], bins : int = 200 , ax : matplotlib . axes . Axes = None ) -> Tuple [ st . rv_continuous , Tuple [ float , ... ]]: \"\"\" Model data by finding best fit candidate distribution among those in DISTRIBUTIONS. Args: data (Union[pd.Series, np.ndarray]): a pandas Series or numpy array with your distribution data. bins (int): the number of bins to decompose your data in before performing fittings. ax (matplotlib.axes.Axes): the matplotlib axis object on which to plot the pdf of tried functions. This should be provided as the ax on which you plotted your distribution. Returns: A tuple containing the scipy.stats generator corresponding to the best fit candidate, and the parameters to give said generator to get this fit. \"\"\" # pylint: disable=too-many-locals logger . debug ( f \"Getting histogram of original data, in { bins } bins\" ) y , x = np . histogram ( data , bins = bins , density = True ) x = ( x + np . roll ( x , - 1 ))[: - 1 ] / 2.0 logger . debug ( \"Creating initial guess\" ) best_distribution = st . norm best_params = ( 0.0 , 1.0 ) best_sse = np . inf # Estimate distribution parameters from data for distribution , distname in DISTRIBUTIONS . items (): try : with warnings . catch_warnings (): # Ignore warnings from data that can't be fit warnings . filterwarnings ( \"ignore\" ) logger . debug ( f \"Trying to fit distribution ' { distname } '\" ) params = distribution . fit ( data ) * args , loc , scale = params logger . debug ( f \"Calculating PDF goodness of fit and error for distribution ' { distname } '\" ) pdf = distribution . pdf ( x , loc = loc , scale = scale , * args ) sse = np . sum ( np . power ( y - pdf , 2.0 )) try : if ax : logger . debug ( f \"Plotting fitted PDF for distribution ' { distname } '\" ) pd . Series ( pdf , x ) . plot ( ax = ax , label = f \" { distname } fit\" , alpha = 1 , lw = 2 ) except Exception : logger . exception ( f \"Plotting distribution ' { distname } ' failed\" ) logger . debug ( f \"Identifying if distribution ' { distname } ' is a better fit than previous tries\" ) if best_sse > sse > 0 : best_distribution = distribution best_params = params best_sse = sse except Exception : logger . exception ( f \"Trying to fit distribution ' { distname } ' failed and aborted\" ) logger . info ( f \"Found a best fit: ' { DISTRIBUTIONS [ best_distribution ] } ' distribution\" ) return best_distribution , best_params def make_pdf ( distribution : st . rv_continuous , params : Tuple [ float , ... ], size : int = 25_000 ) -> pd . Series : \"\"\" Generate a pandas Series for the distributions's Probability Distribution Function. This Series will have axis values as index, and PDF values as values. Args: distribution (st.rv_continuous): a scipy.stats generator object params (Tuple[float, ...]): the parameters for this generator given back by the fit. size (int): the number of points to evaluate. Returns: A pandas Series object with the PDF as values, corresponding axis values as index. \"\"\" # Separate parts of parameters * args , loc , scale = params logger . debug ( \"Getting sane start and end points of distribution\" ) start = ( distribution . ppf ( 0.01 , * args , loc = loc , scale = scale ) if args else distribution . ppf ( 0.01 , loc = loc , scale = scale ) ) end = ( distribution . ppf ( 0.99 , * args , loc = loc , scale = scale ) if args else distribution . ppf ( 0.99 , loc = loc , scale = scale ) ) logger . debug ( \"Building PDF\" ) x = np . linspace ( start , end , size ) y = distribution . pdf ( x , loc = loc , scale = scale , * args ) return pd . Series ( y , x )","title":"Module pyhdtoolkit.maths.stats_fitting"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/#variables","text":"DISTRIBUTIONS","title":"Variables"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/#best_fit_distribution","text":"def best_fit_distribution ( data : Union [ pandas . core . series . Series , numpy . ndarray ], bins : int = 200 , ax : matplotlib . axes . _axes . Axes = None ) -> Tuple [ scipy . stats . _distn_infrastructure . rv_continuous , Tuple [ float , ... ]] Model data by finding best fit candidate distribution among those in DISTRIBUTIONS. Parameters: Name Type Description Default data Union[pd.Series, np.ndarray] a pandas Series or numpy array with your distribution data. None bins int the number of bins to decompose your data in before performing fittings. None ax matplotlib.axes.Axes the matplotlib axis object on which to plot the pdf of tried functions. This should be provided as the ax on which you plotted your distribution. None Returns: Type Description None A tuple containing the scipy.stats generator corresponding to the best fit candidate, and the parameters to give said generator to get this fit. View Source def best_fit_distribution ( data : Union [ pd . Series , np . ndarray ], bins : int = 200 , ax : matplotlib . axes . Axes = None ) -> Tuple [ st . rv_continuous , Tuple [ float , ...]] : \"\"\" Model data by finding best fit candidate distribution among those in DISTRIBUTIONS. Args: data (Union[pd.Series, np.ndarray]): a pandas Series or numpy array with your distribution data. bins (int): the number of bins to decompose your data in before performing fittings. ax (matplotlib.axes.Axes): the matplotlib axis object on which to plot the pdf of tried functions. This should be provided as the ax on which you plotted your distribution. Returns: A tuple containing the scipy.stats generator corresponding to the best fit candidate, and the parameters to give said generator to get this fit. \"\"\" # pylint : disable = too - many - locals logger . debug ( f \"Getting histogram of original data, in {bins} bins\" ) y , x = np . histogram ( data , bins = bins , density = True ) x = ( x + np . roll ( x , - 1 ))[:- 1 ] / 2.0 logger . debug ( \"Creating initial guess\" ) best_distribution = st . norm best_params = ( 0.0 , 1.0 ) best_sse = np . inf # Estimate distribution parameters from data for distribution , distname in DISTRIBUTIONS . items () : try : with warnings . catch_warnings () : # Ignore warnings from data that can't be fit warnings.filterwarnings(\"ignore\") logger.debug(f\"Trying to fit distribution ' { distname } '\") params = distribution.fit(data) *args, loc, scale = params logger.debug(f\"Calculating PDF goodness of fit and error for distribution ' { distname } '\") pdf = distribution.pdf(x, loc=loc, scale=scale, *args) sse = np.sum(np.power(y - pdf, 2.0)) try: if ax: logger.debug(f\"Plotting fitted PDF for distribution ' { distname } '\") pd.Series(pdf, x).plot(ax=ax, label=f\"{distname} fit\", alpha=1, lw=2) except Exception: logger.exception(f\"Plotting distribution ' { distname } ' failed\") logger.debug(f\"Identifying if distribution ' { distname } ' is a better fit than previous tries\") if best_sse > sse > 0: best_distribution = distribution best_params = params best_sse = sse except Exception: logger.exception(f\"Trying to fit distribution ' { distname } ' failed and aborted\") logger.info(f\"Found a best fit: ' { DISTRIBUTIONS [ best_distribution ]} ' distribution \" ) return best_distribution , best_params","title":"best_fit_distribution"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/#make_pdf","text":"def make_pdf ( distribution : scipy . stats . _distn_infrastructure . rv_continuous , params : Tuple [ float , ... ], size : int = 25000 ) -> pandas . core . series . Series Generate a pandas Series for the distributions's Probability Distribution Function. This Series will have axis values as index, and PDF values as values. Parameters: Name Type Description Default distribution st.rv_continuous a scipy.stats generator object None params Tuple[float, ...] the parameters for this generator given back by the fit. None size int the number of points to evaluate. None Returns: Type Description None A pandas Series object with the PDF as values, corresponding axis values as index. View Source def make_pdf ( distribution : st . rv_continuous , params : Tuple [ float , ...], size : int = 25 _000 ) -> pd . Series : \"\"\" Generate a pandas Series for the distributions' s Probability Distribution Function . This Series will have axis values as index , and PDF values as values . Args : distribution ( st . rv_continuous ) : a scipy . stats generator object params ( Tuple [ float , ...]) : the parameters for this generator given back by the fit . size ( int ) : the number of points to evaluate . Returns : A pandas Series object with the PDF as values , corresponding axis values as index . \"\"\" # Separate parts of parameters *args, loc, scale = params logger.debug(\" Getting sane start and end points of distribution \") start = ( distribution.ppf(0.01, *args, loc=loc, scale=scale) if args else distribution.ppf(0.01, loc=loc, scale=scale) ) end = ( distribution.ppf(0.99, *args, loc=loc, scale=scale) if args else distribution.ppf(0.99, loc=loc, scale=scale) ) logger.debug(\" Building PDF \") x = np.linspace(start, end, size) y = distribution.pdf(x, loc=loc, scale=scale, *args) return pd.Series(y, x)","title":"make_pdf"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/#set_distributions_dict","text":"def set_distributions_dict ( dist_dict : Dict [ scipy . stats . _distn_infrastructure . rv_continuous , str ] ) -> None Sets DISTRIBUTIONS as the provided dict. This is useful to define only the ones you want to try out. Parameters: Name Type Description Default dist_dict Dict[st.rv_continuous, str] None dictionnary with the wanted distributions, in the format of DISTRIBUTIONS, aka scipy.stats generator objects as keys, and a string representation of their name as value. None Returns: Type Description None Nothing, but modifies the global DISTRIBUTIONS dict called by other functions. View Source def set_distributions_dict ( dist_dict : Dict [ st . rv_continuous , str ]) -> None : \"\"\" Sets DISTRIBUTIONS as the provided dict. This is useful to define only the ones you want to try out. Args: dist_dict Dict[st.rv_continuous, str]: dictionnary with the wanted distributions, in the format of DISTRIBUTIONS, aka scipy.stats generator objects as keys, and a string representation of their name as value. Returns: Nothing, but modifies the global DISTRIBUTIONS dict called by other functions. \"\"\" # pylint : disable = global - statement logger . debug ( \"Setting tested distributions\" ) global DISTRIBUTIONS DISTRIBUTIONS = dist_dict","title":"set_distributions_dict"},{"location":"reference/pyhdtoolkit/maths/utils/","text":"Module pyhdtoolkit.maths.utils None None View Source from typing import Tuple , Union import numpy as np import pandas as pd from loguru import logger # ----- Miscellaneous Utilites ----- # def get_magnitude ( value : float ) -> int : \"\"\"Return the determined magnitude of the provided value.\"\"\" return int ( np . floor ( np . log10 ( np . abs ( value )))) def get_scaled_values_and_magnitude_string ( values_array : Union [ pd . DataFrame , np . ndarray ], force_magnitude : float = None ) -> Tuple [ Union [ pd . DataFrame , np . ndarray ], str ]: \"\"\" Conveniently scale provided values to the best determined magnitude. Returns scaled values and the magnitude string to use in plots labels. Args: values_array (Union[pd.DataFrame, np.ndarray]): vectorised structure containing the values to scale. force_magnitude (float0: a specific magnitude value to use for the scaling, if desired. Returns: A tuple of the scaled values (same type as the provided ones) and the string to use for the scale in plots labels and legends. Usage: \"\"\" magnitude = get_magnitude ( max ( values_array )) if force_magnitude is None else force_magnitude applied_magnitude = - magnitude logger . trace ( f \"Scaling data by { applied_magnitude } orders of magnitude\" ) scaled_values = values_array * ( 10 ** applied_magnitude ) magnitude_string = \"{\" + f \" { applied_magnitude } \" + \"}\" return scaled_values , magnitude_string Functions get_magnitude def get_magnitude ( value : float ) -> int Return the determined magnitude of the provided value. View Source def get_magnitude ( value : float ) -> int : \"\"\"Return the determined magnitude of the provided value.\"\"\" return int ( np . floor ( np . log10 ( np . abs ( value )))) get_scaled_values_and_magnitude_string def get_scaled_values_and_magnitude_string ( values_array : Union [ pandas . core . frame . DataFrame , numpy . ndarray ], force_magnitude : float = None ) -> Tuple [ Union [ pandas . core . frame . DataFrame , numpy . ndarray ], str ] Conveniently scale provided values to the best determined magnitude. Returns scaled values and the magnitude string to use in plots labels. Parameters: Name Type Description Default values_array Union[pd.DataFrame, np.ndarray] vectorised structure containing the values to scale. None force_magnitude (float0 None a specific magnitude value to use for the scaling, if desired. None Returns: Type Description None A tuple of the scaled values (same type as the provided ones) and the string to use for the scale in plots labels and legends. Usage: | View Source def get_scaled_values_and_magnitude_string ( values_array : Union [ pd . DataFrame , np . ndarray ], force_magnitude : float = None ) -> Tuple [ Union [ pd . DataFrame , np . ndarray ], str ] : \"\"\" Conveniently scale provided values to the best determined magnitude. Returns scaled values and the magnitude string to use in plots labels. Args: values_array (Union[pd.DataFrame, np.ndarray]): vectorised structure containing the values to scale. force_magnitude (float0: a specific magnitude value to use for the scaling, if desired. Returns: A tuple of the scaled values (same type as the provided ones) and the string to use for the scale in plots labels and legends. Usage: \"\"\" magnitude = get_magnitude ( max ( values_array )) if force_magnitude is None else force_magnitude applied_magnitude = - magnitude logger . trace ( f \"Scaling data by {applied_magnitude} orders of magnitude\" ) scaled_values = values_array * ( 10 ** applied_magnitude ) magnitude_string = \"{\" + f \"{applied_magnitude}\" + \"}\" return scaled_values , magnitude_string","title":"Utils"},{"location":"reference/pyhdtoolkit/maths/utils/#module-pyhdtoolkitmathsutils","text":"None None View Source from typing import Tuple , Union import numpy as np import pandas as pd from loguru import logger # ----- Miscellaneous Utilites ----- # def get_magnitude ( value : float ) -> int : \"\"\"Return the determined magnitude of the provided value.\"\"\" return int ( np . floor ( np . log10 ( np . abs ( value )))) def get_scaled_values_and_magnitude_string ( values_array : Union [ pd . DataFrame , np . ndarray ], force_magnitude : float = None ) -> Tuple [ Union [ pd . DataFrame , np . ndarray ], str ]: \"\"\" Conveniently scale provided values to the best determined magnitude. Returns scaled values and the magnitude string to use in plots labels. Args: values_array (Union[pd.DataFrame, np.ndarray]): vectorised structure containing the values to scale. force_magnitude (float0: a specific magnitude value to use for the scaling, if desired. Returns: A tuple of the scaled values (same type as the provided ones) and the string to use for the scale in plots labels and legends. Usage: \"\"\" magnitude = get_magnitude ( max ( values_array )) if force_magnitude is None else force_magnitude applied_magnitude = - magnitude logger . trace ( f \"Scaling data by { applied_magnitude } orders of magnitude\" ) scaled_values = values_array * ( 10 ** applied_magnitude ) magnitude_string = \"{\" + f \" { applied_magnitude } \" + \"}\" return scaled_values , magnitude_string","title":"Module pyhdtoolkit.maths.utils"},{"location":"reference/pyhdtoolkit/maths/utils/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/maths/utils/#get_magnitude","text":"def get_magnitude ( value : float ) -> int Return the determined magnitude of the provided value. View Source def get_magnitude ( value : float ) -> int : \"\"\"Return the determined magnitude of the provided value.\"\"\" return int ( np . floor ( np . log10 ( np . abs ( value ))))","title":"get_magnitude"},{"location":"reference/pyhdtoolkit/maths/utils/#get_scaled_values_and_magnitude_string","text":"def get_scaled_values_and_magnitude_string ( values_array : Union [ pandas . core . frame . DataFrame , numpy . ndarray ], force_magnitude : float = None ) -> Tuple [ Union [ pandas . core . frame . DataFrame , numpy . ndarray ], str ] Conveniently scale provided values to the best determined magnitude. Returns scaled values and the magnitude string to use in plots labels. Parameters: Name Type Description Default values_array Union[pd.DataFrame, np.ndarray] vectorised structure containing the values to scale. None force_magnitude (float0 None a specific magnitude value to use for the scaling, if desired. None Returns: Type Description None A tuple of the scaled values (same type as the provided ones) and the string to use for the scale in plots labels and legends. Usage: | View Source def get_scaled_values_and_magnitude_string ( values_array : Union [ pd . DataFrame , np . ndarray ], force_magnitude : float = None ) -> Tuple [ Union [ pd . DataFrame , np . ndarray ], str ] : \"\"\" Conveniently scale provided values to the best determined magnitude. Returns scaled values and the magnitude string to use in plots labels. Args: values_array (Union[pd.DataFrame, np.ndarray]): vectorised structure containing the values to scale. force_magnitude (float0: a specific magnitude value to use for the scaling, if desired. Returns: A tuple of the scaled values (same type as the provided ones) and the string to use for the scale in plots labels and legends. Usage: \"\"\" magnitude = get_magnitude ( max ( values_array )) if force_magnitude is None else force_magnitude applied_magnitude = - magnitude logger . trace ( f \"Scaling data by {applied_magnitude} orders of magnitude\" ) scaled_values = values_array * ( 10 ** applied_magnitude ) magnitude_string = \"{\" + f \"{applied_magnitude}\" + \"}\" return scaled_values , magnitude_string","title":"get_scaled_values_and_magnitude_string"},{"location":"reference/pyhdtoolkit/models/","text":"Module pyhdtoolkit.models models package ~ ~ ~ ~ ~~ These are various pydantic models useful for data parsing and validation in pyhdtoolkit . View Source \"\"\" models package ~~~~~~~~~~~~~~ These are various `pydantic` models useful for data parsing and validation in `pyhdtoolkit`. :copyright: (c) 2021 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .beam import BeamParameters from .htc import BaseSummary , ClusterSummary , HTCTaskSummary from .madx import MADXBeam Sub-modules pyhdtoolkit.models.beam pyhdtoolkit.models.htc pyhdtoolkit.models.madx","title":"Index"},{"location":"reference/pyhdtoolkit/models/#module-pyhdtoolkitmodels","text":"models package ~ ~ ~ ~ ~~ These are various pydantic models useful for data parsing and validation in pyhdtoolkit . View Source \"\"\" models package ~~~~~~~~~~~~~~ These are various `pydantic` models useful for data parsing and validation in `pyhdtoolkit`. :copyright: (c) 2021 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .beam import BeamParameters from .htc import BaseSummary , ClusterSummary , HTCTaskSummary from .madx import MADXBeam","title":"Module pyhdtoolkit.models"},{"location":"reference/pyhdtoolkit/models/#sub-modules","text":"pyhdtoolkit.models.beam pyhdtoolkit.models.htc pyhdtoolkit.models.madx","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/models/beam/","text":"Module pyhdtoolkit.models.beam Module models.beam Created on 2021.08.03 View Source \"\"\" Module models.beam ------------------ Created on 2021.08.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with `pydantic` models to validate and store data structures used in the `beam` module. \"\"\" from typing import Optional from pydantic import BaseModel class BeamParameters ( BaseModel ): \"\"\" Class to encompass, validate and manipulate properties of a particle beam. \"\"\" pc_GeV : Optional [ float ] # Beam momentum [GeV] B_rho_Tm : Optional [ float ] # Beam rigidity [T/m] E_0_GeV : Optional [ float ] # Particle rest mass energy [GeV] charge : Optional [ float ] # Particle charge in [e] E_tot_GeV : Optional [ float ] # Total beam energy [GeV] E_kin_GeV : Optional [ float ] # Kinectic beam energy [GeV] gamma_r : Optional [ float ] # Relativistic gamma beta_r : Optional [ float ] # Relativistic beta en_x_m : Optional [ float ] # Horizontal normalized emittance [m] en_y_m : Optional [ float ] # Vertical normalized emittance [m] eg_x_m : Optional [ float ] # Horizontal geometrical emittance eg_y_m : Optional [ float ] # Vertical geometrical emittance deltap_p : Optional [ float ] # Momentum deviation Classes BeamParameters class BeamParameters ( __pydantic_self__ , ** data : Any ) View Source class BeamParameters ( BaseModel ) : \"\"\" Class to encompass, validate and manipulate properties of a particle beam. \"\"\" pc_GeV : Optional [ float ] # Beam momentum [ GeV ] B_rho_Tm : Optional [ float ] # Beam rigidity [ T/m ] E_0_GeV : Optional [ float ] # Particle rest mass energy [ GeV ] charge : Optional [ float ] # Particle charge in [ e ] E_tot_GeV : Optional [ float ] # Total beam energy [ GeV ] E_kin_GeV : Optional [ float ] # Kinectic beam energy [ GeV ] gamma_r : Optional [ float ] # Relativistic gamma beta_r : Optional [ float ] # Relativistic beta en_x_m : Optional [ float ] # Horizontal normalized emittance [ m ] en_y_m : Optional [ float ] # Vertical normalized emittance [ m ] eg_x_m : Optional [ float ] # Horizontal geometrical emittance eg_y_m : Optional [ float ] # Vertical geometrical emittance deltap_p : Optional [ float ] # Momentum deviation Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Beam"},{"location":"reference/pyhdtoolkit/models/beam/#module-pyhdtoolkitmodelsbeam","text":"Module models.beam Created on 2021.08.03 View Source \"\"\" Module models.beam ------------------ Created on 2021.08.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with `pydantic` models to validate and store data structures used in the `beam` module. \"\"\" from typing import Optional from pydantic import BaseModel class BeamParameters ( BaseModel ): \"\"\" Class to encompass, validate and manipulate properties of a particle beam. \"\"\" pc_GeV : Optional [ float ] # Beam momentum [GeV] B_rho_Tm : Optional [ float ] # Beam rigidity [T/m] E_0_GeV : Optional [ float ] # Particle rest mass energy [GeV] charge : Optional [ float ] # Particle charge in [e] E_tot_GeV : Optional [ float ] # Total beam energy [GeV] E_kin_GeV : Optional [ float ] # Kinectic beam energy [GeV] gamma_r : Optional [ float ] # Relativistic gamma beta_r : Optional [ float ] # Relativistic beta en_x_m : Optional [ float ] # Horizontal normalized emittance [m] en_y_m : Optional [ float ] # Vertical normalized emittance [m] eg_x_m : Optional [ float ] # Horizontal geometrical emittance eg_y_m : Optional [ float ] # Vertical geometrical emittance deltap_p : Optional [ float ] # Momentum deviation","title":"Module pyhdtoolkit.models.beam"},{"location":"reference/pyhdtoolkit/models/beam/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/models/beam/#beamparameters","text":"class BeamParameters ( __pydantic_self__ , ** data : Any ) View Source class BeamParameters ( BaseModel ) : \"\"\" Class to encompass, validate and manipulate properties of a particle beam. \"\"\" pc_GeV : Optional [ float ] # Beam momentum [ GeV ] B_rho_Tm : Optional [ float ] # Beam rigidity [ T/m ] E_0_GeV : Optional [ float ] # Particle rest mass energy [ GeV ] charge : Optional [ float ] # Particle charge in [ e ] E_tot_GeV : Optional [ float ] # Total beam energy [ GeV ] E_kin_GeV : Optional [ float ] # Kinectic beam energy [ GeV ] gamma_r : Optional [ float ] # Relativistic gamma beta_r : Optional [ float ] # Relativistic beta en_x_m : Optional [ float ] # Horizontal normalized emittance [ m ] en_y_m : Optional [ float ] # Vertical normalized emittance [ m ] eg_x_m : Optional [ float ] # Horizontal geometrical emittance eg_y_m : Optional [ float ] # Vertical geometrical emittance deltap_p : Optional [ float ] # Momentum deviation","title":"BeamParameters"},{"location":"reference/pyhdtoolkit/models/beam/#ancestors-in-mro","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/pyhdtoolkit/models/beam/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/pyhdtoolkit/models/beam/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/models/beam/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/pyhdtoolkit/models/beam/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/pyhdtoolkit/models/beam/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/pyhdtoolkit/models/beam/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/pyhdtoolkit/models/beam/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/pyhdtoolkit/models/beam/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/pyhdtoolkit/models/beam/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/pyhdtoolkit/models/beam/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/pyhdtoolkit/models/beam/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/pyhdtoolkit/models/beam/#methods","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/models/beam/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/pyhdtoolkit/models/beam/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/pyhdtoolkit/models/beam/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/pyhdtoolkit/models/htc/","text":"Module pyhdtoolkit.models.htc Module models.htc Created on 2021.07.30 View Source \"\"\" Module models.htc ----------------- Created on 2021.07.30 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with `pydantic` models to validate and store data obtained by querying the HTCondor queue. \"\"\" from typing import Union from pendulum import DateTime from pydantic import BaseModel class BaseSummary ( BaseModel ): jobs : int completed : int removed : int idle : int running : int held : int suspended : int class ClusterSummary ( BaseModel ): scheduler_id : str query : BaseSummary user : BaseSummary cluster : BaseSummary class HTCTaskSummary ( BaseModel ): owner : str batch_name : int submitted : DateTime done : Union [ int , str ] run : Union [ int , str ] idle : Union [ int , str ] total : int job_ids : str Classes BaseSummary class BaseSummary ( __pydantic_self__ , ** data : Any ) View Source class BaseSummary ( BaseModel ): jobs: int completed: int removed: int idle: int running: int held: int suspended: int Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . ClusterSummary class ClusterSummary ( __pydantic_self__ , ** data : Any ) View Source class ClusterSummary ( BaseModel ): scheduler_id: str query: BaseSummary user: BaseSummary cluster: BaseSummary Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . HTCTaskSummary class HTCTaskSummary ( __pydantic_self__ , ** data : Any ) View Source class HTCTaskSummary ( BaseModel ): owner: str batch_name: int submitted: DateTime done: Union [ int , str ] run: Union [ int , str ] idle: Union [ int , str ] total: int job_ids: str Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"Htc"},{"location":"reference/pyhdtoolkit/models/htc/#module-pyhdtoolkitmodelshtc","text":"Module models.htc Created on 2021.07.30 View Source \"\"\" Module models.htc ----------------- Created on 2021.07.30 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with `pydantic` models to validate and store data obtained by querying the HTCondor queue. \"\"\" from typing import Union from pendulum import DateTime from pydantic import BaseModel class BaseSummary ( BaseModel ): jobs : int completed : int removed : int idle : int running : int held : int suspended : int class ClusterSummary ( BaseModel ): scheduler_id : str query : BaseSummary user : BaseSummary cluster : BaseSummary class HTCTaskSummary ( BaseModel ): owner : str batch_name : int submitted : DateTime done : Union [ int , str ] run : Union [ int , str ] idle : Union [ int , str ] total : int job_ids : str","title":"Module pyhdtoolkit.models.htc"},{"location":"reference/pyhdtoolkit/models/htc/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/models/htc/#basesummary","text":"class BaseSummary ( __pydantic_self__ , ** data : Any ) View Source class BaseSummary ( BaseModel ): jobs: int completed: int removed: int idle: int running: int held: int suspended: int","title":"BaseSummary"},{"location":"reference/pyhdtoolkit/models/htc/#ancestors-in-mro","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/pyhdtoolkit/models/htc/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/pyhdtoolkit/models/htc/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/models/htc/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/pyhdtoolkit/models/htc/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/pyhdtoolkit/models/htc/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/pyhdtoolkit/models/htc/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/pyhdtoolkit/models/htc/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/pyhdtoolkit/models/htc/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/pyhdtoolkit/models/htc/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/pyhdtoolkit/models/htc/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/pyhdtoolkit/models/htc/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/pyhdtoolkit/models/htc/#methods","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/models/htc/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/pyhdtoolkit/models/htc/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/pyhdtoolkit/models/htc/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/pyhdtoolkit/models/htc/#clustersummary","text":"class ClusterSummary ( __pydantic_self__ , ** data : Any ) View Source class ClusterSummary ( BaseModel ): scheduler_id: str query: BaseSummary user: BaseSummary cluster: BaseSummary","title":"ClusterSummary"},{"location":"reference/pyhdtoolkit/models/htc/#ancestors-in-mro_1","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/pyhdtoolkit/models/htc/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/pyhdtoolkit/models/htc/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/models/htc/#construct_1","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/pyhdtoolkit/models/htc/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/pyhdtoolkit/models/htc/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/pyhdtoolkit/models/htc/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/pyhdtoolkit/models/htc/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/pyhdtoolkit/models/htc/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/pyhdtoolkit/models/htc/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/pyhdtoolkit/models/htc/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/pyhdtoolkit/models/htc/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/pyhdtoolkit/models/htc/#methods_1","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/models/htc/#copy_1","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/pyhdtoolkit/models/htc/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/pyhdtoolkit/models/htc/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/pyhdtoolkit/models/htc/#htctasksummary","text":"class HTCTaskSummary ( __pydantic_self__ , ** data : Any ) View Source class HTCTaskSummary ( BaseModel ): owner: str batch_name: int submitted: DateTime done: Union [ int , str ] run: Union [ int , str ] idle: Union [ int , str ] total: int job_ids: str","title":"HTCTaskSummary"},{"location":"reference/pyhdtoolkit/models/htc/#ancestors-in-mro_2","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/pyhdtoolkit/models/htc/#class-variables_2","text":"Config","title":"Class variables"},{"location":"reference/pyhdtoolkit/models/htc/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/models/htc/#construct_2","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/pyhdtoolkit/models/htc/#from_orm_2","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/pyhdtoolkit/models/htc/#parse_file_2","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/pyhdtoolkit/models/htc/#parse_obj_2","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/pyhdtoolkit/models/htc/#parse_raw_2","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/pyhdtoolkit/models/htc/#schema_2","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/pyhdtoolkit/models/htc/#schema_json_2","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/pyhdtoolkit/models/htc/#update_forward_refs_2","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/pyhdtoolkit/models/htc/#validate_2","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/pyhdtoolkit/models/htc/#methods_2","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/models/htc/#copy_2","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/pyhdtoolkit/models/htc/#dict_2","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/pyhdtoolkit/models/htc/#json_2","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/pyhdtoolkit/models/madx/","text":"Module pyhdtoolkit.models.madx Module models.madx Created on 2021.07.30 View Source \"\"\" Module models.madx ------------------ Created on 2021.07.30 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with `pydantic` models to validate and store data obtained by interacting with the `MAD-X` process through `cpymad`. \"\"\" from enum import Enum from pydantic import BaseModel , PositiveFloat , PositiveInt class ParticleEnum ( str , Enum ): \"\"\"Validator Enum defining the accepted particle names in `MAD-X` beams.\"\"\" positron = \"positron\" electron = \"electron\" proton = \"proton\" antiproton = \"antiproton\" posmuon = \"posmuon\" # positive muons negmuon = \"negmuon\" # negative muons ions = \"ions\" class MADXBeam ( BaseModel ): \"\"\"This is a class to encompass and validate `BEAM` attributes from the `MAD-X` process.\"\"\" particle : ParticleEnum # The name of particles in the beam mass : PositiveFloat # The rest mass of the particles in the beam in [GeV] charge : float # The electrical charge of the particles in the beam in units of qp, the proton charge energy : PositiveFloat # Total energy per particle in [GeV] pc : float # Particle momentum times the speed of light, in [GeV] gamma : float # Relativistic factor in [1] beta : float # Relativistic beta in [1] brho : float # Magnetic rigidity of the particles in [T.m] ex : float # Horizontal emittance in [m] ey : float # Vertical emittance in [m] et : float # Longitudinal emittance in [m] exn : float # Normalized horizontal emittance in [m] (beta * gamma * ex) eyn : float # Normalized vertical emittance in [m] *beta * gamma * ey) sigt : float # The bunch length c \u03c3t in [m] sige : float # The relative energy spread \u03c3E /E in [1]. kbunch : PositiveInt # The number of particle bunches in the machine in [1] npart : PositiveInt # The number of particles per bunch in [1] bcurrent : float # The bunch current, in [A] bunched : bool # A logical flag. If set, the beam is treated as bunched whenever this makes sense radiate : bool # A logical flag. If set, synchrotron radiation is considered in all dipole magnets bv : int # integer specifying the direction of the particle movement in the beam line; either +1 or -1 Classes MADXBeam class MADXBeam ( __pydantic_self__ , ** data : Any ) View Source class MADXBeam ( BaseModel ) : \"\"\"This is a class to encompass and validate `BEAM` attributes from the `MAD-X` process.\"\"\" particle : ParticleEnum # The name of particles in the beam mass : PositiveFloat # The rest mass of the particles in the beam in [ GeV ] charge : float # The electrical charge of the particles in the beam in units of qp , the proton charge energy : PositiveFloat # Total energy per particle in [ GeV ] pc : float # Particle momentum times the speed of light , in [ GeV ] gamma : float # Relativistic factor in [ 1 ] beta : float # Relativistic beta in [ 1 ] brho : float # Magnetic rigidity of the particles in [ T.m ] ex : float # Horizontal emittance in [ m ] ey : float # Vertical emittance in [ m ] et : float # Longitudinal emittance in [ m ] exn : float # Normalized horizontal emittance in [ m ] ( beta * gamma * ex ) eyn : float # Normalized vertical emittance in [ m ] * beta * gamma * ey ) sigt : float # The bunch length c \u03c3t in [ m ] sige : float # The relative energy spread \u03c3E / E in [ 1 ] . kbunch : PositiveInt # The number of particle bunches in the machine in [ 1 ] npart : PositiveInt # The number of particles per bunch in [ 1 ] bcurrent : float # The bunch current , in [ A ] bunched : bool # A logical flag . If set , the beam is treated as bunched whenever this makes sense radiate : bool # A logical flag . If set , synchrotron radiation is considered in all dipole magnets bv : int # integer specifying the direction of the particle movement in the beam line ; either + 1 or - 1 Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . ParticleEnum class ParticleEnum ( / , * args , ** kwargs ) View Source class ParticleEnum ( str , Enum ): \"\"\"Validator Enum defining the accepted particle names in `MAD-X` beams.\"\"\" positron = \"positron\" electron = \"electron\" proton = \"proton\" antiproton = \"antiproton\" posmuon = \"posmuon\" # positive muons negmuon = \"negmuon\" # negative muons ions = \"ions\" Ancestors (in MRO) builtins.str enum.Enum Class variables antiproton electron ions name negmuon positron posmuon proton value","title":"Madx"},{"location":"reference/pyhdtoolkit/models/madx/#module-pyhdtoolkitmodelsmadx","text":"Module models.madx Created on 2021.07.30 View Source \"\"\" Module models.madx ------------------ Created on 2021.07.30 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with `pydantic` models to validate and store data obtained by interacting with the `MAD-X` process through `cpymad`. \"\"\" from enum import Enum from pydantic import BaseModel , PositiveFloat , PositiveInt class ParticleEnum ( str , Enum ): \"\"\"Validator Enum defining the accepted particle names in `MAD-X` beams.\"\"\" positron = \"positron\" electron = \"electron\" proton = \"proton\" antiproton = \"antiproton\" posmuon = \"posmuon\" # positive muons negmuon = \"negmuon\" # negative muons ions = \"ions\" class MADXBeam ( BaseModel ): \"\"\"This is a class to encompass and validate `BEAM` attributes from the `MAD-X` process.\"\"\" particle : ParticleEnum # The name of particles in the beam mass : PositiveFloat # The rest mass of the particles in the beam in [GeV] charge : float # The electrical charge of the particles in the beam in units of qp, the proton charge energy : PositiveFloat # Total energy per particle in [GeV] pc : float # Particle momentum times the speed of light, in [GeV] gamma : float # Relativistic factor in [1] beta : float # Relativistic beta in [1] brho : float # Magnetic rigidity of the particles in [T.m] ex : float # Horizontal emittance in [m] ey : float # Vertical emittance in [m] et : float # Longitudinal emittance in [m] exn : float # Normalized horizontal emittance in [m] (beta * gamma * ex) eyn : float # Normalized vertical emittance in [m] *beta * gamma * ey) sigt : float # The bunch length c \u03c3t in [m] sige : float # The relative energy spread \u03c3E /E in [1]. kbunch : PositiveInt # The number of particle bunches in the machine in [1] npart : PositiveInt # The number of particles per bunch in [1] bcurrent : float # The bunch current, in [A] bunched : bool # A logical flag. If set, the beam is treated as bunched whenever this makes sense radiate : bool # A logical flag. If set, synchrotron radiation is considered in all dipole magnets bv : int # integer specifying the direction of the particle movement in the beam line; either +1 or -1","title":"Module pyhdtoolkit.models.madx"},{"location":"reference/pyhdtoolkit/models/madx/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/models/madx/#madxbeam","text":"class MADXBeam ( __pydantic_self__ , ** data : Any ) View Source class MADXBeam ( BaseModel ) : \"\"\"This is a class to encompass and validate `BEAM` attributes from the `MAD-X` process.\"\"\" particle : ParticleEnum # The name of particles in the beam mass : PositiveFloat # The rest mass of the particles in the beam in [ GeV ] charge : float # The electrical charge of the particles in the beam in units of qp , the proton charge energy : PositiveFloat # Total energy per particle in [ GeV ] pc : float # Particle momentum times the speed of light , in [ GeV ] gamma : float # Relativistic factor in [ 1 ] beta : float # Relativistic beta in [ 1 ] brho : float # Magnetic rigidity of the particles in [ T.m ] ex : float # Horizontal emittance in [ m ] ey : float # Vertical emittance in [ m ] et : float # Longitudinal emittance in [ m ] exn : float # Normalized horizontal emittance in [ m ] ( beta * gamma * ex ) eyn : float # Normalized vertical emittance in [ m ] * beta * gamma * ey ) sigt : float # The bunch length c \u03c3t in [ m ] sige : float # The relative energy spread \u03c3E / E in [ 1 ] . kbunch : PositiveInt # The number of particle bunches in the machine in [ 1 ] npart : PositiveInt # The number of particles per bunch in [ 1 ] bcurrent : float # The bunch current , in [ A ] bunched : bool # A logical flag . If set , the beam is treated as bunched whenever this makes sense radiate : bool # A logical flag . If set , synchrotron radiation is considered in all dipole magnets bv : int # integer specifying the direction of the particle movement in the beam line ; either + 1 or - 1","title":"MADXBeam"},{"location":"reference/pyhdtoolkit/models/madx/#ancestors-in-mro","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/pyhdtoolkit/models/madx/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/pyhdtoolkit/models/madx/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/models/madx/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values","title":"construct"},{"location":"reference/pyhdtoolkit/models/madx/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/pyhdtoolkit/models/madx/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/pyhdtoolkit/models/madx/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/pyhdtoolkit/models/madx/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/pyhdtoolkit/models/madx/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/pyhdtoolkit/models/madx/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/pyhdtoolkit/models/madx/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/pyhdtoolkit/models/madx/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/pyhdtoolkit/models/madx/#methods","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/models/madx/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/pyhdtoolkit/models/madx/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/pyhdtoolkit/models/madx/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/pyhdtoolkit/models/madx/#particleenum","text":"class ParticleEnum ( / , * args , ** kwargs ) View Source class ParticleEnum ( str , Enum ): \"\"\"Validator Enum defining the accepted particle names in `MAD-X` beams.\"\"\" positron = \"positron\" electron = \"electron\" proton = \"proton\" antiproton = \"antiproton\" posmuon = \"posmuon\" # positive muons negmuon = \"negmuon\" # negative muons ions = \"ions\"","title":"ParticleEnum"},{"location":"reference/pyhdtoolkit/models/madx/#ancestors-in-mro_1","text":"builtins.str enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/pyhdtoolkit/models/madx/#class-variables_1","text":"antiproton electron ions name negmuon positron posmuon proton value","title":"Class variables"},{"location":"reference/pyhdtoolkit/optics/","text":"Module pyhdtoolkit.optics optics package ~ ~ ~ ~ ~~ These are miscellaneous utilities to perform optics calculation from simulation outputs. View Source \"\"\" optics package ~~~~~~~~~~~~~~ These are miscellaneous utilities to perform optics calculation from simulation outputs. :copyright: (c) 2020 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .beam import Beam , compute_beam_parameters from .ripken import lebedev_beam_size from .twiss import courant_snyder_transform Sub-modules pyhdtoolkit.optics.beam pyhdtoolkit.optics.ripken pyhdtoolkit.optics.twiss","title":"Index"},{"location":"reference/pyhdtoolkit/optics/#module-pyhdtoolkitoptics","text":"optics package ~ ~ ~ ~ ~~ These are miscellaneous utilities to perform optics calculation from simulation outputs. View Source \"\"\" optics package ~~~~~~~~~~~~~~ These are miscellaneous utilities to perform optics calculation from simulation outputs. :copyright: (c) 2020 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .beam import Beam , compute_beam_parameters from .ripken import lebedev_beam_size from .twiss import courant_snyder_transform","title":"Module pyhdtoolkit.optics"},{"location":"reference/pyhdtoolkit/optics/#sub-modules","text":"pyhdtoolkit.optics.beam pyhdtoolkit.optics.ripken pyhdtoolkit.optics.twiss","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/optics/beam/","text":"Module pyhdtoolkit.optics.beam Module optics.beam Created on 2020.11.11 View Source \"\"\" Module optics.beam ------------------ Created on 2020.11.11 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 module implementing various functionality for simple beam parameter calculations. \"\"\" import numpy as np from scipy import constants from pyhdtoolkit.models.beam import BeamParameters def compute_beam_parameters ( pc_gev : float , en_x_m : float , en_y_m : float , deltap_p : float ) -> BeamParameters : \"\"\" Calculate beam parameters from provided values, for proton particles. Args: pc_gev (float): particle momentum. en_x_m (float): horizontal emittance, in meters. en_y_m (float): vertical emittance, in meters. deltap_p (float): momentum deviation. Returns: A `BeamParameters` object with the calculated values. \"\"\" e0_gev = 0.9382720813 e_tot_gev = np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) gamma_r = e_tot_gev / e0_gev beta_r = pc_gev / np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) return BeamParameters ( pc_GeV = pc_gev , B_rho_Tm = 3.3356 * pc_gev , E_0_GeV = e0_gev , E_tot_GeV = e_tot_gev , E_kin_GeV = e_tot_gev - e0_gev , gamma_r = gamma_r , beta_r = beta_r , en_x_m = en_x_m , en_y_m = en_y_m , eg_x_m = en_x_m / gamma_r / beta_r , eg_y_m = en_y_m / gamma_r / beta_r , deltap_p = deltap_p , ) class Beam : \"\"\" Class to encompass functionality. \"\"\" def __init__ ( self , energy : float , emittance : float , m0 : float = constants . physical_constants [ \"proton mass energy equivalent in MeV\" ][ 0 ], ) -> None : \"\"\" Args: energy (float): energy of the particles in your beam, in [GeV]. emittance (float): beam emittance, in [m]. m0 (float): rest mass of the beam's particles in MeV. Defaults to that of a proton. \"\"\" self . energy = energy self . emittance = emittance self . rest_mass = m0 @property def gamma_rel ( self ) -> float : \"\"\"Relativistic gamma.\"\"\" return ( 1e3 * self . energy + self . rest_mass ) / self . rest_mass @property def beta_rel ( self ) -> float : \"\"\"Relativistic beta.\"\"\" return np . sqrt ( 1 + 1 / ( self . gamma_rel ** 2 )) @property def brho ( self ) -> float : \"\"\"Beam rigidity [T/m].\"\"\" return ( 1 / 0.3 ) * self . beta_rel * self . energy / constants . c @property def normalized_emittance ( self ) -> float : \"\"\" Normalized emittance [m]. \"\"\" return self . emittance * self . beta_rel * self . gamma_rel @property def rms_emittance ( self ) -> float : \"\"\" Rms emittance [m]. \"\"\" return self . emittance / ( self . beta_rel * self . gamma_rel ) def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = constants . c ) -> float : \"\"\" Revolution frequency. Args: circumference (float): the machine circumference in [m]. Defaults to that of the LHC. speed (float): the particles' speed in the machine, in [m/s]. Defaults to c. \"\"\" return self . beta_rel * speed / circumference def eta ( self , alpha_p : float ) -> float : \"\"\" Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Args: alpha_p (float): momentum compaction factor. \"\"\" return ( 1 / ( self . gamma_rel ** 2 )) - alpha_p @staticmethod def gamma_transition ( alpha_p : float ) -> float : \"\"\" Relativistic gamma corresponding to the transition energy. Args: alpha_p (float): momentum compaction factor. \"\"\" return np . sqrt ( 1 / alpha_p ) Functions compute_beam_parameters def compute_beam_parameters ( pc_gev : float , en_x_m : float , en_y_m : float , deltap_p : float ) -> pyhdtoolkit . models . beam . BeamParameters Calculate beam parameters from provided values, for proton particles. Parameters: Name Type Description Default pc_gev float particle momentum. None en_x_m float horizontal emittance, in meters. None en_y_m float vertical emittance, in meters. None deltap_p float momentum deviation. None Returns: Type Description None A BeamParameters object with the calculated values. View Source def compute_beam_parameters ( pc_gev : float , en_x_m : float , en_y_m : float , deltap_p : float ) -> BeamParameters : \" \"\" Calculate beam parameters from provided values, for proton particles. Args: pc_gev (float): particle momentum. en_x_m (float): horizontal emittance, in meters. en_y_m (float): vertical emittance, in meters. deltap_p (float): momentum deviation. Returns: A `BeamParameters` object with the calculated values. \"\" \" e0_gev = 0.9382720813 e_tot_gev = np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) gamma_r = e_tot_gev / e0_gev beta_r = pc_gev / np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) return BeamParameters ( pc_GeV = pc_gev , B_rho_Tm = 3.3356 * pc_gev , E_0_GeV = e0_gev , E_tot_GeV = e_tot_gev , E_kin_GeV = e_tot_gev - e0_gev , gamma_r = gamma_r , beta_r = beta_r , en_x_m = en_x_m , en_y_m = en_y_m , eg_x_m = en_x_m / gamma_r / beta_r , eg_y_m = en_y_m / gamma_r / beta_r , deltap_p = deltap_p , ) Classes Beam class Beam ( energy : float , emittance : float , m0 : float = 938.27208816 ) View Source class Beam : \"\"\" Class to encompass functionality. \"\"\" def __init__ ( self , energy : float , emittance : float , m0 : float = constants . physical_constants [ \"proton mass energy equivalent in MeV\" ][ 0 ] , ) -> None : \"\"\" Args: energy (float): energy of the particles in your beam, in [GeV]. emittance (float): beam emittance, in [m]. m0 (float): rest mass of the beam's particles in MeV. Defaults to that of a proton. \"\"\" self . energy = energy self . emittance = emittance self . rest_mass = m0 @property def gamma_rel ( self ) -> float : \"\"\"Relativistic gamma.\"\"\" return ( 1e3 * self . energy + self . rest_mass ) / self . rest_mass @property def beta_rel ( self ) -> float : \"\"\"Relativistic beta.\"\"\" return np . sqrt ( 1 + 1 / ( self . gamma_rel ** 2 )) @property def brho ( self ) -> float : \"\"\"Beam rigidity [T/m].\"\"\" return ( 1 / 0.3 ) * self . beta_rel * self . energy / constants . c @property def normalized_emittance ( self ) -> float : \"\"\" Normalized emittance [m]. \"\"\" return self . emittance * self . beta_rel * self . gamma_rel @property def rms_emittance ( self ) -> float : \"\"\" Rms emittance [m]. \"\"\" return self . emittance / ( self . beta_rel * self . gamma_rel ) def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = constants . c ) -> float : \"\"\" Revolution frequency. Args: circumference (float): the machine circumference in [m]. Defaults to that of the LHC. speed (float): the particles' speed in the machine, in [m/s]. Defaults to c. \"\"\" return self . beta_rel * speed / circumference def eta ( self , alpha_p : float ) -> float : \"\"\" Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Args: alpha_p (float): momentum compaction factor. \"\"\" return ( 1 / ( self . gamma_rel ** 2 )) - alpha_p @staticmethod def gamma_transition ( alpha_p : float ) -> float : \"\"\" Relativistic gamma corresponding to the transition energy. Args: alpha_p (float): momentum compaction factor. \"\"\" return np . sqrt ( 1 / alpha_p ) Static methods gamma_transition def gamma_transition ( alpha_p : float ) -> float Relativistic gamma corresponding to the transition energy. Parameters: Name Type Description Default alpha_p float momentum compaction factor. None View Source @staticmethod def gamma_transition ( alpha_p : float ) -> float : \"\"\" Relativistic gamma corresponding to the transition energy. Args: alpha_p (float): momentum compaction factor. \"\"\" return np . sqrt ( 1 / alpha_p ) Instance variables beta_rel Relativistic beta. brho Beam rigidity [T/m]. gamma_rel Relativistic gamma. normalized_emittance Normalized emittance [m]. rms_emittance Rms emittance [m]. Methods eta def eta ( self , alpha_p : float ) -> float Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Parameters: Name Type Description Default alpha_p float momentum compaction factor. None View Source def eta ( self , alpha_p : float ) -> float : \"\"\" Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Args: alpha_p (float): momentum compaction factor. \"\"\" return ( 1 / ( self . gamma_rel ** 2 )) - alpha_p revolution_frequency def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = 299792458.0 ) -> float Revolution frequency. Parameters: Name Type Description Default circumference float the machine circumference in [m]. Defaults to that of the LHC. that of the LHC speed float the particles' speed in the machine, in [m/s]. Defaults to c. c View Source def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = constants . c ) -> float : \"\"\" Revolution frequency. Args: circumference (float): the machine circumference in [m]. Defaults to that of the LHC. speed (float): the particles' speed in the machine, in [m/s]. Defaults to c. \"\"\" return self . beta_rel * speed / circumference","title":"Beam"},{"location":"reference/pyhdtoolkit/optics/beam/#module-pyhdtoolkitopticsbeam","text":"Module optics.beam Created on 2020.11.11 View Source \"\"\" Module optics.beam ------------------ Created on 2020.11.11 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 module implementing various functionality for simple beam parameter calculations. \"\"\" import numpy as np from scipy import constants from pyhdtoolkit.models.beam import BeamParameters def compute_beam_parameters ( pc_gev : float , en_x_m : float , en_y_m : float , deltap_p : float ) -> BeamParameters : \"\"\" Calculate beam parameters from provided values, for proton particles. Args: pc_gev (float): particle momentum. en_x_m (float): horizontal emittance, in meters. en_y_m (float): vertical emittance, in meters. deltap_p (float): momentum deviation. Returns: A `BeamParameters` object with the calculated values. \"\"\" e0_gev = 0.9382720813 e_tot_gev = np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) gamma_r = e_tot_gev / e0_gev beta_r = pc_gev / np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) return BeamParameters ( pc_GeV = pc_gev , B_rho_Tm = 3.3356 * pc_gev , E_0_GeV = e0_gev , E_tot_GeV = e_tot_gev , E_kin_GeV = e_tot_gev - e0_gev , gamma_r = gamma_r , beta_r = beta_r , en_x_m = en_x_m , en_y_m = en_y_m , eg_x_m = en_x_m / gamma_r / beta_r , eg_y_m = en_y_m / gamma_r / beta_r , deltap_p = deltap_p , ) class Beam : \"\"\" Class to encompass functionality. \"\"\" def __init__ ( self , energy : float , emittance : float , m0 : float = constants . physical_constants [ \"proton mass energy equivalent in MeV\" ][ 0 ], ) -> None : \"\"\" Args: energy (float): energy of the particles in your beam, in [GeV]. emittance (float): beam emittance, in [m]. m0 (float): rest mass of the beam's particles in MeV. Defaults to that of a proton. \"\"\" self . energy = energy self . emittance = emittance self . rest_mass = m0 @property def gamma_rel ( self ) -> float : \"\"\"Relativistic gamma.\"\"\" return ( 1e3 * self . energy + self . rest_mass ) / self . rest_mass @property def beta_rel ( self ) -> float : \"\"\"Relativistic beta.\"\"\" return np . sqrt ( 1 + 1 / ( self . gamma_rel ** 2 )) @property def brho ( self ) -> float : \"\"\"Beam rigidity [T/m].\"\"\" return ( 1 / 0.3 ) * self . beta_rel * self . energy / constants . c @property def normalized_emittance ( self ) -> float : \"\"\" Normalized emittance [m]. \"\"\" return self . emittance * self . beta_rel * self . gamma_rel @property def rms_emittance ( self ) -> float : \"\"\" Rms emittance [m]. \"\"\" return self . emittance / ( self . beta_rel * self . gamma_rel ) def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = constants . c ) -> float : \"\"\" Revolution frequency. Args: circumference (float): the machine circumference in [m]. Defaults to that of the LHC. speed (float): the particles' speed in the machine, in [m/s]. Defaults to c. \"\"\" return self . beta_rel * speed / circumference def eta ( self , alpha_p : float ) -> float : \"\"\" Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Args: alpha_p (float): momentum compaction factor. \"\"\" return ( 1 / ( self . gamma_rel ** 2 )) - alpha_p @staticmethod def gamma_transition ( alpha_p : float ) -> float : \"\"\" Relativistic gamma corresponding to the transition energy. Args: alpha_p (float): momentum compaction factor. \"\"\" return np . sqrt ( 1 / alpha_p )","title":"Module pyhdtoolkit.optics.beam"},{"location":"reference/pyhdtoolkit/optics/beam/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/optics/beam/#compute_beam_parameters","text":"def compute_beam_parameters ( pc_gev : float , en_x_m : float , en_y_m : float , deltap_p : float ) -> pyhdtoolkit . models . beam . BeamParameters Calculate beam parameters from provided values, for proton particles. Parameters: Name Type Description Default pc_gev float particle momentum. None en_x_m float horizontal emittance, in meters. None en_y_m float vertical emittance, in meters. None deltap_p float momentum deviation. None Returns: Type Description None A BeamParameters object with the calculated values. View Source def compute_beam_parameters ( pc_gev : float , en_x_m : float , en_y_m : float , deltap_p : float ) -> BeamParameters : \" \"\" Calculate beam parameters from provided values, for proton particles. Args: pc_gev (float): particle momentum. en_x_m (float): horizontal emittance, in meters. en_y_m (float): vertical emittance, in meters. deltap_p (float): momentum deviation. Returns: A `BeamParameters` object with the calculated values. \"\" \" e0_gev = 0.9382720813 e_tot_gev = np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) gamma_r = e_tot_gev / e0_gev beta_r = pc_gev / np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) return BeamParameters ( pc_GeV = pc_gev , B_rho_Tm = 3.3356 * pc_gev , E_0_GeV = e0_gev , E_tot_GeV = e_tot_gev , E_kin_GeV = e_tot_gev - e0_gev , gamma_r = gamma_r , beta_r = beta_r , en_x_m = en_x_m , en_y_m = en_y_m , eg_x_m = en_x_m / gamma_r / beta_r , eg_y_m = en_y_m / gamma_r / beta_r , deltap_p = deltap_p , )","title":"compute_beam_parameters"},{"location":"reference/pyhdtoolkit/optics/beam/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/optics/beam/#beam","text":"class Beam ( energy : float , emittance : float , m0 : float = 938.27208816 ) View Source class Beam : \"\"\" Class to encompass functionality. \"\"\" def __init__ ( self , energy : float , emittance : float , m0 : float = constants . physical_constants [ \"proton mass energy equivalent in MeV\" ][ 0 ] , ) -> None : \"\"\" Args: energy (float): energy of the particles in your beam, in [GeV]. emittance (float): beam emittance, in [m]. m0 (float): rest mass of the beam's particles in MeV. Defaults to that of a proton. \"\"\" self . energy = energy self . emittance = emittance self . rest_mass = m0 @property def gamma_rel ( self ) -> float : \"\"\"Relativistic gamma.\"\"\" return ( 1e3 * self . energy + self . rest_mass ) / self . rest_mass @property def beta_rel ( self ) -> float : \"\"\"Relativistic beta.\"\"\" return np . sqrt ( 1 + 1 / ( self . gamma_rel ** 2 )) @property def brho ( self ) -> float : \"\"\"Beam rigidity [T/m].\"\"\" return ( 1 / 0.3 ) * self . beta_rel * self . energy / constants . c @property def normalized_emittance ( self ) -> float : \"\"\" Normalized emittance [m]. \"\"\" return self . emittance * self . beta_rel * self . gamma_rel @property def rms_emittance ( self ) -> float : \"\"\" Rms emittance [m]. \"\"\" return self . emittance / ( self . beta_rel * self . gamma_rel ) def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = constants . c ) -> float : \"\"\" Revolution frequency. Args: circumference (float): the machine circumference in [m]. Defaults to that of the LHC. speed (float): the particles' speed in the machine, in [m/s]. Defaults to c. \"\"\" return self . beta_rel * speed / circumference def eta ( self , alpha_p : float ) -> float : \"\"\" Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Args: alpha_p (float): momentum compaction factor. \"\"\" return ( 1 / ( self . gamma_rel ** 2 )) - alpha_p @staticmethod def gamma_transition ( alpha_p : float ) -> float : \"\"\" Relativistic gamma corresponding to the transition energy. Args: alpha_p (float): momentum compaction factor. \"\"\" return np . sqrt ( 1 / alpha_p )","title":"Beam"},{"location":"reference/pyhdtoolkit/optics/beam/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/optics/beam/#gamma_transition","text":"def gamma_transition ( alpha_p : float ) -> float Relativistic gamma corresponding to the transition energy. Parameters: Name Type Description Default alpha_p float momentum compaction factor. None View Source @staticmethod def gamma_transition ( alpha_p : float ) -> float : \"\"\" Relativistic gamma corresponding to the transition energy. Args: alpha_p (float): momentum compaction factor. \"\"\" return np . sqrt ( 1 / alpha_p )","title":"gamma_transition"},{"location":"reference/pyhdtoolkit/optics/beam/#instance-variables","text":"beta_rel Relativistic beta. brho Beam rigidity [T/m]. gamma_rel Relativistic gamma. normalized_emittance Normalized emittance [m]. rms_emittance Rms emittance [m].","title":"Instance variables"},{"location":"reference/pyhdtoolkit/optics/beam/#methods","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/optics/beam/#eta","text":"def eta ( self , alpha_p : float ) -> float Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Parameters: Name Type Description Default alpha_p float momentum compaction factor. None View Source def eta ( self , alpha_p : float ) -> float : \"\"\" Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Args: alpha_p (float): momentum compaction factor. \"\"\" return ( 1 / ( self . gamma_rel ** 2 )) - alpha_p","title":"eta"},{"location":"reference/pyhdtoolkit/optics/beam/#revolution_frequency","text":"def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = 299792458.0 ) -> float Revolution frequency. Parameters: Name Type Description Default circumference float the machine circumference in [m]. Defaults to that of the LHC. that of the LHC speed float the particles' speed in the machine, in [m/s]. Defaults to c. c View Source def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = constants . c ) -> float : \"\"\" Revolution frequency. Args: circumference (float): the machine circumference in [m]. Defaults to that of the LHC. speed (float): the particles' speed in the machine, in [m/s]. Defaults to c. \"\"\" return self . beta_rel * speed / circumference","title":"revolution_frequency"},{"location":"reference/pyhdtoolkit/optics/ripken/","text":"Module pyhdtoolkit.optics.ripken None None View Source from typing import Union import numpy as np from loguru import logger # ----- Setup Utilites ----- # def lebedev_beam_size ( beta1_ : Union [ float , np . ndarray ], beta2_ : Union [ float , np . ndarray ], geom_emit_x : float , geom_emit_y : float ) -> Union [ float , np . ndarray ]: \"\"\" Calculate beam size according to the Lebedev-bogacz formula, based on the Ripken-Mais Twiss parameters. The implementation is that of Eq. (A.3.1) in FERMILAB-PUB-10-383-AD, avaliable at the following link: https://arxiv.org/ftp/arxiv/papers/1207/1207.5526.pdf Args: beta1_ (Union[float, np.ndarray]): value(s) for the beta1x or beta1y Ripken parameter. beta2_ (Union[float, np.ndarray]): value(s) for the beta2x or beta2y Ripken parameter. geom_emit_x (float): geometric emittance of the horizontal plane. geom_emit_y (float): geometric emittante of the vertical plane. Returns: The beam size (horizontal or vertical) according to Lebedev & Bogacz, as sqrt(epsx * beta1_^2 + epsy * beta2_^2). \"\"\" logger . trace ( \"Computing beam size according to Lebedev formula: sqrt(epsx * b1_^2 + epsy * b2_^2)\" ) return np . sqrt ( geom_emit_x * beta1_ + geom_emit_y * beta2_ ) def _beam_size ( coordinates_distribution : np . ndarray , method : str = \"std\" ) -> float : \"\"\" Compute beam size from particle coordinates. Args: coordinates_distribution (np.ndarray): ensemble of coordinates of the particle distributon. method (str): the method of calculation to use, either 'std' (using the standard deviation as the beam size) or 'rms' (root mean square). Returns: The computed beam size. \"\"\" if method == \"std\" : return coordinates_distribution . std () elif method == \"rms\" : return np . sqrt ( np . mean ( np . square ( coordinates_distribution ))) raise NotImplementedError ( f \"Invalid method provided\" ) Functions lebedev_beam_size def lebedev_beam_size ( beta1_ : Union [ float , numpy . ndarray ], beta2_ : Union [ float , numpy . ndarray ], geom_emit_x : float , geom_emit_y : float ) -> Union [ float , numpy . ndarray ] Calculate beam size according to the Lebedev-bogacz formula, based on the Ripken-Mais Twiss parameters. The implementation is that of Eq. (A.3.1) in FERMILAB-PUB-10-383-AD, avaliable at the following link: https://arxiv.org/ftp/arxiv/papers/1207/1207.5526.pdf Parameters: Name Type Description Default beta1_ Union[float, np.ndarray] value(s) for the beta1x or beta1y Ripken parameter. None beta2_ Union[float, np.ndarray] value(s) for the beta2x or beta2y Ripken parameter. None geom_emit_x float geometric emittance of the horizontal plane. None geom_emit_y float geometric emittante of the vertical plane. None Returns: Type Description None The beam size (horizontal or vertical) according to Lebedev & Bogacz, as sqrt(epsx * beta1_^2 + epsy * beta2_^2). View Source def lebedev_beam_size ( beta1_ : Union [ float , np . ndarray ], beta2_ : Union [ float , np . ndarray ], geom_emit_x : float , geom_emit_y : float ) -> Union [ float , np . ndarray ] : \"\"\" Calculate beam size according to the Lebedev-bogacz formula, based on the Ripken-Mais Twiss parameters. The implementation is that of Eq. (A.3.1) in FERMILAB-PUB-10-383-AD, avaliable at the following link: https://arxiv.org/ftp/arxiv/papers/1207/1207.5526.pdf Args: beta1_ (Union[float, np.ndarray]): value(s) for the beta1x or beta1y Ripken parameter. beta2_ (Union[float, np.ndarray]): value(s) for the beta2x or beta2y Ripken parameter. geom_emit_x (float): geometric emittance of the horizontal plane. geom_emit_y (float): geometric emittante of the vertical plane. Returns: The beam size (horizontal or vertical) according to Lebedev & Bogacz, as sqrt(epsx * beta1_^2 + epsy * beta2_^2). \"\"\" logger . trace ( \"Computing beam size according to Lebedev formula: sqrt(epsx * b1_^2 + epsy * b2_^2)\" ) return np . sqrt ( geom_emit_x * beta1_ + geom_emit_y * beta2_ )","title":"Ripken"},{"location":"reference/pyhdtoolkit/optics/ripken/#module-pyhdtoolkitopticsripken","text":"None None View Source from typing import Union import numpy as np from loguru import logger # ----- Setup Utilites ----- # def lebedev_beam_size ( beta1_ : Union [ float , np . ndarray ], beta2_ : Union [ float , np . ndarray ], geom_emit_x : float , geom_emit_y : float ) -> Union [ float , np . ndarray ]: \"\"\" Calculate beam size according to the Lebedev-bogacz formula, based on the Ripken-Mais Twiss parameters. The implementation is that of Eq. (A.3.1) in FERMILAB-PUB-10-383-AD, avaliable at the following link: https://arxiv.org/ftp/arxiv/papers/1207/1207.5526.pdf Args: beta1_ (Union[float, np.ndarray]): value(s) for the beta1x or beta1y Ripken parameter. beta2_ (Union[float, np.ndarray]): value(s) for the beta2x or beta2y Ripken parameter. geom_emit_x (float): geometric emittance of the horizontal plane. geom_emit_y (float): geometric emittante of the vertical plane. Returns: The beam size (horizontal or vertical) according to Lebedev & Bogacz, as sqrt(epsx * beta1_^2 + epsy * beta2_^2). \"\"\" logger . trace ( \"Computing beam size according to Lebedev formula: sqrt(epsx * b1_^2 + epsy * b2_^2)\" ) return np . sqrt ( geom_emit_x * beta1_ + geom_emit_y * beta2_ ) def _beam_size ( coordinates_distribution : np . ndarray , method : str = \"std\" ) -> float : \"\"\" Compute beam size from particle coordinates. Args: coordinates_distribution (np.ndarray): ensemble of coordinates of the particle distributon. method (str): the method of calculation to use, either 'std' (using the standard deviation as the beam size) or 'rms' (root mean square). Returns: The computed beam size. \"\"\" if method == \"std\" : return coordinates_distribution . std () elif method == \"rms\" : return np . sqrt ( np . mean ( np . square ( coordinates_distribution ))) raise NotImplementedError ( f \"Invalid method provided\" )","title":"Module pyhdtoolkit.optics.ripken"},{"location":"reference/pyhdtoolkit/optics/ripken/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/optics/ripken/#lebedev_beam_size","text":"def lebedev_beam_size ( beta1_ : Union [ float , numpy . ndarray ], beta2_ : Union [ float , numpy . ndarray ], geom_emit_x : float , geom_emit_y : float ) -> Union [ float , numpy . ndarray ] Calculate beam size according to the Lebedev-bogacz formula, based on the Ripken-Mais Twiss parameters. The implementation is that of Eq. (A.3.1) in FERMILAB-PUB-10-383-AD, avaliable at the following link: https://arxiv.org/ftp/arxiv/papers/1207/1207.5526.pdf Parameters: Name Type Description Default beta1_ Union[float, np.ndarray] value(s) for the beta1x or beta1y Ripken parameter. None beta2_ Union[float, np.ndarray] value(s) for the beta2x or beta2y Ripken parameter. None geom_emit_x float geometric emittance of the horizontal plane. None geom_emit_y float geometric emittante of the vertical plane. None Returns: Type Description None The beam size (horizontal or vertical) according to Lebedev & Bogacz, as sqrt(epsx * beta1_^2 + epsy * beta2_^2). View Source def lebedev_beam_size ( beta1_ : Union [ float , np . ndarray ], beta2_ : Union [ float , np . ndarray ], geom_emit_x : float , geom_emit_y : float ) -> Union [ float , np . ndarray ] : \"\"\" Calculate beam size according to the Lebedev-bogacz formula, based on the Ripken-Mais Twiss parameters. The implementation is that of Eq. (A.3.1) in FERMILAB-PUB-10-383-AD, avaliable at the following link: https://arxiv.org/ftp/arxiv/papers/1207/1207.5526.pdf Args: beta1_ (Union[float, np.ndarray]): value(s) for the beta1x or beta1y Ripken parameter. beta2_ (Union[float, np.ndarray]): value(s) for the beta2x or beta2y Ripken parameter. geom_emit_x (float): geometric emittance of the horizontal plane. geom_emit_y (float): geometric emittante of the vertical plane. Returns: The beam size (horizontal or vertical) according to Lebedev & Bogacz, as sqrt(epsx * beta1_^2 + epsy * beta2_^2). \"\"\" logger . trace ( \"Computing beam size according to Lebedev formula: sqrt(epsx * b1_^2 + epsy * b2_^2)\" ) return np . sqrt ( geom_emit_x * beta1_ + geom_emit_y * beta2_ )","title":"lebedev_beam_size"},{"location":"reference/pyhdtoolkit/optics/twiss/","text":"Module pyhdtoolkit.optics.twiss Module optics.twiss Created on 2020.09.07 View Source \"\"\" Module optics.twiss ------------------- Created on 2020.09.07 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 module implementing various functionality for optics calculations from / to twiss parameters. \"\"\" import numpy as np def courant_snyder_transform ( u_vector : np . ndarray , alpha : float , beta : float ) -> np . ndarray : \"\"\" Perform the Courant-Snyder transform on rergular (nonchaotic) phase-space coordinatess. Specifically, if considering the horizontal plane and noting U = (x, px) the phase-space vector, it returns U_bar = (x_bar, px_bar) according to the transform: U_bar = P * U, where P = [1/sqrt(beta_x) 0 ] [alpha_x/sqrt(beta_x) sqrt(beta_x)] Args: u_vector (np.ndarray): two-dimentional array of phase-space (spatial and momenta) coordinates, either horizontal or vertical. alpha (float): alpha twiss parameter in the appropriate plane. beta (float): beta twiss parameter in the appropriate plane. Returns: The normal phase-space coordinates from the Courant-Snyder transform. \"\"\" p_matrix = np . array ([[ 1 / np . sqrt ( beta ), 0 ], [ alpha / np . sqrt ( beta ), np . sqrt ( beta )]]) return p_matrix @ u_vector Functions courant_snyder_transform def courant_snyder_transform ( u_vector : numpy . ndarray , alpha : float , beta : float ) -> numpy . ndarray Perform the Courant-Snyder transform on rergular (nonchaotic) phase-space coordinatess. Specifically, if considering the horizontal plane and noting U = (x, px) the phase-space vector, it returns U_bar = (x_bar, px_bar) according to the transform: U_bar = P * U, where P = [1/sqrt(beta_x) 0 ] [alpha_x/sqrt(beta_x) sqrt(beta_x)] Parameters: Name Type Description Default u_vector np.ndarray two-dimentional array of phase-space (spatial and momenta) coordinates, either horizontal or vertical. None alpha float alpha twiss parameter in the appropriate plane. None beta float beta twiss parameter in the appropriate plane. None Returns: Type Description None The normal phase-space coordinates from the Courant-Snyder transform. View Source def courant_snyder_transform ( u_vector : np . ndarray , alpha : float , beta : float ) -> np . ndarray : \"\"\" Perform the Courant-Snyder transform on rergular (nonchaotic) phase-space coordinatess. Specifically, if considering the horizontal plane and noting U = (x, px) the phase-space vector, it returns U_bar = (x_bar, px_bar) according to the transform: U_bar = P * U, where P = [1/sqrt(beta_x) 0 ] [alpha_x/sqrt(beta_x) sqrt(beta_x)] Args: u_vector (np.ndarray): two-dimentional array of phase-space (spatial and momenta) coordinates, either horizontal or vertical. alpha (float): alpha twiss parameter in the appropriate plane. beta (float): beta twiss parameter in the appropriate plane. Returns: The normal phase-space coordinates from the Courant-Snyder transform. \"\"\" p_matrix = np . array ([[ 1 / np . sqrt ( beta ), 0 ], [ alpha / np . sqrt ( beta ), np . sqrt ( beta )]]) return p_matrix @ u_vector","title":"Twiss"},{"location":"reference/pyhdtoolkit/optics/twiss/#module-pyhdtoolkitopticstwiss","text":"Module optics.twiss Created on 2020.09.07 View Source \"\"\" Module optics.twiss ------------------- Created on 2020.09.07 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 module implementing various functionality for optics calculations from / to twiss parameters. \"\"\" import numpy as np def courant_snyder_transform ( u_vector : np . ndarray , alpha : float , beta : float ) -> np . ndarray : \"\"\" Perform the Courant-Snyder transform on rergular (nonchaotic) phase-space coordinatess. Specifically, if considering the horizontal plane and noting U = (x, px) the phase-space vector, it returns U_bar = (x_bar, px_bar) according to the transform: U_bar = P * U, where P = [1/sqrt(beta_x) 0 ] [alpha_x/sqrt(beta_x) sqrt(beta_x)] Args: u_vector (np.ndarray): two-dimentional array of phase-space (spatial and momenta) coordinates, either horizontal or vertical. alpha (float): alpha twiss parameter in the appropriate plane. beta (float): beta twiss parameter in the appropriate plane. Returns: The normal phase-space coordinates from the Courant-Snyder transform. \"\"\" p_matrix = np . array ([[ 1 / np . sqrt ( beta ), 0 ], [ alpha / np . sqrt ( beta ), np . sqrt ( beta )]]) return p_matrix @ u_vector","title":"Module pyhdtoolkit.optics.twiss"},{"location":"reference/pyhdtoolkit/optics/twiss/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/optics/twiss/#courant_snyder_transform","text":"def courant_snyder_transform ( u_vector : numpy . ndarray , alpha : float , beta : float ) -> numpy . ndarray Perform the Courant-Snyder transform on rergular (nonchaotic) phase-space coordinatess. Specifically, if considering the horizontal plane and noting U = (x, px) the phase-space vector, it returns U_bar = (x_bar, px_bar) according to the transform: U_bar = P * U, where P = [1/sqrt(beta_x) 0 ] [alpha_x/sqrt(beta_x) sqrt(beta_x)] Parameters: Name Type Description Default u_vector np.ndarray two-dimentional array of phase-space (spatial and momenta) coordinates, either horizontal or vertical. None alpha float alpha twiss parameter in the appropriate plane. None beta float beta twiss parameter in the appropriate plane. None Returns: Type Description None The normal phase-space coordinates from the Courant-Snyder transform. View Source def courant_snyder_transform ( u_vector : np . ndarray , alpha : float , beta : float ) -> np . ndarray : \"\"\" Perform the Courant-Snyder transform on rergular (nonchaotic) phase-space coordinatess. Specifically, if considering the horizontal plane and noting U = (x, px) the phase-space vector, it returns U_bar = (x_bar, px_bar) according to the transform: U_bar = P * U, where P = [1/sqrt(beta_x) 0 ] [alpha_x/sqrt(beta_x) sqrt(beta_x)] Args: u_vector (np.ndarray): two-dimentional array of phase-space (spatial and momenta) coordinates, either horizontal or vertical. alpha (float): alpha twiss parameter in the appropriate plane. beta (float): beta twiss parameter in the appropriate plane. Returns: The normal phase-space coordinates from the Courant-Snyder transform. \"\"\" p_matrix = np . array ([[ 1 / np . sqrt ( beta ), 0 ], [ alpha / np . sqrt ( beta ), np . sqrt ( beta )]]) return p_matrix @ u_vector","title":"courant_snyder_transform"},{"location":"reference/pyhdtoolkit/plotting/","text":"Module pyhdtoolkit.plotting plotting package ~ ~ ~ ~ ~ ~ ~ These are miscellaneous utilities to integrate to my plots. View Source \"\"\" plotting package ~~~~~~~~~~~~~~~~~~~ These are miscellaneous utilities to integrate to my plots. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .helpers import AnnotationsPlotter Sub-modules pyhdtoolkit.plotting.helpers","title":"Index"},{"location":"reference/pyhdtoolkit/plotting/#module-pyhdtoolkitplotting","text":"plotting package ~ ~ ~ ~ ~ ~ ~ These are miscellaneous utilities to integrate to my plots. View Source \"\"\" plotting package ~~~~~~~~~~~~~~~~~~~ These are miscellaneous utilities to integrate to my plots. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .helpers import AnnotationsPlotter","title":"Module pyhdtoolkit.plotting"},{"location":"reference/pyhdtoolkit/plotting/#sub-modules","text":"pyhdtoolkit.plotting.helpers","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/plotting/helpers/","text":"Module pyhdtoolkit.plotting.helpers Module plotting.helpers Created on 2019.06.15 View Source \"\"\" Module plotting.helpers ----------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions for more descriptive plots. \"\"\" from typing import Tuple import matplotlib.axes class AnnotationsPlotter : \"\"\" A class to encapsulate all useful plotting additional tidbits. \"\"\" @staticmethod def set_arrow_label ( axis : matplotlib . axes . Axes , label : str , arrow_position : Tuple [ float , float ], label_position : Tuple [ float , float ], color : str = \"k\" , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ** kwargs , ) -> matplotlib . text . Annotation : \"\"\" Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes `Axes` instance. Original code from Guido Sterbini. Args: axis (matplotlib.axes.Axes): a matplotlib axis to plot on. label (str): label text to print on the axis. arrow_position (Tuple[float, float]): where on the plot to point the tip of the arrow. label_position (Tuple[float, float]): where on the plot the text label (and thus start of the arrow) is. color (str): color parameter for your arrow and label. Defaults to 'k'. arrow_arc_rad (float): angle value defining the upwards / downwards shape of and bending of the arrow. fontsize (int): text size in the box Returns: A matploblit text annotation object. \"\"\" return axis . annotate ( label , xy = arrow_position , xycoords = \"data\" , xytext = label_position , textcoords = \"data\" , size = fontsize , color = color , va = \"center\" , ha = \"center\" , bbox = dict ( boxstyle = \"round4\" , fc = \"w\" , color = color , lw = 2 ), arrowprops = dict ( arrowstyle = \"-|>\" , connectionstyle = \"arc3,rad=\" + str ( arrow_arc_rad ), fc = \"w\" , color = color , lw = 2 , ), ** kwargs , ) Classes AnnotationsPlotter class AnnotationsPlotter ( / , * args , ** kwargs ) View Source class AnnotationsPlotter : \" \"\" A class to encapsulate all useful plotting additional tidbits. \"\" \" @staticmethod def set _arrow_label ( axis : matplotlib . axes . Axes , label : str , arrow_position : Tuple [ float , float ] , label_position : Tuple [ float , float ] , color : str = \"k\" , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ** kwargs , ) -> matplotlib . text . Annotation : \" \"\" Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes `Axes` instance. Original code from Guido Sterbini. Args: axis (matplotlib.axes.Axes): a matplotlib axis to plot on. label (str): label text to print on the axis. arrow_position (Tuple[float, float]): where on the plot to point the tip of the arrow. label_position (Tuple[float, float]): where on the plot the text label (and thus start of the arrow) is. color (str): color parameter for your arrow and label. Defaults to 'k'. arrow_arc_rad (float): angle value defining the upwards / downwards shape of and bending of the arrow. fontsize (int): text size in the box Returns: A matploblit text annotation object. \"\" \" return axis . annotate ( label , xy = arrow_position , xycoords = \"data\" , xytext = label_position , textcoords = \"data\" , size = fontsize , color = color , va = \"center\" , ha = \"center\" , bbox = dict ( boxstyle = \"round4\" , fc = \"w\" , color = color , lw = 2 ), arrowprops = dict ( arrowstyle = \"-|>\" , connectionstyle = \"arc3,rad=\" + str ( arrow_arc_rad ), fc = \"w\" , color = color , lw = 2 , ), ** kwargs , ) Static methods set_arrow_label def set_arrow_label ( axis : matplotlib . axes . _axes . Axes , label : str , arrow_position : Tuple [ float , float ], label_position : Tuple [ float , float ], color : str = 'k' , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ** kwargs ) -> matplotlib . text . Annotation Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes Axes instance. Original code from Guido Sterbini. Parameters: Name Type Description Default axis matplotlib.axes.Axes a matplotlib axis to plot on. None label str label text to print on the axis. None arrow_position Tuple[float, float] where on the plot to point the tip of the arrow. None label_position Tuple[float, float] where on the plot the text label (and thus start of the arrow) is. None color str color parameter for your arrow and label. Defaults to 'k'. 'k' arrow_arc_rad float angle value defining the upwards / downwards shape of and bending of the arrow. None fontsize int text size in the box None Returns: Type Description None A matploblit text annotation object. View Source @staticmethod def set _arrow_label ( axis : matplotlib . axes . Axes , label : str , arrow_position : Tuple [ float , float ] , label_position : Tuple [ float , float ] , color : str = \"k\" , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ** kwargs , ) -> matplotlib . text . Annotation : \" \"\" Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes `Axes` instance. Original code from Guido Sterbini. Args: axis (matplotlib.axes.Axes): a matplotlib axis to plot on. label (str): label text to print on the axis. arrow_position (Tuple[float, float]): where on the plot to point the tip of the arrow. label_position (Tuple[float, float]): where on the plot the text label (and thus start of the arrow) is. color (str): color parameter for your arrow and label. Defaults to 'k'. arrow_arc_rad (float): angle value defining the upwards / downwards shape of and bending of the arrow. fontsize (int): text size in the box Returns: A matploblit text annotation object. \"\" \" return axis . annotate ( label , xy = arrow_position , xycoords = \"data\" , xytext = label_position , textcoords = \"data\" , size = fontsize , color = color , va = \"center\" , ha = \"center\" , bbox = dict ( boxstyle = \"round4\" , fc = \"w\" , color = color , lw = 2 ), arrowprops = dict ( arrowstyle = \"-|>\" , connectionstyle = \"arc3,rad=\" + str ( arrow_arc_rad ), fc = \"w\" , color = color , lw = 2 , ), ** kwargs , )","title":"Helpers"},{"location":"reference/pyhdtoolkit/plotting/helpers/#module-pyhdtoolkitplottinghelpers","text":"Module plotting.helpers Created on 2019.06.15 View Source \"\"\" Module plotting.helpers ----------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions for more descriptive plots. \"\"\" from typing import Tuple import matplotlib.axes class AnnotationsPlotter : \"\"\" A class to encapsulate all useful plotting additional tidbits. \"\"\" @staticmethod def set_arrow_label ( axis : matplotlib . axes . Axes , label : str , arrow_position : Tuple [ float , float ], label_position : Tuple [ float , float ], color : str = \"k\" , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ** kwargs , ) -> matplotlib . text . Annotation : \"\"\" Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes `Axes` instance. Original code from Guido Sterbini. Args: axis (matplotlib.axes.Axes): a matplotlib axis to plot on. label (str): label text to print on the axis. arrow_position (Tuple[float, float]): where on the plot to point the tip of the arrow. label_position (Tuple[float, float]): where on the plot the text label (and thus start of the arrow) is. color (str): color parameter for your arrow and label. Defaults to 'k'. arrow_arc_rad (float): angle value defining the upwards / downwards shape of and bending of the arrow. fontsize (int): text size in the box Returns: A matploblit text annotation object. \"\"\" return axis . annotate ( label , xy = arrow_position , xycoords = \"data\" , xytext = label_position , textcoords = \"data\" , size = fontsize , color = color , va = \"center\" , ha = \"center\" , bbox = dict ( boxstyle = \"round4\" , fc = \"w\" , color = color , lw = 2 ), arrowprops = dict ( arrowstyle = \"-|>\" , connectionstyle = \"arc3,rad=\" + str ( arrow_arc_rad ), fc = \"w\" , color = color , lw = 2 , ), ** kwargs , )","title":"Module pyhdtoolkit.plotting.helpers"},{"location":"reference/pyhdtoolkit/plotting/helpers/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/plotting/helpers/#annotationsplotter","text":"class AnnotationsPlotter ( / , * args , ** kwargs ) View Source class AnnotationsPlotter : \" \"\" A class to encapsulate all useful plotting additional tidbits. \"\" \" @staticmethod def set _arrow_label ( axis : matplotlib . axes . Axes , label : str , arrow_position : Tuple [ float , float ] , label_position : Tuple [ float , float ] , color : str = \"k\" , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ** kwargs , ) -> matplotlib . text . Annotation : \" \"\" Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes `Axes` instance. Original code from Guido Sterbini. Args: axis (matplotlib.axes.Axes): a matplotlib axis to plot on. label (str): label text to print on the axis. arrow_position (Tuple[float, float]): where on the plot to point the tip of the arrow. label_position (Tuple[float, float]): where on the plot the text label (and thus start of the arrow) is. color (str): color parameter for your arrow and label. Defaults to 'k'. arrow_arc_rad (float): angle value defining the upwards / downwards shape of and bending of the arrow. fontsize (int): text size in the box Returns: A matploblit text annotation object. \"\" \" return axis . annotate ( label , xy = arrow_position , xycoords = \"data\" , xytext = label_position , textcoords = \"data\" , size = fontsize , color = color , va = \"center\" , ha = \"center\" , bbox = dict ( boxstyle = \"round4\" , fc = \"w\" , color = color , lw = 2 ), arrowprops = dict ( arrowstyle = \"-|>\" , connectionstyle = \"arc3,rad=\" + str ( arrow_arc_rad ), fc = \"w\" , color = color , lw = 2 , ), ** kwargs , )","title":"AnnotationsPlotter"},{"location":"reference/pyhdtoolkit/plotting/helpers/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/plotting/helpers/#set_arrow_label","text":"def set_arrow_label ( axis : matplotlib . axes . _axes . Axes , label : str , arrow_position : Tuple [ float , float ], label_position : Tuple [ float , float ], color : str = 'k' , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ** kwargs ) -> matplotlib . text . Annotation Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes Axes instance. Original code from Guido Sterbini. Parameters: Name Type Description Default axis matplotlib.axes.Axes a matplotlib axis to plot on. None label str label text to print on the axis. None arrow_position Tuple[float, float] where on the plot to point the tip of the arrow. None label_position Tuple[float, float] where on the plot the text label (and thus start of the arrow) is. None color str color parameter for your arrow and label. Defaults to 'k'. 'k' arrow_arc_rad float angle value defining the upwards / downwards shape of and bending of the arrow. None fontsize int text size in the box None Returns: Type Description None A matploblit text annotation object. View Source @staticmethod def set _arrow_label ( axis : matplotlib . axes . Axes , label : str , arrow_position : Tuple [ float , float ] , label_position : Tuple [ float , float ] , color : str = \"k\" , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ** kwargs , ) -> matplotlib . text . Annotation : \" \"\" Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes `Axes` instance. Original code from Guido Sterbini. Args: axis (matplotlib.axes.Axes): a matplotlib axis to plot on. label (str): label text to print on the axis. arrow_position (Tuple[float, float]): where on the plot to point the tip of the arrow. label_position (Tuple[float, float]): where on the plot the text label (and thus start of the arrow) is. color (str): color parameter for your arrow and label. Defaults to 'k'. arrow_arc_rad (float): angle value defining the upwards / downwards shape of and bending of the arrow. fontsize (int): text size in the box Returns: A matploblit text annotation object. \"\" \" return axis . annotate ( label , xy = arrow_position , xycoords = \"data\" , xytext = label_position , textcoords = \"data\" , size = fontsize , color = color , va = \"center\" , ha = \"center\" , bbox = dict ( boxstyle = \"round4\" , fc = \"w\" , color = color , lw = 2 ), arrowprops = dict ( arrowstyle = \"-|>\" , connectionstyle = \"arc3,rad=\" + str ( arrow_arc_rad ), fc = \"w\" , color = color , lw = 2 , ), ** kwargs , )","title":"set_arrow_label"},{"location":"reference/pyhdtoolkit/utils/","text":"Module pyhdtoolkit.utils utils package ~ ~ ~ ~ ~ ~ ~ These are miscellaneous modules with functions that sometime tunr out useful to my workflow. These are mainly wrappers around lower-level tools, or simply just additional niceties. View Source \"\"\" utils package ~~~~~~~~~~~~~~~~~~~ These are miscellaneous modules with functions that sometime tunr out useful to my workflow. These are mainly wrappers around lower-level tools, or simply just additional niceties. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .cmdline import CommandLine from .executors import MultiProcessor , MultiThreader from .printutil import END , Background , Foreground , Styles Sub-modules pyhdtoolkit.utils.cmdline pyhdtoolkit.utils.contexts pyhdtoolkit.utils.defaults pyhdtoolkit.utils.executors pyhdtoolkit.utils.htc_monitor pyhdtoolkit.utils.operations pyhdtoolkit.utils.printutil Variables python3 END","title":"Index"},{"location":"reference/pyhdtoolkit/utils/#module-pyhdtoolkitutils","text":"utils package ~ ~ ~ ~ ~ ~ ~ These are miscellaneous modules with functions that sometime tunr out useful to my workflow. These are mainly wrappers around lower-level tools, or simply just additional niceties. View Source \"\"\" utils package ~~~~~~~~~~~~~~~~~~~ These are miscellaneous modules with functions that sometime tunr out useful to my workflow. These are mainly wrappers around lower-level tools, or simply just additional niceties. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .cmdline import CommandLine from .executors import MultiProcessor , MultiThreader from .printutil import END , Background , Foreground , Styles","title":"Module pyhdtoolkit.utils"},{"location":"reference/pyhdtoolkit/utils/#sub-modules","text":"pyhdtoolkit.utils.cmdline pyhdtoolkit.utils.contexts pyhdtoolkit.utils.defaults pyhdtoolkit.utils.executors pyhdtoolkit.utils.htc_monitor pyhdtoolkit.utils.operations pyhdtoolkit.utils.printutil","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/utils/#variables","text":"python3 END","title":"Variables"},{"location":"reference/pyhdtoolkit/utils/cmdline/","text":"Module pyhdtoolkit.utils.cmdline Module utils.cmdline Created on 2019.11.06 View Source \"\"\" Module utils.cmdline -------------------- Created on 2019.11.06 :author: Felix Soubelet (felix.soubelet@cern.ch) Utility script to help run commands and access the commandline. \"\"\" import errno import os import signal import subprocess from typing import Mapping , Optional , Tuple from loguru import logger from pyhdtoolkit.utils.contexts import timeit class CommandLine : \"\"\" A high-level object to encapsulate the different methods for interacting with the commandline. \"\"\" @staticmethod def check_pid_exists ( pid : int ) -> bool : \"\"\" Check whether the given PID exists in the current process table. Args: pid (int): the Process ID you want to check. Returns: A boolean stating the result. \"\"\" if pid == 0 : # According to \"man 2 kill\", PID 0 refers to <<every process in the process group of # the calling process>>. Best not to go any further. logger . warning ( \"PID 0 refers to 'every process in calling processes', and should be untouched\" ) return True try : # Sending SIG 0 only checks if process has terminated, we're not actually terminating it os . kill ( pid , 0 ) except OSError as pid_checkout_error : if pid_checkout_error . errno == errno . ESRCH : # ERROR \"No such process\" return False if ( pid_checkout_error . errno == errno . EPERM ): # ERROR \"Operation not permitted\" -> there's a process to deny access to. return True # According to \"man 2 kill\" possible error values are (EINVAL, EPERM, ESRCH), therefore # we should never get here. If so let's be explicit in considering this an error. logger . exception ( \"Could not figure out the provided PID for some reason\" ) raise pid_checkout_error return True @staticmethod def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Optional [ int ], bytes ]: \"\"\" Run command based on `subprocess.Popen` and return the tuple of `(returncode, stdout)`. Note that `stderr` is redirected to `stdout`. `shell` is same to parameter of `Popen`. If the process does not terminate after `timeout` seconds, a `TimeoutExpired` exception will be raised. Args: command (str): string, the command you want to run. shell (bool): same as `Popen` argument. Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to run the command. In other words, using an intermediate shell means that variables, glob patterns, and other special shell features in the command string are processed before the command is ran. Defaults to True. env (Mapping): mapping that defines the environment variables for the new process. timeout (float): same as `Popen.communicate` argument, number of seconds to wait for a response before raising a TimeoutExpired exception. Returns: The tuple of (returncode, stdout). Beware, the stdout will be a byte array (id est b'some returned text'). This output, returned as stdout, needs to be decoded properly before you do anything with it, especially if you intend to log it into a file. While it will most likely be 'utf-8', the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on. Usage: CommandLine.run('echo hello') -> (0, b'hello\\r\\n') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') \"\"\" with timeit ( lambda spanned : logger . info ( f \"Ran command ' { command } ' in a subprocess, in: { spanned : .4f } seconds\" ) ): process = subprocess . Popen ( command , shell = shell , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , env = env ) stdout , _ = process . communicate ( timeout = timeout ) if process . poll () != 0 : logger . warning ( f \"Subprocess command ' { command } ' finished with exit code: { process . poll () } \" ) else : logger . success ( f \"Subprocess command ' { command } ' finished with exit code: { process . poll () } \" ) return process . poll (), stdout @staticmethod def terminate ( pid : int ) -> bool : \"\"\" Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Args: pid (int): the process ID to kill. Returns: A boolean stating the success of the operation. \"\"\" if CommandLine . check_pid_exists ( pid ): os . kill ( pid , signal . SIGTERM ) logger . debug ( f \"Process { pid } has successfully been terminated.\" ) return True logger . error ( f \"Process with ID { pid } could not be terminated.\" ) return False Classes CommandLine class CommandLine ( / , * args , ** kwargs ) View Source class CommandLine : \" \"\" A high-level object to encapsulate the different methods for interacting with the commandline. \"\" \" @staticmethod def check_pid_exists ( pid : int ) -> bool : \" \"\" Check whether the given PID exists in the current process table. Args: pid (int): the Process ID you want to check. Returns: A boolean stating the result. \"\" \" if pid == 0 : # According to \"man 2 kill\", PID 0 refers to <<every process in the process group of # the calling process>>. Best not to go any further. logger . warning ( \"PID 0 refers to 'every process in calling processes', and should be untouched\" ) return True try : # Sending SIG 0 only checks if process has terminated, we're not actually terminating it os . kill ( pid , 0 ) except OSError as pid_checkout_error : if pid_checkout_error . errno == errno . ESRCH : # ERROR \"No such process\" return False if ( pid_checkout_error . errno == errno . EPERM ) : # ERROR \"Operation not permitted\" -> there's a process to deny access to. return True # According to \"man 2 kill\" possible error values are (EINVAL, EPERM, ESRCH), therefore # we should never get here. If so let's be explicit in considering this an error. logger . exception ( \"Could not figure out the provided PID for some reason\" ) raise pid_checkout_error return True @staticmethod def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Optional [ int ] , bytes ] : \" \"\" Run command based on `subprocess.Popen` and return the tuple of `(returncode, stdout)`. Note that `stderr` is redirected to `stdout`. `shell` is same to parameter of `Popen`. If the process does not terminate after `timeout` seconds, a `TimeoutExpired` exception will be raised. Args: command (str): string, the command you want to run. shell (bool): same as `Popen` argument. Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to run the command. In other words, using an intermediate shell means that variables, glob patterns, and other special shell features in the command string are processed before the command is ran. Defaults to True. env (Mapping): mapping that defines the environment variables for the new process. timeout (float): same as `Popen.communicate` argument, number of seconds to wait for a response before raising a TimeoutExpired exception. Returns: The tuple of (returncode, stdout). Beware, the stdout will be a byte array (id est b'some returned text'). This output, returned as stdout, needs to be decoded properly before you do anything with it, especially if you intend to log it into a file. While it will most likely be 'utf-8', the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on. Usage: CommandLine.run('echo hello') -> (0, b'hello \\r\\n ') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') \"\" \" with timeit ( lambda spanned : logger . info ( f \"Ran command '{command}' in a subprocess, in: {spanned:.4f} seconds\" ) ) : process = subprocess . Popen ( command , shell = shell , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , env = env ) stdout , _ = process . communicate ( timeout = timeout ) if process . poll () != 0 : logger . warning ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) else : logger . success ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) return process . poll (), stdout @staticmethod def terminate ( pid : int ) -> bool : \" \"\" Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Args: pid (int): the process ID to kill. Returns: A boolean stating the success of the operation. \"\" \" if CommandLine . check_pid_exists ( pid ) : os . kill ( pid , signal . SIGTERM ) logger . debug ( f \"Process {pid} has successfully been terminated.\" ) return True logger . error ( f \"Process with ID {pid} could not be terminated.\" ) return False Static methods check_pid_exists def check_pid_exists ( pid : int ) -> bool Check whether the given PID exists in the current process table. Parameters: Name Type Description Default pid int the Process ID you want to check. None Returns: Type Description None A boolean stating the result. View Source @staticmethod def check_pid_exists ( pid : int ) -> bool : \"\"\" Check whether the given PID exists in the current process table. Args: pid (int): the Process ID you want to check. Returns: A boolean stating the result. \"\"\" if pid == 0 : # According to \"man 2 kill\" , PID 0 refers to << every process in the process group of # the calling process >> . Best not to go any further . logger . warning ( \"PID 0 refers to 'every process in calling processes', and should be untouched\" ) return True try : # Sending SIG 0 only checks if process has terminated , we 're not actually terminating it os.kill(pid, 0) except OSError as pid_checkout_error: if pid_checkout_error.errno == errno.ESRCH: # ERROR \"No such process\" return False if ( pid_checkout_error.errno == errno.EPERM ): # ERROR \"Operation not permitted\" -> there' s a process to deny access to . return True # According to \"man 2 kill\" possible error values are ( EINVAL , EPERM , ESRCH ), therefore # we should never get here . If so let ' s be explicit in considering this an error . logger . exception ( \"Could not figure out the provided PID for some reason\" ) raise pid_checkout_error return True run def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Union [ int , NoneType ], bytes ] Run command based on subprocess.Popen and return the tuple of (returncode, stdout) . Note that stderr is redirected to stdout . shell is same to parameter of Popen . If the process does not terminate after timeout seconds, a TimeoutExpired exception will be raised. Args : command ( str ) : string , the command you want to run . shell ( bool ) : same as `Popen` argument . Set ting the shell argument to a true value causes subprocess to spawn an intermediate shell process , and tell it to run the command . In other words , using an intermediate shell means that variables , glob patterns , and other special shell features in the command string are processed before the command is ran . Defaults to True . env ( Mapping ) : mapping that defines the environment variables for the new process . timeout ( float ) : same as `Popen.communicate` argument , number of seconds to wait for a response before raising a TimeoutExpired exception . Returns : The tuple of ( returncode , stdout ). Beware , the stdout will be a byte array ( id est b 'some returned text' ). This output , returned as stdout , needs to be decoded properly before you do anything with it , especially if you intend to log it into a file . While it will most likely be 'utf-8' , the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on . Usage : CommandLine . run ( 'echo hello' ) -> ( 0 , b 'hello ') modified_env = os . environ . copy () modified_env [ 'ENV_VAR' ] = new_value CommandLine . run ( 'echo $ENV_VAR' , env = modified_env ) -> ( 0 , b 'new_value' ) View Source @staticmethod def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Optional [ int ] , bytes ] : \" \"\" Run command based on `subprocess.Popen` and return the tuple of `(returncode, stdout)`. Note that `stderr` is redirected to `stdout`. `shell` is same to parameter of `Popen`. If the process does not terminate after `timeout` seconds, a `TimeoutExpired` exception will be raised. Args: command (str): string, the command you want to run. shell (bool): same as `Popen` argument. Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to run the command. In other words, using an intermediate shell means that variables, glob patterns, and other special shell features in the command string are processed before the command is ran. Defaults to True. env (Mapping): mapping that defines the environment variables for the new process. timeout (float): same as `Popen.communicate` argument, number of seconds to wait for a response before raising a TimeoutExpired exception. Returns: The tuple of (returncode, stdout). Beware, the stdout will be a byte array (id est b'some returned text'). This output, returned as stdout, needs to be decoded properly before you do anything with it, especially if you intend to log it into a file. While it will most likely be 'utf-8', the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on. Usage: CommandLine.run('echo hello') -> (0, b'hello \\r\\n ') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') \"\" \" with timeit ( lambda spanned : logger . info ( f \"Ran command '{command}' in a subprocess, in: {spanned:.4f} seconds\" ) ) : process = subprocess . Popen ( command , shell = shell , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , env = env ) stdout , _ = process . communicate ( timeout = timeout ) if process . poll () != 0 : logger . warning ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) else : logger . success ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) return process . poll (), stdout terminate def terminate ( pid : int ) -> bool Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Parameters: Name Type Description Default pid int the process ID to kill. None Returns: Type Description None A boolean stating the success of the operation. View Source @ staticmethod def terminate ( pid : int ) -> bool : \"\"\" Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Args: pid (int): the process ID to kill. Returns: A boolean stating the success of the operation. \"\"\" if CommandLine . check_pid_exists ( pid ): os . kill ( pid , signal . SIGTERM ) logger . debug ( f \"Process {pid} has successfully been terminated.\" ) return True logger . error ( f \"Process with ID {pid} could not be terminated.\" ) return False","title":"Cmdline"},{"location":"reference/pyhdtoolkit/utils/cmdline/#module-pyhdtoolkitutilscmdline","text":"Module utils.cmdline Created on 2019.11.06 View Source \"\"\" Module utils.cmdline -------------------- Created on 2019.11.06 :author: Felix Soubelet (felix.soubelet@cern.ch) Utility script to help run commands and access the commandline. \"\"\" import errno import os import signal import subprocess from typing import Mapping , Optional , Tuple from loguru import logger from pyhdtoolkit.utils.contexts import timeit class CommandLine : \"\"\" A high-level object to encapsulate the different methods for interacting with the commandline. \"\"\" @staticmethod def check_pid_exists ( pid : int ) -> bool : \"\"\" Check whether the given PID exists in the current process table. Args: pid (int): the Process ID you want to check. Returns: A boolean stating the result. \"\"\" if pid == 0 : # According to \"man 2 kill\", PID 0 refers to <<every process in the process group of # the calling process>>. Best not to go any further. logger . warning ( \"PID 0 refers to 'every process in calling processes', and should be untouched\" ) return True try : # Sending SIG 0 only checks if process has terminated, we're not actually terminating it os . kill ( pid , 0 ) except OSError as pid_checkout_error : if pid_checkout_error . errno == errno . ESRCH : # ERROR \"No such process\" return False if ( pid_checkout_error . errno == errno . EPERM ): # ERROR \"Operation not permitted\" -> there's a process to deny access to. return True # According to \"man 2 kill\" possible error values are (EINVAL, EPERM, ESRCH), therefore # we should never get here. If so let's be explicit in considering this an error. logger . exception ( \"Could not figure out the provided PID for some reason\" ) raise pid_checkout_error return True @staticmethod def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Optional [ int ], bytes ]: \"\"\" Run command based on `subprocess.Popen` and return the tuple of `(returncode, stdout)`. Note that `stderr` is redirected to `stdout`. `shell` is same to parameter of `Popen`. If the process does not terminate after `timeout` seconds, a `TimeoutExpired` exception will be raised. Args: command (str): string, the command you want to run. shell (bool): same as `Popen` argument. Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to run the command. In other words, using an intermediate shell means that variables, glob patterns, and other special shell features in the command string are processed before the command is ran. Defaults to True. env (Mapping): mapping that defines the environment variables for the new process. timeout (float): same as `Popen.communicate` argument, number of seconds to wait for a response before raising a TimeoutExpired exception. Returns: The tuple of (returncode, stdout). Beware, the stdout will be a byte array (id est b'some returned text'). This output, returned as stdout, needs to be decoded properly before you do anything with it, especially if you intend to log it into a file. While it will most likely be 'utf-8', the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on. Usage: CommandLine.run('echo hello') -> (0, b'hello\\r\\n') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') \"\"\" with timeit ( lambda spanned : logger . info ( f \"Ran command ' { command } ' in a subprocess, in: { spanned : .4f } seconds\" ) ): process = subprocess . Popen ( command , shell = shell , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , env = env ) stdout , _ = process . communicate ( timeout = timeout ) if process . poll () != 0 : logger . warning ( f \"Subprocess command ' { command } ' finished with exit code: { process . poll () } \" ) else : logger . success ( f \"Subprocess command ' { command } ' finished with exit code: { process . poll () } \" ) return process . poll (), stdout @staticmethod def terminate ( pid : int ) -> bool : \"\"\" Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Args: pid (int): the process ID to kill. Returns: A boolean stating the success of the operation. \"\"\" if CommandLine . check_pid_exists ( pid ): os . kill ( pid , signal . SIGTERM ) logger . debug ( f \"Process { pid } has successfully been terminated.\" ) return True logger . error ( f \"Process with ID { pid } could not be terminated.\" ) return False","title":"Module pyhdtoolkit.utils.cmdline"},{"location":"reference/pyhdtoolkit/utils/cmdline/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/utils/cmdline/#commandline","text":"class CommandLine ( / , * args , ** kwargs ) View Source class CommandLine : \" \"\" A high-level object to encapsulate the different methods for interacting with the commandline. \"\" \" @staticmethod def check_pid_exists ( pid : int ) -> bool : \" \"\" Check whether the given PID exists in the current process table. Args: pid (int): the Process ID you want to check. Returns: A boolean stating the result. \"\" \" if pid == 0 : # According to \"man 2 kill\", PID 0 refers to <<every process in the process group of # the calling process>>. Best not to go any further. logger . warning ( \"PID 0 refers to 'every process in calling processes', and should be untouched\" ) return True try : # Sending SIG 0 only checks if process has terminated, we're not actually terminating it os . kill ( pid , 0 ) except OSError as pid_checkout_error : if pid_checkout_error . errno == errno . ESRCH : # ERROR \"No such process\" return False if ( pid_checkout_error . errno == errno . EPERM ) : # ERROR \"Operation not permitted\" -> there's a process to deny access to. return True # According to \"man 2 kill\" possible error values are (EINVAL, EPERM, ESRCH), therefore # we should never get here. If so let's be explicit in considering this an error. logger . exception ( \"Could not figure out the provided PID for some reason\" ) raise pid_checkout_error return True @staticmethod def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Optional [ int ] , bytes ] : \" \"\" Run command based on `subprocess.Popen` and return the tuple of `(returncode, stdout)`. Note that `stderr` is redirected to `stdout`. `shell` is same to parameter of `Popen`. If the process does not terminate after `timeout` seconds, a `TimeoutExpired` exception will be raised. Args: command (str): string, the command you want to run. shell (bool): same as `Popen` argument. Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to run the command. In other words, using an intermediate shell means that variables, glob patterns, and other special shell features in the command string are processed before the command is ran. Defaults to True. env (Mapping): mapping that defines the environment variables for the new process. timeout (float): same as `Popen.communicate` argument, number of seconds to wait for a response before raising a TimeoutExpired exception. Returns: The tuple of (returncode, stdout). Beware, the stdout will be a byte array (id est b'some returned text'). This output, returned as stdout, needs to be decoded properly before you do anything with it, especially if you intend to log it into a file. While it will most likely be 'utf-8', the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on. Usage: CommandLine.run('echo hello') -> (0, b'hello \\r\\n ') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') \"\" \" with timeit ( lambda spanned : logger . info ( f \"Ran command '{command}' in a subprocess, in: {spanned:.4f} seconds\" ) ) : process = subprocess . Popen ( command , shell = shell , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , env = env ) stdout , _ = process . communicate ( timeout = timeout ) if process . poll () != 0 : logger . warning ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) else : logger . success ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) return process . poll (), stdout @staticmethod def terminate ( pid : int ) -> bool : \" \"\" Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Args: pid (int): the process ID to kill. Returns: A boolean stating the success of the operation. \"\" \" if CommandLine . check_pid_exists ( pid ) : os . kill ( pid , signal . SIGTERM ) logger . debug ( f \"Process {pid} has successfully been terminated.\" ) return True logger . error ( f \"Process with ID {pid} could not be terminated.\" ) return False","title":"CommandLine"},{"location":"reference/pyhdtoolkit/utils/cmdline/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/cmdline/#check_pid_exists","text":"def check_pid_exists ( pid : int ) -> bool Check whether the given PID exists in the current process table. Parameters: Name Type Description Default pid int the Process ID you want to check. None Returns: Type Description None A boolean stating the result. View Source @staticmethod def check_pid_exists ( pid : int ) -> bool : \"\"\" Check whether the given PID exists in the current process table. Args: pid (int): the Process ID you want to check. Returns: A boolean stating the result. \"\"\" if pid == 0 : # According to \"man 2 kill\" , PID 0 refers to << every process in the process group of # the calling process >> . Best not to go any further . logger . warning ( \"PID 0 refers to 'every process in calling processes', and should be untouched\" ) return True try : # Sending SIG 0 only checks if process has terminated , we 're not actually terminating it os.kill(pid, 0) except OSError as pid_checkout_error: if pid_checkout_error.errno == errno.ESRCH: # ERROR \"No such process\" return False if ( pid_checkout_error.errno == errno.EPERM ): # ERROR \"Operation not permitted\" -> there' s a process to deny access to . return True # According to \"man 2 kill\" possible error values are ( EINVAL , EPERM , ESRCH ), therefore # we should never get here . If so let ' s be explicit in considering this an error . logger . exception ( \"Could not figure out the provided PID for some reason\" ) raise pid_checkout_error return True","title":"check_pid_exists"},{"location":"reference/pyhdtoolkit/utils/cmdline/#run","text":"def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Union [ int , NoneType ], bytes ] Run command based on subprocess.Popen and return the tuple of (returncode, stdout) . Note that stderr is redirected to stdout . shell is same to parameter of Popen . If the process does not terminate after timeout seconds, a TimeoutExpired exception will be raised. Args : command ( str ) : string , the command you want to run . shell ( bool ) : same as `Popen` argument . Set ting the shell argument to a true value causes subprocess to spawn an intermediate shell process , and tell it to run the command . In other words , using an intermediate shell means that variables , glob patterns , and other special shell features in the command string are processed before the command is ran . Defaults to True . env ( Mapping ) : mapping that defines the environment variables for the new process . timeout ( float ) : same as `Popen.communicate` argument , number of seconds to wait for a response before raising a TimeoutExpired exception . Returns : The tuple of ( returncode , stdout ). Beware , the stdout will be a byte array ( id est b 'some returned text' ). This output , returned as stdout , needs to be decoded properly before you do anything with it , especially if you intend to log it into a file . While it will most likely be 'utf-8' , the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on . Usage : CommandLine . run ( 'echo hello' ) -> ( 0 , b 'hello ') modified_env = os . environ . copy () modified_env [ 'ENV_VAR' ] = new_value CommandLine . run ( 'echo $ENV_VAR' , env = modified_env ) -> ( 0 , b 'new_value' ) View Source @staticmethod def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Optional [ int ] , bytes ] : \" \"\" Run command based on `subprocess.Popen` and return the tuple of `(returncode, stdout)`. Note that `stderr` is redirected to `stdout`. `shell` is same to parameter of `Popen`. If the process does not terminate after `timeout` seconds, a `TimeoutExpired` exception will be raised. Args: command (str): string, the command you want to run. shell (bool): same as `Popen` argument. Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to run the command. In other words, using an intermediate shell means that variables, glob patterns, and other special shell features in the command string are processed before the command is ran. Defaults to True. env (Mapping): mapping that defines the environment variables for the new process. timeout (float): same as `Popen.communicate` argument, number of seconds to wait for a response before raising a TimeoutExpired exception. Returns: The tuple of (returncode, stdout). Beware, the stdout will be a byte array (id est b'some returned text'). This output, returned as stdout, needs to be decoded properly before you do anything with it, especially if you intend to log it into a file. While it will most likely be 'utf-8', the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on. Usage: CommandLine.run('echo hello') -> (0, b'hello \\r\\n ') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') \"\" \" with timeit ( lambda spanned : logger . info ( f \"Ran command '{command}' in a subprocess, in: {spanned:.4f} seconds\" ) ) : process = subprocess . Popen ( command , shell = shell , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , env = env ) stdout , _ = process . communicate ( timeout = timeout ) if process . poll () != 0 : logger . warning ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) else : logger . success ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) return process . poll (), stdout","title":"run"},{"location":"reference/pyhdtoolkit/utils/cmdline/#terminate","text":"def terminate ( pid : int ) -> bool Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Parameters: Name Type Description Default pid int the process ID to kill. None Returns: Type Description None A boolean stating the success of the operation. View Source @ staticmethod def terminate ( pid : int ) -> bool : \"\"\" Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Args: pid (int): the process ID to kill. Returns: A boolean stating the success of the operation. \"\"\" if CommandLine . check_pid_exists ( pid ): os . kill ( pid , signal . SIGTERM ) logger . debug ( f \"Process {pid} has successfully been terminated.\" ) return True logger . error ( f \"Process with ID {pid} could not be terminated.\" ) return False","title":"terminate"},{"location":"reference/pyhdtoolkit/utils/contexts/","text":"Module pyhdtoolkit.utils.contexts Module utils.contexts Provides contexts to use functions in. View Source \"\"\" Module utils.contexts --------------------- Provides contexts to use functions in. \"\"\" import time from contextlib import contextmanager from typing import Callable , Iterator @contextmanager def timeit ( function : Callable ) -> Iterator [ None ]: \"\"\" Returns the time elapsed when executing code in the context via `function`. Original code from @jaimecp89 Args: function (Callable): any callable taking one argument. Was conceived with a lambda in mind. Returns: The elapsed time as an argument for the provided function. Usage: with timeit(lambda spanned: logger.debug(f'Did some stuff in {spanned} seconds')): some_stuff() some_other_stuff() \"\"\" start_time = time . time () try : yield finally : time_used = time . time () - start_time function ( time_used ) Functions timeit def timeit ( function : Callable ) -> Iterator [ NoneType ] Returns the time elapsed when executing code in the context via function . Original code from @jaimecp89 Parameters: Name Type Description Default function Callable any callable taking one argument. Was conceived with a lambda in mind. None Returns: Type Description None The elapsed time as an argument for the provided function. Usage: with timeit(lambda spanned: logger.debug(f'Did some stuff in {spanned} seconds')): some_stuff() some_other_stuff() | View Source @contextmanager def timeit ( function : Callable ) -> Iterator [ None ] : \"\"\" Returns the time elapsed when executing code in the context via `function`. Original code from @jaimecp89 Args: function (Callable): any callable taking one argument. Was conceived with a lambda in mind. Returns: The elapsed time as an argument for the provided function. Usage: with timeit(lambda spanned: logger.debug(f'Did some stuff in {spanned} seconds')): some_stuff() some_other_stuff() \"\"\" start_time = time . time () try : yield finally : time_used = time . time () - start_time function ( time_used )","title":"Contexts"},{"location":"reference/pyhdtoolkit/utils/contexts/#module-pyhdtoolkitutilscontexts","text":"Module utils.contexts Provides contexts to use functions in. View Source \"\"\" Module utils.contexts --------------------- Provides contexts to use functions in. \"\"\" import time from contextlib import contextmanager from typing import Callable , Iterator @contextmanager def timeit ( function : Callable ) -> Iterator [ None ]: \"\"\" Returns the time elapsed when executing code in the context via `function`. Original code from @jaimecp89 Args: function (Callable): any callable taking one argument. Was conceived with a lambda in mind. Returns: The elapsed time as an argument for the provided function. Usage: with timeit(lambda spanned: logger.debug(f'Did some stuff in {spanned} seconds')): some_stuff() some_other_stuff() \"\"\" start_time = time . time () try : yield finally : time_used = time . time () - start_time function ( time_used )","title":"Module pyhdtoolkit.utils.contexts"},{"location":"reference/pyhdtoolkit/utils/contexts/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/utils/contexts/#timeit","text":"def timeit ( function : Callable ) -> Iterator [ NoneType ] Returns the time elapsed when executing code in the context via function . Original code from @jaimecp89 Parameters: Name Type Description Default function Callable any callable taking one argument. Was conceived with a lambda in mind. None Returns: Type Description None The elapsed time as an argument for the provided function. Usage: with timeit(lambda spanned: logger.debug(f'Did some stuff in {spanned} seconds')): some_stuff() some_other_stuff() | View Source @contextmanager def timeit ( function : Callable ) -> Iterator [ None ] : \"\"\" Returns the time elapsed when executing code in the context via `function`. Original code from @jaimecp89 Args: function (Callable): any callable taking one argument. Was conceived with a lambda in mind. Returns: The elapsed time as an argument for the provided function. Usage: with timeit(lambda spanned: logger.debug(f'Did some stuff in {spanned} seconds')): some_stuff() some_other_stuff() \"\"\" start_time = time . time () try : yield finally : time_used = time . time () - start_time function ( time_used )","title":"timeit"},{"location":"reference/pyhdtoolkit/utils/defaults/","text":"Module pyhdtoolkit.utils.defaults Module utils.defaults Created on 2019.11.12 View Source \"\"\" Module utils.defaults --------------------- Created on 2019.11.12 :author: Felix Soubelet (felix.soubelet@cern.ch) Provides defaults to import for different settings. \"\"\" import sys from pathlib import Path from typing import Dict , NewType , Union import matplotlib import matplotlib.pyplot as plt from loguru import logger ANACONDA_INSTALL = Path () . home () / \"anaconda3\" OMC_PYTHON = ANACONDA_INSTALL / \"envs\" / \"OMC\" / \"bin\" / \"python\" WORK_REPOSITORIES = Path . home () / \"Repositories\" / \"Work\" BETABEAT_REPO = WORK_REPOSITORIES / \"Beta-Beat.src\" OMC3_REPO = WORK_REPOSITORIES / \"omc3\" TBT_CONVERTER_SCRIPT = OMC3_REPO / \"omc3\" / \"tbt_converter.py\" LOGURU_FORMAT = ( \"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | \" \"<level> {level: <8} </level> | \" \"<cyan> {name} </cyan>:<cyan> {line} </cyan> - \" \"<level> {message} </level>\" ) PlotSetting = NewType ( \"PlotSetting\" , Union [ float , bool , str , tuple ]) # Set those with matplotlib.pyplot.rcParams.update(PLOT_PARAMS). # Will ALWAYS be overwritten by later on definition PLOT_PARAMS : Dict [ str , PlotSetting ] = { # ------ Patches ------ # \"patch.linewidth\" : 1.5 , # Width of patches edge lines # ------ Fonts ------ # \"font.family\" : \"sans-serif\" , # Font family \"font.style\" : \"normal\" , # Style to apply to text font \"font.weight\" : \"bold\" , # Bold font \"font.size\" : 25 , # Default font size of elements \"font.sans-serif\" : \"Helvetica\" , # Sans-Serif font to use # ----- Mathtext ----- # \"mathtext.default\" : \"regular\" , # default font for math # ------ Text ------ # \"text.usetex\" : True , # Use LaTeX for text handling (Set to False if you don't have a local installation) \"text.latex.preamble\" : r \"\\usepackage{amsmath, amssymb}\" , # \\boldmath\", # Be careful with the preamble # ------ Axes ------ # \"axes.linewidth\" : 2 , # Linewidth of axes edges \"axes.grid\" : True , # Do display grid \"axes.titlesize\" : 30 , # Fontsize of the axes title \"axes.labelsize\" : 30 , # Fontsize of the x and y axis labels \"axes.labelweight\" : \"bold\" , # Bold labels \"axes.formatter.limits\" : ( - 4 , 5 ), # Switch to scientific notations when order of magnitude reaches 1e3 \"axes.formatter.use_mathtext\" : True , # Format with i.e 10^{4} instead of 1e4 \"axes.formatter.useoffset\" : False , # Do not use the annoying offset on top of yticks # ------ Date Formats ------ # \"date.autoformatter.year\" : \"%Y\" , # AutoDateFormatter setting for years display \"date.autoformatter.month\" : \"%Y-%m\" , # AutoDateFormatter setting for months display \"date.autoformatter.day\" : \"%Y-%m- %d \" , # AutoDateFormatter setting for days display \"date.autoformatter.hour\" : \"%m- %d %H\" , # AutoDateFormatter setting for seconds display \"date.autoformatter.minute\" : \" %d %H:%M\" , # AutoDateFormatter setting for minutes display \"date.autoformatter.second\" : \"%H:%M:%S\" , # AutoDateFormatter setting for seconds display \"date.autoformatter.microsecond\" : \"%M:%S. %f \" , # AutoDateFormatter setting for microseconds # ------ Horizontal Ticks ------ # \"xtick.major.size\" : 8 , # Size (length) of the major xtick locators \"xtick.minor.size\" : 5 , # Size (length) of the minor xtick locators \"xtick.major.width\" : 1.5 , # Width of the major xtick locators \"xtick.minor.width\" : 0.6 , # Width of the minor xtick locators \"xtick.labelsize\" : 25 , # Fontsize of the x axis tick labels \"xtick.direction\" : \"in\" , # Show xticks towards inside of figure \"xtick.minor.visible\" : True , # Show minor xtick locators # ------ Vertical Ticks ------ # \"ytick.major.size\" : 8 , # Size (length) of the major ytick locators \"ytick.minor.size\" : 5 , # Size (length) of the minor ytick locators \"ytick.major.width\" : 1.5 , # Width of the major ytick locators \"ytick.minor.width\" : 0.6 , # Width of the minor ytick locators \"ytick.labelsize\" : 25 , # Fontsize of the y axis tick labels \"ytick.direction\" : \"in\" , # Show yticks towards inside of figure \"ytick.minor.visible\" : True , # Show minor ytick locators # ----- Grid ----- # \"grid.linestyle\" : \"--\" , # Which linestyle for grid lines \"grid.linewidth\" : 1.3 , # Width of the grid lines # ------- Legend ------ # \"legend.loc\" : \"best\" , # Default legend location \"legend.frameon\" : True , # Make a dedicated patch for the legend \"legend.fancybox\" : True , # Use rounded box for legend background \"legend.fontsize\" : 22 , # Legend text font size \"legend.title_fontsize\" : 23 , # Legend title text font size # ------ Figure ------ # \"figure.titlesize\" : 35 , # Size of the figure title \"figure.figsize\" : ( 16 , 10 ), # Default size of the figures \"figure.dpi\" : 300 , # Figure dots per inch \"figure.subplot.left\" : 0.15 , # Left side of the subplots of the figure \"figure.subplot.right\" : 0.90 , # Right side of the subplots of the figure \"figure.subplot.bottom\" : 0.15 , # Bottom side of the subplots of the figure \"figure.subplot.top\" : 0.90 , # Top side of the subplots of the figure \"figure.autolayout\" : True , # Adjust subplot params to fit the figure (tight_layout) # ------ Saving ------ # \"savefig.dpi\" : 1000 , # Saved figure dots per inch \"savefig.format\" : \"pdf\" , # Saved figure file format \"savefig.bbox\" : \"tight\" , # Careful: incompatible with pipe-based animation backends } def config_logger ( level : str = \"INFO\" , ** kwargs ) -> None : \"\"\" Resets the logger object from loguru, with `sys.stdout` as a sink and the aforedefined format. This comes down to personnal preference. Any additional keyword argument used is transmitted to the `logger.add` call. \"\"\" logger . remove () logger . add ( sys . stdout , format = LOGURU_FORMAT , level = level . upper (), ** kwargs ) def install_mpl_style () -> None : \"\"\" Will create a `phd.mplstyle` file in the appropriate directories from the `PLOT_PARAMS` defined in this module. This enables one to use the style without importing `PLOT_PARAMS` and updating the rcParams, but instead simply using `plt.style.use(\"phd\")`. Sometimes, matplotlib will not look for the file in its global config directory, but in the activated environment's site-packages data. The file is installed in both places. \"\"\" logger . info ( \"Installing matplotlib style\" ) style_content : str = \" \\n \" . join ( f \" { option } : { setting } \" for option , setting in PLOT_PARAMS . items ()) mpl_config_stylelib = Path ( matplotlib . get_configdir ()) / \"stylelib\" mpl_env_stylelib = Path ( plt . style . core . BASE_LIBRARY_PATH ) logger . debug ( \"Ensuring matplotlib 'stylelib' directory exists\" ) mpl_config_stylelib . mkdir ( parents = True , exist_ok = True ) mpl_env_stylelib . mkdir ( parents = True , exist_ok = True ) config_style_file = mpl_config_stylelib / \"phd.mplstyle\" env_style_file = mpl_env_stylelib / \"phd.mplstyle\" logger . debug ( f \"Creating style file at ' { config_style_file . absolute () } '\" ) config_style_file . write_text ( style_content . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" )) logger . debug ( f \"Creating style file at ' { env_style_file . absolute () } '\" ) env_style_file . write_text ( style_content . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" )) logger . success ( \"You can now use it with 'plt.style.use( \\\" phd \\\" )'\" ) Variables ANACONDA_INSTALL BETABEAT_REPO LOGURU_FORMAT OMC3_REPO OMC_PYTHON PLOT_PARAMS TBT_CONVERTER_SCRIPT WORK_REPOSITORIES Functions PlotSetting def PlotSetting ( x ) View Source def new_type ( x ) : return x config_logger def config_logger ( level : str = 'INFO' , ** kwargs ) -> None Resets the logger object from loguru, with sys.stdout as a sink and the aforedefined format. This comes down to personnal preference. Any additional keyword argument used is transmitted to the logger.add call. View Source def config_logger ( level : str = \"INFO\" , ** kwargs ) -> None : \"\"\" Resets the logger object from loguru, with `sys.stdout` as a sink and the aforedefined format. This comes down to personnal preference. Any additional keyword argument used is transmitted to the `logger.add` call. \"\"\" logger . remove () logger . add ( sys . stdout , format = LOGURU_FORMAT , level = level . upper (), ** kwargs ) install_mpl_style def install_mpl_style ( ) -> None Will create a phd.mplstyle file in the appropriate directories from the PLOT_PARAMS defined in this module. This enables one to use the style without importing PLOT_PARAMS and updating the rcParams, but instead simply using plt.style.use(\"phd\") . Sometimes, matplotlib will not look for the file in its global config directory, but in the activated environment's site-packages data. The file is installed in both places. View Source def install_mpl_style () -> None : \" \"\" Will create a `phd.mplstyle` file in the appropriate directories from the `PLOT_PARAMS` defined in this module. This enables one to use the style without importing `PLOT_PARAMS` and updating the rcParams, but instead simply using `plt.style.use(\" phd \")`. Sometimes, matplotlib will not look for the file in its global config directory, but in the activated environment's site-packages data. The file is installed in both places. \"\" \" logger . info ( \"Installing matplotlib style\" ) style_content : str = \" \\n \" . join ( f \"{option} : {setting}\" for option , set ting in PLOT_PARAMS . items ()) mpl_config_stylelib = Path ( matplotlib . get_configdir ()) / \"stylelib\" mpl_env_stylelib = Path ( plt . style . core . BASE_LIBRARY_PATH ) logger . debug ( \"Ensuring matplotlib 'stylelib' directory exists\" ) mpl_config_stylelib . mkdir ( parents = True , exist_ok = True ) mpl_env_stylelib . mkdir ( parents = True , exist_ok = True ) config_style_file = mpl_config_stylelib / \"phd.mplstyle\" env_style_file = mpl_env_stylelib / \"phd.mplstyle\" logger . debug ( f \"Creating style file at '{config_style_file.absolute()}'\" ) config_style_file . write_text ( style_content . replace ( \"(\" , \"\" ). replace ( \")\" , \"\" )) logger . debug ( f \"Creating style file at '{env_style_file.absolute()}'\" ) env_style_file . write_text ( style_content . replace ( \"(\" , \"\" ). replace ( \")\" , \"\" )) logger . success ( \"You can now use it with 'plt.style.use( \\\" phd \\\" )'\" )","title":"Defaults"},{"location":"reference/pyhdtoolkit/utils/defaults/#module-pyhdtoolkitutilsdefaults","text":"Module utils.defaults Created on 2019.11.12 View Source \"\"\" Module utils.defaults --------------------- Created on 2019.11.12 :author: Felix Soubelet (felix.soubelet@cern.ch) Provides defaults to import for different settings. \"\"\" import sys from pathlib import Path from typing import Dict , NewType , Union import matplotlib import matplotlib.pyplot as plt from loguru import logger ANACONDA_INSTALL = Path () . home () / \"anaconda3\" OMC_PYTHON = ANACONDA_INSTALL / \"envs\" / \"OMC\" / \"bin\" / \"python\" WORK_REPOSITORIES = Path . home () / \"Repositories\" / \"Work\" BETABEAT_REPO = WORK_REPOSITORIES / \"Beta-Beat.src\" OMC3_REPO = WORK_REPOSITORIES / \"omc3\" TBT_CONVERTER_SCRIPT = OMC3_REPO / \"omc3\" / \"tbt_converter.py\" LOGURU_FORMAT = ( \"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | \" \"<level> {level: <8} </level> | \" \"<cyan> {name} </cyan>:<cyan> {line} </cyan> - \" \"<level> {message} </level>\" ) PlotSetting = NewType ( \"PlotSetting\" , Union [ float , bool , str , tuple ]) # Set those with matplotlib.pyplot.rcParams.update(PLOT_PARAMS). # Will ALWAYS be overwritten by later on definition PLOT_PARAMS : Dict [ str , PlotSetting ] = { # ------ Patches ------ # \"patch.linewidth\" : 1.5 , # Width of patches edge lines # ------ Fonts ------ # \"font.family\" : \"sans-serif\" , # Font family \"font.style\" : \"normal\" , # Style to apply to text font \"font.weight\" : \"bold\" , # Bold font \"font.size\" : 25 , # Default font size of elements \"font.sans-serif\" : \"Helvetica\" , # Sans-Serif font to use # ----- Mathtext ----- # \"mathtext.default\" : \"regular\" , # default font for math # ------ Text ------ # \"text.usetex\" : True , # Use LaTeX for text handling (Set to False if you don't have a local installation) \"text.latex.preamble\" : r \"\\usepackage{amsmath, amssymb}\" , # \\boldmath\", # Be careful with the preamble # ------ Axes ------ # \"axes.linewidth\" : 2 , # Linewidth of axes edges \"axes.grid\" : True , # Do display grid \"axes.titlesize\" : 30 , # Fontsize of the axes title \"axes.labelsize\" : 30 , # Fontsize of the x and y axis labels \"axes.labelweight\" : \"bold\" , # Bold labels \"axes.formatter.limits\" : ( - 4 , 5 ), # Switch to scientific notations when order of magnitude reaches 1e3 \"axes.formatter.use_mathtext\" : True , # Format with i.e 10^{4} instead of 1e4 \"axes.formatter.useoffset\" : False , # Do not use the annoying offset on top of yticks # ------ Date Formats ------ # \"date.autoformatter.year\" : \"%Y\" , # AutoDateFormatter setting for years display \"date.autoformatter.month\" : \"%Y-%m\" , # AutoDateFormatter setting for months display \"date.autoformatter.day\" : \"%Y-%m- %d \" , # AutoDateFormatter setting for days display \"date.autoformatter.hour\" : \"%m- %d %H\" , # AutoDateFormatter setting for seconds display \"date.autoformatter.minute\" : \" %d %H:%M\" , # AutoDateFormatter setting for minutes display \"date.autoformatter.second\" : \"%H:%M:%S\" , # AutoDateFormatter setting for seconds display \"date.autoformatter.microsecond\" : \"%M:%S. %f \" , # AutoDateFormatter setting for microseconds # ------ Horizontal Ticks ------ # \"xtick.major.size\" : 8 , # Size (length) of the major xtick locators \"xtick.minor.size\" : 5 , # Size (length) of the minor xtick locators \"xtick.major.width\" : 1.5 , # Width of the major xtick locators \"xtick.minor.width\" : 0.6 , # Width of the minor xtick locators \"xtick.labelsize\" : 25 , # Fontsize of the x axis tick labels \"xtick.direction\" : \"in\" , # Show xticks towards inside of figure \"xtick.minor.visible\" : True , # Show minor xtick locators # ------ Vertical Ticks ------ # \"ytick.major.size\" : 8 , # Size (length) of the major ytick locators \"ytick.minor.size\" : 5 , # Size (length) of the minor ytick locators \"ytick.major.width\" : 1.5 , # Width of the major ytick locators \"ytick.minor.width\" : 0.6 , # Width of the minor ytick locators \"ytick.labelsize\" : 25 , # Fontsize of the y axis tick labels \"ytick.direction\" : \"in\" , # Show yticks towards inside of figure \"ytick.minor.visible\" : True , # Show minor ytick locators # ----- Grid ----- # \"grid.linestyle\" : \"--\" , # Which linestyle for grid lines \"grid.linewidth\" : 1.3 , # Width of the grid lines # ------- Legend ------ # \"legend.loc\" : \"best\" , # Default legend location \"legend.frameon\" : True , # Make a dedicated patch for the legend \"legend.fancybox\" : True , # Use rounded box for legend background \"legend.fontsize\" : 22 , # Legend text font size \"legend.title_fontsize\" : 23 , # Legend title text font size # ------ Figure ------ # \"figure.titlesize\" : 35 , # Size of the figure title \"figure.figsize\" : ( 16 , 10 ), # Default size of the figures \"figure.dpi\" : 300 , # Figure dots per inch \"figure.subplot.left\" : 0.15 , # Left side of the subplots of the figure \"figure.subplot.right\" : 0.90 , # Right side of the subplots of the figure \"figure.subplot.bottom\" : 0.15 , # Bottom side of the subplots of the figure \"figure.subplot.top\" : 0.90 , # Top side of the subplots of the figure \"figure.autolayout\" : True , # Adjust subplot params to fit the figure (tight_layout) # ------ Saving ------ # \"savefig.dpi\" : 1000 , # Saved figure dots per inch \"savefig.format\" : \"pdf\" , # Saved figure file format \"savefig.bbox\" : \"tight\" , # Careful: incompatible with pipe-based animation backends } def config_logger ( level : str = \"INFO\" , ** kwargs ) -> None : \"\"\" Resets the logger object from loguru, with `sys.stdout` as a sink and the aforedefined format. This comes down to personnal preference. Any additional keyword argument used is transmitted to the `logger.add` call. \"\"\" logger . remove () logger . add ( sys . stdout , format = LOGURU_FORMAT , level = level . upper (), ** kwargs ) def install_mpl_style () -> None : \"\"\" Will create a `phd.mplstyle` file in the appropriate directories from the `PLOT_PARAMS` defined in this module. This enables one to use the style without importing `PLOT_PARAMS` and updating the rcParams, but instead simply using `plt.style.use(\"phd\")`. Sometimes, matplotlib will not look for the file in its global config directory, but in the activated environment's site-packages data. The file is installed in both places. \"\"\" logger . info ( \"Installing matplotlib style\" ) style_content : str = \" \\n \" . join ( f \" { option } : { setting } \" for option , setting in PLOT_PARAMS . items ()) mpl_config_stylelib = Path ( matplotlib . get_configdir ()) / \"stylelib\" mpl_env_stylelib = Path ( plt . style . core . BASE_LIBRARY_PATH ) logger . debug ( \"Ensuring matplotlib 'stylelib' directory exists\" ) mpl_config_stylelib . mkdir ( parents = True , exist_ok = True ) mpl_env_stylelib . mkdir ( parents = True , exist_ok = True ) config_style_file = mpl_config_stylelib / \"phd.mplstyle\" env_style_file = mpl_env_stylelib / \"phd.mplstyle\" logger . debug ( f \"Creating style file at ' { config_style_file . absolute () } '\" ) config_style_file . write_text ( style_content . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" )) logger . debug ( f \"Creating style file at ' { env_style_file . absolute () } '\" ) env_style_file . write_text ( style_content . replace ( \"(\" , \"\" ) . replace ( \")\" , \"\" )) logger . success ( \"You can now use it with 'plt.style.use( \\\" phd \\\" )'\" )","title":"Module pyhdtoolkit.utils.defaults"},{"location":"reference/pyhdtoolkit/utils/defaults/#variables","text":"ANACONDA_INSTALL BETABEAT_REPO LOGURU_FORMAT OMC3_REPO OMC_PYTHON PLOT_PARAMS TBT_CONVERTER_SCRIPT WORK_REPOSITORIES","title":"Variables"},{"location":"reference/pyhdtoolkit/utils/defaults/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/utils/defaults/#plotsetting","text":"def PlotSetting ( x ) View Source def new_type ( x ) : return x","title":"PlotSetting"},{"location":"reference/pyhdtoolkit/utils/defaults/#config_logger","text":"def config_logger ( level : str = 'INFO' , ** kwargs ) -> None Resets the logger object from loguru, with sys.stdout as a sink and the aforedefined format. This comes down to personnal preference. Any additional keyword argument used is transmitted to the logger.add call. View Source def config_logger ( level : str = \"INFO\" , ** kwargs ) -> None : \"\"\" Resets the logger object from loguru, with `sys.stdout` as a sink and the aforedefined format. This comes down to personnal preference. Any additional keyword argument used is transmitted to the `logger.add` call. \"\"\" logger . remove () logger . add ( sys . stdout , format = LOGURU_FORMAT , level = level . upper (), ** kwargs )","title":"config_logger"},{"location":"reference/pyhdtoolkit/utils/defaults/#install_mpl_style","text":"def install_mpl_style ( ) -> None Will create a phd.mplstyle file in the appropriate directories from the PLOT_PARAMS defined in this module. This enables one to use the style without importing PLOT_PARAMS and updating the rcParams, but instead simply using plt.style.use(\"phd\") . Sometimes, matplotlib will not look for the file in its global config directory, but in the activated environment's site-packages data. The file is installed in both places. View Source def install_mpl_style () -> None : \" \"\" Will create a `phd.mplstyle` file in the appropriate directories from the `PLOT_PARAMS` defined in this module. This enables one to use the style without importing `PLOT_PARAMS` and updating the rcParams, but instead simply using `plt.style.use(\" phd \")`. Sometimes, matplotlib will not look for the file in its global config directory, but in the activated environment's site-packages data. The file is installed in both places. \"\" \" logger . info ( \"Installing matplotlib style\" ) style_content : str = \" \\n \" . join ( f \"{option} : {setting}\" for option , set ting in PLOT_PARAMS . items ()) mpl_config_stylelib = Path ( matplotlib . get_configdir ()) / \"stylelib\" mpl_env_stylelib = Path ( plt . style . core . BASE_LIBRARY_PATH ) logger . debug ( \"Ensuring matplotlib 'stylelib' directory exists\" ) mpl_config_stylelib . mkdir ( parents = True , exist_ok = True ) mpl_env_stylelib . mkdir ( parents = True , exist_ok = True ) config_style_file = mpl_config_stylelib / \"phd.mplstyle\" env_style_file = mpl_env_stylelib / \"phd.mplstyle\" logger . debug ( f \"Creating style file at '{config_style_file.absolute()}'\" ) config_style_file . write_text ( style_content . replace ( \"(\" , \"\" ). replace ( \")\" , \"\" )) logger . debug ( f \"Creating style file at '{env_style_file.absolute()}'\" ) env_style_file . write_text ( style_content . replace ( \"(\" , \"\" ). replace ( \")\" , \"\" )) logger . success ( \"You can now use it with 'plt.style.use( \\\" phd \\\" )'\" )","title":"install_mpl_style"},{"location":"reference/pyhdtoolkit/utils/executors/","text":"Module pyhdtoolkit.utils.executors Module utils.executors Created on 2019.12.09 View Source \"\"\" Module utils.executors ---------------------- Created on 2019.12.09 :author: Felix Soubelet A module providing two classes to execute functions + arguments couples through either a multiprocessing approach, or a multithreading approach. Here are a few tidbits to keep in mind: - There can only be one thread running at any given time in a python process because of the GIL. - Multiprocessing is parallelism. Multithreading is concurrency. - Multiprocessing is for increasing speed. Multithreading is for hiding latency. - Multiprocessing is best for computations. Multithreading is best for IO. - If you have CPU heavy tasks, use multiprocessing with n_process = n_cores and never more. Never. - If you have IO heavy tasks, use multithreading with n_threads = m * n_cores with m a number bigger than 1 that you can tweak on your own. Try many values and choose the one with the best speedup because there isn\u2019t a general rule. For instance the default value of m in ThreadPoolExecutor is set to 5 which I think is quite random. \"\"\" from concurrent import futures from typing import Callable , List from loguru import logger class MultiProcessor : \"\"\" A class to easily wrap a multi-processing context manager call to a function. Reminder: multiprocessing is good for cpu-heavy tasks. Reminder: only picklable objects can be executed and returned. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_cpu_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ] : \"\"\" Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_processes (int): the number of processes to fire up. No more than your number of cores! If n_processes is `None` or not given, ProcessPoolExecutor will default it to the number of processors on the machine. Returns: A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multiprocessing with {n_processes} processes\" ) with futures . ProcessPoolExecutor ( n_processes ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_processes} processes finished\" ) return list ( results ) class MultiThreader : \"\"\" A class to easily wrap a multi-threading context manager call to a function. Reminder: multithreading is good for IO-heavy tasks. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_io_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ] : \"\"\" Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_threads (int): the number of threads to fire up. If n_threads is `None` or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. Returns: A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multithreading with {n_threads} threads\" ) with futures . ThreadPoolExecutor ( n_threads ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_threads} threads finished\" ) return list ( results ) Classes MultiProcessor class MultiProcessor ( / , * args , ** kwargs ) View Source class MultiProcessor : \"\"\" A class to easily wrap a multi-processing context manager call to a function. Reminder: multiprocessing is good for cpu-heavy tasks. Reminder: only picklable objects can be executed and returned. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_cpu_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_processes (int): the number of processes to fire up. No more than your number of cores! If n_processes is `None` or not given, ProcessPoolExecutor will default it to the number of processors on the machine. Returns: A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multiprocessing with {n_processes} processes\" ) with futures . ProcessPoolExecutor ( n_processes ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_processes} processes finished\" ) return list ( results ) Static methods execute_function def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ] Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Parameters: Name Type Description Default func Callable the function to call. None func_args list list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. None n_processes int the number of processes to fire up. No more than your number of cores! If n_processes is None or not given, ProcessPoolExecutor will default it to the number of processors on the machine. None Returns: Type Description None A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. View Source @ staticmethod def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_processes (int): the number of processes to fire up. No more than your number of cores! If n_processes is `None` or not given, ProcessPoolExecutor will default it to the number of processors on the machine. Returns: A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multiprocessing with {n_processes} processes\" ) with futures . ProcessPoolExecutor ( n_processes ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_processes} processes finished\" ) return list ( results ) MultiThreader class MultiThreader ( / , * args , ** kwargs ) View Source class MultiThreader : \"\"\" A class to easily wrap a multi-threading context manager call to a function. Reminder: multithreading is good for IO-heavy tasks. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_io_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_threads (int): the number of threads to fire up. If n_threads is `None` or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. Returns: A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multithreading with {n_threads} threads\" ) with futures . ThreadPoolExecutor ( n_threads ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_threads} threads finished\" ) return list ( results ) Static methods execute_function def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ] Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Parameters: Name Type Description Default func Callable the function to call. None func_args list list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. None n_threads int the number of threads to fire up. If n_threads is None or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. None Returns: Type Description None A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. View Source @ staticmethod def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_threads (int): the number of threads to fire up. If n_threads is `None` or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. Returns: A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multithreading with {n_threads} threads\" ) with futures . ThreadPoolExecutor ( n_threads ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_threads} threads finished\" ) return list ( results )","title":"Executors"},{"location":"reference/pyhdtoolkit/utils/executors/#module-pyhdtoolkitutilsexecutors","text":"Module utils.executors Created on 2019.12.09 View Source \"\"\" Module utils.executors ---------------------- Created on 2019.12.09 :author: Felix Soubelet A module providing two classes to execute functions + arguments couples through either a multiprocessing approach, or a multithreading approach. Here are a few tidbits to keep in mind: - There can only be one thread running at any given time in a python process because of the GIL. - Multiprocessing is parallelism. Multithreading is concurrency. - Multiprocessing is for increasing speed. Multithreading is for hiding latency. - Multiprocessing is best for computations. Multithreading is best for IO. - If you have CPU heavy tasks, use multiprocessing with n_process = n_cores and never more. Never. - If you have IO heavy tasks, use multithreading with n_threads = m * n_cores with m a number bigger than 1 that you can tweak on your own. Try many values and choose the one with the best speedup because there isn\u2019t a general rule. For instance the default value of m in ThreadPoolExecutor is set to 5 which I think is quite random. \"\"\" from concurrent import futures from typing import Callable , List from loguru import logger class MultiProcessor : \"\"\" A class to easily wrap a multi-processing context manager call to a function. Reminder: multiprocessing is good for cpu-heavy tasks. Reminder: only picklable objects can be executed and returned. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_cpu_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ] : \"\"\" Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_processes (int): the number of processes to fire up. No more than your number of cores! If n_processes is `None` or not given, ProcessPoolExecutor will default it to the number of processors on the machine. Returns: A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multiprocessing with {n_processes} processes\" ) with futures . ProcessPoolExecutor ( n_processes ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_processes} processes finished\" ) return list ( results ) class MultiThreader : \"\"\" A class to easily wrap a multi-threading context manager call to a function. Reminder: multithreading is good for IO-heavy tasks. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_io_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ] : \"\"\" Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_threads (int): the number of threads to fire up. If n_threads is `None` or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. Returns: A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multithreading with {n_threads} threads\" ) with futures . ThreadPoolExecutor ( n_threads ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_threads} threads finished\" ) return list ( results )","title":"Module pyhdtoolkit.utils.executors"},{"location":"reference/pyhdtoolkit/utils/executors/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/utils/executors/#multiprocessor","text":"class MultiProcessor ( / , * args , ** kwargs ) View Source class MultiProcessor : \"\"\" A class to easily wrap a multi-processing context manager call to a function. Reminder: multiprocessing is good for cpu-heavy tasks. Reminder: only picklable objects can be executed and returned. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_cpu_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_processes (int): the number of processes to fire up. No more than your number of cores! If n_processes is `None` or not given, ProcessPoolExecutor will default it to the number of processors on the machine. Returns: A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multiprocessing with {n_processes} processes\" ) with futures . ProcessPoolExecutor ( n_processes ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_processes} processes finished\" ) return list ( results )","title":"MultiProcessor"},{"location":"reference/pyhdtoolkit/utils/executors/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/executors/#execute_function","text":"def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ] Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Parameters: Name Type Description Default func Callable the function to call. None func_args list list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. None n_processes int the number of processes to fire up. No more than your number of cores! If n_processes is None or not given, ProcessPoolExecutor will default it to the number of processors on the machine. None Returns: Type Description None A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. View Source @ staticmethod def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_processes (int): the number of processes to fire up. No more than your number of cores! If n_processes is `None` or not given, ProcessPoolExecutor will default it to the number of processors on the machine. Returns: A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multiprocessing with {n_processes} processes\" ) with futures . ProcessPoolExecutor ( n_processes ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_processes} processes finished\" ) return list ( results )","title":"execute_function"},{"location":"reference/pyhdtoolkit/utils/executors/#multithreader","text":"class MultiThreader ( / , * args , ** kwargs ) View Source class MultiThreader : \"\"\" A class to easily wrap a multi-threading context manager call to a function. Reminder: multithreading is good for IO-heavy tasks. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_io_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_threads (int): the number of threads to fire up. If n_threads is `None` or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. Returns: A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multithreading with {n_threads} threads\" ) with futures . ThreadPoolExecutor ( n_threads ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_threads} threads finished\" ) return list ( results )","title":"MultiThreader"},{"location":"reference/pyhdtoolkit/utils/executors/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/executors/#execute_function_1","text":"def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ] Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Parameters: Name Type Description Default func Callable the function to call. None func_args list list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. None n_threads int the number of threads to fire up. If n_threads is None or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. None Returns: Type Description None A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. View Source @ staticmethod def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_threads (int): the number of threads to fire up. If n_threads is `None` or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. Returns: A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multithreading with {n_threads} threads\" ) with futures . ThreadPoolExecutor ( n_threads ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_threads} threads finished\" ) return list ( results )","title":"execute_function"},{"location":"reference/pyhdtoolkit/utils/htc_monitor/","text":"Module pyhdtoolkit.utils.htc_monitor Module utils.htc_monitor Created on 2021.04.22 View Source \"\"\" Module utils.htc_monitor ------------------------ Created on 2021.04.22 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with utility to query the HTCondor queue, process the returned data and display it nicely. \"\"\" import re import time from typing import List , Tuple import pendulum from loguru import logger from rich import box from rich.console import RenderGroup from rich.live import Live from rich.panel import Panel from rich.table import Table from pyhdtoolkit.models.htc import BaseSummary , ClusterSummary , HTCTaskSummary from pyhdtoolkit.utils import defaults from pyhdtoolkit.utils.cmdline import CommandLine defaults . config_logger ( level = \"ERROR\" ) # ----- Data ----- # TASK_COLUMNS_SETTINGS = { \"OWNER\" : dict ( justify = \"left\" , header_style = \"bold\" , style = \"bold\" , no_wrap = True ), \"BATCH_NAME\" : dict ( justify = \"center\" , header_style = \"magenta\" , style = \"magenta\" , no_wrap = True ), \"SUBMITTED\" : dict ( justify = \"center\" , header_style = \"medium_turquoise\" , style = \"medium_turquoise\" , no_wrap = True ), \"DONE\" : dict ( justify = \"right\" , header_style = \"bold green3\" , style = \"bold green3\" , no_wrap = True ), \"RUNNING\" : dict ( justify = \"right\" , header_style = \"bold cornflower_blue\" , style = \"bold cornflower_blue\" , no_wrap = True ), \"IDLE\" : dict ( justify = \"right\" , header_style = \"bold dark_orange3\" , style = \"bold dark_orange3\" , no_wrap = True ), \"TOTAL\" : dict ( justify = \"right\" , style = \"bold\" , no_wrap = True ), \"JOB_IDS\" : dict ( justify = \"right\" , no_wrap = True ), } CLUSTER_COLUMNS_SETTINGS = { \"SOURCE\" : dict ( justify = \"left\" , header_style = \"bold\" , style = \"bold\" , no_wrap = True ), \"JOBS\" : dict ( justify = \"right\" , header_style = \"bold\" , style = \"bold\" , no_wrap = True ), \"COMPLETED\" : dict ( justify = \"right\" , header_style = \"bold green3\" , style = \"bold green3\" , no_wrap = True ), \"RUNNING\" : dict ( justify = \"right\" , header_style = \"bold cornflower_blue\" , style = \"bold cornflower_blue\" , no_wrap = True ), \"IDLE\" : dict ( justify = \"right\" , header_style = \"bold dark_orange3\" , style = \"bold dark_orange3\" , no_wrap = True ), \"HELD\" : dict ( justify = \"right\" , header_style = \"bold gold1\" , style = \"bold gold1\" , no_wrap = True ), \"SUSPENDED\" : dict ( justify = \"right\" , header_style = \"bold slate_blue1\" , style = \"bold slate_blue1\" , no_wrap = True ), \"REMOVED\" : dict ( justify = \"right\" , header_style = \"bold red3\" , style = \"bold red3\" , no_wrap = True ), } # ----- HTCondor Querying / Processing ----- # def query_condor_q () -> str : \"\"\" Returns a decoded string with the result of the 'condor_q' command, to get status on your jobs. \"\"\" return_code , raw_result = CommandLine . run ( \"condor_q\" ) condor_status = raw_result . decode () . strip () if return_code == 0 : return condor_status else : raise ChildProcessError ( \"Checking htcondor status failed\" ) def read_condor_q ( report : str ) -> Tuple [ List [ HTCTaskSummary ], ClusterSummary ]: \"\"\" Split information from different parts of the condor_q output into one data structure. Args: report (str): the utf-8 decoded string returned by the 'condor_q' command. Returns: A tuple of: - A list of each task summary given by 'condor_q', each as a validated HTCTaskSummary object, - A validated ClusterSummary object with scheduler identification and summaries of the user as well as all users' statistics on this scheduler cluster. Example Usage: condor_q_output = get_the_string_as_you_wish(...) tasks, cluster = read_condor_q(condor_q_outout) \"\"\" tasks : List [ HTCTaskSummary ] = [] next_line_is_task_report = False for line in report . split ( \" \\n \" ): if line . startswith ( \"-- Schedd:\" ): # extract scheduler information scheduler_id = _process_scheduler_information_line ( line ) elif line . startswith ( \"OWNER\" ): # headers line before we get task reports next_line_is_task_report = True elif next_line_is_task_report : # extract task report information if line != \" \\n \" and line != \"\" : tasks . append ( _process_task_summary_line ( line )) else : # an empty line denotes the end of the task report(s) next_line_is_task_report = False else : # extract cluster information, only 3 lines here querying_owner = tasks [ 0 ] . owner if tasks else \"(\\D+)\" if \"query\" in line : # first line query_summary = _process_cluster_summary_line ( line , \"query\" ) elif \"all users\" in line : # last line full_summary = _process_cluster_summary_line ( line , \"all users\" ) elif line != \" \\n \" and line != \"\" : # user line, whoever the user is owner_summary = _process_cluster_summary_line ( line , querying_owner ) cluster_summary = ClusterSummary ( scheduler_id = scheduler_id , query = query_summary , user = owner_summary , cluster = full_summary ) return tasks , cluster_summary # ----- Output Formating ----- # def make_tasks_table ( tasks : List [ HTCTaskSummary ]) -> Table : table = _default_tasks_table () date_display_format = \"dddd, D MMM YY at LT (zz)\" # example: Wednesday, 21 Apr 21 9:04 PM (CEST) for task in tasks : table . add_row ( task . owner , str ( task . batch_name ), task . submitted . format ( fmt = date_display_format ), str ( task . done ), str ( task . run ), str ( task . idle ), str ( task . total ), task . job_ids , ) return table def make_cluster_table ( owner_name : str , cluster : ClusterSummary ) -> Table : table = _default_cluster_table () for i , source in enumerate ([ \"query\" , \"user\" , \"cluster\" ]): table . add_row ( \"Query\" if i == 0 else ( \"All Users\" if i == 2 else owner_name ), str ( cluster . dict ()[ source ][ \"jobs\" ]), str ( cluster . dict ()[ source ][ \"completed\" ]), str ( cluster . dict ()[ source ][ \"running\" ]), str ( cluster . dict ()[ source ][ \"idle\" ]), str ( cluster . dict ()[ source ][ \"held\" ]), str ( cluster . dict ()[ source ][ \"suspended\" ]), str ( cluster . dict ()[ source ][ \"removed\" ]), ) return table # ----- Helpers ----- # def _process_scheduler_information_line ( line : str ) -> str : \"\"\"Extract only the 'Schedd: <cluster>.cern.ch' part oof the scheduler information line\"\"\" result = re . search ( r \"Schedd: (.*).cern.ch\" , line ) return result . group ( 1 ) def _process_task_summary_line ( line : str ) -> HTCTaskSummary : \"\"\" Extract the various information in a task summary line, validated and returned as an HTCTaskSummary object. \"\"\" line_elements = line . split () return HTCTaskSummary ( owner = line_elements [ 0 ], batch_name = line_elements [ 2 ], # line_elements[1] is the 'ID:' part, we don't need it submitted = pendulum . from_format ( f \" { line_elements [ 3 ] } { line_elements [ 4 ] } \" , fmt = \"MM/D HH:mm\" , tz = \"Europe/Paris\" ), # Geneva timezone is Paris timezone, done = line_elements [ 5 ], run = line_elements [ 6 ], idle = line_elements [ 7 ], total = line_elements [ 8 ], job_ids = line_elements [ 9 ], ) def _process_cluster_summary_line ( line : str , query : str = None ) -> BaseSummary : \"\"\" Beware if no jobs are running we can't have taken querying_owner info from tasks summaries, so we need to match a wildcard word by giving querying_owner=(\\D+). This would add a match to the regex search, and we need to look one match further for the wanted information. \"\"\" result = re . search ( rf \"Total for { query } : (\\d+) jobs; (\\d+) completed, \" \"(\\d+) removed, (\\d+) idle, (\\d+) running, (\\d+) held, (\\d+) suspended\" , line , ) first_interesting_match_index = 1 if query != \"(\\D+)\" else 2 return BaseSummary ( jobs = result . group ( first_interesting_match_index ), completed = result . group ( first_interesting_match_index + 1 ), removed = result . group ( first_interesting_match_index + 2 ), idle = result . group ( first_interesting_match_index + 3 ), running = result . group ( first_interesting_match_index + 4 ), held = result . group ( first_interesting_match_index + 5 ), suspended = result . group ( first_interesting_match_index + 6 ), ) def _default_tasks_table () -> Table : \"\"\"Create the default structure for the Tasks Table, hard coded columns and no rows added.\"\"\" table = Table ( width = 120 , box = box . SIMPLE_HEAVY ) for header , header_col_settings in TASK_COLUMNS_SETTINGS . items (): table . add_column ( header , ** header_col_settings ) return table def _default_cluster_table () -> Table : \"\"\"Create the default structure for the Cluster Table, hard coded columns and no rows added.\"\"\" table = Table ( width = 120 , box = box . HORIZONTALS ) for header , header_col_settings in CLUSTER_COLUMNS_SETTINGS . items (): table . add_column ( header , ** header_col_settings ) return table # ----- Executable ----- # @logger . catch () def main (): def generate_renderable () -> RenderGroup : \"\"\" Function called to update the live display, fetches data from htcondor, does the processing and returns a RenderGroup with both Panels. \"\"\" condor_string = query_condor_q () user_tasks , cluster_info = read_condor_q ( condor_string ) owner = user_tasks [ 0 ] . owner if user_tasks else \"User\" tasks_table = make_tasks_table ( user_tasks ) cluster_table = make_cluster_table ( owner , cluster_info ) return RenderGroup ( Panel ( tasks_table , title = f \"Scheduler: { cluster_info . scheduler_id } .cern.ch\" , expand = False , border_style = \"scope.border\" , ), Panel ( cluster_table , title = f \" { cluster_info . scheduler_id } Statistics\" , expand = False , border_style = \"scope.border\" , ), ) with Live ( generate_renderable (), refresh_per_second = 0.25 ) as live : live . console . log ( \"Querying HTCondor Queue - Refreshed Every 5 Minutes \\n \" ) while True : try : live . update ( generate_renderable ()) time . sleep ( 300 ) except KeyboardInterrupt : live . console . log ( \"Exiting Program\" ) break if __name__ == \"__main__\" : main () Variables CLUSTER_COLUMNS_SETTINGS TASK_COLUMNS_SETTINGS Functions main def main ( ) View Source @logger . catch () def main () : def generate_renderable () -> RenderGroup : \"\"\" Function called to update the live display, fetches data from htcondor, does the processing and returns a RenderGroup with both Panels. \"\"\" condor_string = query_condor_q () user_tasks , cluster_info = read_condor_q ( condor_string ) owner = user_tasks [ 0 ] . owner if user_tasks else \"User\" tasks_table = make_tasks_table ( user_tasks ) cluster_table = make_cluster_table ( owner , cluster_info ) return RenderGroup ( Panel ( tasks_table , title = f \"Scheduler: {cluster_info.scheduler_id}.cern.ch\" , expand = False , border_style = \"scope.border\" , ), Panel ( cluster_table , title = f \"{cluster_info.scheduler_id} Statistics\" , expand = False , border_style = \"scope.border\" , ), ) with Live ( generate_renderable (), refresh_per_second = 0.25 ) as live : live . console . log ( \"Querying HTCondor Queue - Refreshed Every 5 Minutes\\n\" ) while True : try : live . update ( generate_renderable ()) time . sleep ( 300 ) except KeyboardInterrupt : live . console . log ( \"Exiting Program\" ) break make_cluster_table def make_cluster_table ( owner_name : str , cluster : pyhdtoolkit . models . htc . ClusterSummary ) -> rich . table . Table View Source def make_cluster_table ( owner_name : str , cluster : ClusterSummary ) -> Table : table = _default_cluster_table () for i , source in enumerate ( [ \"query\", \"user\", \"cluster\" ] ) : table . add_row ( \"Query\" if i == 0 else ( \"All Users\" if i == 2 else owner_name ), str ( cluster . dict () [ source ][ \"jobs\" ] ), str ( cluster . dict () [ source ][ \"completed\" ] ), str ( cluster . dict () [ source ][ \"running\" ] ), str ( cluster . dict () [ source ][ \"idle\" ] ), str ( cluster . dict () [ source ][ \"held\" ] ), str ( cluster . dict () [ source ][ \"suspended\" ] ), str ( cluster . dict () [ source ][ \"removed\" ] ), ) return table make_tasks_table def make_tasks_table ( tasks : List [ pyhdtoolkit . models . htc . HTCTaskSummary ] ) -> rich . table . Table View Source def make_tasks_table ( tasks : List [ HTCTaskSummary ] ) -> Table : table = _default_tasks_table () date_display_format = \"dddd, D MMM YY at LT (zz)\" # example : Wednesday , 21 Apr 21 9 : 04 PM ( CEST ) for task in tasks : table . add_row ( task . owner , str ( task . batch_name ), task . submitted . format ( fmt = date_display_format ), str ( task . done ), str ( task . run ), str ( task . idle ), str ( task . total ), task . job_ids , ) return table query_condor_q def query_condor_q ( ) -> str Returns a decoded string with the result of the 'condor_q' command, to get status on your jobs. View Source def query_condor_q () -> str : \"\"\" Returns a decoded string with the result of the ' condor_q ' command, to get status on your jobs. \"\"\" return_code , raw_result = CommandLine . run ( \"condor_q\" ) condor_status = raw_result . decode (). strip () if return_code == 0 : return condor_status else : raise ChildProcessError ( \"Checking htcondor status failed\" ) read_condor_q def read_condor_q ( report : str ) -> Tuple [ List [ pyhdtoolkit . models . htc . HTCTaskSummary ], pyhdtoolkit . models . htc . ClusterSummary ] Split information from different parts of the condor_q output into one data structure. Parameters: Name Type Description Default report str the utf-8 decoded string returned by the 'condor_q' command. None Returns: Type Description None A tuple of: - A list of each task summary given by 'condor_q', each as a validated HTCTaskSummary object, - A validated ClusterSummary object with scheduler identification and summaries of the user as well as all users' statistics on this scheduler cluster. Example Usage: condor_q_output = get_the_string_as_you_wish(...) tasks, cluster = read_condor_q(condor_q_outout) | View Source def read_condor_q ( report : str ) -> Tuple [ List[HTCTaskSummary ] , ClusterSummary ]: \"\"\" Split information from different parts of the condor_q output into one data structure. Args: report (str): the utf-8 decoded string returned by the 'condor_q' command. Returns: A tuple of: - A list of each task summary given by 'condor_q', each as a validated HTCTaskSummary object, - A validated ClusterSummary object with scheduler identification and summaries of the user as well as all users' statistics on this scheduler cluster. Example Usage: condor_q_output = get_the_string_as_you_wish(...) tasks, cluster = read_condor_q(condor_q_outout) \"\"\" tasks : List [ HTCTaskSummary ] = [] next_line_is_task_report = False for line in report . split ( \"\\n\" ) : if line . startswith ( \"-- Schedd:\" ) : # extract scheduler information scheduler_id = _process_scheduler_information_line ( line ) elif line . startswith ( \"OWNER\" ) : # headers line before we get task reports next_line_is_task_report = True elif next_line_is_task_report : # extract task report information if line != \"\\n\" and line != \"\" : tasks . append ( _process_task_summary_line ( line )) else : # an empty line denotes the end of the task report ( s ) next_line_is_task_report = False else : # extract cluster information , only 3 lines here querying_owner = tasks [ 0 ] . owner if tasks else \"(\\D+)\" if \"query\" in line : # first line query_summary = _process_cluster_summary_line ( line , \"query\" ) elif \"all users\" in line : # last line full_summary = _process_cluster_summary_line ( line , \"all users\" ) elif line != \"\\n\" and line != \"\" : # user line , whoever the user is owner_summary = _process_cluster_summary_line ( line , querying_owner ) cluster_summary = ClusterSummary ( scheduler_id = scheduler_id , query = query_summary , user = owner_summary , cluster = full_summary ) return tasks , cluster_summary","title":"Htc Monitor"},{"location":"reference/pyhdtoolkit/utils/htc_monitor/#module-pyhdtoolkitutilshtc_monitor","text":"Module utils.htc_monitor Created on 2021.04.22 View Source \"\"\" Module utils.htc_monitor ------------------------ Created on 2021.04.22 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with utility to query the HTCondor queue, process the returned data and display it nicely. \"\"\" import re import time from typing import List , Tuple import pendulum from loguru import logger from rich import box from rich.console import RenderGroup from rich.live import Live from rich.panel import Panel from rich.table import Table from pyhdtoolkit.models.htc import BaseSummary , ClusterSummary , HTCTaskSummary from pyhdtoolkit.utils import defaults from pyhdtoolkit.utils.cmdline import CommandLine defaults . config_logger ( level = \"ERROR\" ) # ----- Data ----- # TASK_COLUMNS_SETTINGS = { \"OWNER\" : dict ( justify = \"left\" , header_style = \"bold\" , style = \"bold\" , no_wrap = True ), \"BATCH_NAME\" : dict ( justify = \"center\" , header_style = \"magenta\" , style = \"magenta\" , no_wrap = True ), \"SUBMITTED\" : dict ( justify = \"center\" , header_style = \"medium_turquoise\" , style = \"medium_turquoise\" , no_wrap = True ), \"DONE\" : dict ( justify = \"right\" , header_style = \"bold green3\" , style = \"bold green3\" , no_wrap = True ), \"RUNNING\" : dict ( justify = \"right\" , header_style = \"bold cornflower_blue\" , style = \"bold cornflower_blue\" , no_wrap = True ), \"IDLE\" : dict ( justify = \"right\" , header_style = \"bold dark_orange3\" , style = \"bold dark_orange3\" , no_wrap = True ), \"TOTAL\" : dict ( justify = \"right\" , style = \"bold\" , no_wrap = True ), \"JOB_IDS\" : dict ( justify = \"right\" , no_wrap = True ), } CLUSTER_COLUMNS_SETTINGS = { \"SOURCE\" : dict ( justify = \"left\" , header_style = \"bold\" , style = \"bold\" , no_wrap = True ), \"JOBS\" : dict ( justify = \"right\" , header_style = \"bold\" , style = \"bold\" , no_wrap = True ), \"COMPLETED\" : dict ( justify = \"right\" , header_style = \"bold green3\" , style = \"bold green3\" , no_wrap = True ), \"RUNNING\" : dict ( justify = \"right\" , header_style = \"bold cornflower_blue\" , style = \"bold cornflower_blue\" , no_wrap = True ), \"IDLE\" : dict ( justify = \"right\" , header_style = \"bold dark_orange3\" , style = \"bold dark_orange3\" , no_wrap = True ), \"HELD\" : dict ( justify = \"right\" , header_style = \"bold gold1\" , style = \"bold gold1\" , no_wrap = True ), \"SUSPENDED\" : dict ( justify = \"right\" , header_style = \"bold slate_blue1\" , style = \"bold slate_blue1\" , no_wrap = True ), \"REMOVED\" : dict ( justify = \"right\" , header_style = \"bold red3\" , style = \"bold red3\" , no_wrap = True ), } # ----- HTCondor Querying / Processing ----- # def query_condor_q () -> str : \"\"\" Returns a decoded string with the result of the 'condor_q' command, to get status on your jobs. \"\"\" return_code , raw_result = CommandLine . run ( \"condor_q\" ) condor_status = raw_result . decode () . strip () if return_code == 0 : return condor_status else : raise ChildProcessError ( \"Checking htcondor status failed\" ) def read_condor_q ( report : str ) -> Tuple [ List [ HTCTaskSummary ], ClusterSummary ]: \"\"\" Split information from different parts of the condor_q output into one data structure. Args: report (str): the utf-8 decoded string returned by the 'condor_q' command. Returns: A tuple of: - A list of each task summary given by 'condor_q', each as a validated HTCTaskSummary object, - A validated ClusterSummary object with scheduler identification and summaries of the user as well as all users' statistics on this scheduler cluster. Example Usage: condor_q_output = get_the_string_as_you_wish(...) tasks, cluster = read_condor_q(condor_q_outout) \"\"\" tasks : List [ HTCTaskSummary ] = [] next_line_is_task_report = False for line in report . split ( \" \\n \" ): if line . startswith ( \"-- Schedd:\" ): # extract scheduler information scheduler_id = _process_scheduler_information_line ( line ) elif line . startswith ( \"OWNER\" ): # headers line before we get task reports next_line_is_task_report = True elif next_line_is_task_report : # extract task report information if line != \" \\n \" and line != \"\" : tasks . append ( _process_task_summary_line ( line )) else : # an empty line denotes the end of the task report(s) next_line_is_task_report = False else : # extract cluster information, only 3 lines here querying_owner = tasks [ 0 ] . owner if tasks else \"(\\D+)\" if \"query\" in line : # first line query_summary = _process_cluster_summary_line ( line , \"query\" ) elif \"all users\" in line : # last line full_summary = _process_cluster_summary_line ( line , \"all users\" ) elif line != \" \\n \" and line != \"\" : # user line, whoever the user is owner_summary = _process_cluster_summary_line ( line , querying_owner ) cluster_summary = ClusterSummary ( scheduler_id = scheduler_id , query = query_summary , user = owner_summary , cluster = full_summary ) return tasks , cluster_summary # ----- Output Formating ----- # def make_tasks_table ( tasks : List [ HTCTaskSummary ]) -> Table : table = _default_tasks_table () date_display_format = \"dddd, D MMM YY at LT (zz)\" # example: Wednesday, 21 Apr 21 9:04 PM (CEST) for task in tasks : table . add_row ( task . owner , str ( task . batch_name ), task . submitted . format ( fmt = date_display_format ), str ( task . done ), str ( task . run ), str ( task . idle ), str ( task . total ), task . job_ids , ) return table def make_cluster_table ( owner_name : str , cluster : ClusterSummary ) -> Table : table = _default_cluster_table () for i , source in enumerate ([ \"query\" , \"user\" , \"cluster\" ]): table . add_row ( \"Query\" if i == 0 else ( \"All Users\" if i == 2 else owner_name ), str ( cluster . dict ()[ source ][ \"jobs\" ]), str ( cluster . dict ()[ source ][ \"completed\" ]), str ( cluster . dict ()[ source ][ \"running\" ]), str ( cluster . dict ()[ source ][ \"idle\" ]), str ( cluster . dict ()[ source ][ \"held\" ]), str ( cluster . dict ()[ source ][ \"suspended\" ]), str ( cluster . dict ()[ source ][ \"removed\" ]), ) return table # ----- Helpers ----- # def _process_scheduler_information_line ( line : str ) -> str : \"\"\"Extract only the 'Schedd: <cluster>.cern.ch' part oof the scheduler information line\"\"\" result = re . search ( r \"Schedd: (.*).cern.ch\" , line ) return result . group ( 1 ) def _process_task_summary_line ( line : str ) -> HTCTaskSummary : \"\"\" Extract the various information in a task summary line, validated and returned as an HTCTaskSummary object. \"\"\" line_elements = line . split () return HTCTaskSummary ( owner = line_elements [ 0 ], batch_name = line_elements [ 2 ], # line_elements[1] is the 'ID:' part, we don't need it submitted = pendulum . from_format ( f \" { line_elements [ 3 ] } { line_elements [ 4 ] } \" , fmt = \"MM/D HH:mm\" , tz = \"Europe/Paris\" ), # Geneva timezone is Paris timezone, done = line_elements [ 5 ], run = line_elements [ 6 ], idle = line_elements [ 7 ], total = line_elements [ 8 ], job_ids = line_elements [ 9 ], ) def _process_cluster_summary_line ( line : str , query : str = None ) -> BaseSummary : \"\"\" Beware if no jobs are running we can't have taken querying_owner info from tasks summaries, so we need to match a wildcard word by giving querying_owner=(\\D+). This would add a match to the regex search, and we need to look one match further for the wanted information. \"\"\" result = re . search ( rf \"Total for { query } : (\\d+) jobs; (\\d+) completed, \" \"(\\d+) removed, (\\d+) idle, (\\d+) running, (\\d+) held, (\\d+) suspended\" , line , ) first_interesting_match_index = 1 if query != \"(\\D+)\" else 2 return BaseSummary ( jobs = result . group ( first_interesting_match_index ), completed = result . group ( first_interesting_match_index + 1 ), removed = result . group ( first_interesting_match_index + 2 ), idle = result . group ( first_interesting_match_index + 3 ), running = result . group ( first_interesting_match_index + 4 ), held = result . group ( first_interesting_match_index + 5 ), suspended = result . group ( first_interesting_match_index + 6 ), ) def _default_tasks_table () -> Table : \"\"\"Create the default structure for the Tasks Table, hard coded columns and no rows added.\"\"\" table = Table ( width = 120 , box = box . SIMPLE_HEAVY ) for header , header_col_settings in TASK_COLUMNS_SETTINGS . items (): table . add_column ( header , ** header_col_settings ) return table def _default_cluster_table () -> Table : \"\"\"Create the default structure for the Cluster Table, hard coded columns and no rows added.\"\"\" table = Table ( width = 120 , box = box . HORIZONTALS ) for header , header_col_settings in CLUSTER_COLUMNS_SETTINGS . items (): table . add_column ( header , ** header_col_settings ) return table # ----- Executable ----- # @logger . catch () def main (): def generate_renderable () -> RenderGroup : \"\"\" Function called to update the live display, fetches data from htcondor, does the processing and returns a RenderGroup with both Panels. \"\"\" condor_string = query_condor_q () user_tasks , cluster_info = read_condor_q ( condor_string ) owner = user_tasks [ 0 ] . owner if user_tasks else \"User\" tasks_table = make_tasks_table ( user_tasks ) cluster_table = make_cluster_table ( owner , cluster_info ) return RenderGroup ( Panel ( tasks_table , title = f \"Scheduler: { cluster_info . scheduler_id } .cern.ch\" , expand = False , border_style = \"scope.border\" , ), Panel ( cluster_table , title = f \" { cluster_info . scheduler_id } Statistics\" , expand = False , border_style = \"scope.border\" , ), ) with Live ( generate_renderable (), refresh_per_second = 0.25 ) as live : live . console . log ( \"Querying HTCondor Queue - Refreshed Every 5 Minutes \\n \" ) while True : try : live . update ( generate_renderable ()) time . sleep ( 300 ) except KeyboardInterrupt : live . console . log ( \"Exiting Program\" ) break if __name__ == \"__main__\" : main ()","title":"Module pyhdtoolkit.utils.htc_monitor"},{"location":"reference/pyhdtoolkit/utils/htc_monitor/#variables","text":"CLUSTER_COLUMNS_SETTINGS TASK_COLUMNS_SETTINGS","title":"Variables"},{"location":"reference/pyhdtoolkit/utils/htc_monitor/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/utils/htc_monitor/#main","text":"def main ( ) View Source @logger . catch () def main () : def generate_renderable () -> RenderGroup : \"\"\" Function called to update the live display, fetches data from htcondor, does the processing and returns a RenderGroup with both Panels. \"\"\" condor_string = query_condor_q () user_tasks , cluster_info = read_condor_q ( condor_string ) owner = user_tasks [ 0 ] . owner if user_tasks else \"User\" tasks_table = make_tasks_table ( user_tasks ) cluster_table = make_cluster_table ( owner , cluster_info ) return RenderGroup ( Panel ( tasks_table , title = f \"Scheduler: {cluster_info.scheduler_id}.cern.ch\" , expand = False , border_style = \"scope.border\" , ), Panel ( cluster_table , title = f \"{cluster_info.scheduler_id} Statistics\" , expand = False , border_style = \"scope.border\" , ), ) with Live ( generate_renderable (), refresh_per_second = 0.25 ) as live : live . console . log ( \"Querying HTCondor Queue - Refreshed Every 5 Minutes\\n\" ) while True : try : live . update ( generate_renderable ()) time . sleep ( 300 ) except KeyboardInterrupt : live . console . log ( \"Exiting Program\" ) break","title":"main"},{"location":"reference/pyhdtoolkit/utils/htc_monitor/#make_cluster_table","text":"def make_cluster_table ( owner_name : str , cluster : pyhdtoolkit . models . htc . ClusterSummary ) -> rich . table . Table View Source def make_cluster_table ( owner_name : str , cluster : ClusterSummary ) -> Table : table = _default_cluster_table () for i , source in enumerate ( [ \"query\", \"user\", \"cluster\" ] ) : table . add_row ( \"Query\" if i == 0 else ( \"All Users\" if i == 2 else owner_name ), str ( cluster . dict () [ source ][ \"jobs\" ] ), str ( cluster . dict () [ source ][ \"completed\" ] ), str ( cluster . dict () [ source ][ \"running\" ] ), str ( cluster . dict () [ source ][ \"idle\" ] ), str ( cluster . dict () [ source ][ \"held\" ] ), str ( cluster . dict () [ source ][ \"suspended\" ] ), str ( cluster . dict () [ source ][ \"removed\" ] ), ) return table","title":"make_cluster_table"},{"location":"reference/pyhdtoolkit/utils/htc_monitor/#make_tasks_table","text":"def make_tasks_table ( tasks : List [ pyhdtoolkit . models . htc . HTCTaskSummary ] ) -> rich . table . Table View Source def make_tasks_table ( tasks : List [ HTCTaskSummary ] ) -> Table : table = _default_tasks_table () date_display_format = \"dddd, D MMM YY at LT (zz)\" # example : Wednesday , 21 Apr 21 9 : 04 PM ( CEST ) for task in tasks : table . add_row ( task . owner , str ( task . batch_name ), task . submitted . format ( fmt = date_display_format ), str ( task . done ), str ( task . run ), str ( task . idle ), str ( task . total ), task . job_ids , ) return table","title":"make_tasks_table"},{"location":"reference/pyhdtoolkit/utils/htc_monitor/#query_condor_q","text":"def query_condor_q ( ) -> str Returns a decoded string with the result of the 'condor_q' command, to get status on your jobs. View Source def query_condor_q () -> str : \"\"\" Returns a decoded string with the result of the ' condor_q ' command, to get status on your jobs. \"\"\" return_code , raw_result = CommandLine . run ( \"condor_q\" ) condor_status = raw_result . decode (). strip () if return_code == 0 : return condor_status else : raise ChildProcessError ( \"Checking htcondor status failed\" )","title":"query_condor_q"},{"location":"reference/pyhdtoolkit/utils/htc_monitor/#read_condor_q","text":"def read_condor_q ( report : str ) -> Tuple [ List [ pyhdtoolkit . models . htc . HTCTaskSummary ], pyhdtoolkit . models . htc . ClusterSummary ] Split information from different parts of the condor_q output into one data structure. Parameters: Name Type Description Default report str the utf-8 decoded string returned by the 'condor_q' command. None Returns: Type Description None A tuple of: - A list of each task summary given by 'condor_q', each as a validated HTCTaskSummary object, - A validated ClusterSummary object with scheduler identification and summaries of the user as well as all users' statistics on this scheduler cluster. Example Usage: condor_q_output = get_the_string_as_you_wish(...) tasks, cluster = read_condor_q(condor_q_outout) | View Source def read_condor_q ( report : str ) -> Tuple [ List[HTCTaskSummary ] , ClusterSummary ]: \"\"\" Split information from different parts of the condor_q output into one data structure. Args: report (str): the utf-8 decoded string returned by the 'condor_q' command. Returns: A tuple of: - A list of each task summary given by 'condor_q', each as a validated HTCTaskSummary object, - A validated ClusterSummary object with scheduler identification and summaries of the user as well as all users' statistics on this scheduler cluster. Example Usage: condor_q_output = get_the_string_as_you_wish(...) tasks, cluster = read_condor_q(condor_q_outout) \"\"\" tasks : List [ HTCTaskSummary ] = [] next_line_is_task_report = False for line in report . split ( \"\\n\" ) : if line . startswith ( \"-- Schedd:\" ) : # extract scheduler information scheduler_id = _process_scheduler_information_line ( line ) elif line . startswith ( \"OWNER\" ) : # headers line before we get task reports next_line_is_task_report = True elif next_line_is_task_report : # extract task report information if line != \"\\n\" and line != \"\" : tasks . append ( _process_task_summary_line ( line )) else : # an empty line denotes the end of the task report ( s ) next_line_is_task_report = False else : # extract cluster information , only 3 lines here querying_owner = tasks [ 0 ] . owner if tasks else \"(\\D+)\" if \"query\" in line : # first line query_summary = _process_cluster_summary_line ( line , \"query\" ) elif \"all users\" in line : # last line full_summary = _process_cluster_summary_line ( line , \"all users\" ) elif line != \"\\n\" and line != \"\" : # user line , whoever the user is owner_summary = _process_cluster_summary_line ( line , querying_owner ) cluster_summary = ClusterSummary ( scheduler_id = scheduler_id , query = query_summary , user = owner_summary , cluster = full_summary ) return tasks , cluster_summary","title":"read_condor_q"},{"location":"reference/pyhdtoolkit/utils/operations/","text":"Module pyhdtoolkit.utils.operations Module utils.operations Created on 2019.11.12 View Source \"\"\" Module utils.operations ----------------------- Created on 2019.11.12 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection classes with utility functions to perform common / convenient operations on the classic Python structures. \"\"\" import copy import itertools import math import random import re from functools import reduce from typing import Callable , Dict , List , Sequence , Tuple , Union class ListOperations : \"\"\" A class to group some common / useful operations on lists. \"\"\" @staticmethod def all_unique ( sequence : Sequence ) -> bool : \"\"\" Returns True if all the values in a flat list are unique, False otherwise. Args: sequence (Sequence): a sequence of elements. Returns: True if all elements are unique, False otherwise. \"\"\" return len ( sequence ) == len ( set ( sequence )) @staticmethod def average_by ( sequence : Sequence , function : Callable = lambda x : x ) -> float : \"\"\" Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Args: sequence (Sequence): a sequence of elements. function (Callable): function to apply to elements of the sequence. Returns: The average of each element's result through `function`. Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 \"\"\" return float ( sum ( map ( function , sequence ), 0.0 ) / len ( sequence )) @staticmethod def bifurcate ( sequence : Sequence , filters : List [ bool ]) -> Sequence : \"\"\" Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Args: sequence (Sequence): a sequence of elements. filters (List[bool]): a list of booleans. Returns: A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] \"\"\" return [ [ x for i , x in enumerate ( sequence ) if filters [ i ]], [ x for i , x in enumerate ( sequence ) if not filters [ i ]], ] @staticmethod def bifurcate_by ( sequence : Sequence , function : Callable ) -> list : \"\"\" Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of lst, that should return a boolean. Returns: A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] \"\"\" return [[ x for x in sequence if function ( x )], [ x for x in sequence if not function ( x )]] @staticmethod def chunk_list ( sequence : Sequence , size : int ) -> Sequence : \"\"\" Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Args: sequence (Sequence): a sequence of elements. size (int): the size of the wanted sublists. Returns: A list of lists of length `size` (except maybe the last element), with elements from `lst`. Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] \"\"\" if size > len ( sequence ) : return sequence return list ( map ( lambda x : sequence [ x * size : x * size + size ], list ( range ( math . ceil ( len ( sequence ) / size ))),) ) @staticmethod def deep_flatten ( sequence : Sequence ) -> list : \"\"\" Deep flattens a list, no matter the nesting levels. This is a recursive approach. Args: sequence (Sequence): a sequence of elements. Returns: A list with all elements of `lst`, but flattened. Usage: deep_flatten([[\" a \", \" b \"], [1, 2], None, [True, False]]) -> [\" a \", \" b \", 1, 2, None True, False] \"\"\" return ( [ elem for sublist in sequence for elem in ListOperations . deep_flatten ( sublist )] if isinstance ( sequence , list ) else [ sequence ] ) @staticmethod def eval_none ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False \"\"\" return not any ( map ( function , sequence )) @staticmethod def eval_some ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False \"\"\" return any ( map ( function , sequence )) @staticmethod def get_indices ( element , sequence : Sequence ) -> List [ int ] : \"\"\" Return all array indices at which number is located. Args: element: any reference element to check. sequence (Sequence): a sequence containing objects comparable to `elements`. A string can be compared to an int in Python, custom objects probably won't be comparable. Returns: A list of all indices at which `element` is found in `sequence`. Empty list if `element` is not present in `sequence` at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] \"\"\" return [ i for ( y , i ) in zip ( sequence , range ( len ( sequence ))) if element == y ] @staticmethod def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ] : \"\"\" Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of `sequence` that should return a boolean. Returns: A dict with keys \" True \" and \" False \", each having as value a list of all elements of `lst` that were evaluated to respectively `True` or `False` through `function`. Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} \"\"\" groups = {} for key in list ( map ( function , sequence )) : groups [ key ] = [ item for item in sequence if function ( item ) == key ] return groups @staticmethod def has_duplicates ( sequence : Sequence ) -> bool : \"\"\" Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Args: sequence (Sequence): a sequence of elements. Returns: A boolean indicating the presence of duplicates in `lst`. Usage: has_duplicates([1, 2, 1]) -> True \"\"\" return len ( sequence ) ! = len ( set ( sequence )) @staticmethod def sample ( sequence : Sequence ) -> list : \"\"\" Returns a random element from an array. Args: sequence (Sequence): a sequence of elements. Returns: A random element from `lst` in a list (to manage potentially nested lists as input). \"\"\" return sequence [ random . randint ( 0 , len ( sequence ) - 1 )] @staticmethod def sanitize_list ( sequence : Sequence ) -> list : \"\"\" Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Args: sequence (Sequence): a sequence of elements. Returns: The sequence without falsy values. Usage: sanitize_list([1, False, \" a \", 2, \"\", None, 6, 0]) -> [1, \" a \", 2, 6] \"\"\" return list ( filter ( bool , sequence )) @staticmethod def shuffle ( sequence : Sequence ) -> Sequence : \"\"\" Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm (https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) to reorder the elements. Args: sequence (Sequence): a sequence of elements. Returns: The `lst` with original elements at a random index. \"\"\" temp_list = copy . deepcopy ( sequence ) amount_to_shuffle = len ( temp_list ) while amount_to_shuffle > 1 : rand_index = int ( math . floor ( random . random () * amount_to_shuffle )) amount_to_shuffle -= 1 temp_list [ rand_index ], temp_list [ amount_to_shuffle ] = ( temp_list [ amount_to_shuffle ], temp_list [ rand_index ], ) return temp_list @staticmethod def spread ( sequence : Sequence ) -> list : \"\"\" Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in `lst` are iterables! Args: sequence (Sequence): a sequence of elements. Returns: The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] \"\"\" return list ( itertools . chain . from_iterable ( sequence )) @staticmethod def symmetric_difference_by ( lst_1: Sequence , lst_2: Sequence , function : Callable ) -> list : \"\"\" Returns the symmetric difference (https://en.wikipedia.org/wiki/Symmetric_difference) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] \"\"\" _ lst_1 , _ lst_2 = set ( map ( function , lst_1 )), set ( map ( function , lst_2 )) return [ item for item in lst_1 if function ( item ) not in _ lst_2 ] + [ item for item in lst_2 if function ( item ) not in _ lst_1 ] @staticmethod def union_by ( lst_1: Sequence , lst_2: Sequence , function : Callable ) -> list : \"\"\" Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union (https://en.wikipedia.org/wiki/Union_(set_theory)) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] \"\"\" _ lst_1 = set ( map ( function , lst_1 )) return sorted ( list ( set ( lst_1 + [ item for item in lst_2 if function ( item ) not in _ lst_1 ]))) @staticmethod def zipper ( * args , fillvalue = None ) -> list : \"\"\" Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Args: *args: a number (>= 2) of different iterables. fillvalue: value to use in case of length mismatch. Returns: A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\" a \", \" b \", \" c \"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] \"\"\" max_length = max ( len ( lst ) for lst in args ) return [ [ args [ k ][ i ] if i < len ( args [ k ]) else fillvalue for k in range ( len ( args ))] for i in range ( max_length ) ] class MiscellaneousOperations : \"\"\" A class to group some misc. operations that don't pertain to classic structures. \"\"\" @staticmethod def longest_item ( * args ) : \"\"\" Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Args: *args: any number (>= 2) of iterables. Returns: The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) \"\"\" return max ( args , key = len ) @staticmethod def map_values ( obj : dict , function : Callable ) -> dict : \"\"\" Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Args: obj: a dictionary. function (Callable): a callable on values of `obj`. Returns: A new dictionary with the results. Usage: map_values( {\" a \": list(range(5)), \" b \": list(range(10)), \" c \": list(range(15))}, lambda x: len(x) ) -> {\" a \": 5, \" b \": 10, \" c \": 15} \"\"\" ret = {} for key in obj : ret [ key ] = function ( obj [ key ]) return ret class NumberOperations : \"\"\" A class to group some common / useful operations on numbers. \"\"\" @staticmethod def clamp_number ( num : Union [ int , float ], a_val: Union [ int , float ], b_val: Union [ int , float ] ) -> Union [ int , float ] : \"\"\" Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Args: num (Union[int, float]): a number (float / int) a_val (Union[int, float]): a number (float / int) b_val (Union[int, float]): a number (float / int) Returns: A number (float / int), being the nearest to `num` in the range [`a_val`, `b_val`]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 \"\"\" return max ( min ( num , max ( a_val , b_val )), min ( a_val , b_val )) @staticmethod def degrees_to_radians ( deg_value: Union [ int , float ], decompose : bool = False ) -> Union [ Tuple [ float , str , str ], int , float ] : \"\"\" Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Args: deg_value (Union[int, float]): angle value in degrees. decompose (bool): boolean option to return a more verbose result. Defaults to False. Returns: The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \" pi \", \" rad \") \"\"\" if decompose : return deg_value / 180 , \"pi\" , \"rad\" return ( deg_value * math . pi ) / 180.0 @staticmethod def greatest_common_divisor ( numbers_list: Sequence ) -> Union [ int , float ] : \"\"\" Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in `numbers_list`. Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 \"\"\" return reduce ( math . gcd , numbers_list ) @staticmethod def is_divisible_by ( dividend : Union [ int , float ], divisor : Union [ int , float ]) -> bool : \"\"\" Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Args: dividend (Union[int, float]): a number. divisor (Union[int, float]): a number. Returns: A boolean stating if `dividend` can be divided by `divisor`. \"\"\" return dividend % divisor == 0 @staticmethod def least_common_multiple ( * args ) -> int : \"\"\" Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 \"\"\" numbers = list ( ListOperations . spread ( list ( args ))) def _ lcm ( number1 , number2 ) : \"\"\"A least common multiple method for two numbers only\"\"\" return int ( number1 * number2 / math . gcd ( number1 , number2 )) return reduce ( lambda x , y : _ lcm ( x , y ), numbers ) @staticmethod def radians_to_degrees ( rad_value: Union [ int , float ]) -> Union [ int , float ] : \"\"\" Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Args: rad_value (Union[int, float]): angle value in degrees. Returns: The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 \"\"\" return ( rad_value * 180.0 ) / math . pi class StringOperations : \"\"\" A class to group some common / useful operations on strings. \"\"\" @staticmethod def camel_case ( text : str ) -> str : \"\"\" Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Args: text (str): a string. Returns: The same string best adapted to camel_case. Usage: camel_case(\" a_snake_case_name \") -> \" aSnakeCaseName \" camel_case(\" A Title Case Name \") -> \" aTitleCaseName \" \"\"\" text = re . sub ( r \"(\\s|_|-)+\" , \" \" , text ). title (). replace ( \" \" , \"\" ) return text [ 0 ]. lower () + text [ 1 : ] @staticmethod def capitalize ( text : str , lower_rest: bool = False ) -> str : \"\"\" Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Args: text (str): a string. lower_rest (bool): boolean option to lower all elements starting from the second. Returns: The `string`, capitalized. Usage: capitalize(\" astringtocapitalize \") -> \" Astringtocapitalize \" capitalize(\" astRIngTocApItalizE \", lower_rest=True) -> \" Astringtocapitalize \" \"\"\" return text [ : 1 ]. upper () + ( text [ 1 : ]. lower () if lower_rest else text [ 1 : ]) @staticmethod def is_anagram ( str_1: str , str_2: str ) -> bool : \"\"\" Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Args: str_1 (str): a string. str_2 (str): a string. Returns: A boolean stating whether `str_1` is an anagram of `str_2` or not. Usage: is_anagram(\" Tom Marvolo Riddle \", \" I am Lord Voldemort \") -> True is_anagram(\" A first string \", \" Definitely not an anagram \") -> False \"\"\" _ str1 , _ str2 = ( str_1 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), str_2 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), ) return sorted ( _ str1 . lower ()) == sorted ( _ str2 . lower ()) @staticmethod def is_palindrome ( text : str ) -> bool : \"\"\" Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Args: text (str): a string. Returns: A boolean stating whether `string` is a palindrome or not. Usage: is_palindrome(\" racecar \") -> True is_palindrome(\" definitelynot \") -> False \"\"\" s_reverse = re . sub ( r \" [ \\ W_ ] \", \"\", text.lower()) return s_reverse == s_reverse[::-1] @staticmethod def kebab_case(text: str) -> str: \"\"\" Converts a string to kebab - case . Break the string into words and combine them adding - as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to kebab_case . Usage : kebab_case ( \"camel Case\" ) -> \"camel-case\" kebab_case ( \"snake_case\" ) -> \"snake-case\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" - \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \", lambda mo: mo.group(0).lower(), text, ), ) @staticmethod def snake_case(text: str) -> str: \"\"\" Converts a string to snake_case . Break the string into words and combine them adding _ as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to snake_case . Usage : snake_case ( \"A bunch of words\" ) -> \"a_bunch_of_words\" snake_case ( \"camelCase\" ) -> \"camelcase\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" _ \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \" , lambda mo : mo . group ( 0 ). lower (), text , ), ) Classes ListOperations class ListOperations ( / , * args , ** kwargs ) View Source class ListOperations : \"\"\" A class to group some common / useful operations on lists. \"\"\" @ staticmethod def all_unique ( sequence : Sequence ) -> bool : \"\"\" Returns True if all the values in a flat list are unique, False otherwise. Args: sequence (Sequence): a sequence of elements. Returns: True if all elements are unique, False otherwise. \"\"\" return len ( sequence ) == len ( set ( sequence )) @ staticmethod def average_by ( sequence : Sequence , function : Callable = lambda x : x ) -> float : \"\"\" Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Args: sequence (Sequence): a sequence of elements. function (Callable): function to apply to elements of the sequence. Returns: The average of each element's result through `function`. Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 \"\"\" return float ( sum ( map ( function , sequence ), 0.0 ) / len ( sequence )) @ staticmethod def bifurcate ( sequence : Sequence , filters : List [ bool ]) -> Sequence : \"\"\" Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Args: sequence (Sequence): a sequence of elements. filters (List[bool]): a list of booleans. Returns: A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] \"\"\" return [ [ x for i , x in enumerate ( sequence ) if filters [ i ]], [ x for i , x in enumerate ( sequence ) if not filters [ i ]], ] @ staticmethod def bifurcate_by ( sequence : Sequence , function : Callable ) -> list : \"\"\" Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of lst, that should return a boolean. Returns: A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] \"\"\" return [[ x for x in sequence if function ( x )], [ x for x in sequence if not function ( x )]] @ staticmethod def chunk_list ( sequence : Sequence , size : int ) -> Sequence : \"\"\" Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Args: sequence (Sequence): a sequence of elements. size (int): the size of the wanted sublists. Returns: A list of lists of length `size` (except maybe the last element), with elements from `lst`. Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] \"\"\" if size > len ( sequence ): return sequence return list ( map ( lambda x : sequence [ x * size : x * size + size ], list ( range ( math . ceil ( len ( sequence ) / size ))),) ) @ staticmethod def deep_flatten ( sequence : Sequence ) -> list : \"\"\" Deep flattens a list, no matter the nesting levels. This is a recursive approach. Args: sequence (Sequence): a sequence of elements. Returns: A list with all elements of `lst`, but flattened. Usage: deep_flatten([[\"a\", \"b\"], [1, 2], None, [True, False]]) -> [\"a\", \"b\", 1, 2, None True, False] \"\"\" return ( [ elem for sublist in sequence for elem in ListOperations . deep_flatten ( sublist )] if isinstance ( sequence , list ) else [ sequence ] ) @ staticmethod def eval_none ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False \"\"\" return not any ( map ( function , sequence )) @ staticmethod def eval_some ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False \"\"\" return any ( map ( function , sequence )) @ staticmethod def get_indices ( element , sequence : Sequence ) -> List [ int ]: \"\"\" Return all array indices at which number is located. Args: element: any reference element to check. sequence (Sequence): a sequence containing objects comparable to `elements`. A string can be compared to an int in Python, custom objects probably won't be comparable. Returns: A list of all indices at which `element` is found in `sequence`. Empty list if `element` is not present in `sequence` at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] \"\"\" return [ i for ( y , i ) in zip ( sequence , range ( len ( sequence ))) if element == y ] @ staticmethod def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ]: \"\"\" Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of `sequence` that should return a boolean. Returns: A dict with keys \"True\" and \"False\", each having as value a list of all elements of `lst` that were evaluated to respectively `True` or `False` through `function`. Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} \"\"\" groups = {} for key in list ( map ( function , sequence )): groups [ key ] = [ item for item in sequence if function ( item ) == key ] return groups @ staticmethod def has_duplicates ( sequence : Sequence ) -> bool : \"\"\" Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Args: sequence (Sequence): a sequence of elements. Returns: A boolean indicating the presence of duplicates in `lst`. Usage: has_duplicates([1, 2, 1]) -> True \"\"\" return len ( sequence ) != len ( set ( sequence )) @ staticmethod def sample ( sequence : Sequence ) -> list : \"\"\" Returns a random element from an array. Args: sequence (Sequence): a sequence of elements. Returns: A random element from `lst` in a list (to manage potentially nested lists as input). \"\"\" return sequence [ random . randint ( 0 , len ( sequence ) - 1 )] @ staticmethod def sanitize_list ( sequence : Sequence ) -> list : \"\"\" Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Args: sequence (Sequence): a sequence of elements. Returns: The sequence without falsy values. Usage: sanitize_list([1, False, \"a\", 2, \"\", None, 6, 0]) -> [1, \"a\", 2, 6] \"\"\" return list ( filter ( bool , sequence )) @ staticmethod def shuffle ( sequence : Sequence ) -> Sequence : \"\"\" Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm (https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) to reorder the elements. Args: sequence (Sequence): a sequence of elements. Returns: The `lst` with original elements at a random index. \"\"\" temp_list = copy . deepcopy ( sequence ) amount_to_shuffle = len ( temp_list ) while amount_to_shuffle > 1 : rand_index = int ( math . floor ( random . random () * amount_to_shuffle )) amount_to_shuffle -= 1 temp_list [ rand_index ], temp_list [ amount_to_shuffle ] = ( temp_list [ amount_to_shuffle ], temp_list [ rand_index ], ) return temp_list @ staticmethod def spread ( sequence : Sequence ) -> list : \"\"\" Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in `lst` are iterables! Args: sequence (Sequence): a sequence of elements. Returns: The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] \"\"\" return list ( itertools . chain . from_iterable ( sequence )) @ staticmethod def symmetric_difference_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \"\"\" Returns the symmetric difference (https://en.wikipedia.org/wiki/Symmetric_difference) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] \"\"\" _lst_1 , _lst_2 = set ( map ( function , lst_1 )), set ( map ( function , lst_2 )) return [ item for item in lst_1 if function ( item ) not in _lst_2 ] + [ item for item in lst_2 if function ( item ) not in _lst_1 ] @ staticmethod def union_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \"\"\" Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union (https://en.wikipedia.org/wiki/Union_(set_theory)) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] \"\"\" _lst_1 = set ( map ( function , lst_1 )) return sorted ( list ( set ( lst_1 + [ item for item in lst_2 if function ( item ) not in _lst_1 ]))) @ staticmethod def zipper ( * args , fillvalue = None ) -> list : \"\"\" Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Args: *args: a number (>= 2) of different iterables. fillvalue: value to use in case of length mismatch. Returns: A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\"a\", \"b\", \"c\"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] \"\"\" max_length = max ( len ( lst ) for lst in args ) return [ [ args [ k ][ i ] if i < len ( args [ k ]) else fillvalue for k in range ( len ( args ))] for i in range ( max_length ) ] Static methods all_unique def all_unique ( sequence : Sequence ) -> bool Returns True if all the values in a flat list are unique, False otherwise. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None True if all elements are unique, False otherwise. View Source @staticmethod def all_unique ( sequence : Sequence ) -> bool : \"\"\" Returns True if all the values in a flat list are unique, False otherwise. Args: sequence (Sequence): a sequence of elements. Returns: True if all elements are unique, False otherwise. \"\"\" return len ( sequence ) == len ( set ( sequence )) average_by def average_by ( sequence : Sequence , function : Callable = < function ListOperations .< lambda > at 0x7f8c6343d320 > ) -> float Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable function to apply to elements of the sequence. None Returns: Type Description None The average of each element's result through function . Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 | View Source @staticmethod def average_by ( sequence : Sequence , function : Callable = lambda x : x ) -> float : \" \"\" Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Args: sequence (Sequence): a sequence of elements. function (Callable): function to apply to elements of the sequence. Returns: The average of each element's result through `function`. Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 \"\" \" return float ( sum ( map ( function , sequence ), 0.0 ) / len ( sequence )) bifurcate def bifurcate ( sequence : Sequence , filters : List [ bool ] ) -> Sequence Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None filters List[bool] a list of booleans. None Returns: Type Description None A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] | View Source @staticmethod def bifurcate ( sequence : Sequence , filters : List [ bool ] ) -> Sequence : \"\"\" Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Args: sequence (Sequence): a sequence of elements. filters (List[bool]): a list of booleans. Returns: A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] \"\"\" return [ [x for i, x in enumerate(sequence) if filters[i ] ] , [ x for i, x in enumerate(sequence) if not filters[i ] ] , ] bifurcate_by def bifurcate_by ( sequence : Sequence , function : Callable ) -> list Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on the elements of lst, that should return a boolean. None Returns: Type Description None A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] | View Source @staticmethod def bifurcate_by ( sequence : Sequence , function : Callable ) -> list : \"\"\" Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of lst, that should return a boolean. Returns: A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] \"\"\" return [ [x for x in sequence if function(x) ] , [ x for x in sequence if not function(x) ] ] chunk_list def chunk_list ( sequence : Sequence , size : int ) -> Sequence Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None size int the size of the wanted sublists. None Returns: Type Description None A list of lists of length size (except maybe the last element), with elements from lst . Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] | View Source @staticmethod def chunk_list ( sequence : Sequence , size : int ) -> Sequence : \" \"\" Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Args: sequence (Sequence): a sequence of elements. size (int): the size of the wanted sublists. Returns: A list of lists of length `size` (except maybe the last element), with elements from `lst`. Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] \"\" \" if size > len ( sequence ) : return sequence return list ( map ( lambda x : sequence [ x * size : x * size + size ] , list ( range ( math . ceil ( len ( sequence ) / size ))),) ) deep_flatten def deep_flatten ( sequence : Sequence ) -> list Deep flattens a list, no matter the nesting levels. This is a recursive approach. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None A list with all elements of lst , but flattened. Usage: deep_flatten([[\"a\", \"b\"], [1, 2], None, [True, False]]) -> [\"a\", \"b\", 1, 2, None True, False] | View Source @staticmethod def deep_flatten ( sequence : Sequence ) -> list : \"\"\" Deep flattens a list, no matter the nesting levels. This is a recursive approach. Args: sequence (Sequence): a sequence of elements. Returns: A list with all elements of `lst`, but flattened. Usage: deep_flatten([[\" a \", \" b \"], [1, 2], None, [True, False]]) -> [\" a \", \" b \", 1, 2, None True, False] \"\"\" return ( [ elem for sublist in sequence for elem in ListOperations.deep_flatten(sublist) ] if isinstance ( sequence , list ) else [ sequence ] ) eval_none def eval_none ( sequence : Sequence , function : Callable = < function ListOperations .< lambda > at 0x7f8c6343d680 > ) -> bool Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on elements of sequence that should return a boolean. None Returns: Type Description None A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False | View Source @staticmethod def eval_none ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \" \"\" Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False \"\" \" return not any ( map ( function , sequence )) eval_some def eval_some ( sequence : Sequence , function : Callable = < function ListOperations .< lambda > at 0x7f8c6343d7a0 > ) -> bool Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on elements of sequence that should return a boolean. None Returns: Type Description None A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False | View Source @staticmethod def eval_some ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \" \"\" Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False \"\" \" return any ( map ( function , sequence )) get_indices def get_indices ( element , sequence : Sequence ) -> List [ int ] Return all array indices at which number is located. Parameters: Name Type Description Default element None any reference element to check. None sequence Sequence a sequence containing objects comparable to elements . A string can be compared to an int in Python, custom objects probably won't be comparable. None Returns: Type Description None A list of all indices at which element is found in sequence . Empty list if element is not present in sequence at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] | View Source @staticmethod def get_indices ( element , sequence : Sequence ) -> List [ int ] : \" \"\" Return all array indices at which number is located. Args: element: any reference element to check. sequence (Sequence): a sequence containing objects comparable to `elements`. A string can be compared to an int in Python, custom objects probably won't be comparable. Returns: A list of all indices at which `element` is found in `sequence`. Empty list if `element` is not present in `sequence` at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] \"\" \" return [ i for ( y , i ) in zip ( sequence , range ( len ( sequence ))) if element == y ] group_by def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ] Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on the elements of sequence that should return a boolean. None Returns: Type Description None A dict with keys \"True\" and \"False\", each having as value a list of all elements of lst that were evaluated to respectively True or False through function . Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} | View Source @staticmethod def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ] : \" \"\" Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of `sequence` that should return a boolean. Returns: A dict with keys \" True \" and \" False \", each having as value a list of all elements of `lst` that were evaluated to respectively `True` or `False` through `function`. Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} \"\" \" groups = {} for key in list ( map ( function , sequence )) : groups [ key ] = [ item for item in sequence if function ( item ) == key ] return groups has_duplicates def has_duplicates ( sequence : Sequence ) -> bool Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None A boolean indicating the presence of duplicates in lst . Usage: has_duplicates([1, 2, 1]) -> True | View Source @staticmethod def has_duplicates ( sequence : Sequence ) -> bool : \" \"\" Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Args: sequence (Sequence): a sequence of elements. Returns: A boolean indicating the presence of duplicates in `lst`. Usage: has_duplicates([1, 2, 1]) -> True \"\" \" return len ( sequence ) != len ( set ( sequence )) sample def sample ( sequence : Sequence ) -> list Returns a random element from an array. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None A random element from lst in a list (to manage potentially nested lists as input). View Source @staticmethod def sample ( sequence : Sequence ) -> list : \" \"\" Returns a random element from an array. Args: sequence (Sequence): a sequence of elements. Returns: A random element from `lst` in a list (to manage potentially nested lists as input). \"\" \" return sequence [ random . randint ( 0 , len ( sequence ) - 1 ) ] sanitize_list def sanitize_list ( sequence : Sequence ) -> list Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None The sequence without falsy values. Usage: sanitize_list([1, False, \"a\", 2, \"\", None, 6, 0]) -> [1, \"a\", 2, 6] | View Source @staticmethod def sanitize_list ( sequence : Sequence ) -> list : \"\"\" Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Args: sequence (Sequence): a sequence of elements. Returns: The sequence without falsy values. Usage: sanitize_list([1, False, \" a \", 2, \"\", None, 6, 0]) -> [1, \" a \", 2, 6] \"\"\" return list ( filter ( bool , sequence )) shuffle def shuffle ( sequence : Sequence ) -> Sequence Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm ( https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle ) to reorder the elements. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None The lst with original elements at a random index. View Source @staticmethod def shuffle ( sequence : Sequence ) -> Sequence : \"\"\" Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm (https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) to reorder the elements. Args: sequence (Sequence): a sequence of elements. Returns: The `lst` with original elements at a random index. \"\"\" temp_list = copy . deepcopy ( sequence ) amount_to_shuffle = len ( temp_list ) while amount_to_shuffle > 1 : rand_index = int ( math . floor ( random . random () * amount_to_shuffle )) amount_to_shuffle -= 1 temp_list [ rand_index ] , temp_list [ amount_to_shuffle ] = ( temp_list [ amount_to_shuffle ] , temp_list [ rand_index ] , ) return temp_list spread def spread ( sequence : Sequence ) -> list Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in lst are iterables! Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] | View Source @staticmethod def spread ( sequence : Sequence ) -> list : \" \"\" Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in `lst` are iterables! Args: sequence (Sequence): a sequence of elements. Returns: The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] \"\" \" return list ( itertools . chain . from_iterable ( sequence )) symmetric_difference_by def symmetric_difference_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list Returns the symmetric difference ( https://en.wikipedia.org/wiki/Symmetric_difference ) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Parameters: Name Type Description Default lst_1 Sequence a sequence of elements. None lst_2 Sequence a sequence of elements. None function Callable a callable on elements of lst_1 and lst_2 . None Returns: Type Description None A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] | View Source @staticmethod def symmetric_difference_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \" \"\" Returns the symmetric difference (https://en.wikipedia.org/wiki/Symmetric_difference) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] \"\" \" _lst_1 , _lst_2 = set ( map ( function , lst_1 )), set ( map ( function , lst_2 )) return [ item for item in lst_1 if function ( item ) not in _lst_2 ] + [ item for item in lst_2 if function ( item ) not in _lst_1 ] union_by def union_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union ( https://en.wikipedia.org/wiki/Union_(set_theory )) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Parameters: Name Type Description Default lst_1 Sequence a sequence of elements. None lst_2 Sequence a sequence of elements. None function Callable a callable on elements of lst_1 and lst_2 . None Returns: Type Description None A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] | View Source @staticmethod def union_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \" \"\" Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union (https://en.wikipedia.org/wiki/Union_(set_theory)) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] \"\" \" _lst_1 = set ( map ( function , lst_1 )) return sorted ( list ( set ( lst_1 + [ item for item in lst_2 if function ( item ) not in _lst_1 ] ))) zipper def zipper ( * args , fillvalue = None ) -> list Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Parameters: Name Type Description Default *args None a number (>= 2) of different iterables. None fillvalue None value to use in case of length mismatch. None Returns: Type Description None A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\"a\", \"b\", \"c\"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] | View Source @staticmethod def zipper ( * args , fillvalue = None ) -> list : \"\"\" Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Args: *args: a number (>= 2) of different iterables. fillvalue: value to use in case of length mismatch. Returns: A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\" a \", \" b \", \" c \"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] \"\"\" max_length = max ( len ( lst ) for lst in args ) return [ [args[k ][ i ] if i < len ( args [ k ] ) else fillvalue for k in range ( len ( args )) ] for i in range ( max_length ) ] MiscellaneousOperations class MiscellaneousOperations ( / , * args , ** kwargs ) View Source class MiscellaneousOperations : \"\"\" A class to group some misc. operations that don't pertain to classic structures. \"\"\" @staticmethod def longest_item ( * args ) : \"\"\" Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Args: *args: any number (>= 2) of iterables. Returns: The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) \"\"\" return max ( args , key = len ) @staticmethod def map_values ( obj : dict , function : Callable ) -> dict : \"\"\" Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Args: obj: a dictionary. function (Callable): a callable on values of `obj`. Returns: A new dictionary with the results. Usage: map_values( {\" a \": list(range(5)), \" b \": list(range(10)), \" c \": list(range(15))}, lambda x: len(x) ) -> {\" a \": 5, \" b \": 10, \" c \": 15} \"\"\" ret = {} for key in obj : ret [ key ] = function ( obj [ key ] ) return ret Static methods longest_item def longest_item ( * args ) Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Parameters: Name Type Description Default *args None any number (>= 2) of iterables. None Returns: Type Description None The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) | View Source @staticmethod def longest_item ( * args ) : \"\"\" Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Args: *args: any number (>= 2) of iterables. Returns: The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) \"\"\" return max ( args , key = len ) map_values def map_values ( obj : dict , function : Callable ) -> dict Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Parameters: Name Type Description Default obj None a dictionary. None function Callable a callable on values of obj . None Returns: Type Description None A new dictionary with the results. Usage: map_values( {\"a\": list(range(5)), \"b\": list(range(10)), \"c\": list(range(15))}, lambda x: len(x) ) -> {\"a\": 5, \"b\": 10, \"c\": 15} | View Source @staticmethod def map_values ( obj : dict , function : Callable ) -> dict : \"\"\" Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Args: obj: a dictionary. function (Callable): a callable on values of `obj`. Returns: A new dictionary with the results. Usage: map_values( {\" a \": list(range(5)), \" b \": list(range(10)), \" c \": list(range(15))}, lambda x: len(x) ) -> {\" a \": 5, \" b \": 10, \" c \": 15} \"\"\" ret = {} for key in obj : ret [ key ] = function ( obj [ key ] ) return ret NumberOperations class NumberOperations ( / , * args , ** kwargs ) View Source class NumberOperations : \" \"\" A class to group some common / useful operations on numbers. \"\" \" @staticmethod def clamp_number ( num : Union [ int , float ] , a_val : Union [ int , float ] , b_val : Union [ int , float ] ) -> Union [ int , float ] : \" \"\" Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Args: num (Union[int, float]): a number (float / int) a_val (Union[int, float]): a number (float / int) b_val (Union[int, float]): a number (float / int) Returns: A number (float / int), being the nearest to `num` in the range [`a_val`, `b_val`]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 \"\" \" return max ( min ( num , max ( a_val , b_val )), min ( a_val , b_val )) @staticmethod def degrees_to_radians ( deg_value : Union [ int , float ] , decompose : bool = False ) -> Union [ Tuple [ float , str , str ] , int , float ] : \" \"\" Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Args: deg_value (Union[int, float]): angle value in degrees. decompose (bool): boolean option to return a more verbose result. Defaults to False. Returns: The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \" pi \", \" rad \") \"\" \" if decompose : return deg_value / 180 , \"pi\" , \"rad\" return ( deg_value * math . pi ) / 180.0 @staticmethod def greatest_common_divisor ( numbers_list : Sequence ) -> Union [ int , float ] : \" \"\" Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in `numbers_list`. Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 \"\" \" return reduce ( math . gcd , numbers_list ) @staticmethod def is_divisible_by ( dividend : Union [ int , float ] , divisor : Union [ int , float ] ) -> bool : \" \"\" Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Args: dividend (Union[int, float]): a number. divisor (Union[int, float]): a number. Returns: A boolean stating if `dividend` can be divided by `divisor`. \"\" \" return dividend % divisor == 0 @staticmethod def least_common_multiple ( * args ) -> int : \" \"\" Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 \"\" \" numbers = list ( ListOperations . spread ( list ( args ))) def _lcm ( number1 , number2 ) : \" \"\" A least common multiple method for two numbers only \"\" \" return int ( number1 * number2 / math . gcd ( number1 , number2 )) return reduce ( lambda x , y : _lcm ( x , y ), numbers ) @staticmethod def radians_to_degrees ( rad_value : Union [ int , float ] ) -> Union [ int , float ] : \" \"\" Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Args: rad_value (Union[int, float]): angle value in degrees. Returns: The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 \"\" \" return ( rad_value * 180.0 ) / math . pi Static methods clamp_number def clamp_number ( num : Union [ int , float ], a_val : Union [ int , float ], b_val : Union [ int , float ] ) -> Union [ int , float ] Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Parameters: Name Type Description Default num Union[int, float] a number (float / int) None a_val Union[int, float] a number (float / int) None b_val Union[int, float] a number (float / int) None Returns: Type Description None A number (float / int), being the nearest to num in the range [ a_val , b_val ]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 | View Source @staticmethod def clamp_number ( num : Union [ int , float ] , a_val : Union [ int , float ] , b_val : Union [ int , float ] ) -> Union [ int , float ] : \" \"\" Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Args: num (Union[int, float]): a number (float / int) a_val (Union[int, float]): a number (float / int) b_val (Union[int, float]): a number (float / int) Returns: A number (float / int), being the nearest to `num` in the range [`a_val`, `b_val`]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 \"\" \" return max ( min ( num , max ( a_val , b_val )), min ( a_val , b_val )) degrees_to_radians def degrees_to_radians ( deg_value : Union [ int , float ], decompose : bool = False ) -> Union [ Tuple [ float , str , str ], int , float ] Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Parameters: Name Type Description Default deg_value Union[int, float] angle value in degrees. None decompose bool boolean option to return a more verbose result. Defaults to False. False Returns: Type Description None The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \"pi\", \"rad\") | View Source @staticmethod def degrees_to_radians ( deg_value : Union [ int, float ] , decompose : bool = False ) -> Union [ Tuple[float, str, str ] , int , float ]: \"\"\" Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Args: deg_value (Union[int, float]): angle value in degrees. decompose (bool): boolean option to return a more verbose result. Defaults to False. Returns: The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \" pi \", \" rad \") \"\"\" if decompose : return deg_value / 180 , \"pi\" , \"rad\" return ( deg_value * math . pi ) / 180.0 greatest_common_divisor def greatest_common_divisor ( numbers_list : Sequence ) -> Union [ int , float ] Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in numbers_list . Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 View Source @staticmethod def greatest_common_divisor ( numbers_list : Sequence ) -> Union [ int , float ] : \" \"\" Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in `numbers_list`. Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 \"\" \" return reduce ( math . gcd , numbers_list ) is_divisible_by def is_divisible_by ( dividend : Union [ int , float ], divisor : Union [ int , float ] ) -> bool Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Parameters: Name Type Description Default dividend Union[int, float] a number. None divisor Union[int, float] a number. None Returns: Type Description None A boolean stating if dividend can be divided by divisor . View Source @staticmethod def is_divisible_by ( dividend : Union [ int , float ] , divisor : Union [ int , float ] ) -> bool : \" \"\" Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Args: dividend (Union[int, float]): a number. divisor (Union[int, float]): a number. Returns: A boolean stating if `dividend` can be divided by `divisor`. \"\" \" return dividend % divisor == 0 least_common_multiple def least_common_multiple ( * args ) -> int Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 View Source @staticmethod def least_common_multiple ( * args ) -> int : \"\"\" Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 \"\"\" numbers = list ( ListOperations . spread ( list ( args ))) def _lcm ( number1 , number2 ) : \"\"\"A least common multiple method for two numbers only\"\"\" return int ( number1 * number2 / math . gcd ( number1 , number2 )) return reduce ( lambda x , y : _lcm ( x , y ), numbers ) radians_to_degrees def radians_to_degrees ( rad_value : Union [ int , float ] ) -> Union [ int , float ] Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Parameters: Name Type Description Default rad_value Union[int, float] angle value in degrees. None Returns: Type Description None The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 | View Source @staticmethod def radians_to_degrees ( rad_value : Union [ int, float ] ) -> Union [ int, float ] : \"\"\" Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Args: rad_value (Union[int, float]): angle value in degrees. Returns: The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 \"\"\" return ( rad_value * 180.0 ) / math . pi StringOperations class StringOperations ( / , * args , ** kwargs ) View Source class StringOperations : \"\"\" A class to group some common / useful operations on strings. \"\"\" @staticmethod def camel_case ( text : str ) -> str : \"\"\" Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Args: text (str): a string. Returns: The same string best adapted to camel_case. Usage: camel_case(\" a_snake_case_name \") -> \" aSnakeCaseName \" camel_case(\" A Title Case Name \") -> \" aTitleCaseName \" \"\"\" text = re . sub ( r \"(\\s|_|-)+\" , \" \" , text ). title (). replace ( \" \" , \"\" ) return text [ 0 ]. lower () + text [ 1 : ] @staticmethod def capitalize ( text : str , lower_rest: bool = False ) -> str : \"\"\" Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Args: text (str): a string. lower_rest (bool): boolean option to lower all elements starting from the second. Returns: The `string`, capitalized. Usage: capitalize(\" astringtocapitalize \") -> \" Astringtocapitalize \" capitalize(\" astRIngTocApItalizE \", lower_rest=True) -> \" Astringtocapitalize \" \"\"\" return text [ : 1 ]. upper () + ( text [ 1 : ]. lower () if lower_rest else text [ 1 : ]) @staticmethod def is_anagram ( str_1: str , str_2: str ) -> bool : \"\"\" Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Args: str_1 (str): a string. str_2 (str): a string. Returns: A boolean stating whether `str_1` is an anagram of `str_2` or not. Usage: is_anagram(\" Tom Marvolo Riddle \", \" I am Lord Voldemort \") -> True is_anagram(\" A first string \", \" Definitely not an anagram \") -> False \"\"\" _ str1 , _ str2 = ( str_1 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), str_2 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), ) return sorted ( _ str1 . lower ()) == sorted ( _ str2 . lower ()) @staticmethod def is_palindrome ( text : str ) -> bool : \"\"\" Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Args: text (str): a string. Returns: A boolean stating whether `string` is a palindrome or not. Usage: is_palindrome(\" racecar \") -> True is_palindrome(\" definitelynot \") -> False \"\"\" s_reverse = re . sub ( r \" [ \\ W_ ] \", \"\", text.lower()) return s_reverse == s_reverse[::-1] @staticmethod def kebab_case(text: str) -> str: \"\"\" Converts a string to kebab - case . Break the string into words and combine them adding - as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to kebab_case . Usage : kebab_case ( \"camel Case\" ) -> \"camel-case\" kebab_case ( \"snake_case\" ) -> \"snake-case\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" - \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \", lambda mo: mo.group(0).lower(), text, ), ) @staticmethod def snake_case(text: str) -> str: \"\"\" Converts a string to snake_case . Break the string into words and combine them adding _ as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to snake_case . Usage : snake_case ( \"A bunch of words\" ) -> \"a_bunch_of_words\" snake_case ( \"camelCase\" ) -> \"camelcase\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" _ \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \" , lambda mo : mo . group ( 0 ). lower (), text , ), ) Static methods camel_case def camel_case ( text : str ) -> str Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Parameters: Name Type Description Default text str a string. None Returns: Type Description None The same string best adapted to camel_case. Usage: camel_case(\"a_snake_case_name\") -> \"aSnakeCaseName\" camel_case(\"A Title Case Name\") -> \"aTitleCaseName\" | View Source @staticmethod def camel_case ( text : str ) -> str : \"\"\" Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Args: text (str): a string. Returns: The same string best adapted to camel_case. Usage: camel_case(\" a_snake_case_name \") -> \" aSnakeCaseName \" camel_case(\" A Title Case Name \") -> \" aTitleCaseName \" \"\"\" text = re . sub ( r \"(\\s|_|-)+\" , \" \" , text ). title (). replace ( \" \" , \"\" ) return text [ 0 ] . lower () + text [ 1: ] capitalize def capitalize ( text : str , lower_rest : bool = False ) -> str Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Parameters: Name Type Description Default text str a string. None lower_rest bool boolean option to lower all elements starting from the second. None Returns: Type Description None The string , capitalized. Usage: capitalize(\"astringtocapitalize\") -> \"Astringtocapitalize\" capitalize(\"astRIngTocApItalizE\", lower_rest=True) -> \"Astringtocapitalize\" | View Source @staticmethod def capitalize ( text : str , lower_rest : bool = False ) -> str : \" \"\" Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Args: text (str): a string. lower_rest (bool): boolean option to lower all elements starting from the second. Returns: The `string`, capitalized. Usage: capitalize(\" astringtocapitalize \") -> \" Astringtocapitalize \" capitalize(\" astRIngTocApItalizE \", lower_rest=True) -> \" Astringtocapitalize \" \"\" \" return text [ : 1 ] . upper () + ( text [ 1 : ] . lower () if lower_rest else text [ 1 : ] ) is_anagram def is_anagram ( str_1 : str , str_2 : str ) -> bool Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Parameters: Name Type Description Default str_1 str a string. None str_2 str a string. None Returns: Type Description None A boolean stating whether str_1 is an anagram of str_2 or not. Usage: is_anagram(\"Tom Marvolo Riddle\", \"I am Lord Voldemort\") -> True is_anagram(\"A first string\", \"Definitely not an anagram\") -> False | View Source @staticmethod def is_anagram ( str_1 : str , str_2 : str ) -> bool : \" \"\" Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Args: str_1 (str): a string. str_2 (str): a string. Returns: A boolean stating whether `str_1` is an anagram of `str_2` or not. Usage: is_anagram(\" Tom Marvolo Riddle \", \" I am Lord Voldemort \") -> True is_anagram(\" A first string \", \" Definitely not an anagram \") -> False \"\" \" _str1 , _str2 = ( str_1 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), str_2 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), ) return sorted ( _str1 . lower ()) == sorted ( _str2 . lower ()) is_palindrome def is_palindrome ( text : str ) -> bool Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Parameters: Name Type Description Default text str a string. None Returns: Type Description None A boolean stating whether string is a palindrome or not. Usage: is_palindrome(\"racecar\") -> True is_palindrome(\"definitelynot\") -> False | View Source @staticmethod def is_palindrome ( text : str ) -> bool : \"\"\" Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Args: text (str): a string. Returns: A boolean stating whether `string` is a palindrome or not. Usage: is_palindrome(\" racecar \") -> True is_palindrome(\" definitelynot \") -> False \"\"\" s_reverse = re . sub ( r \" [ \\ W_ ] \", \" \" , text . lower ()) return s_reverse == s_reverse [ ::- 1 ] kebab_case def kebab_case ( text : str ) -> str Converts a string to kebab-case. Break the string into words and combine them adding - as a separator, using a regexp. Parameters: Name Type Description Default text str a string. None Returns: Type Description None The same string best adapted to kebab_case. Usage: kebab_case(\"camel Case\") -> \"camel-case\" kebab_case(\"snake_case\") -> \"snake-case\" | View Source @staticmethod def kebab_case ( text : str ) -> str : \"\"\" Converts a string to kebab-case. Break the string into words and combine them adding - as a separator, using a regexp. Args: text (str): a string. Returns: The same string best adapted to kebab_case. Usage: kebab_case(\" camel Case \") -> \" camel - case \" kebab_case(\" snake_case \") -> \" snake - case \" \"\"\" return re . sub ( r \"(\\s|_|-)+\" , \"-\" , re . sub ( r \"[A-Z]{2,}(?=[A-Z][a-z]+[0-9]*|\\b)|[A-Z]?[a-z]+[0-9]*|[A-Z]|[0-9]+\" , lambda mo : mo . group ( 0 ). lower (), text , ), ) snake_case def snake_case ( text : str ) -> str Converts a string to snake_case. Break the string into words and combine them adding _ as a separator, using a regexp. Parameters: Name Type Description Default text str a string. None Returns: Type Description None The same string best adapted to snake_case. Usage: snake_case(\"A bunch of words\") -> \"a_bunch_of_words\" snake_case(\"camelCase\") -> \"camelcase\" | View Source @staticmethod def snake_case ( text : str ) -> str : \"\"\" Converts a string to snake_case. Break the string into words and combine them adding _ as a separator, using a regexp. Args: text (str): a string. Returns: The same string best adapted to snake_case. Usage: snake_case(\" A bunch of words \") -> \" a_bunch_of_words \" snake_case(\" camelCase \") -> \" camelcase \" \"\"\" return re . sub ( r \"(\\s|_|-)+\" , \"_\" , re . sub ( r \"[A-Z]{2,}(?=[A-Z][a-z]+[0-9]*|\\b)|[A-Z]?[a-z]+[0-9]*|[A-Z]|[0-9]+\" , lambda mo : mo . group ( 0 ). lower (), text , ), )","title":"Operations"},{"location":"reference/pyhdtoolkit/utils/operations/#module-pyhdtoolkitutilsoperations","text":"Module utils.operations Created on 2019.11.12 View Source \"\"\" Module utils.operations ----------------------- Created on 2019.11.12 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection classes with utility functions to perform common / convenient operations on the classic Python structures. \"\"\" import copy import itertools import math import random import re from functools import reduce from typing import Callable , Dict , List , Sequence , Tuple , Union class ListOperations : \"\"\" A class to group some common / useful operations on lists. \"\"\" @staticmethod def all_unique ( sequence : Sequence ) -> bool : \"\"\" Returns True if all the values in a flat list are unique, False otherwise. Args: sequence (Sequence): a sequence of elements. Returns: True if all elements are unique, False otherwise. \"\"\" return len ( sequence ) == len ( set ( sequence )) @staticmethod def average_by ( sequence : Sequence , function : Callable = lambda x : x ) -> float : \"\"\" Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Args: sequence (Sequence): a sequence of elements. function (Callable): function to apply to elements of the sequence. Returns: The average of each element's result through `function`. Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 \"\"\" return float ( sum ( map ( function , sequence ), 0.0 ) / len ( sequence )) @staticmethod def bifurcate ( sequence : Sequence , filters : List [ bool ]) -> Sequence : \"\"\" Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Args: sequence (Sequence): a sequence of elements. filters (List[bool]): a list of booleans. Returns: A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] \"\"\" return [ [ x for i , x in enumerate ( sequence ) if filters [ i ]], [ x for i , x in enumerate ( sequence ) if not filters [ i ]], ] @staticmethod def bifurcate_by ( sequence : Sequence , function : Callable ) -> list : \"\"\" Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of lst, that should return a boolean. Returns: A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] \"\"\" return [[ x for x in sequence if function ( x )], [ x for x in sequence if not function ( x )]] @staticmethod def chunk_list ( sequence : Sequence , size : int ) -> Sequence : \"\"\" Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Args: sequence (Sequence): a sequence of elements. size (int): the size of the wanted sublists. Returns: A list of lists of length `size` (except maybe the last element), with elements from `lst`. Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] \"\"\" if size > len ( sequence ) : return sequence return list ( map ( lambda x : sequence [ x * size : x * size + size ], list ( range ( math . ceil ( len ( sequence ) / size ))),) ) @staticmethod def deep_flatten ( sequence : Sequence ) -> list : \"\"\" Deep flattens a list, no matter the nesting levels. This is a recursive approach. Args: sequence (Sequence): a sequence of elements. Returns: A list with all elements of `lst`, but flattened. Usage: deep_flatten([[\" a \", \" b \"], [1, 2], None, [True, False]]) -> [\" a \", \" b \", 1, 2, None True, False] \"\"\" return ( [ elem for sublist in sequence for elem in ListOperations . deep_flatten ( sublist )] if isinstance ( sequence , list ) else [ sequence ] ) @staticmethod def eval_none ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False \"\"\" return not any ( map ( function , sequence )) @staticmethod def eval_some ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False \"\"\" return any ( map ( function , sequence )) @staticmethod def get_indices ( element , sequence : Sequence ) -> List [ int ] : \"\"\" Return all array indices at which number is located. Args: element: any reference element to check. sequence (Sequence): a sequence containing objects comparable to `elements`. A string can be compared to an int in Python, custom objects probably won't be comparable. Returns: A list of all indices at which `element` is found in `sequence`. Empty list if `element` is not present in `sequence` at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] \"\"\" return [ i for ( y , i ) in zip ( sequence , range ( len ( sequence ))) if element == y ] @staticmethod def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ] : \"\"\" Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of `sequence` that should return a boolean. Returns: A dict with keys \" True \" and \" False \", each having as value a list of all elements of `lst` that were evaluated to respectively `True` or `False` through `function`. Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} \"\"\" groups = {} for key in list ( map ( function , sequence )) : groups [ key ] = [ item for item in sequence if function ( item ) == key ] return groups @staticmethod def has_duplicates ( sequence : Sequence ) -> bool : \"\"\" Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Args: sequence (Sequence): a sequence of elements. Returns: A boolean indicating the presence of duplicates in `lst`. Usage: has_duplicates([1, 2, 1]) -> True \"\"\" return len ( sequence ) ! = len ( set ( sequence )) @staticmethod def sample ( sequence : Sequence ) -> list : \"\"\" Returns a random element from an array. Args: sequence (Sequence): a sequence of elements. Returns: A random element from `lst` in a list (to manage potentially nested lists as input). \"\"\" return sequence [ random . randint ( 0 , len ( sequence ) - 1 )] @staticmethod def sanitize_list ( sequence : Sequence ) -> list : \"\"\" Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Args: sequence (Sequence): a sequence of elements. Returns: The sequence without falsy values. Usage: sanitize_list([1, False, \" a \", 2, \"\", None, 6, 0]) -> [1, \" a \", 2, 6] \"\"\" return list ( filter ( bool , sequence )) @staticmethod def shuffle ( sequence : Sequence ) -> Sequence : \"\"\" Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm (https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) to reorder the elements. Args: sequence (Sequence): a sequence of elements. Returns: The `lst` with original elements at a random index. \"\"\" temp_list = copy . deepcopy ( sequence ) amount_to_shuffle = len ( temp_list ) while amount_to_shuffle > 1 : rand_index = int ( math . floor ( random . random () * amount_to_shuffle )) amount_to_shuffle -= 1 temp_list [ rand_index ], temp_list [ amount_to_shuffle ] = ( temp_list [ amount_to_shuffle ], temp_list [ rand_index ], ) return temp_list @staticmethod def spread ( sequence : Sequence ) -> list : \"\"\" Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in `lst` are iterables! Args: sequence (Sequence): a sequence of elements. Returns: The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] \"\"\" return list ( itertools . chain . from_iterable ( sequence )) @staticmethod def symmetric_difference_by ( lst_1: Sequence , lst_2: Sequence , function : Callable ) -> list : \"\"\" Returns the symmetric difference (https://en.wikipedia.org/wiki/Symmetric_difference) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] \"\"\" _ lst_1 , _ lst_2 = set ( map ( function , lst_1 )), set ( map ( function , lst_2 )) return [ item for item in lst_1 if function ( item ) not in _ lst_2 ] + [ item for item in lst_2 if function ( item ) not in _ lst_1 ] @staticmethod def union_by ( lst_1: Sequence , lst_2: Sequence , function : Callable ) -> list : \"\"\" Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union (https://en.wikipedia.org/wiki/Union_(set_theory)) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] \"\"\" _ lst_1 = set ( map ( function , lst_1 )) return sorted ( list ( set ( lst_1 + [ item for item in lst_2 if function ( item ) not in _ lst_1 ]))) @staticmethod def zipper ( * args , fillvalue = None ) -> list : \"\"\" Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Args: *args: a number (>= 2) of different iterables. fillvalue: value to use in case of length mismatch. Returns: A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\" a \", \" b \", \" c \"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] \"\"\" max_length = max ( len ( lst ) for lst in args ) return [ [ args [ k ][ i ] if i < len ( args [ k ]) else fillvalue for k in range ( len ( args ))] for i in range ( max_length ) ] class MiscellaneousOperations : \"\"\" A class to group some misc. operations that don't pertain to classic structures. \"\"\" @staticmethod def longest_item ( * args ) : \"\"\" Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Args: *args: any number (>= 2) of iterables. Returns: The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) \"\"\" return max ( args , key = len ) @staticmethod def map_values ( obj : dict , function : Callable ) -> dict : \"\"\" Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Args: obj: a dictionary. function (Callable): a callable on values of `obj`. Returns: A new dictionary with the results. Usage: map_values( {\" a \": list(range(5)), \" b \": list(range(10)), \" c \": list(range(15))}, lambda x: len(x) ) -> {\" a \": 5, \" b \": 10, \" c \": 15} \"\"\" ret = {} for key in obj : ret [ key ] = function ( obj [ key ]) return ret class NumberOperations : \"\"\" A class to group some common / useful operations on numbers. \"\"\" @staticmethod def clamp_number ( num : Union [ int , float ], a_val: Union [ int , float ], b_val: Union [ int , float ] ) -> Union [ int , float ] : \"\"\" Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Args: num (Union[int, float]): a number (float / int) a_val (Union[int, float]): a number (float / int) b_val (Union[int, float]): a number (float / int) Returns: A number (float / int), being the nearest to `num` in the range [`a_val`, `b_val`]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 \"\"\" return max ( min ( num , max ( a_val , b_val )), min ( a_val , b_val )) @staticmethod def degrees_to_radians ( deg_value: Union [ int , float ], decompose : bool = False ) -> Union [ Tuple [ float , str , str ], int , float ] : \"\"\" Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Args: deg_value (Union[int, float]): angle value in degrees. decompose (bool): boolean option to return a more verbose result. Defaults to False. Returns: The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \" pi \", \" rad \") \"\"\" if decompose : return deg_value / 180 , \"pi\" , \"rad\" return ( deg_value * math . pi ) / 180.0 @staticmethod def greatest_common_divisor ( numbers_list: Sequence ) -> Union [ int , float ] : \"\"\" Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in `numbers_list`. Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 \"\"\" return reduce ( math . gcd , numbers_list ) @staticmethod def is_divisible_by ( dividend : Union [ int , float ], divisor : Union [ int , float ]) -> bool : \"\"\" Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Args: dividend (Union[int, float]): a number. divisor (Union[int, float]): a number. Returns: A boolean stating if `dividend` can be divided by `divisor`. \"\"\" return dividend % divisor == 0 @staticmethod def least_common_multiple ( * args ) -> int : \"\"\" Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 \"\"\" numbers = list ( ListOperations . spread ( list ( args ))) def _ lcm ( number1 , number2 ) : \"\"\"A least common multiple method for two numbers only\"\"\" return int ( number1 * number2 / math . gcd ( number1 , number2 )) return reduce ( lambda x , y : _ lcm ( x , y ), numbers ) @staticmethod def radians_to_degrees ( rad_value: Union [ int , float ]) -> Union [ int , float ] : \"\"\" Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Args: rad_value (Union[int, float]): angle value in degrees. Returns: The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 \"\"\" return ( rad_value * 180.0 ) / math . pi class StringOperations : \"\"\" A class to group some common / useful operations on strings. \"\"\" @staticmethod def camel_case ( text : str ) -> str : \"\"\" Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Args: text (str): a string. Returns: The same string best adapted to camel_case. Usage: camel_case(\" a_snake_case_name \") -> \" aSnakeCaseName \" camel_case(\" A Title Case Name \") -> \" aTitleCaseName \" \"\"\" text = re . sub ( r \"(\\s|_|-)+\" , \" \" , text ). title (). replace ( \" \" , \"\" ) return text [ 0 ]. lower () + text [ 1 : ] @staticmethod def capitalize ( text : str , lower_rest: bool = False ) -> str : \"\"\" Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Args: text (str): a string. lower_rest (bool): boolean option to lower all elements starting from the second. Returns: The `string`, capitalized. Usage: capitalize(\" astringtocapitalize \") -> \" Astringtocapitalize \" capitalize(\" astRIngTocApItalizE \", lower_rest=True) -> \" Astringtocapitalize \" \"\"\" return text [ : 1 ]. upper () + ( text [ 1 : ]. lower () if lower_rest else text [ 1 : ]) @staticmethod def is_anagram ( str_1: str , str_2: str ) -> bool : \"\"\" Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Args: str_1 (str): a string. str_2 (str): a string. Returns: A boolean stating whether `str_1` is an anagram of `str_2` or not. Usage: is_anagram(\" Tom Marvolo Riddle \", \" I am Lord Voldemort \") -> True is_anagram(\" A first string \", \" Definitely not an anagram \") -> False \"\"\" _ str1 , _ str2 = ( str_1 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), str_2 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), ) return sorted ( _ str1 . lower ()) == sorted ( _ str2 . lower ()) @staticmethod def is_palindrome ( text : str ) -> bool : \"\"\" Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Args: text (str): a string. Returns: A boolean stating whether `string` is a palindrome or not. Usage: is_palindrome(\" racecar \") -> True is_palindrome(\" definitelynot \") -> False \"\"\" s_reverse = re . sub ( r \" [ \\ W_ ] \", \"\", text.lower()) return s_reverse == s_reverse[::-1] @staticmethod def kebab_case(text: str) -> str: \"\"\" Converts a string to kebab - case . Break the string into words and combine them adding - as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to kebab_case . Usage : kebab_case ( \"camel Case\" ) -> \"camel-case\" kebab_case ( \"snake_case\" ) -> \"snake-case\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" - \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \", lambda mo: mo.group(0).lower(), text, ), ) @staticmethod def snake_case(text: str) -> str: \"\"\" Converts a string to snake_case . Break the string into words and combine them adding _ as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to snake_case . Usage : snake_case ( \"A bunch of words\" ) -> \"a_bunch_of_words\" snake_case ( \"camelCase\" ) -> \"camelcase\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" _ \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \" , lambda mo : mo . group ( 0 ). lower (), text , ), )","title":"Module pyhdtoolkit.utils.operations"},{"location":"reference/pyhdtoolkit/utils/operations/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/utils/operations/#listoperations","text":"class ListOperations ( / , * args , ** kwargs ) View Source class ListOperations : \"\"\" A class to group some common / useful operations on lists. \"\"\" @ staticmethod def all_unique ( sequence : Sequence ) -> bool : \"\"\" Returns True if all the values in a flat list are unique, False otherwise. Args: sequence (Sequence): a sequence of elements. Returns: True if all elements are unique, False otherwise. \"\"\" return len ( sequence ) == len ( set ( sequence )) @ staticmethod def average_by ( sequence : Sequence , function : Callable = lambda x : x ) -> float : \"\"\" Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Args: sequence (Sequence): a sequence of elements. function (Callable): function to apply to elements of the sequence. Returns: The average of each element's result through `function`. Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 \"\"\" return float ( sum ( map ( function , sequence ), 0.0 ) / len ( sequence )) @ staticmethod def bifurcate ( sequence : Sequence , filters : List [ bool ]) -> Sequence : \"\"\" Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Args: sequence (Sequence): a sequence of elements. filters (List[bool]): a list of booleans. Returns: A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] \"\"\" return [ [ x for i , x in enumerate ( sequence ) if filters [ i ]], [ x for i , x in enumerate ( sequence ) if not filters [ i ]], ] @ staticmethod def bifurcate_by ( sequence : Sequence , function : Callable ) -> list : \"\"\" Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of lst, that should return a boolean. Returns: A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] \"\"\" return [[ x for x in sequence if function ( x )], [ x for x in sequence if not function ( x )]] @ staticmethod def chunk_list ( sequence : Sequence , size : int ) -> Sequence : \"\"\" Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Args: sequence (Sequence): a sequence of elements. size (int): the size of the wanted sublists. Returns: A list of lists of length `size` (except maybe the last element), with elements from `lst`. Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] \"\"\" if size > len ( sequence ): return sequence return list ( map ( lambda x : sequence [ x * size : x * size + size ], list ( range ( math . ceil ( len ( sequence ) / size ))),) ) @ staticmethod def deep_flatten ( sequence : Sequence ) -> list : \"\"\" Deep flattens a list, no matter the nesting levels. This is a recursive approach. Args: sequence (Sequence): a sequence of elements. Returns: A list with all elements of `lst`, but flattened. Usage: deep_flatten([[\"a\", \"b\"], [1, 2], None, [True, False]]) -> [\"a\", \"b\", 1, 2, None True, False] \"\"\" return ( [ elem for sublist in sequence for elem in ListOperations . deep_flatten ( sublist )] if isinstance ( sequence , list ) else [ sequence ] ) @ staticmethod def eval_none ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False \"\"\" return not any ( map ( function , sequence )) @ staticmethod def eval_some ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False \"\"\" return any ( map ( function , sequence )) @ staticmethod def get_indices ( element , sequence : Sequence ) -> List [ int ]: \"\"\" Return all array indices at which number is located. Args: element: any reference element to check. sequence (Sequence): a sequence containing objects comparable to `elements`. A string can be compared to an int in Python, custom objects probably won't be comparable. Returns: A list of all indices at which `element` is found in `sequence`. Empty list if `element` is not present in `sequence` at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] \"\"\" return [ i for ( y , i ) in zip ( sequence , range ( len ( sequence ))) if element == y ] @ staticmethod def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ]: \"\"\" Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of `sequence` that should return a boolean. Returns: A dict with keys \"True\" and \"False\", each having as value a list of all elements of `lst` that were evaluated to respectively `True` or `False` through `function`. Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} \"\"\" groups = {} for key in list ( map ( function , sequence )): groups [ key ] = [ item for item in sequence if function ( item ) == key ] return groups @ staticmethod def has_duplicates ( sequence : Sequence ) -> bool : \"\"\" Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Args: sequence (Sequence): a sequence of elements. Returns: A boolean indicating the presence of duplicates in `lst`. Usage: has_duplicates([1, 2, 1]) -> True \"\"\" return len ( sequence ) != len ( set ( sequence )) @ staticmethod def sample ( sequence : Sequence ) -> list : \"\"\" Returns a random element from an array. Args: sequence (Sequence): a sequence of elements. Returns: A random element from `lst` in a list (to manage potentially nested lists as input). \"\"\" return sequence [ random . randint ( 0 , len ( sequence ) - 1 )] @ staticmethod def sanitize_list ( sequence : Sequence ) -> list : \"\"\" Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Args: sequence (Sequence): a sequence of elements. Returns: The sequence without falsy values. Usage: sanitize_list([1, False, \"a\", 2, \"\", None, 6, 0]) -> [1, \"a\", 2, 6] \"\"\" return list ( filter ( bool , sequence )) @ staticmethod def shuffle ( sequence : Sequence ) -> Sequence : \"\"\" Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm (https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) to reorder the elements. Args: sequence (Sequence): a sequence of elements. Returns: The `lst` with original elements at a random index. \"\"\" temp_list = copy . deepcopy ( sequence ) amount_to_shuffle = len ( temp_list ) while amount_to_shuffle > 1 : rand_index = int ( math . floor ( random . random () * amount_to_shuffle )) amount_to_shuffle -= 1 temp_list [ rand_index ], temp_list [ amount_to_shuffle ] = ( temp_list [ amount_to_shuffle ], temp_list [ rand_index ], ) return temp_list @ staticmethod def spread ( sequence : Sequence ) -> list : \"\"\" Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in `lst` are iterables! Args: sequence (Sequence): a sequence of elements. Returns: The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] \"\"\" return list ( itertools . chain . from_iterable ( sequence )) @ staticmethod def symmetric_difference_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \"\"\" Returns the symmetric difference (https://en.wikipedia.org/wiki/Symmetric_difference) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] \"\"\" _lst_1 , _lst_2 = set ( map ( function , lst_1 )), set ( map ( function , lst_2 )) return [ item for item in lst_1 if function ( item ) not in _lst_2 ] + [ item for item in lst_2 if function ( item ) not in _lst_1 ] @ staticmethod def union_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \"\"\" Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union (https://en.wikipedia.org/wiki/Union_(set_theory)) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] \"\"\" _lst_1 = set ( map ( function , lst_1 )) return sorted ( list ( set ( lst_1 + [ item for item in lst_2 if function ( item ) not in _lst_1 ]))) @ staticmethod def zipper ( * args , fillvalue = None ) -> list : \"\"\" Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Args: *args: a number (>= 2) of different iterables. fillvalue: value to use in case of length mismatch. Returns: A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\"a\", \"b\", \"c\"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] \"\"\" max_length = max ( len ( lst ) for lst in args ) return [ [ args [ k ][ i ] if i < len ( args [ k ]) else fillvalue for k in range ( len ( args ))] for i in range ( max_length ) ]","title":"ListOperations"},{"location":"reference/pyhdtoolkit/utils/operations/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/operations/#all_unique","text":"def all_unique ( sequence : Sequence ) -> bool Returns True if all the values in a flat list are unique, False otherwise. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None True if all elements are unique, False otherwise. View Source @staticmethod def all_unique ( sequence : Sequence ) -> bool : \"\"\" Returns True if all the values in a flat list are unique, False otherwise. Args: sequence (Sequence): a sequence of elements. Returns: True if all elements are unique, False otherwise. \"\"\" return len ( sequence ) == len ( set ( sequence ))","title":"all_unique"},{"location":"reference/pyhdtoolkit/utils/operations/#average_by","text":"def average_by ( sequence : Sequence , function : Callable = < function ListOperations .< lambda > at 0x7f8c6343d320 > ) -> float Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable function to apply to elements of the sequence. None Returns: Type Description None The average of each element's result through function . Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 | View Source @staticmethod def average_by ( sequence : Sequence , function : Callable = lambda x : x ) -> float : \" \"\" Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Args: sequence (Sequence): a sequence of elements. function (Callable): function to apply to elements of the sequence. Returns: The average of each element's result through `function`. Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 \"\" \" return float ( sum ( map ( function , sequence ), 0.0 ) / len ( sequence ))","title":"average_by"},{"location":"reference/pyhdtoolkit/utils/operations/#bifurcate","text":"def bifurcate ( sequence : Sequence , filters : List [ bool ] ) -> Sequence Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None filters List[bool] a list of booleans. None Returns: Type Description None A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] | View Source @staticmethod def bifurcate ( sequence : Sequence , filters : List [ bool ] ) -> Sequence : \"\"\" Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Args: sequence (Sequence): a sequence of elements. filters (List[bool]): a list of booleans. Returns: A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] \"\"\" return [ [x for i, x in enumerate(sequence) if filters[i ] ] , [ x for i, x in enumerate(sequence) if not filters[i ] ] , ]","title":"bifurcate"},{"location":"reference/pyhdtoolkit/utils/operations/#bifurcate_by","text":"def bifurcate_by ( sequence : Sequence , function : Callable ) -> list Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on the elements of lst, that should return a boolean. None Returns: Type Description None A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] | View Source @staticmethod def bifurcate_by ( sequence : Sequence , function : Callable ) -> list : \"\"\" Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of lst, that should return a boolean. Returns: A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] \"\"\" return [ [x for x in sequence if function(x) ] , [ x for x in sequence if not function(x) ] ]","title":"bifurcate_by"},{"location":"reference/pyhdtoolkit/utils/operations/#chunk_list","text":"def chunk_list ( sequence : Sequence , size : int ) -> Sequence Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None size int the size of the wanted sublists. None Returns: Type Description None A list of lists of length size (except maybe the last element), with elements from lst . Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] | View Source @staticmethod def chunk_list ( sequence : Sequence , size : int ) -> Sequence : \" \"\" Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Args: sequence (Sequence): a sequence of elements. size (int): the size of the wanted sublists. Returns: A list of lists of length `size` (except maybe the last element), with elements from `lst`. Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] \"\" \" if size > len ( sequence ) : return sequence return list ( map ( lambda x : sequence [ x * size : x * size + size ] , list ( range ( math . ceil ( len ( sequence ) / size ))),) )","title":"chunk_list"},{"location":"reference/pyhdtoolkit/utils/operations/#deep_flatten","text":"def deep_flatten ( sequence : Sequence ) -> list Deep flattens a list, no matter the nesting levels. This is a recursive approach. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None A list with all elements of lst , but flattened. Usage: deep_flatten([[\"a\", \"b\"], [1, 2], None, [True, False]]) -> [\"a\", \"b\", 1, 2, None True, False] | View Source @staticmethod def deep_flatten ( sequence : Sequence ) -> list : \"\"\" Deep flattens a list, no matter the nesting levels. This is a recursive approach. Args: sequence (Sequence): a sequence of elements. Returns: A list with all elements of `lst`, but flattened. Usage: deep_flatten([[\" a \", \" b \"], [1, 2], None, [True, False]]) -> [\" a \", \" b \", 1, 2, None True, False] \"\"\" return ( [ elem for sublist in sequence for elem in ListOperations.deep_flatten(sublist) ] if isinstance ( sequence , list ) else [ sequence ] )","title":"deep_flatten"},{"location":"reference/pyhdtoolkit/utils/operations/#eval_none","text":"def eval_none ( sequence : Sequence , function : Callable = < function ListOperations .< lambda > at 0x7f8c6343d680 > ) -> bool Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on elements of sequence that should return a boolean. None Returns: Type Description None A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False | View Source @staticmethod def eval_none ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \" \"\" Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False \"\" \" return not any ( map ( function , sequence ))","title":"eval_none"},{"location":"reference/pyhdtoolkit/utils/operations/#eval_some","text":"def eval_some ( sequence : Sequence , function : Callable = < function ListOperations .< lambda > at 0x7f8c6343d7a0 > ) -> bool Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on elements of sequence that should return a boolean. None Returns: Type Description None A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False | View Source @staticmethod def eval_some ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \" \"\" Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False \"\" \" return any ( map ( function , sequence ))","title":"eval_some"},{"location":"reference/pyhdtoolkit/utils/operations/#get_indices","text":"def get_indices ( element , sequence : Sequence ) -> List [ int ] Return all array indices at which number is located. Parameters: Name Type Description Default element None any reference element to check. None sequence Sequence a sequence containing objects comparable to elements . A string can be compared to an int in Python, custom objects probably won't be comparable. None Returns: Type Description None A list of all indices at which element is found in sequence . Empty list if element is not present in sequence at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] | View Source @staticmethod def get_indices ( element , sequence : Sequence ) -> List [ int ] : \" \"\" Return all array indices at which number is located. Args: element: any reference element to check. sequence (Sequence): a sequence containing objects comparable to `elements`. A string can be compared to an int in Python, custom objects probably won't be comparable. Returns: A list of all indices at which `element` is found in `sequence`. Empty list if `element` is not present in `sequence` at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] \"\" \" return [ i for ( y , i ) in zip ( sequence , range ( len ( sequence ))) if element == y ]","title":"get_indices"},{"location":"reference/pyhdtoolkit/utils/operations/#group_by","text":"def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ] Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on the elements of sequence that should return a boolean. None Returns: Type Description None A dict with keys \"True\" and \"False\", each having as value a list of all elements of lst that were evaluated to respectively True or False through function . Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} | View Source @staticmethod def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ] : \" \"\" Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of `sequence` that should return a boolean. Returns: A dict with keys \" True \" and \" False \", each having as value a list of all elements of `lst` that were evaluated to respectively `True` or `False` through `function`. Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} \"\" \" groups = {} for key in list ( map ( function , sequence )) : groups [ key ] = [ item for item in sequence if function ( item ) == key ] return groups","title":"group_by"},{"location":"reference/pyhdtoolkit/utils/operations/#has_duplicates","text":"def has_duplicates ( sequence : Sequence ) -> bool Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None A boolean indicating the presence of duplicates in lst . Usage: has_duplicates([1, 2, 1]) -> True | View Source @staticmethod def has_duplicates ( sequence : Sequence ) -> bool : \" \"\" Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Args: sequence (Sequence): a sequence of elements. Returns: A boolean indicating the presence of duplicates in `lst`. Usage: has_duplicates([1, 2, 1]) -> True \"\" \" return len ( sequence ) != len ( set ( sequence ))","title":"has_duplicates"},{"location":"reference/pyhdtoolkit/utils/operations/#sample","text":"def sample ( sequence : Sequence ) -> list Returns a random element from an array. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None A random element from lst in a list (to manage potentially nested lists as input). View Source @staticmethod def sample ( sequence : Sequence ) -> list : \" \"\" Returns a random element from an array. Args: sequence (Sequence): a sequence of elements. Returns: A random element from `lst` in a list (to manage potentially nested lists as input). \"\" \" return sequence [ random . randint ( 0 , len ( sequence ) - 1 ) ]","title":"sample"},{"location":"reference/pyhdtoolkit/utils/operations/#sanitize_list","text":"def sanitize_list ( sequence : Sequence ) -> list Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None The sequence without falsy values. Usage: sanitize_list([1, False, \"a\", 2, \"\", None, 6, 0]) -> [1, \"a\", 2, 6] | View Source @staticmethod def sanitize_list ( sequence : Sequence ) -> list : \"\"\" Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Args: sequence (Sequence): a sequence of elements. Returns: The sequence without falsy values. Usage: sanitize_list([1, False, \" a \", 2, \"\", None, 6, 0]) -> [1, \" a \", 2, 6] \"\"\" return list ( filter ( bool , sequence ))","title":"sanitize_list"},{"location":"reference/pyhdtoolkit/utils/operations/#shuffle","text":"def shuffle ( sequence : Sequence ) -> Sequence Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm ( https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle ) to reorder the elements. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None The lst with original elements at a random index. View Source @staticmethod def shuffle ( sequence : Sequence ) -> Sequence : \"\"\" Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm (https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) to reorder the elements. Args: sequence (Sequence): a sequence of elements. Returns: The `lst` with original elements at a random index. \"\"\" temp_list = copy . deepcopy ( sequence ) amount_to_shuffle = len ( temp_list ) while amount_to_shuffle > 1 : rand_index = int ( math . floor ( random . random () * amount_to_shuffle )) amount_to_shuffle -= 1 temp_list [ rand_index ] , temp_list [ amount_to_shuffle ] = ( temp_list [ amount_to_shuffle ] , temp_list [ rand_index ] , ) return temp_list","title":"shuffle"},{"location":"reference/pyhdtoolkit/utils/operations/#spread","text":"def spread ( sequence : Sequence ) -> list Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in lst are iterables! Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] | View Source @staticmethod def spread ( sequence : Sequence ) -> list : \" \"\" Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in `lst` are iterables! Args: sequence (Sequence): a sequence of elements. Returns: The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] \"\" \" return list ( itertools . chain . from_iterable ( sequence ))","title":"spread"},{"location":"reference/pyhdtoolkit/utils/operations/#symmetric_difference_by","text":"def symmetric_difference_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list Returns the symmetric difference ( https://en.wikipedia.org/wiki/Symmetric_difference ) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Parameters: Name Type Description Default lst_1 Sequence a sequence of elements. None lst_2 Sequence a sequence of elements. None function Callable a callable on elements of lst_1 and lst_2 . None Returns: Type Description None A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] | View Source @staticmethod def symmetric_difference_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \" \"\" Returns the symmetric difference (https://en.wikipedia.org/wiki/Symmetric_difference) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] \"\" \" _lst_1 , _lst_2 = set ( map ( function , lst_1 )), set ( map ( function , lst_2 )) return [ item for item in lst_1 if function ( item ) not in _lst_2 ] + [ item for item in lst_2 if function ( item ) not in _lst_1 ]","title":"symmetric_difference_by"},{"location":"reference/pyhdtoolkit/utils/operations/#union_by","text":"def union_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union ( https://en.wikipedia.org/wiki/Union_(set_theory )) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Parameters: Name Type Description Default lst_1 Sequence a sequence of elements. None lst_2 Sequence a sequence of elements. None function Callable a callable on elements of lst_1 and lst_2 . None Returns: Type Description None A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] | View Source @staticmethod def union_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \" \"\" Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union (https://en.wikipedia.org/wiki/Union_(set_theory)) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] \"\" \" _lst_1 = set ( map ( function , lst_1 )) return sorted ( list ( set ( lst_1 + [ item for item in lst_2 if function ( item ) not in _lst_1 ] )))","title":"union_by"},{"location":"reference/pyhdtoolkit/utils/operations/#zipper","text":"def zipper ( * args , fillvalue = None ) -> list Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Parameters: Name Type Description Default *args None a number (>= 2) of different iterables. None fillvalue None value to use in case of length mismatch. None Returns: Type Description None A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\"a\", \"b\", \"c\"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] | View Source @staticmethod def zipper ( * args , fillvalue = None ) -> list : \"\"\" Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Args: *args: a number (>= 2) of different iterables. fillvalue: value to use in case of length mismatch. Returns: A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\" a \", \" b \", \" c \"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] \"\"\" max_length = max ( len ( lst ) for lst in args ) return [ [args[k ][ i ] if i < len ( args [ k ] ) else fillvalue for k in range ( len ( args )) ] for i in range ( max_length ) ]","title":"zipper"},{"location":"reference/pyhdtoolkit/utils/operations/#miscellaneousoperations","text":"class MiscellaneousOperations ( / , * args , ** kwargs ) View Source class MiscellaneousOperations : \"\"\" A class to group some misc. operations that don't pertain to classic structures. \"\"\" @staticmethod def longest_item ( * args ) : \"\"\" Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Args: *args: any number (>= 2) of iterables. Returns: The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) \"\"\" return max ( args , key = len ) @staticmethod def map_values ( obj : dict , function : Callable ) -> dict : \"\"\" Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Args: obj: a dictionary. function (Callable): a callable on values of `obj`. Returns: A new dictionary with the results. Usage: map_values( {\" a \": list(range(5)), \" b \": list(range(10)), \" c \": list(range(15))}, lambda x: len(x) ) -> {\" a \": 5, \" b \": 10, \" c \": 15} \"\"\" ret = {} for key in obj : ret [ key ] = function ( obj [ key ] ) return ret","title":"MiscellaneousOperations"},{"location":"reference/pyhdtoolkit/utils/operations/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/operations/#longest_item","text":"def longest_item ( * args ) Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Parameters: Name Type Description Default *args None any number (>= 2) of iterables. None Returns: Type Description None The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) | View Source @staticmethod def longest_item ( * args ) : \"\"\" Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Args: *args: any number (>= 2) of iterables. Returns: The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) \"\"\" return max ( args , key = len )","title":"longest_item"},{"location":"reference/pyhdtoolkit/utils/operations/#map_values","text":"def map_values ( obj : dict , function : Callable ) -> dict Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Parameters: Name Type Description Default obj None a dictionary. None function Callable a callable on values of obj . None Returns: Type Description None A new dictionary with the results. Usage: map_values( {\"a\": list(range(5)), \"b\": list(range(10)), \"c\": list(range(15))}, lambda x: len(x) ) -> {\"a\": 5, \"b\": 10, \"c\": 15} | View Source @staticmethod def map_values ( obj : dict , function : Callable ) -> dict : \"\"\" Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Args: obj: a dictionary. function (Callable): a callable on values of `obj`. Returns: A new dictionary with the results. Usage: map_values( {\" a \": list(range(5)), \" b \": list(range(10)), \" c \": list(range(15))}, lambda x: len(x) ) -> {\" a \": 5, \" b \": 10, \" c \": 15} \"\"\" ret = {} for key in obj : ret [ key ] = function ( obj [ key ] ) return ret","title":"map_values"},{"location":"reference/pyhdtoolkit/utils/operations/#numberoperations","text":"class NumberOperations ( / , * args , ** kwargs ) View Source class NumberOperations : \" \"\" A class to group some common / useful operations on numbers. \"\" \" @staticmethod def clamp_number ( num : Union [ int , float ] , a_val : Union [ int , float ] , b_val : Union [ int , float ] ) -> Union [ int , float ] : \" \"\" Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Args: num (Union[int, float]): a number (float / int) a_val (Union[int, float]): a number (float / int) b_val (Union[int, float]): a number (float / int) Returns: A number (float / int), being the nearest to `num` in the range [`a_val`, `b_val`]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 \"\" \" return max ( min ( num , max ( a_val , b_val )), min ( a_val , b_val )) @staticmethod def degrees_to_radians ( deg_value : Union [ int , float ] , decompose : bool = False ) -> Union [ Tuple [ float , str , str ] , int , float ] : \" \"\" Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Args: deg_value (Union[int, float]): angle value in degrees. decompose (bool): boolean option to return a more verbose result. Defaults to False. Returns: The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \" pi \", \" rad \") \"\" \" if decompose : return deg_value / 180 , \"pi\" , \"rad\" return ( deg_value * math . pi ) / 180.0 @staticmethod def greatest_common_divisor ( numbers_list : Sequence ) -> Union [ int , float ] : \" \"\" Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in `numbers_list`. Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 \"\" \" return reduce ( math . gcd , numbers_list ) @staticmethod def is_divisible_by ( dividend : Union [ int , float ] , divisor : Union [ int , float ] ) -> bool : \" \"\" Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Args: dividend (Union[int, float]): a number. divisor (Union[int, float]): a number. Returns: A boolean stating if `dividend` can be divided by `divisor`. \"\" \" return dividend % divisor == 0 @staticmethod def least_common_multiple ( * args ) -> int : \" \"\" Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 \"\" \" numbers = list ( ListOperations . spread ( list ( args ))) def _lcm ( number1 , number2 ) : \" \"\" A least common multiple method for two numbers only \"\" \" return int ( number1 * number2 / math . gcd ( number1 , number2 )) return reduce ( lambda x , y : _lcm ( x , y ), numbers ) @staticmethod def radians_to_degrees ( rad_value : Union [ int , float ] ) -> Union [ int , float ] : \" \"\" Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Args: rad_value (Union[int, float]): angle value in degrees. Returns: The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 \"\" \" return ( rad_value * 180.0 ) / math . pi","title":"NumberOperations"},{"location":"reference/pyhdtoolkit/utils/operations/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/operations/#clamp_number","text":"def clamp_number ( num : Union [ int , float ], a_val : Union [ int , float ], b_val : Union [ int , float ] ) -> Union [ int , float ] Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Parameters: Name Type Description Default num Union[int, float] a number (float / int) None a_val Union[int, float] a number (float / int) None b_val Union[int, float] a number (float / int) None Returns: Type Description None A number (float / int), being the nearest to num in the range [ a_val , b_val ]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 | View Source @staticmethod def clamp_number ( num : Union [ int , float ] , a_val : Union [ int , float ] , b_val : Union [ int , float ] ) -> Union [ int , float ] : \" \"\" Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Args: num (Union[int, float]): a number (float / int) a_val (Union[int, float]): a number (float / int) b_val (Union[int, float]): a number (float / int) Returns: A number (float / int), being the nearest to `num` in the range [`a_val`, `b_val`]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 \"\" \" return max ( min ( num , max ( a_val , b_val )), min ( a_val , b_val ))","title":"clamp_number"},{"location":"reference/pyhdtoolkit/utils/operations/#degrees_to_radians","text":"def degrees_to_radians ( deg_value : Union [ int , float ], decompose : bool = False ) -> Union [ Tuple [ float , str , str ], int , float ] Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Parameters: Name Type Description Default deg_value Union[int, float] angle value in degrees. None decompose bool boolean option to return a more verbose result. Defaults to False. False Returns: Type Description None The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \"pi\", \"rad\") | View Source @staticmethod def degrees_to_radians ( deg_value : Union [ int, float ] , decompose : bool = False ) -> Union [ Tuple[float, str, str ] , int , float ]: \"\"\" Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Args: deg_value (Union[int, float]): angle value in degrees. decompose (bool): boolean option to return a more verbose result. Defaults to False. Returns: The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \" pi \", \" rad \") \"\"\" if decompose : return deg_value / 180 , \"pi\" , \"rad\" return ( deg_value * math . pi ) / 180.0","title":"degrees_to_radians"},{"location":"reference/pyhdtoolkit/utils/operations/#greatest_common_divisor","text":"def greatest_common_divisor ( numbers_list : Sequence ) -> Union [ int , float ] Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in numbers_list . Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 View Source @staticmethod def greatest_common_divisor ( numbers_list : Sequence ) -> Union [ int , float ] : \" \"\" Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in `numbers_list`. Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 \"\" \" return reduce ( math . gcd , numbers_list )","title":"greatest_common_divisor"},{"location":"reference/pyhdtoolkit/utils/operations/#is_divisible_by","text":"def is_divisible_by ( dividend : Union [ int , float ], divisor : Union [ int , float ] ) -> bool Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Parameters: Name Type Description Default dividend Union[int, float] a number. None divisor Union[int, float] a number. None Returns: Type Description None A boolean stating if dividend can be divided by divisor . View Source @staticmethod def is_divisible_by ( dividend : Union [ int , float ] , divisor : Union [ int , float ] ) -> bool : \" \"\" Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Args: dividend (Union[int, float]): a number. divisor (Union[int, float]): a number. Returns: A boolean stating if `dividend` can be divided by `divisor`. \"\" \" return dividend % divisor == 0","title":"is_divisible_by"},{"location":"reference/pyhdtoolkit/utils/operations/#least_common_multiple","text":"def least_common_multiple ( * args ) -> int Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 View Source @staticmethod def least_common_multiple ( * args ) -> int : \"\"\" Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 \"\"\" numbers = list ( ListOperations . spread ( list ( args ))) def _lcm ( number1 , number2 ) : \"\"\"A least common multiple method for two numbers only\"\"\" return int ( number1 * number2 / math . gcd ( number1 , number2 )) return reduce ( lambda x , y : _lcm ( x , y ), numbers )","title":"least_common_multiple"},{"location":"reference/pyhdtoolkit/utils/operations/#radians_to_degrees","text":"def radians_to_degrees ( rad_value : Union [ int , float ] ) -> Union [ int , float ] Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Parameters: Name Type Description Default rad_value Union[int, float] angle value in degrees. None Returns: Type Description None The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 | View Source @staticmethod def radians_to_degrees ( rad_value : Union [ int, float ] ) -> Union [ int, float ] : \"\"\" Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Args: rad_value (Union[int, float]): angle value in degrees. Returns: The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 \"\"\" return ( rad_value * 180.0 ) / math . pi","title":"radians_to_degrees"},{"location":"reference/pyhdtoolkit/utils/operations/#stringoperations","text":"class StringOperations ( / , * args , ** kwargs ) View Source class StringOperations : \"\"\" A class to group some common / useful operations on strings. \"\"\" @staticmethod def camel_case ( text : str ) -> str : \"\"\" Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Args: text (str): a string. Returns: The same string best adapted to camel_case. Usage: camel_case(\" a_snake_case_name \") -> \" aSnakeCaseName \" camel_case(\" A Title Case Name \") -> \" aTitleCaseName \" \"\"\" text = re . sub ( r \"(\\s|_|-)+\" , \" \" , text ). title (). replace ( \" \" , \"\" ) return text [ 0 ]. lower () + text [ 1 : ] @staticmethod def capitalize ( text : str , lower_rest: bool = False ) -> str : \"\"\" Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Args: text (str): a string. lower_rest (bool): boolean option to lower all elements starting from the second. Returns: The `string`, capitalized. Usage: capitalize(\" astringtocapitalize \") -> \" Astringtocapitalize \" capitalize(\" astRIngTocApItalizE \", lower_rest=True) -> \" Astringtocapitalize \" \"\"\" return text [ : 1 ]. upper () + ( text [ 1 : ]. lower () if lower_rest else text [ 1 : ]) @staticmethod def is_anagram ( str_1: str , str_2: str ) -> bool : \"\"\" Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Args: str_1 (str): a string. str_2 (str): a string. Returns: A boolean stating whether `str_1` is an anagram of `str_2` or not. Usage: is_anagram(\" Tom Marvolo Riddle \", \" I am Lord Voldemort \") -> True is_anagram(\" A first string \", \" Definitely not an anagram \") -> False \"\"\" _ str1 , _ str2 = ( str_1 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), str_2 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), ) return sorted ( _ str1 . lower ()) == sorted ( _ str2 . lower ()) @staticmethod def is_palindrome ( text : str ) -> bool : \"\"\" Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Args: text (str): a string. Returns: A boolean stating whether `string` is a palindrome or not. Usage: is_palindrome(\" racecar \") -> True is_palindrome(\" definitelynot \") -> False \"\"\" s_reverse = re . sub ( r \" [ \\ W_ ] \", \"\", text.lower()) return s_reverse == s_reverse[::-1] @staticmethod def kebab_case(text: str) -> str: \"\"\" Converts a string to kebab - case . Break the string into words and combine them adding - as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to kebab_case . Usage : kebab_case ( \"camel Case\" ) -> \"camel-case\" kebab_case ( \"snake_case\" ) -> \"snake-case\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" - \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \", lambda mo: mo.group(0).lower(), text, ), ) @staticmethod def snake_case(text: str) -> str: \"\"\" Converts a string to snake_case . Break the string into words and combine them adding _ as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to snake_case . Usage : snake_case ( \"A bunch of words\" ) -> \"a_bunch_of_words\" snake_case ( \"camelCase\" ) -> \"camelcase\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" _ \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \" , lambda mo : mo . group ( 0 ). lower (), text , ), )","title":"StringOperations"},{"location":"reference/pyhdtoolkit/utils/operations/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/operations/#camel_case","text":"def camel_case ( text : str ) -> str Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Parameters: Name Type Description Default text str a string. None Returns: Type Description None The same string best adapted to camel_case. Usage: camel_case(\"a_snake_case_name\") -> \"aSnakeCaseName\" camel_case(\"A Title Case Name\") -> \"aTitleCaseName\" | View Source @staticmethod def camel_case ( text : str ) -> str : \"\"\" Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Args: text (str): a string. Returns: The same string best adapted to camel_case. Usage: camel_case(\" a_snake_case_name \") -> \" aSnakeCaseName \" camel_case(\" A Title Case Name \") -> \" aTitleCaseName \" \"\"\" text = re . sub ( r \"(\\s|_|-)+\" , \" \" , text ). title (). replace ( \" \" , \"\" ) return text [ 0 ] . lower () + text [ 1: ]","title":"camel_case"},{"location":"reference/pyhdtoolkit/utils/operations/#capitalize","text":"def capitalize ( text : str , lower_rest : bool = False ) -> str Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Parameters: Name Type Description Default text str a string. None lower_rest bool boolean option to lower all elements starting from the second. None Returns: Type Description None The string , capitalized. Usage: capitalize(\"astringtocapitalize\") -> \"Astringtocapitalize\" capitalize(\"astRIngTocApItalizE\", lower_rest=True) -> \"Astringtocapitalize\" | View Source @staticmethod def capitalize ( text : str , lower_rest : bool = False ) -> str : \" \"\" Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Args: text (str): a string. lower_rest (bool): boolean option to lower all elements starting from the second. Returns: The `string`, capitalized. Usage: capitalize(\" astringtocapitalize \") -> \" Astringtocapitalize \" capitalize(\" astRIngTocApItalizE \", lower_rest=True) -> \" Astringtocapitalize \" \"\" \" return text [ : 1 ] . upper () + ( text [ 1 : ] . lower () if lower_rest else text [ 1 : ] )","title":"capitalize"},{"location":"reference/pyhdtoolkit/utils/operations/#is_anagram","text":"def is_anagram ( str_1 : str , str_2 : str ) -> bool Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Parameters: Name Type Description Default str_1 str a string. None str_2 str a string. None Returns: Type Description None A boolean stating whether str_1 is an anagram of str_2 or not. Usage: is_anagram(\"Tom Marvolo Riddle\", \"I am Lord Voldemort\") -> True is_anagram(\"A first string\", \"Definitely not an anagram\") -> False | View Source @staticmethod def is_anagram ( str_1 : str , str_2 : str ) -> bool : \" \"\" Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Args: str_1 (str): a string. str_2 (str): a string. Returns: A boolean stating whether `str_1` is an anagram of `str_2` or not. Usage: is_anagram(\" Tom Marvolo Riddle \", \" I am Lord Voldemort \") -> True is_anagram(\" A first string \", \" Definitely not an anagram \") -> False \"\" \" _str1 , _str2 = ( str_1 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), str_2 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), ) return sorted ( _str1 . lower ()) == sorted ( _str2 . lower ())","title":"is_anagram"},{"location":"reference/pyhdtoolkit/utils/operations/#is_palindrome","text":"def is_palindrome ( text : str ) -> bool Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Parameters: Name Type Description Default text str a string. None Returns: Type Description None A boolean stating whether string is a palindrome or not. Usage: is_palindrome(\"racecar\") -> True is_palindrome(\"definitelynot\") -> False | View Source @staticmethod def is_palindrome ( text : str ) -> bool : \"\"\" Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Args: text (str): a string. Returns: A boolean stating whether `string` is a palindrome or not. Usage: is_palindrome(\" racecar \") -> True is_palindrome(\" definitelynot \") -> False \"\"\" s_reverse = re . sub ( r \" [ \\ W_ ] \", \" \" , text . lower ()) return s_reverse == s_reverse [ ::- 1 ]","title":"is_palindrome"},{"location":"reference/pyhdtoolkit/utils/operations/#kebab_case","text":"def kebab_case ( text : str ) -> str Converts a string to kebab-case. Break the string into words and combine them adding - as a separator, using a regexp. Parameters: Name Type Description Default text str a string. None Returns: Type Description None The same string best adapted to kebab_case. Usage: kebab_case(\"camel Case\") -> \"camel-case\" kebab_case(\"snake_case\") -> \"snake-case\" | View Source @staticmethod def kebab_case ( text : str ) -> str : \"\"\" Converts a string to kebab-case. Break the string into words and combine them adding - as a separator, using a regexp. Args: text (str): a string. Returns: The same string best adapted to kebab_case. Usage: kebab_case(\" camel Case \") -> \" camel - case \" kebab_case(\" snake_case \") -> \" snake - case \" \"\"\" return re . sub ( r \"(\\s|_|-)+\" , \"-\" , re . sub ( r \"[A-Z]{2,}(?=[A-Z][a-z]+[0-9]*|\\b)|[A-Z]?[a-z]+[0-9]*|[A-Z]|[0-9]+\" , lambda mo : mo . group ( 0 ). lower (), text , ), )","title":"kebab_case"},{"location":"reference/pyhdtoolkit/utils/operations/#snake_case","text":"def snake_case ( text : str ) -> str Converts a string to snake_case. Break the string into words and combine them adding _ as a separator, using a regexp. Parameters: Name Type Description Default text str a string. None Returns: Type Description None The same string best adapted to snake_case. Usage: snake_case(\"A bunch of words\") -> \"a_bunch_of_words\" snake_case(\"camelCase\") -> \"camelcase\" | View Source @staticmethod def snake_case ( text : str ) -> str : \"\"\" Converts a string to snake_case. Break the string into words and combine them adding _ as a separator, using a regexp. Args: text (str): a string. Returns: The same string best adapted to snake_case. Usage: snake_case(\" A bunch of words \") -> \" a_bunch_of_words \" snake_case(\" camelCase \") -> \" camelcase \" \"\"\" return re . sub ( r \"(\\s|_|-)+\" , \"_\" , re . sub ( r \"[A-Z]{2,}(?=[A-Z][a-z]+[0-9]*|\\b)|[A-Z]?[a-z]+[0-9]*|[A-Z]|[0-9]+\" , lambda mo : mo . group ( 0 ). lower (), text , ), )","title":"snake_case"},{"location":"reference/pyhdtoolkit/utils/printutil/","text":"Module pyhdtoolkit.utils.printutil Module utils.printutil Created on 2019.12.11 View Source \"\"\" Module utils.printutil ---------------------- Created on 2019.12.11 :author: Felix Soubelet (felix.soubelet@cern.ch) A class utility class to allow me printing text in color, bold, etc. \"\"\" END = \"\\033[0m\" class Background : \"\"\" ANSI color escape sequences for the background of a terminal output. \"\"\" black = \"\\033[40m\" blue = \"\\033[44m\" cyan = \"\\033[46m\" green = \"\\033[42m\" grey = \"\\033[47m\" magenta = \"\\033[45m\" red = \"\\033[41m\" yellow = \"\\033[43m\" class Foreground : \"\"\" ANSI color escape sequences for the foreground of a terminal output. \"\"\" blue = \"\\033[94m\" cyan = \"\\033[96m\" dark_blue = \"\\033[34m\" dark_cyan = \"\\033[36m\" dark_green = \"\\033[32m\" dark_grey = \"\\033[90m\" dark_red = \"\\033[31m\" dark_yellow = \"\\033[33m\" green = \"\\033[92m\" grey = \"\\033[37m\" magenta = \"\\033[35m\" pink = \"\\033[95m\" red = \"\\033[91m\" yellow = \"\\033[93m\" white = \"\\033[30m\" class Styles : \"\"\" ANSI style escape sequences for a terminal output. \"\"\" all_off = \"\\033[0m\" bold = \"\\033[1m\" concealed = \"\\033[7m\" disable = \"\\033[02m\" reverse = \"\\033[7m\" strikethrough = \"\\033[09m\" underscore = \"\\033[4m\" Variables END Classes Background class Background ( / , * args , ** kwargs ) View Source class Background: \"\"\" ANSI color escape sequences for the background of a terminal output. \"\"\" black = \"\\033[40m\" blue = \"\\033[44m\" cyan = \"\\033[46m\" green = \"\\033[42m\" grey = \"\\033[47m\" magenta = \"\\033[45m\" red = \"\\033[41m\" yellow = \"\\033[43m\" Class variables black blue cyan green grey magenta red yellow Foreground class Foreground ( / , * args , ** kwargs ) View Source class Foreground: \"\"\" ANSI color escape sequences for the foreground of a terminal output . \"\"\" blue = \" \\033 [94m\" cyan = \" \\033 [96m\" dark_blue = \" \\033 [34m\" dark_cyan = \" \\033 [36m\" dark_green = \" \\033 [32m\" dark_grey = \" \\033 [90m\" dark_red = \" \\033 [31m\" dark_yellow = \" \\033 [33m\" green = \" \\033 [92m\" grey = \" \\033 [37m\" magenta = \" \\033 [35m\" pink = \" \\033 [95m\" red = \" \\033 [91m\" yellow = \" \\033 [93m\" white = \" \\033 [30m\" Class variables blue cyan dark_blue dark_cyan dark_green dark_grey dark_red dark_yellow green grey magenta pink red white yellow Styles class Styles ( / , * args , ** kwargs ) View Source class Styles: \"\"\" ANSI style escape sequences for a terminal output. \"\"\" all_off = \"\\033[0m\" bold = \"\\033[1m\" concealed = \"\\033[7m\" disable = \"\\033[02m\" reverse = \"\\033[7m\" strikethrough = \"\\033[09m\" underscore = \"\\033[4m\" Class variables all_off bold concealed disable reverse strikethrough underscore","title":"Printutil"},{"location":"reference/pyhdtoolkit/utils/printutil/#module-pyhdtoolkitutilsprintutil","text":"Module utils.printutil Created on 2019.12.11 View Source \"\"\" Module utils.printutil ---------------------- Created on 2019.12.11 :author: Felix Soubelet (felix.soubelet@cern.ch) A class utility class to allow me printing text in color, bold, etc. \"\"\" END = \"\\033[0m\" class Background : \"\"\" ANSI color escape sequences for the background of a terminal output. \"\"\" black = \"\\033[40m\" blue = \"\\033[44m\" cyan = \"\\033[46m\" green = \"\\033[42m\" grey = \"\\033[47m\" magenta = \"\\033[45m\" red = \"\\033[41m\" yellow = \"\\033[43m\" class Foreground : \"\"\" ANSI color escape sequences for the foreground of a terminal output. \"\"\" blue = \"\\033[94m\" cyan = \"\\033[96m\" dark_blue = \"\\033[34m\" dark_cyan = \"\\033[36m\" dark_green = \"\\033[32m\" dark_grey = \"\\033[90m\" dark_red = \"\\033[31m\" dark_yellow = \"\\033[33m\" green = \"\\033[92m\" grey = \"\\033[37m\" magenta = \"\\033[35m\" pink = \"\\033[95m\" red = \"\\033[91m\" yellow = \"\\033[93m\" white = \"\\033[30m\" class Styles : \"\"\" ANSI style escape sequences for a terminal output. \"\"\" all_off = \"\\033[0m\" bold = \"\\033[1m\" concealed = \"\\033[7m\" disable = \"\\033[02m\" reverse = \"\\033[7m\" strikethrough = \"\\033[09m\" underscore = \"\\033[4m\"","title":"Module pyhdtoolkit.utils.printutil"},{"location":"reference/pyhdtoolkit/utils/printutil/#variables","text":"END","title":"Variables"},{"location":"reference/pyhdtoolkit/utils/printutil/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/utils/printutil/#background","text":"class Background ( / , * args , ** kwargs ) View Source class Background: \"\"\" ANSI color escape sequences for the background of a terminal output. \"\"\" black = \"\\033[40m\" blue = \"\\033[44m\" cyan = \"\\033[46m\" green = \"\\033[42m\" grey = \"\\033[47m\" magenta = \"\\033[45m\" red = \"\\033[41m\" yellow = \"\\033[43m\"","title":"Background"},{"location":"reference/pyhdtoolkit/utils/printutil/#class-variables","text":"black blue cyan green grey magenta red yellow","title":"Class variables"},{"location":"reference/pyhdtoolkit/utils/printutil/#foreground","text":"class Foreground ( / , * args , ** kwargs ) View Source class Foreground: \"\"\" ANSI color escape sequences for the foreground of a terminal output . \"\"\" blue = \" \\033 [94m\" cyan = \" \\033 [96m\" dark_blue = \" \\033 [34m\" dark_cyan = \" \\033 [36m\" dark_green = \" \\033 [32m\" dark_grey = \" \\033 [90m\" dark_red = \" \\033 [31m\" dark_yellow = \" \\033 [33m\" green = \" \\033 [92m\" grey = \" \\033 [37m\" magenta = \" \\033 [35m\" pink = \" \\033 [95m\" red = \" \\033 [91m\" yellow = \" \\033 [93m\" white = \" \\033 [30m\"","title":"Foreground"},{"location":"reference/pyhdtoolkit/utils/printutil/#class-variables_1","text":"blue cyan dark_blue dark_cyan dark_green dark_grey dark_red dark_yellow green grey magenta pink red white yellow","title":"Class variables"},{"location":"reference/pyhdtoolkit/utils/printutil/#styles","text":"class Styles ( / , * args , ** kwargs ) View Source class Styles: \"\"\" ANSI style escape sequences for a terminal output. \"\"\" all_off = \"\\033[0m\" bold = \"\\033[1m\" concealed = \"\\033[7m\" disable = \"\\033[02m\" reverse = \"\\033[7m\" strikethrough = \"\\033[09m\" underscore = \"\\033[4m\"","title":"Styles"},{"location":"reference/pyhdtoolkit/utils/printutil/#class-variables_2","text":"all_off bold concealed disable reverse strikethrough underscore","title":"Class variables"}]}