{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pyhdtoolkit \u267b\ufe0f An all-in-one package for Python work in my PhD \u267b\ufe0f Link to documentation . License Copyright \u00a9 2019 Felix Soubelet. MIT License","title":"Home"},{"location":"#license","text":"Copyright \u00a9 2019 Felix Soubelet. MIT License","title":"License"},{"location":"docs/About_PyhDToolkit/","text":"About PyhDToolkit Purpose This package is meant to be an all-in-one collection of utilities and scripts I use in my PhD work. Most of the codes here have their use in my day-to-day work, but not necessarily in our team's softwares. Functionality For now, PyhDToolkit provides some of the following features: Useful tools to integrate with cpymad , a Python bindings library for the MAD-X code, including generators, matching routines and plotting utilities. A maths module to incorporate useful methods used in analysis. A plotting module for my favorite defaults and helpers. An optics module for particle accelerator physics related calculations and analysis. A scripts module to handle different simulations setups. A tfstools module similar to cpymadtools , with functionality revolving around handling tfs files and plotting their contents. A utils module for various utilities. Roadmap In addition to developping current modules, more will be added to better incorporate with the workhorse softwares of the OMC team, notably tfs-pandas and omc3 . Foreseen development includes: Expansion of existing modules, particularly the optics module to include most simple calculations on beam properties. An omcwrapper module to handle different usecases of the omc3 package. A sixtracklibtools module for utility functions surrounding the use of sixtracklib .","title":"About"},{"location":"docs/About_PyhDToolkit/#about-pyhdtoolkit","text":"","title":"About PyhDToolkit"},{"location":"docs/About_PyhDToolkit/#purpose","text":"This package is meant to be an all-in-one collection of utilities and scripts I use in my PhD work. Most of the codes here have their use in my day-to-day work, but not necessarily in our team's softwares.","title":"Purpose"},{"location":"docs/About_PyhDToolkit/#functionality","text":"For now, PyhDToolkit provides some of the following features: Useful tools to integrate with cpymad , a Python bindings library for the MAD-X code, including generators, matching routines and plotting utilities. A maths module to incorporate useful methods used in analysis. A plotting module for my favorite defaults and helpers. An optics module for particle accelerator physics related calculations and analysis. A scripts module to handle different simulations setups. A tfstools module similar to cpymadtools , with functionality revolving around handling tfs files and plotting their contents. A utils module for various utilities.","title":"Functionality"},{"location":"docs/About_PyhDToolkit/#roadmap","text":"In addition to developping current modules, more will be added to better incorporate with the workhorse softwares of the OMC team, notably tfs-pandas and omc3 . Foreseen development includes: Expansion of existing modules, particularly the optics module to include most simple calculations on beam properties. An omcwrapper module to handle different usecases of the omc3 package. A sixtracklibtools module for utility functions surrounding the use of sixtracklib .","title":"Roadmap"},{"location":"docs/Getting_Started/","text":"Getting Started Installation This code is compatible with Python 3.7+ . There are two possible methods for using this package: either as a Python package with pip , or as a Docker image. With pip You can now install this simply in a virtual environment with: > pip install pyhdtoolkit Installation in a virtual environment Don't know what a virtual environment is or how to set it up? Here is a good primer on virtual environments by RealPython. How about a development environment? Sure thing. This repository uses Poetry as a packaging and build tool. To set yourself up, get a local copy through VCS and run: poetry install This repository follows the Google docstring format, uses Black as a code formatter with a default enforced line length of 100 characters, and Pylint as a linter. You can format the code with make format and lint it (which will format first) with make lint . Testing builds are ensured after each commit through Github Actions. You can run tests locally with the predefined make tests , or through poetry run pytest <options> for customized options. How can I easily reproduce your research done with this? This repository comes with an environment.yml file to reproduce my work conda environment, feel free to use it. If you checked out from version control, you can install this environment and add it to your ipython kernel by running make condaenv . You can also make use of a fully-fetched one through Docker as explained below. With Docker You can directly pull a pre-built image from Dockerhub (with tag latest being an automated build) with: > docker pull fsoubelet/simenv You can then run a server from within the container and bind a local directory to work on. Assuming you pulled the provided image from Dockerhub, run a jupyterlab server on port 8888 with the command: > docker run --rm -p 8888 :8888 -e JUPYTER_ENABLE_LAB = yes -v <host_dir_to_mount>:/home/jovyan/work fsoubelet/simenv","title":"Getting Started"},{"location":"docs/Getting_Started/#getting-started","text":"","title":"Getting Started"},{"location":"docs/Getting_Started/#installation","text":"This code is compatible with Python 3.7+ . There are two possible methods for using this package: either as a Python package with pip , or as a Docker image.","title":"Installation"},{"location":"docs/Getting_Started/#with-pip","text":"You can now install this simply in a virtual environment with: > pip install pyhdtoolkit Installation in a virtual environment Don't know what a virtual environment is or how to set it up? Here is a good primer on virtual environments by RealPython. How about a development environment? Sure thing. This repository uses Poetry as a packaging and build tool. To set yourself up, get a local copy through VCS and run: poetry install This repository follows the Google docstring format, uses Black as a code formatter with a default enforced line length of 100 characters, and Pylint as a linter. You can format the code with make format and lint it (which will format first) with make lint . Testing builds are ensured after each commit through Github Actions. You can run tests locally with the predefined make tests , or through poetry run pytest <options> for customized options. How can I easily reproduce your research done with this? This repository comes with an environment.yml file to reproduce my work conda environment, feel free to use it. If you checked out from version control, you can install this environment and add it to your ipython kernel by running make condaenv . You can also make use of a fully-fetched one through Docker as explained below.","title":"With pip"},{"location":"docs/Getting_Started/#with-docker","text":"You can directly pull a pre-built image from Dockerhub (with tag latest being an automated build) with: > docker pull fsoubelet/simenv You can then run a server from within the container and bind a local directory to work on. Assuming you pulled the provided image from Dockerhub, run a jupyterlab server on port 8888 with the command: > docker run --rm -p 8888 :8888 -e JUPYTER_ENABLE_LAB = yes -v <host_dir_to_mount>:/home/jovyan/work fsoubelet/simenv","title":"With Docker"},{"location":"reference/pyhdtoolkit/","text":"Module pyhdtoolkit pyhdtoolkit Library ~ ~ ~ ~ ~ ~ ~ pyhdtoolkit is a utility library, written in Python, for my PhD needs. Mainly particle accelerator physics studies and plotting. View Source \"\"\" pyhdtoolkit Library ~~~~~~~~~~~~~~~~~~~ pyhdtoolkit is a utility library, written in Python, for my PhD needs. Mainly particle accelerator physics studies and plotting. :copyright: (c) 2019-2020 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" # Set default logging handler to avoid \"No handler found\" warnings. __title__ = \"pyhdtoolkit\" __description__ = \"An all-in-one toolkit package to easy my Python work in my PhD.\" __url__ = \"https://github.com/fsoubelet/PyhDToolkit\" __version__ = \"0.7.0\" __author__ = \"Felix Soubelet\" __author_email__ = \"felix.soubelet@cern.ch\" __license__ = \"MIT\" Sub-modules pyhdtoolkit.cpymadtools pyhdtoolkit.maths pyhdtoolkit.optics pyhdtoolkit.plotting pyhdtoolkit.scripts pyhdtoolkit.tfstools pyhdtoolkit.utils","title":"Index"},{"location":"reference/pyhdtoolkit/#module-pyhdtoolkit","text":"pyhdtoolkit Library ~ ~ ~ ~ ~ ~ ~ pyhdtoolkit is a utility library, written in Python, for my PhD needs. Mainly particle accelerator physics studies and plotting. View Source \"\"\" pyhdtoolkit Library ~~~~~~~~~~~~~~~~~~~ pyhdtoolkit is a utility library, written in Python, for my PhD needs. Mainly particle accelerator physics studies and plotting. :copyright: (c) 2019-2020 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" # Set default logging handler to avoid \"No handler found\" warnings. __title__ = \"pyhdtoolkit\" __description__ = \"An all-in-one toolkit package to easy my Python work in my PhD.\" __url__ = \"https://github.com/fsoubelet/PyhDToolkit\" __version__ = \"0.7.0\" __author__ = \"Felix Soubelet\" __author_email__ = \"felix.soubelet@cern.ch\" __license__ = \"MIT\"","title":"Module pyhdtoolkit"},{"location":"reference/pyhdtoolkit/#sub-modules","text":"pyhdtoolkit.cpymadtools pyhdtoolkit.maths pyhdtoolkit.optics pyhdtoolkit.plotting pyhdtoolkit.scripts pyhdtoolkit.tfstools pyhdtoolkit.utils","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/cpymadtools/","text":"Module pyhdtoolkit.cpymadtools cpymadtools package ~ ~ ~ ~ ~ ~ ~ cpymadtools is a collection of utilities that integrate within my workflow with the cpymad library. View Source \"\"\" cpymadtools package ~~~~~~~~~~~~~~~~~~~ cpymadtools is a collection of utilities that integrate within my workflow with the `cpymad` library. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .errors import switch_magnetic_errors from .generators import LatticeGenerator from .latwiss import plot_latwiss , plot_machine_survey from .matching import get_closest_tune_approach , get_lhc_tune_and_chroma_knobs , match_tunes_and_chromaticities from .orbit import get_current_orbit_setup , lhc_orbit_variables , setup_lhc_orbit from .parameters import beam_parameters from .plotters import AperturePlotter , DynamicAperturePlotter , PhaseSpacePlotter , TuneDiagramPlotter from .ptc import get_amplitude_detuning , get_rdts from .special import ( apply_lhc_colinearity_knob , apply_lhc_coupling_knob , apply_lhc_rigidity_waist_shift_knob , deactivate_lhc_arc_sextupoles , make_sixtrack_output , power_landau_octupoles , ) Sub-modules pyhdtoolkit.cpymadtools.constants pyhdtoolkit.cpymadtools.errors pyhdtoolkit.cpymadtools.generators pyhdtoolkit.cpymadtools.latwiss pyhdtoolkit.cpymadtools.matching pyhdtoolkit.cpymadtools.orbit pyhdtoolkit.cpymadtools.parameters pyhdtoolkit.cpymadtools.plotters pyhdtoolkit.cpymadtools.ptc pyhdtoolkit.cpymadtools.special","title":"Index"},{"location":"reference/pyhdtoolkit/cpymadtools/#module-pyhdtoolkitcpymadtools","text":"cpymadtools package ~ ~ ~ ~ ~ ~ ~ cpymadtools is a collection of utilities that integrate within my workflow with the cpymad library. View Source \"\"\" cpymadtools package ~~~~~~~~~~~~~~~~~~~ cpymadtools is a collection of utilities that integrate within my workflow with the `cpymad` library. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .errors import switch_magnetic_errors from .generators import LatticeGenerator from .latwiss import plot_latwiss , plot_machine_survey from .matching import get_closest_tune_approach , get_lhc_tune_and_chroma_knobs , match_tunes_and_chromaticities from .orbit import get_current_orbit_setup , lhc_orbit_variables , setup_lhc_orbit from .parameters import beam_parameters from .plotters import AperturePlotter , DynamicAperturePlotter , PhaseSpacePlotter , TuneDiagramPlotter from .ptc import get_amplitude_detuning , get_rdts from .special import ( apply_lhc_colinearity_knob , apply_lhc_coupling_knob , apply_lhc_rigidity_waist_shift_knob , deactivate_lhc_arc_sextupoles , make_sixtrack_output , power_landau_octupoles , )","title":"Module pyhdtoolkit.cpymadtools"},{"location":"reference/pyhdtoolkit/cpymadtools/#sub-modules","text":"pyhdtoolkit.cpymadtools.constants pyhdtoolkit.cpymadtools.errors pyhdtoolkit.cpymadtools.generators pyhdtoolkit.cpymadtools.latwiss pyhdtoolkit.cpymadtools.matching pyhdtoolkit.cpymadtools.orbit pyhdtoolkit.cpymadtools.parameters pyhdtoolkit.cpymadtools.plotters pyhdtoolkit.cpymadtools.ptc pyhdtoolkit.cpymadtools.special","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/cpymadtools/constants/","text":"Module pyhdtoolkit.cpymadtools.constants Module cpymadtools.constants Created on 2020.02.02 View Source \"\"\" Module cpymadtools.constants ---------------------------- Created on 2020.02.02 :author: Felix Soubelet (felix.soubelet@cern.ch) Specific constants to be used in cpymadtools functions, to help with consistency. \"\"\" LHC_CROSSING_SCHEMES = { \"flat\" : {}, \"lhc_inj\" : { \"on_x1\" : - 170 , \"on_sep1\" : - 2 , \"on_x2\" : 170 , \"on_sep2\" : 3.5 , \"on_x5\" : 170 , \"on_sep5\" : 2 , \"on_x8\" : - 170 , \"on_sep8\" : - 3.5 , \"phi_IR1\" : 90 , \"phi_IR5\" : 0 , }, \"lhc_top\" : { \"on_x1\" : - 160 , \"on_sep1\" : - 0.55 , \"on_x2\" : 200 , \"on_sep2\" : 1.4 , \"on_x5\" : 160 , \"on_sep5\" : 0.55 , \"on_oh5\" : - 1.8 , \"on_x8\" : - 250 , \"on_sep8\" : - 1 , \"phi_IR1\" : 90 , \"phi_IR5\" : 0 , }, \"hllhc_inj\" : { \"on_x1\" : 295 , \"on_sep1\" : - 2 , \"on_x2\" : 170 , \"on_sep2\" : 3.5 , \"on_x5\" : 295 , \"on_sep5\" : 2 , \"on_x8\" : - 170 , \"on_sep8\" : - 3.5 , # phis should be set by optics }, \"hllhc_top\" : { \"on_x1\" : 250 , \"on_sep1\" : - 0.75 , # 0.55 \"on_x2\" : 170 , \"on_sep2\" : 1 , # 1.4 \"on_x5\" : 250 , \"on_sep5\" : 0.75 , # 0.55 # 'on_oh5': -1.8, \"on_x8\" : - 200 , # - 250 \"on_sep8\" : - 1 , \"on_crab1\" : - 190 , \"on_crab5\" : - 190 , # phis should be set by optics }, } # All values are defined as multiples of 0.3 / Energy CORRECTOR_LIMITS = { \"HLLHC\" : dict ( # MQSX1 = mvars [ 'kmax_MQSXF'], MQSX1 = 0.600 / 0.050 , # 0.6 T . m @ 50 mm in IR1&IR5 MQSX2 = 1.360 / 0.017 , # 1.36 T @ 17 mm in IR2&IR8 # MCSX1 = mvars [ 'kmax_MCSXF'], MCSX1 = 0.050 * 2 / ( 0.050 ** 2 ), # 0.050 Tm @ 50 mm in IR1&IR5 MCSX2 = 0.028 * 2 / ( 0.017 ** 2 ), # 0.028 T @ 17 mm in IR2&IR8 # MCSSX1 = mvars [ 'kmax_MCSSXF'], MCSSX1 = 0.050 * 2 / ( 0.050 ** 2 ), # 0.050 Tm @ 50 mm in IR1&IR5 MCSSX2 = 0.11 * 2 / ( 0.017 ** 2 ), # 0.11 T @ 17 mm in IR2&IR8 # MCOX1 = mvars [ 'kmax_MCOXF'], MCOX1 = 0.030 * 6 / ( 0.050 ** 3 ), # 0.030 Tm @ 50 mm in IR1&IR5 MCOX2 = 0.045 * 6 / ( 0.017 ** 3 ), # 0.045 T @ 17 mm in IR2&IR8 # MCOSX1 = mvars [ 'kmax_MCOSXF'], MCOSX1 = 0.030 * 6 / ( 0.050 ** 3 ), # 0.030 Tm @ 50 mm in IR1&IR5 MCOSX2 = 0.048 * 6 / ( 0.017 ** 3 ), # 0.048 T @ 17 mm in IR2&IR8 # MCDX1 = mvars [ 'kmax_MCDXF'], MCDX1 = 0.030 * 24 / ( 0.050 ** 4 ), # 0.030 Tm @ 50 mm in IR1&IR5 # MCDSX1 = mvars [ 'kmax_MCDSXF'], MCDSX1 = 0.030 * 24 / ( 0.050 ** 4 ), # 0.030 Tm @ 50 mm in IR1&IR5 # MCTX1 = mvars [ 'kmax_MCTXF'], MCTX1 = 0.07 * 120 / ( 0.050 ** 5 ), # 0.070 Tm @ 50 mm in IR1&IR5 MCTX2 = 0.01 * 120 / ( 0.017 ** 5 ), # 0.010 Tm @ 17 mm in IR1&IR5 # MCTSX1 = mvars [ 'kmax_MCTSXF'], MCTSX1 = 0.07 * 120 / ( 0.050 ** 5 ), # 0.070 Tm @ 50 mm in IR1&IR5 MQT = 120 , # 120 T / m MQS = 120 , # 120 T / m MS = 1.280 * 2 / ( 0.017 ** 2 ), # 1.28 T @ 17 mm MSS = 1.280 * 2 / ( 0.017 ** 2 ), # 1.28 T @ 17 mm MCS = 0.471 * 2 / ( 0.017 ** 2 ), # 0.471 T @ 17 mm MCO = 0.040 * 6 / ( 0.017 ** 3 ), # 0.04 T @ 17 mm MCD = 0.100 * 24 / ( 0.017 ** 4 ), # 0.1 T @ 17 mm MO = 0.29 * 6 / ( 0.017 ** 3 ), # 0.29 T @ 17 mm ) } FD_FAMILIES = { \"MO\" , \"MS\" , \"MQT\" } # Magnets that have F and D families TWO_FAMILIES = { \"MS\" } # Magnets that have 1 and 2 families SPECIAL_FAMILIES = { \"MQS\" } # Magnets in every second arc Variables CORRECTOR_LIMITS FD_FAMILIES LHC_CROSSING_SCHEMES SPECIAL_FAMILIES TWO_FAMILIES","title":"Constants"},{"location":"reference/pyhdtoolkit/cpymadtools/constants/#module-pyhdtoolkitcpymadtoolsconstants","text":"Module cpymadtools.constants Created on 2020.02.02 View Source \"\"\" Module cpymadtools.constants ---------------------------- Created on 2020.02.02 :author: Felix Soubelet (felix.soubelet@cern.ch) Specific constants to be used in cpymadtools functions, to help with consistency. \"\"\" LHC_CROSSING_SCHEMES = { \"flat\" : {}, \"lhc_inj\" : { \"on_x1\" : - 170 , \"on_sep1\" : - 2 , \"on_x2\" : 170 , \"on_sep2\" : 3.5 , \"on_x5\" : 170 , \"on_sep5\" : 2 , \"on_x8\" : - 170 , \"on_sep8\" : - 3.5 , \"phi_IR1\" : 90 , \"phi_IR5\" : 0 , }, \"lhc_top\" : { \"on_x1\" : - 160 , \"on_sep1\" : - 0.55 , \"on_x2\" : 200 , \"on_sep2\" : 1.4 , \"on_x5\" : 160 , \"on_sep5\" : 0.55 , \"on_oh5\" : - 1.8 , \"on_x8\" : - 250 , \"on_sep8\" : - 1 , \"phi_IR1\" : 90 , \"phi_IR5\" : 0 , }, \"hllhc_inj\" : { \"on_x1\" : 295 , \"on_sep1\" : - 2 , \"on_x2\" : 170 , \"on_sep2\" : 3.5 , \"on_x5\" : 295 , \"on_sep5\" : 2 , \"on_x8\" : - 170 , \"on_sep8\" : - 3.5 , # phis should be set by optics }, \"hllhc_top\" : { \"on_x1\" : 250 , \"on_sep1\" : - 0.75 , # 0.55 \"on_x2\" : 170 , \"on_sep2\" : 1 , # 1.4 \"on_x5\" : 250 , \"on_sep5\" : 0.75 , # 0.55 # 'on_oh5': -1.8, \"on_x8\" : - 200 , # - 250 \"on_sep8\" : - 1 , \"on_crab1\" : - 190 , \"on_crab5\" : - 190 , # phis should be set by optics }, } # All values are defined as multiples of 0.3 / Energy CORRECTOR_LIMITS = { \"HLLHC\" : dict ( # MQSX1 = mvars [ 'kmax_MQSXF'], MQSX1 = 0.600 / 0.050 , # 0.6 T . m @ 50 mm in IR1&IR5 MQSX2 = 1.360 / 0.017 , # 1.36 T @ 17 mm in IR2&IR8 # MCSX1 = mvars [ 'kmax_MCSXF'], MCSX1 = 0.050 * 2 / ( 0.050 ** 2 ), # 0.050 Tm @ 50 mm in IR1&IR5 MCSX2 = 0.028 * 2 / ( 0.017 ** 2 ), # 0.028 T @ 17 mm in IR2&IR8 # MCSSX1 = mvars [ 'kmax_MCSSXF'], MCSSX1 = 0.050 * 2 / ( 0.050 ** 2 ), # 0.050 Tm @ 50 mm in IR1&IR5 MCSSX2 = 0.11 * 2 / ( 0.017 ** 2 ), # 0.11 T @ 17 mm in IR2&IR8 # MCOX1 = mvars [ 'kmax_MCOXF'], MCOX1 = 0.030 * 6 / ( 0.050 ** 3 ), # 0.030 Tm @ 50 mm in IR1&IR5 MCOX2 = 0.045 * 6 / ( 0.017 ** 3 ), # 0.045 T @ 17 mm in IR2&IR8 # MCOSX1 = mvars [ 'kmax_MCOSXF'], MCOSX1 = 0.030 * 6 / ( 0.050 ** 3 ), # 0.030 Tm @ 50 mm in IR1&IR5 MCOSX2 = 0.048 * 6 / ( 0.017 ** 3 ), # 0.048 T @ 17 mm in IR2&IR8 # MCDX1 = mvars [ 'kmax_MCDXF'], MCDX1 = 0.030 * 24 / ( 0.050 ** 4 ), # 0.030 Tm @ 50 mm in IR1&IR5 # MCDSX1 = mvars [ 'kmax_MCDSXF'], MCDSX1 = 0.030 * 24 / ( 0.050 ** 4 ), # 0.030 Tm @ 50 mm in IR1&IR5 # MCTX1 = mvars [ 'kmax_MCTXF'], MCTX1 = 0.07 * 120 / ( 0.050 ** 5 ), # 0.070 Tm @ 50 mm in IR1&IR5 MCTX2 = 0.01 * 120 / ( 0.017 ** 5 ), # 0.010 Tm @ 17 mm in IR1&IR5 # MCTSX1 = mvars [ 'kmax_MCTSXF'], MCTSX1 = 0.07 * 120 / ( 0.050 ** 5 ), # 0.070 Tm @ 50 mm in IR1&IR5 MQT = 120 , # 120 T / m MQS = 120 , # 120 T / m MS = 1.280 * 2 / ( 0.017 ** 2 ), # 1.28 T @ 17 mm MSS = 1.280 * 2 / ( 0.017 ** 2 ), # 1.28 T @ 17 mm MCS = 0.471 * 2 / ( 0.017 ** 2 ), # 0.471 T @ 17 mm MCO = 0.040 * 6 / ( 0.017 ** 3 ), # 0.04 T @ 17 mm MCD = 0.100 * 24 / ( 0.017 ** 4 ), # 0.1 T @ 17 mm MO = 0.29 * 6 / ( 0.017 ** 3 ), # 0.29 T @ 17 mm ) } FD_FAMILIES = { \"MO\" , \"MS\" , \"MQT\" } # Magnets that have F and D families TWO_FAMILIES = { \"MS\" } # Magnets that have 1 and 2 families SPECIAL_FAMILIES = { \"MQS\" } # Magnets in every second arc","title":"Module pyhdtoolkit.cpymadtools.constants"},{"location":"reference/pyhdtoolkit/cpymadtools/constants/#variables","text":"CORRECTOR_LIMITS FD_FAMILIES LHC_CROSSING_SCHEMES SPECIAL_FAMILIES TWO_FAMILIES","title":"Variables"},{"location":"reference/pyhdtoolkit/cpymadtools/errors/","text":"Module pyhdtoolkit.cpymadtools.errors Module cpymadtools.errors Created on 2020.02.03 View Source \"\"\" Module cpymadtools.errors ------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X errors setups and manipulatioins with a cpymad.madx.Madx object, mainly for LHC and HLLHC machines. \"\"\" from cpymad.madx import Madx from loguru import logger # ----- Utlites ----- # def switch_magnetic_errors ( cpymad_instance : Madx , ** kwargs ) -> None : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Applies magnetic field orders. This will only work for LHC and HLLHC machines. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. Keyword Args: default: sets global default to this value. Defaults to `False`. AB#: sets the default for all of that order, the order being the # number. A# or B#: sets the default for systematic and random of this id. A#s, B#r etc.: sets the specific value. In all kwargs, the order # should be in the range (1...15), where 1 == dipolar field. \"\"\" logger . debug ( \"Setting magnetic errors\" ) global_default = kwargs . get ( \"default\" , False ) for order in range ( 1 , 16 ): logger . trace ( f \"Setting up for order {order}\" ) order_default = kwargs . get ( f \"AB{order:d}\" , global_default ) for ab in \"AB\" : ab_default = kwargs . get ( f \"{ab}{order:d}\" , order_default ) for sr in \"sr\" : name = f \"{ab}{order:d}{sr}\" error_value = int ( kwargs . get ( name , ab_default )) logger . trace ( f \"Setting global for 'ON_{name}' to {error_value}\" ) cpymad_instance . globals [ f \"ON_{name}\" ] = error_value Functions switch_magnetic_errors def switch_magnetic_errors ( cpymad_instance : cpymad . madx . Madx , ** kwargs ) -> None INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Applies magnetic field orders. This will only work for LHC and HLLHC machines. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. Keyword Args: None default None sets global default to this value. Defaults to False . False AB# None sets the default for all of that order, the order being the # number. None A# or B# None sets the default for systematic and random of this id. None A#s, B#r etc. None sets the specific value. In all kwargs, the order # should be in the range (1...15), where 1 == dipolar field. None View Source def switch_magnetic_errors ( cpymad_instance : Madx , ** kwargs ) -> None : \" \"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Applies magnetic field orders. This will only work for LHC and HLLHC machines. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. Keyword Args: default: sets global default to this value. Defaults to `False`. AB#: sets the default for all of that order, the order being the # number. A# or B#: sets the default for systematic and random of this id. A#s, B#r etc.: sets the specific value. In all kwargs, the order # should be in the range (1...15), where 1 == dipolar field. \"\" \" logger . debug ( \"Setting magnetic errors\" ) global_default = kwargs . get ( \"default\" , False ) for order in range ( 1 , 16 ) : logger . trace ( f \"Setting up for order {order}\" ) order_default = kwargs . get ( f \"AB{order:d}\" , global_default ) for ab in \"AB\" : ab_default = kwargs . get ( f \"{ab}{order:d}\" , order_default ) for sr in \"sr\" : name = f \"{ab}{order:d}{sr}\" error_value = int ( kwargs . get ( name , ab_default )) logger . trace ( f \"Setting global for 'ON_{name}' to {error_value}\" ) cpymad_instance . globals [ f \"ON_{name}\" ] = error_value","title":"Errors"},{"location":"reference/pyhdtoolkit/cpymadtools/errors/#module-pyhdtoolkitcpymadtoolserrors","text":"Module cpymadtools.errors Created on 2020.02.03 View Source \"\"\" Module cpymadtools.errors ------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X errors setups and manipulatioins with a cpymad.madx.Madx object, mainly for LHC and HLLHC machines. \"\"\" from cpymad.madx import Madx from loguru import logger # ----- Utlites ----- # def switch_magnetic_errors ( cpymad_instance : Madx , ** kwargs ) -> None : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Applies magnetic field orders. This will only work for LHC and HLLHC machines. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. Keyword Args: default: sets global default to this value. Defaults to `False`. AB#: sets the default for all of that order, the order being the # number. A# or B#: sets the default for systematic and random of this id. A#s, B#r etc.: sets the specific value. In all kwargs, the order # should be in the range (1...15), where 1 == dipolar field. \"\"\" logger . debug ( \"Setting magnetic errors\" ) global_default = kwargs . get ( \"default\" , False ) for order in range ( 1 , 16 ): logger . trace ( f \"Setting up for order {order}\" ) order_default = kwargs . get ( f \"AB{order:d}\" , global_default ) for ab in \"AB\" : ab_default = kwargs . get ( f \"{ab}{order:d}\" , order_default ) for sr in \"sr\" : name = f \"{ab}{order:d}{sr}\" error_value = int ( kwargs . get ( name , ab_default )) logger . trace ( f \"Setting global for 'ON_{name}' to {error_value}\" ) cpymad_instance . globals [ f \"ON_{name}\" ] = error_value","title":"Module pyhdtoolkit.cpymadtools.errors"},{"location":"reference/pyhdtoolkit/cpymadtools/errors/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/errors/#switch_magnetic_errors","text":"def switch_magnetic_errors ( cpymad_instance : cpymad . madx . Madx , ** kwargs ) -> None INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Applies magnetic field orders. This will only work for LHC and HLLHC machines. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. Keyword Args: None default None sets global default to this value. Defaults to False . False AB# None sets the default for all of that order, the order being the # number. None A# or B# None sets the default for systematic and random of this id. None A#s, B#r etc. None sets the specific value. In all kwargs, the order # should be in the range (1...15), where 1 == dipolar field. None View Source def switch_magnetic_errors ( cpymad_instance : Madx , ** kwargs ) -> None : \" \"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Applies magnetic field orders. This will only work for LHC and HLLHC machines. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. Keyword Args: default: sets global default to this value. Defaults to `False`. AB#: sets the default for all of that order, the order being the # number. A# or B#: sets the default for systematic and random of this id. A#s, B#r etc.: sets the specific value. In all kwargs, the order # should be in the range (1...15), where 1 == dipolar field. \"\" \" logger . debug ( \"Setting magnetic errors\" ) global_default = kwargs . get ( \"default\" , False ) for order in range ( 1 , 16 ) : logger . trace ( f \"Setting up for order {order}\" ) order_default = kwargs . get ( f \"AB{order:d}\" , global_default ) for ab in \"AB\" : ab_default = kwargs . get ( f \"{ab}{order:d}\" , order_default ) for sr in \"sr\" : name = f \"{ab}{order:d}{sr}\" error_value = int ( kwargs . get ( name , ab_default )) logger . trace ( f \"Setting global for 'ON_{name}' to {error_value}\" ) cpymad_instance . globals [ f \"ON_{name}\" ] = error_value","title":"switch_magnetic_errors"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/","text":"Module pyhdtoolkit.cpymadtools.generators Module cpymadtools.generators Created on 2019.06.15 View Source \"\"\" Module cpymadtools.generators ----------------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions for generating different lattices for cpymad.madx.Madx input. \"\"\" # ----- Utlites ----- # class LatticeGenerator : \"\"\" A simple class to handle said functions. \"\"\" @staticmethod def generate_base_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_onesext_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, ks1, 0}; mod: multipole, knl:={0, 0, ks2, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_oneoct_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, 0, koct}; mod: multipole, knl:={0, 0, 0, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_tripleterrors_study_reference () -> str : \"\"\" Generate generic script for reference Twiss, to use in a `cpymad.madx.Madx` object. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 twiss; \"\"\" @staticmethod def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str : \"\"\" Generate generic script for tf_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. tf_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For field errors ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma Rr = 0.05; ! Radius for field errors (??) ON_B2R = 1; ! Activate field errors B2r = {tf_error}; ! Set field errors magnitude -> Units of B2 error (will be in 1E-4) exec, SetEfcomp_Q; ! Assign field errors !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" @staticmethod def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str : \"\"\" Generate generic script for ms_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. ms_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet MSErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For longitudinal missalignments ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma ealign, ds := {ms_error} * 1E-3 * TGAUSS(GCUTR); ! Gaussian missalignments in meters !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" Classes LatticeGenerator class LatticeGenerator ( / , * args , ** kwargs ) View Source class LatticeGenerator : \"\"\" A simple class to handle said functions. \"\"\" @staticmethod def generate_base_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_onesext_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, ks1, 0}; mod: multipole, knl:={0, 0, ks2, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_oneoct_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, 0, koct}; mod: multipole, knl:={0, 0, 0, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_tripleterrors_study_reference () -> str : \"\"\" Generate generic script for reference Twiss, to use in a `cpymad.madx.Madx` object. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 twiss; \"\"\" @staticmethod def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str : \"\"\" Generate generic script for tf_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. tf_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For field errors ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma Rr = 0.05; ! Radius for field errors (??) ON_B2R = 1; ! Activate field errors B2r = {tf_error}; ! Set field errors magnitude -> Units of B2 error (will be in 1E-4) exec, SetEfcomp_Q; ! Assign field errors !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" @staticmethod def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str : \"\"\" Generate generic script for ms_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. ms_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet MSErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For longitudinal missalignments ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma ealign, ds := {ms_error} * 1E-3 * TGAUSS(GCUTR); ! Gaussian missalignments in meters !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" Static methods generate_base_cas_lattice def generate_base_cas_lattice ( ) -> str Simple function to help unclutter the notebook. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_base_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" generate_oneoct_cas_lattice def generate_oneoct_cas_lattice ( ) -> str Simple function to help unclutter the notebook. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_oneoct_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, 0, koct}; mod: multipole, knl:={0, 0, 0, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" generate_onesext_cas_lattice def generate_onesext_cas_lattice ( ) -> str Simple function to help unclutter the notebook. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_onesext_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, ks1, 0}; mod: multipole, knl:={0, 0, ks2, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" generate_tripleterrors_study_mserror_job def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str Generate generic script for ms_error Twiss, to use in a cpymad.madx.Madx object. Parameters: Name Type Description Default rand_seed str the random seed to provide MAD for the errors distributions. None ms_error str the misalignment error value (along the s axis). None Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str : \"\"\" Generate generic script for ms_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. ms_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet MSErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For longitudinal missalignments ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma ealign, ds := {ms_error} * 1E-3 * TGAUSS(GCUTR); ! Gaussian missalignments in meters !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" generate_tripleterrors_study_reference def generate_tripleterrors_study_reference ( ) -> str Generate generic script for reference Twiss, to use in a cpymad.madx.Madx object. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_tripleterrors_study_reference () -> str : \"\"\" Generate generic script for reference Twiss, to use in a `cpymad.madx.Madx` object. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 twiss; \"\"\" generate_tripleterrors_study_tferror_job def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str Generate generic script for tf_error Twiss, to use in a cpymad.madx.Madx object. Parameters: Name Type Description Default rand_seed str the random seed to provide MAD for the errors distributions. None tf_error str the misalignment error value (along the s axis). None Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str : \"\"\" Generate generic script for tf_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. tf_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For field errors ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma Rr = 0.05; ! Radius for field errors (??) ON_B2R = 1; ! Activate field errors B2r = {tf_error}; ! Set field errors magnitude -> Units of B2 error (will be in 1E-4) exec, SetEfcomp_Q; ! Assign field errors !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\"","title":"Generators"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#module-pyhdtoolkitcpymadtoolsgenerators","text":"Module cpymadtools.generators Created on 2019.06.15 View Source \"\"\" Module cpymadtools.generators ----------------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions for generating different lattices for cpymad.madx.Madx input. \"\"\" # ----- Utlites ----- # class LatticeGenerator : \"\"\" A simple class to handle said functions. \"\"\" @staticmethod def generate_base_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_onesext_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, ks1, 0}; mod: multipole, knl:={0, 0, ks2, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_oneoct_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, 0, koct}; mod: multipole, knl:={0, 0, 0, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_tripleterrors_study_reference () -> str : \"\"\" Generate generic script for reference Twiss, to use in a `cpymad.madx.Madx` object. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 twiss; \"\"\" @staticmethod def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str : \"\"\" Generate generic script for tf_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. tf_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For field errors ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma Rr = 0.05; ! Radius for field errors (??) ON_B2R = 1; ! Activate field errors B2r = {tf_error}; ! Set field errors magnitude -> Units of B2 error (will be in 1E-4) exec, SetEfcomp_Q; ! Assign field errors !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" @staticmethod def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str : \"\"\" Generate generic script for ms_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. ms_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet MSErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For longitudinal missalignments ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma ealign, ds := {ms_error} * 1E-3 * TGAUSS(GCUTR); ! Gaussian missalignments in meters !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\"","title":"Module pyhdtoolkit.cpymadtools.generators"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#latticegenerator","text":"class LatticeGenerator ( / , * args , ** kwargs ) View Source class LatticeGenerator : \"\"\" A simple class to handle said functions. \"\"\" @staticmethod def generate_base_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_onesext_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, ks1, 0}; mod: multipole, knl:={0, 0, ks2, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_oneoct_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, 0, koct}; mod: multipole, knl:={0, 0, 0, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\" @staticmethod def generate_tripleterrors_study_reference () -> str : \"\"\" Generate generic script for reference Twiss, to use in a `cpymad.madx.Madx` object. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 twiss; \"\"\" @staticmethod def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str : \"\"\" Generate generic script for tf_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. tf_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For field errors ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma Rr = 0.05; ! Radius for field errors (??) ON_B2R = 1; ! Activate field errors B2r = {tf_error}; ! Set field errors magnitude -> Units of B2 error (will be in 1E-4) exec, SetEfcomp_Q; ! Assign field errors !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\" @staticmethod def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str : \"\"\" Generate generic script for ms_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. ms_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet MSErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For longitudinal missalignments ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma ealign, ds := {ms_error} * 1E-3 * TGAUSS(GCUTR); ! Gaussian missalignments in meters !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\"","title":"LatticeGenerator"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#generate_base_cas_lattice","text":"def generate_base_cas_lattice ( ) -> str Simple function to help unclutter the notebook. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_base_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\"","title":"generate_base_cas_lattice"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#generate_oneoct_cas_lattice","text":"def generate_oneoct_cas_lattice ( ) -> str Simple function to help unclutter the notebook. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_oneoct_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, 0, koct}; mod: multipole, knl:={0, 0, 0, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\"","title":"generate_oneoct_cas_lattice"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#generate_onesext_cas_lattice","text":"def generate_onesext_cas_lattice ( ) -> str Simple function to help unclutter the notebook. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_onesext_cas_lattice () -> str : \"\"\" Simple function to help unclutter the notebook. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" option, -info, -warn; TITLE, \u2019CAS2019 Project Team 3\u2019; ! PARAMETERS circumference = 1000.0; ncell = 24; lcell = circumference/ncell; lq = 3.00; angleBending = 2.0*pi/(4*ncell); ! STRENGTHS kqf = 0.0228 * lq; kqd = -0.0228 * lq; lsex = 0.00001; ks1 = 0; ks2 = 0; ! ELEMENTS mb:multipole, knl:={angleBending}; qf: multipole, knl:={0, kqf}; qd: multipole, knl:={0, kqd}; msf: multipole, knl:={0, 0, ksf}; msd: multipole, knl:={0, 0, ksd}; mof: multipole, knl:={0, 0, ks1, 0}; mod: multipole, knl:={0, 0, ks2, 0}; ! DECLARE SEQUENCE CAS3: sequence, refer=centre, l=circumference; start_machine: marker, at = 0; n = 1; qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mof: mof, at=(n-1)*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mod: mod, at=(n-1)*lcell + 0.50*lcell + 3*lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; while (n < ncell+1) { qf: qf, at=(n-1)*lcell; msf: msf, at=(n-1)*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.15*lcell; mb: mb, at=(n-1)*lcell + 0.35*lcell; qd: qd, at=(n-1)*lcell + 0.50*lcell; msd: msd, at=(n-1)*lcell + 0.50*lcell + lsex/2.0; mb: mb, at=(n-1)*lcell + 0.65*lcell; mb: mb, at=(n-1)*lcell + 0.85*lcell; at=(n-1)*lcell; n = n + 1; } end_machine: marker at=circumference; endsequence; ! MAKE BEAM beam, particle=proton, sequence=CAS3, energy=20.0; ! ACTIVATE SEQUENCE use, sequence=CAS3; select,flag=interpolate, class=drift, slice=5, range=#s/#e; ksf = 0; ksd = 0; ! INTERPOLATE select, flag=interpolate, class=drift, slice=10, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=5, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; ! TWISS select,flag=twiss, clear; select,flag=twiss, column=name ,s, x, y, betx, bety, mux, muy, dx, dy; twiss; \"\"\"","title":"generate_onesext_cas_lattice"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#generate_tripleterrors_study_mserror_job","text":"def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str Generate generic script for ms_error Twiss, to use in a cpymad.madx.Madx object. Parameters: Name Type Description Default rand_seed str the random seed to provide MAD for the errors distributions. None ms_error str the misalignment error value (along the s axis). None Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_tripleterrors_study_mserror_job ( rand_seed : str , ms_error : str ) -> str : \"\"\" Generate generic script for ms_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. ms_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet MSErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For longitudinal missalignments ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma ealign, ds := {ms_error} * 1E-3 * TGAUSS(GCUTR); ! Gaussian missalignments in meters !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\"","title":"generate_tripleterrors_study_mserror_job"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#generate_tripleterrors_study_reference","text":"def generate_tripleterrors_study_reference ( ) -> str Generate generic script for reference Twiss, to use in a cpymad.madx.Madx object. Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_tripleterrors_study_reference () -> str : \"\"\" Generate generic script for reference Twiss, to use in a `cpymad.madx.Madx` object. Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 twiss; \"\"\"","title":"generate_tripleterrors_study_reference"},{"location":"reference/pyhdtoolkit/cpymadtools/generators/#generate_tripleterrors_study_tferror_job","text":"def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str Generate generic script for tf_error Twiss, to use in a cpymad.madx.Madx object. Parameters: Name Type Description Default rand_seed str the random seed to provide MAD for the errors distributions. None tf_error str the misalignment error value (along the s axis). None Returns: Type Description None A string you can input into your cpymad.madx.Madx object. View Source @staticmethod def generate_tripleterrors_study_tferror_job ( rand_seed : str , tf_error : str ) -> str : \"\"\" Generate generic script for tf_error Twiss, to use in a `cpymad.madx.Madx` object. Args: rand_seed (str): the random seed to provide MAD for the errors distributions. tf_error (str): the misalignment error value (along the s axis). Returns: A string you can input into your `cpymad.madx.Madx` object. \"\"\" return f \"\"\" !####################### Make macros available ####################### option, -echo, -warn, -info; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / beta_beat . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / lhc . macros . madx \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / madx / lib / hllhc . macros . madx \"; title, \" HLLHC Triplet TFErrors to Beta - Beating \"; !####################### Call optics files ####################### call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / lhcrunIII . seq \"; call, file = \" / afs / cern . ch / work / f / fesoubel / public / Repositories / Beta - Beat . src / model / accelerators / lhc / hllhc1 .3 / main . seq \"; call, file = \" / afs / cern . ch / eng / lhc / optics / V6 .5 / errors / Esubroutines . madx \"; !####################### Calling modifiers for 15cm optics ####################### call, file = \" / afs / cern . ch / eng / lhc / optics / HLLHCV1 .3 / opt_150_150_150_150 . madx \"; !####################### Create beam ####################### exec, define_nominal_beams(); !####################### Flatten and set START point at ? ####################### exec, cycle_sequences(); !####################### Default crossing scheme ####################### exec, set_default_crossing_scheme(); !####################### Selecting to use Beam 1 ####################### use, period = LHCB1; !####################### Tune matching and Twiss nominal ####################### option, echo, warn, info; exec, match_tunes(62.31, 60.32, 1); ! Since we're using beam 1 exec, do_twiss_elements(LHCB1, \" . / twiss_nominal . dat \", 0.0); !####################### For field errors ####################### eoption, add, seed = {rand_seed}; ! Different seed every time select, flag=error, clear; select, flag=error, pattern = ^MQXF.*[RL][15]; ! Only triplets quadrupoles around IP1 and IP5 GCUTR = 3; ! Cut gaussians at 3 sigma Rr = 0.05; ! Radius for field errors (??) ON_B2R = 1; ! Activate field errors B2r = {tf_error}; ! Set field errors magnitude -> Units of B2 error (will be in 1E-4) exec, SetEfcomp_Q; ! Assign field errors !####################### Saving errors to file ####################### !esave, file=\" . / errors_file . dat \"; ! Will save the errors of chosen type. !####################### Tune matching and Twiss with errors ####################### exec, match_tunes(62.31, 60.32, 1); exec, do_twiss_elements(LHCB1, \" . / twiss_errors . dat \", 0.0); \"\"\"","title":"generate_tripleterrors_study_tferror_job"},{"location":"reference/pyhdtoolkit/cpymadtools/latwiss/","text":"Module pyhdtoolkit.cpymadtools.latwiss Module cpymadtools.latwiss Created on 2019.06.15 View Source \"\"\" Module cpymadtools.latwiss -------------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions to elegantly plot the Twiss parameters output of a cpymad.madx.Madx instance after it has ran, or machine survey. \"\"\" from typing import Dict , Tuple import matplotlib.axes import matplotlib.patches as patches import matplotlib.pyplot as plt import pandas as pd from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.plotting.settings import PLOT_PARAMS plt . rcParams . update ( PLOT_PARAMS ) # ----- Utilities ----- # def _plot_lattice_series ( ax : matplotlib . axes . Axes , series : pd . DataFrame , height : float = 1.0 , v_offset : float = 0.0 , color : str = \"r\" , alpha : float = 0.5 , lw : int = 3 , ) -> None : \"\"\" Plots the layout of your machine as a patch of rectangles for different element types. Original code from Guido Sterbini. Args: ax (matplotlib.axes.Axes): an existing matplotlib.axis `Axes` object to act on. series (pd.DataFrame): a dataframe with your elements' data. height (float): value to reach for the patch on the y axis. v_offset (float): vertical offset for the patch. color (str): color kwarg to transmit to pyplot. alpha (float): alpha kwarg to transmit to pyplot. lw (int): linewidth kwarg to transmit to pyplot. \"\"\" ax . add_patch ( patches . Rectangle ( ( series . s - series . l , v_offset - height / 2.0 ), series . l , # width height , # height color = color , alpha = alpha , lw = lw , ) ) def plot_latwiss ( cpymad_instance : Madx , title : str , figsize : Tuple [ int , int ] = ( 16 , 10 ), savefig : str = None , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_sextupoles : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ), ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is heavily refactored code, but the original is from Guido Sterbini. WARNING: This will FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_sextupoles (bool): if True, sextupole patches will be plotted on the layout subplot of the figure. Defaults to False. Sextupoles are plotted in yellow. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint: disable=too-many-arguments logger . debug ( \"Getting Twiss dataframe from cpymad\" ) twiss_df = cpymad_instance . table . twiss . dframe () figure = plt . figure ( figsize = figsize ) # Create a subplot for the lattice patches (takes a third of figure) logger . trace ( \"Setting up element patches subplots\" ) quadrupole_patches_axis = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) dipole_patches_axis = quadrupole_patches_axis . twinx () quadrupole_patches_axis . set_ylabel ( \"1/f=K1L [m$^{-1}$]\" , color = \"red\" ) # quadrupole in red quadrupole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"red\" ) dipole_patches_axis . set_ylabel ( \"$ \\\\ theta$=K0L [rad]\" , color = \"blue\" ) # dipoles in blue dipole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"blue\" ) quadrupole_patches_axis . set_ylim ( k1l_lim ) dipole_patches_axis . set_ylim ( k0l_lim ) dipole_patches_axis . grid ( False ) quadrupole_patches_axis . set_title ( title ) quadrupole_patches_axis . plot ( twiss_df . s , 0 * twiss_df . s , \"k\" ) # All elements can be defined as a 'multipole', but dipoles can also be defined as # 'rbend' or 'sbend', quadrupoles as 'quadrupoles' and sextupoles as 'sextupoles' logger . debug ( \"Extracting element-specific dataframes\" ) quadrupoles_df = twiss_df [ ( twiss_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( twiss_df . name . str . contains ( \"Q\" , case = False )) ] dipoles_df = twiss_df [ ( twiss_df . keyword . isin ([ \"multipole\" , \"rbend\" , \"sbend\" ])) & ( twiss_df . name . str . contains ( \"B\" , case = False )) ] sextupoles_df = twiss_df [ ( twiss_df . keyword . isin ([ \"multipole\" , \"sextupole\" ])) & ( twiss_df . name . str . contains ( \"S\" , case = False )) ] # Plotting dipole patches, beware 'sbend' and 'rbend' have an 'angle' value and not a 'k0l' if plot_dipoles : logger . debug ( \"Plotting dipole patches\" ) for _ , dipole in dipoles_df . iterrows (): if dipole . k0l != 0 : logger . trace ( \"Plotting dipole element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . k0l , v_offset = dipole . k0l / 2 , color = \"b\" , ) if dipole . angle != 0 : logger . trace ( \"Plotting 'sbend' / 'rbend' element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . angle , v_offset = dipole . angle / 2 , color = \"b\" , ) # Plotting the quadrupole patches if plot_quadrupoles : logger . debug ( \"Plotting quadrupole patches\" ) for _ , quad in quadrupoles_df . iterrows (): _plot_lattice_series ( quadrupole_patches_axis , quad , height = quad . k1l , v_offset = quad . k1l / 2 , color = \"r\" ) # Plotting the sextupole patches if plot_sextupoles : logger . debug ( \"Plotting sextupole patches\" ) for _ , sext in sextupoles_df . iterrows (): _plot_lattice_series ( dipole_patches_axis , sext , height = sext . k2l , v_offset = sext . k2l / 2 , color = \"y\" ) # Plotting beta functions on remaining two thirds of the figure logger . trace ( \"Setting up betatron functions subplot\" ) betatron_axis = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 2 , sharex = quadrupole_patches_axis ) betatron_axis . plot ( twiss_df . s , twiss_df . betx , label = \"$ \\\\ beta_x$\" , lw = 1.5 ) betatron_axis . plot ( twiss_df . s , twiss_df . bety , label = \"$ \\\\ beta_y$\" , lw = 1.5 ) betatron_axis . legend ( loc = 2 ) betatron_axis . set_ylabel ( \"$ \\\\ beta$-functions [m]\" ) if beta_ylim : logger . debug ( \"Setting ylim for betatron functions plot\" ) betatron_axis . set_ylim ( beta_ylim ) betatron_axis . set_xlabel ( \"s [m]\" ) # Plotting the dispersion logger . trace ( \"Setting up dispersion functions subplot\" ) dispertion_axis = betatron_axis . twinx () dispertion_axis . plot ( twiss_df . s , twiss_df . dx , color = \"brown\" , label = \"$D_x$\" , lw = 2 ) dispertion_axis . plot ( twiss_df . s , twiss_df . dy , ls = \"-.\" , color = \"sienna\" , label = \"$D_y$\" , lw = 2 ) dispertion_axis . legend ( loc = 1 ) dispertion_axis . set_ylabel ( \"Dispersions [m]\" , color = \"brown\" ) dispertion_axis . tick_params ( axis = \"y\" , labelcolor = \"brown\" ) dispertion_axis . grid ( False ) if disp_ylim : logger . debug ( \"Setting ylim for dispersion plot\" ) dispertion_axis . set_ylim ( disp_ylim ) if xlimits : logger . debug ( \"Setting xlim for longitudinal coordinate\" ) plt . xlim ( xlimits ) if savefig : logger . info ( f \"Saving latwiss plot as {savefig}\" ) plt . savefig ( savefig , format = \"pdf\" , dpi = 500 ) return figure def plot_machine_survey ( cpymad_instance : Madx , title : str = \"Machine Layout\" , figsize : Tuple [ int , int ] = ( 16 , 11 ), savefig : str = None , show_elements : bool = False , high_orders : bool = False , ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing the machine geometry in 2D. Original code is from Guido Sterbini. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. show_elements (bool): if True, will try to plot by differentiating elements. Experimental, defaults to False. high_orders (bool): if True, plot sextupoles and octupoles when show_elements is True, otherwise only up to quadrupoles. Defaults to False. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" logger . debug ( \"Getting machine survey from cpymad\" ) cpymad_instance . input ( \"survey;\" ) survey = cpymad_instance . table . survey . dframe () figure = plt . figure ( figsize = figsize ) if show_elements : logger . debug ( \"Plotting survey with elements differentiation\" ) element_dfs = _make_survey_groups ( survey ) plt . scatter ( element_dfs [ \"dipoles\" ] . z , element_dfs [ \"dipoles\" ] . x , marker = \".\" , c = element_dfs [ \"dipoles\" ] . s , cmap = \"copper\" , label = \"Dipoles\" , ) plt . scatter ( element_dfs [ \"quad_foc\" ] . z , element_dfs [ \"quad_foc\" ] . x , marker = \"o\" , color = \"blue\" , label = \"QF\" , ) plt . scatter ( element_dfs [ \"quad_defoc\" ] . z , element_dfs [ \"quad_defoc\" ] . x , marker = \"o\" , color = \"red\" , label = \"QD\" , ) if high_orders : logger . debug ( \"Plotting high order magnetic elements (up to octupoles)\" ) plt . scatter ( element_dfs [ \"sextupoles\" ] . z , element_dfs [ \"sextupoles\" ] . x , marker = \".\" , color = \"m\" , label = \"MS\" , ) plt . scatter ( element_dfs [ \"octupoles\" ] . z , element_dfs [ \"octupoles\" ] . x , marker = \".\" , color = \"cyan\" , label = \"MO\" , ) plt . legend ( loc = 2 ) else : logger . debug ( \"Plotting survey without elements differentiation\" ) plt . scatter ( survey . z , survey . x , c = survey . s ) plt . axis ( \"equal\" ) plt . colorbar () . set_label ( \"s [m]\" ) plt . xlabel ( \"z [m]\" ) plt . ylabel ( \"x [m]\" ) plt . title ( title ) if savefig : logger . info ( f \"Saving machine survey plot as {savefig}\" ) plt . savefig ( savefig , format = \"pdf\" , dpi = 500 ) return figure # ----- Helpers ----- # def _make_survey_groups ( survey_df : pd . DataFrame ) -> Dict [ str , pd . DataFrame ]: \"\"\" Gets a survey dataframe and returns different sub-dataframes corresponding to different magnetic elements. Args: survey_df (pd.DataFrame): machine survey dataframe obtained from your Madx instance, with <instance>.table.survey.dframe(). Returns: A dictionary containing a dataframe for dipoles, focusing quadrupoles, defocusing quadrupoles, sextupoles and octupoles. The keys are self-explanatory. \"\"\" logger . debug ( \"Getting different element groups dframes from MAD-X survey\" ) return { \"dipoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"sbend\" , \"rbend\" ])) & ( survey_df . name . str . contains ( \"B\" , case = False )) ], \"quad_foc\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) & ( survey_df . name . str . contains ( \"F\" , case = False )) ], \"quad_defoc\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) & ( survey_df . name . str . contains ( \"D\" , case = False )) ], \"sextupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"sextupole\" ])) & ( survey_df . name . str . contains ( \"S\" , case = False )) ], \"octupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"octupole\" ])) & ( survey_df . name . str . contains ( \"O\" , case = False )) ], } Variables PLOT_PARAMS Functions plot_latwiss def plot_latwiss ( cpymad_instance : cpymad . madx . Madx , title : str , figsize : Tuple [ int , int ] = ( 16 , 10 ), savefig : str = None , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_sextupoles : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ) ) -> matplotlib . figure . Figure Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is heavily refactored code, but the original is from Guido Sterbini. WARNING: This will FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None title str title of your plot. None figsize Tuple[int, int] size of the figure, defaults to (16, 10). None savefig str will save the figure if this is not None, using the string value passed. None xlimits Tuple[float, float] will implement xlim (for the s coordinate) if this is not None, using the tuple passed. None plot_dipoles bool if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. None plot_quadrupoles bool if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. None plot_sextupoles bool if True, sextupole patches will be plotted on the layout subplot of the figure. Defaults to False. Sextupoles are plotted in yellow. None disp_ylim Tuple[float, float] vertical axis limits for the dispersion values. Defaults to (-10, 125). None beta_ylim Tuple[float, float] vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. None k0l_lim Tuple[float, float] vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). None k1l_lim Tuple[float, float] vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source def plot_latwiss ( cpymad_instance : Madx , title : str , figsize : Tuple [ int, int ] = ( 16 , 10 ), savefig : str = None , xlimits : Tuple [ float, float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_sextupoles : bool = False , disp_ylim : Tuple [ float, float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float, float ] = None , k0l_lim : Tuple [ float, float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float, float ] = ( - 0.08 , 0.08 ), ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is heavily refactored code, but the original is from Guido Sterbini. WARNING: This will FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_sextupoles (bool): if True, sextupole patches will be plotted on the layout subplot of the figure. Defaults to False. Sextupoles are plotted in yellow. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint : disable = too - many - arguments logger . debug ( \"Getting Twiss dataframe from cpymad\" ) twiss_df = cpymad_instance . table . twiss . dframe () figure = plt . figure ( figsize = figsize ) # Create a subplot for the lattice patches ( takes a third of figure ) logger . trace ( \"Setting up element patches subplots\" ) quadrupole_patches_axis = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) dipole_patches_axis = quadrupole_patches_axis . twinx () quadrupole_patches_axis . set_ylabel ( \"1/f=K1L [m$^{-1}$]\" , color = \"red\" ) # quadrupole in red quadrupole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"red\" ) dipole_patches_axis . set_ylabel ( \"$\\\\theta$=K0L [rad]\" , color = \"blue\" ) # dipoles in blue dipole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"blue\" ) quadrupole_patches_axis . set_ylim ( k1l_lim ) dipole_patches_axis . set_ylim ( k0l_lim ) dipole_patches_axis . grid ( False ) quadrupole_patches_axis . set_title ( title ) quadrupole_patches_axis . plot ( twiss_df . s , 0 * twiss_df . s , \"k\" ) # All elements can be defined as a 'multipole' , but dipoles can also be defined as # 'rbend' or 'sbend' , quadrupoles as 'quadrupoles' and sextupoles as 'sextupoles' logger . debug ( \"Extracting element-specific dataframes\" ) quadrupoles_df = twiss_df [ (twiss_df.keyword.isin([\"multipole\", \"quadrupole\" ] )) & ( twiss_df . name . str . contains ( \"Q\" , case = False )) ] dipoles_df = twiss_df [ (twiss_df.keyword.isin([\"multipole\", \"rbend\", \"sbend\" ] )) & ( twiss_df . name . str . contains ( \"B\" , case = False )) ] sextupoles_df = twiss_df [ (twiss_df.keyword.isin([\"multipole\", \"sextupole\" ] )) & ( twiss_df . name . str . contains ( \"S\" , case = False )) ] # Plotting dipole patches , beware 'sbend' and 'rbend' have an 'angle' value and not a 'k0l' if plot_dipoles : logger . debug ( \"Plotting dipole patches\" ) for _ , dipole in dipoles_df . iterrows () : if dipole . k0l != 0 : logger . trace ( \"Plotting dipole element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . k0l , v_offset = dipole . k0l / 2 , color = \"b\" , ) if dipole . angle != 0 : logger . trace ( \"Plotting 'sbend' / 'rbend' element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . angle , v_offset = dipole . angle / 2 , color = \"b\" , ) # Plotting the quadrupole patches if plot_quadrupoles : logger . debug ( \"Plotting quadrupole patches\" ) for _ , quad in quadrupoles_df . iterrows () : _plot_lattice_series ( quadrupole_patches_axis , quad , height = quad . k1l , v_offset = quad . k1l / 2 , color = \"r\" ) # Plotting the sextupole patches if plot_sextupoles : logger . debug ( \"Plotting sextupole patches\" ) for _ , sext in sextupoles_df . iterrows () : _plot_lattice_series ( dipole_patches_axis , sext , height = sext . k2l , v_offset = sext . k2l / 2 , color = \"y\" ) # Plotting beta functions on remaining two thirds of the figure logger . trace ( \"Setting up betatron functions subplot\" ) betatron_axis = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 2 , sharex = quadrupole_patches_axis ) betatron_axis . plot ( twiss_df . s , twiss_df . betx , label = \"$\\\\beta_x$\" , lw = 1.5 ) betatron_axis . plot ( twiss_df . s , twiss_df . bety , label = \"$\\\\beta_y$\" , lw = 1.5 ) betatron_axis . legend ( loc = 2 ) betatron_axis . set_ylabel ( \"$\\\\beta$-functions [m]\" ) if beta_ylim : logger . debug ( \"Setting ylim for betatron functions plot\" ) betatron_axis . set_ylim ( beta_ylim ) betatron_axis . set_xlabel ( \"s [m]\" ) # Plotting the dispersion logger . trace ( \"Setting up dispersion functions subplot\" ) dispertion_axis = betatron_axis . twinx () dispertion_axis . plot ( twiss_df . s , twiss_df . dx , color = \"brown\" , label = \"$D_x$\" , lw = 2 ) dispertion_axis . plot ( twiss_df . s , twiss_df . dy , ls = \"-.\" , color = \"sienna\" , label = \"$D_y$\" , lw = 2 ) dispertion_axis . legend ( loc = 1 ) dispertion_axis . set_ylabel ( \"Dispersions [m]\" , color = \"brown\" ) dispertion_axis . tick_params ( axis = \"y\" , labelcolor = \"brown\" ) dispertion_axis . grid ( False ) if disp_ylim : logger . debug ( \"Setting ylim for dispersion plot\" ) dispertion_axis . set_ylim ( disp_ylim ) if xlimits : logger . debug ( \"Setting xlim for longitudinal coordinate\" ) plt . xlim ( xlimits ) if savefig : logger . info ( f \"Saving latwiss plot as {savefig}\" ) plt . savefig ( savefig , format = \"pdf\" , dpi = 500 ) return figure plot_machine_survey def plot_machine_survey ( cpymad_instance : cpymad . madx . Madx , title : str = 'Machine Layout' , figsize : Tuple [ int , int ] = ( 16 , 11 ), savefig : str = None , show_elements : bool = False , high_orders : bool = False ) -> matplotlib . figure . Figure Provided with an active Cpymad class after having ran a script, will create a plot representing the machine geometry in 2D. Original code is from Guido Sterbini. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None title str title of your plot. None figsize Tuple[int, int] size of the figure, defaults to (16, 10). None savefig str will save the figure if this is not None, using the string value passed. None show_elements bool if True, will try to plot by differentiating elements. Experimental, defaults to False. None high_orders bool if True, plot sextupoles and octupoles when show_elements is True, otherwise only up to quadrupoles. Defaults to False. None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source def plot_machine_survey ( cpymad_instance : Madx , title : str = \"Machine Layout\" , figsize : Tuple [ int, int ] = ( 16 , 11 ), savefig : str = None , show_elements : bool = False , high_orders : bool = False , ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing the machine geometry in 2D. Original code is from Guido Sterbini. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. show_elements (bool): if True, will try to plot by differentiating elements. Experimental, defaults to False. high_orders (bool): if True, plot sextupoles and octupoles when show_elements is True, otherwise only up to quadrupoles. Defaults to False. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" logger . debug ( \"Getting machine survey from cpymad\" ) cpymad_instance . input ( \"survey;\" ) survey = cpymad_instance . table . survey . dframe () figure = plt . figure ( figsize = figsize ) if show_elements : logger . debug ( \"Plotting survey with elements differentiation\" ) element_dfs = _make_survey_groups ( survey ) plt . scatter ( element_dfs [ \"dipoles\" ] . z , element_dfs [ \"dipoles\" ] . x , marker = \".\" , c = element_dfs [ \"dipoles\" ] . s , cmap = \"copper\" , label = \"Dipoles\" , ) plt . scatter ( element_dfs [ \"quad_foc\" ] . z , element_dfs [ \"quad_foc\" ] . x , marker = \"o\" , color = \"blue\" , label = \"QF\" , ) plt . scatter ( element_dfs [ \"quad_defoc\" ] . z , element_dfs [ \"quad_defoc\" ] . x , marker = \"o\" , color = \"red\" , label = \"QD\" , ) if high_orders : logger . debug ( \"Plotting high order magnetic elements (up to octupoles)\" ) plt . scatter ( element_dfs [ \"sextupoles\" ] . z , element_dfs [ \"sextupoles\" ] . x , marker = \".\" , color = \"m\" , label = \"MS\" , ) plt . scatter ( element_dfs [ \"octupoles\" ] . z , element_dfs [ \"octupoles\" ] . x , marker = \".\" , color = \"cyan\" , label = \"MO\" , ) plt . legend ( loc = 2 ) else : logger . debug ( \"Plotting survey without elements differentiation\" ) plt . scatter ( survey . z , survey . x , c = survey . s ) plt . axis ( \"equal\" ) plt . colorbar (). set_label ( \"s [m]\" ) plt . xlabel ( \"z [m]\" ) plt . ylabel ( \"x [m]\" ) plt . title ( title ) if savefig : logger . info ( f \"Saving machine survey plot as {savefig}\" ) plt . savefig ( savefig , format = \"pdf\" , dpi = 500 ) return figure","title":"Latwiss"},{"location":"reference/pyhdtoolkit/cpymadtools/latwiss/#module-pyhdtoolkitcpymadtoolslatwiss","text":"Module cpymadtools.latwiss Created on 2019.06.15 View Source \"\"\" Module cpymadtools.latwiss -------------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions to elegantly plot the Twiss parameters output of a cpymad.madx.Madx instance after it has ran, or machine survey. \"\"\" from typing import Dict , Tuple import matplotlib.axes import matplotlib.patches as patches import matplotlib.pyplot as plt import pandas as pd from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.plotting.settings import PLOT_PARAMS plt . rcParams . update ( PLOT_PARAMS ) # ----- Utilities ----- # def _plot_lattice_series ( ax : matplotlib . axes . Axes , series : pd . DataFrame , height : float = 1.0 , v_offset : float = 0.0 , color : str = \"r\" , alpha : float = 0.5 , lw : int = 3 , ) -> None : \"\"\" Plots the layout of your machine as a patch of rectangles for different element types. Original code from Guido Sterbini. Args: ax (matplotlib.axes.Axes): an existing matplotlib.axis `Axes` object to act on. series (pd.DataFrame): a dataframe with your elements' data. height (float): value to reach for the patch on the y axis. v_offset (float): vertical offset for the patch. color (str): color kwarg to transmit to pyplot. alpha (float): alpha kwarg to transmit to pyplot. lw (int): linewidth kwarg to transmit to pyplot. \"\"\" ax . add_patch ( patches . Rectangle ( ( series . s - series . l , v_offset - height / 2.0 ), series . l , # width height , # height color = color , alpha = alpha , lw = lw , ) ) def plot_latwiss ( cpymad_instance : Madx , title : str , figsize : Tuple [ int , int ] = ( 16 , 10 ), savefig : str = None , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_sextupoles : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ), ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is heavily refactored code, but the original is from Guido Sterbini. WARNING: This will FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_sextupoles (bool): if True, sextupole patches will be plotted on the layout subplot of the figure. Defaults to False. Sextupoles are plotted in yellow. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint: disable=too-many-arguments logger . debug ( \"Getting Twiss dataframe from cpymad\" ) twiss_df = cpymad_instance . table . twiss . dframe () figure = plt . figure ( figsize = figsize ) # Create a subplot for the lattice patches (takes a third of figure) logger . trace ( \"Setting up element patches subplots\" ) quadrupole_patches_axis = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) dipole_patches_axis = quadrupole_patches_axis . twinx () quadrupole_patches_axis . set_ylabel ( \"1/f=K1L [m$^{-1}$]\" , color = \"red\" ) # quadrupole in red quadrupole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"red\" ) dipole_patches_axis . set_ylabel ( \"$ \\\\ theta$=K0L [rad]\" , color = \"blue\" ) # dipoles in blue dipole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"blue\" ) quadrupole_patches_axis . set_ylim ( k1l_lim ) dipole_patches_axis . set_ylim ( k0l_lim ) dipole_patches_axis . grid ( False ) quadrupole_patches_axis . set_title ( title ) quadrupole_patches_axis . plot ( twiss_df . s , 0 * twiss_df . s , \"k\" ) # All elements can be defined as a 'multipole', but dipoles can also be defined as # 'rbend' or 'sbend', quadrupoles as 'quadrupoles' and sextupoles as 'sextupoles' logger . debug ( \"Extracting element-specific dataframes\" ) quadrupoles_df = twiss_df [ ( twiss_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( twiss_df . name . str . contains ( \"Q\" , case = False )) ] dipoles_df = twiss_df [ ( twiss_df . keyword . isin ([ \"multipole\" , \"rbend\" , \"sbend\" ])) & ( twiss_df . name . str . contains ( \"B\" , case = False )) ] sextupoles_df = twiss_df [ ( twiss_df . keyword . isin ([ \"multipole\" , \"sextupole\" ])) & ( twiss_df . name . str . contains ( \"S\" , case = False )) ] # Plotting dipole patches, beware 'sbend' and 'rbend' have an 'angle' value and not a 'k0l' if plot_dipoles : logger . debug ( \"Plotting dipole patches\" ) for _ , dipole in dipoles_df . iterrows (): if dipole . k0l != 0 : logger . trace ( \"Plotting dipole element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . k0l , v_offset = dipole . k0l / 2 , color = \"b\" , ) if dipole . angle != 0 : logger . trace ( \"Plotting 'sbend' / 'rbend' element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . angle , v_offset = dipole . angle / 2 , color = \"b\" , ) # Plotting the quadrupole patches if plot_quadrupoles : logger . debug ( \"Plotting quadrupole patches\" ) for _ , quad in quadrupoles_df . iterrows (): _plot_lattice_series ( quadrupole_patches_axis , quad , height = quad . k1l , v_offset = quad . k1l / 2 , color = \"r\" ) # Plotting the sextupole patches if plot_sextupoles : logger . debug ( \"Plotting sextupole patches\" ) for _ , sext in sextupoles_df . iterrows (): _plot_lattice_series ( dipole_patches_axis , sext , height = sext . k2l , v_offset = sext . k2l / 2 , color = \"y\" ) # Plotting beta functions on remaining two thirds of the figure logger . trace ( \"Setting up betatron functions subplot\" ) betatron_axis = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 2 , sharex = quadrupole_patches_axis ) betatron_axis . plot ( twiss_df . s , twiss_df . betx , label = \"$ \\\\ beta_x$\" , lw = 1.5 ) betatron_axis . plot ( twiss_df . s , twiss_df . bety , label = \"$ \\\\ beta_y$\" , lw = 1.5 ) betatron_axis . legend ( loc = 2 ) betatron_axis . set_ylabel ( \"$ \\\\ beta$-functions [m]\" ) if beta_ylim : logger . debug ( \"Setting ylim for betatron functions plot\" ) betatron_axis . set_ylim ( beta_ylim ) betatron_axis . set_xlabel ( \"s [m]\" ) # Plotting the dispersion logger . trace ( \"Setting up dispersion functions subplot\" ) dispertion_axis = betatron_axis . twinx () dispertion_axis . plot ( twiss_df . s , twiss_df . dx , color = \"brown\" , label = \"$D_x$\" , lw = 2 ) dispertion_axis . plot ( twiss_df . s , twiss_df . dy , ls = \"-.\" , color = \"sienna\" , label = \"$D_y$\" , lw = 2 ) dispertion_axis . legend ( loc = 1 ) dispertion_axis . set_ylabel ( \"Dispersions [m]\" , color = \"brown\" ) dispertion_axis . tick_params ( axis = \"y\" , labelcolor = \"brown\" ) dispertion_axis . grid ( False ) if disp_ylim : logger . debug ( \"Setting ylim for dispersion plot\" ) dispertion_axis . set_ylim ( disp_ylim ) if xlimits : logger . debug ( \"Setting xlim for longitudinal coordinate\" ) plt . xlim ( xlimits ) if savefig : logger . info ( f \"Saving latwiss plot as {savefig}\" ) plt . savefig ( savefig , format = \"pdf\" , dpi = 500 ) return figure def plot_machine_survey ( cpymad_instance : Madx , title : str = \"Machine Layout\" , figsize : Tuple [ int , int ] = ( 16 , 11 ), savefig : str = None , show_elements : bool = False , high_orders : bool = False , ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing the machine geometry in 2D. Original code is from Guido Sterbini. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. show_elements (bool): if True, will try to plot by differentiating elements. Experimental, defaults to False. high_orders (bool): if True, plot sextupoles and octupoles when show_elements is True, otherwise only up to quadrupoles. Defaults to False. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" logger . debug ( \"Getting machine survey from cpymad\" ) cpymad_instance . input ( \"survey;\" ) survey = cpymad_instance . table . survey . dframe () figure = plt . figure ( figsize = figsize ) if show_elements : logger . debug ( \"Plotting survey with elements differentiation\" ) element_dfs = _make_survey_groups ( survey ) plt . scatter ( element_dfs [ \"dipoles\" ] . z , element_dfs [ \"dipoles\" ] . x , marker = \".\" , c = element_dfs [ \"dipoles\" ] . s , cmap = \"copper\" , label = \"Dipoles\" , ) plt . scatter ( element_dfs [ \"quad_foc\" ] . z , element_dfs [ \"quad_foc\" ] . x , marker = \"o\" , color = \"blue\" , label = \"QF\" , ) plt . scatter ( element_dfs [ \"quad_defoc\" ] . z , element_dfs [ \"quad_defoc\" ] . x , marker = \"o\" , color = \"red\" , label = \"QD\" , ) if high_orders : logger . debug ( \"Plotting high order magnetic elements (up to octupoles)\" ) plt . scatter ( element_dfs [ \"sextupoles\" ] . z , element_dfs [ \"sextupoles\" ] . x , marker = \".\" , color = \"m\" , label = \"MS\" , ) plt . scatter ( element_dfs [ \"octupoles\" ] . z , element_dfs [ \"octupoles\" ] . x , marker = \".\" , color = \"cyan\" , label = \"MO\" , ) plt . legend ( loc = 2 ) else : logger . debug ( \"Plotting survey without elements differentiation\" ) plt . scatter ( survey . z , survey . x , c = survey . s ) plt . axis ( \"equal\" ) plt . colorbar () . set_label ( \"s [m]\" ) plt . xlabel ( \"z [m]\" ) plt . ylabel ( \"x [m]\" ) plt . title ( title ) if savefig : logger . info ( f \"Saving machine survey plot as {savefig}\" ) plt . savefig ( savefig , format = \"pdf\" , dpi = 500 ) return figure # ----- Helpers ----- # def _make_survey_groups ( survey_df : pd . DataFrame ) -> Dict [ str , pd . DataFrame ]: \"\"\" Gets a survey dataframe and returns different sub-dataframes corresponding to different magnetic elements. Args: survey_df (pd.DataFrame): machine survey dataframe obtained from your Madx instance, with <instance>.table.survey.dframe(). Returns: A dictionary containing a dataframe for dipoles, focusing quadrupoles, defocusing quadrupoles, sextupoles and octupoles. The keys are self-explanatory. \"\"\" logger . debug ( \"Getting different element groups dframes from MAD-X survey\" ) return { \"dipoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"sbend\" , \"rbend\" ])) & ( survey_df . name . str . contains ( \"B\" , case = False )) ], \"quad_foc\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) & ( survey_df . name . str . contains ( \"F\" , case = False )) ], \"quad_defoc\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) & ( survey_df . name . str . contains ( \"D\" , case = False )) ], \"sextupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"sextupole\" ])) & ( survey_df . name . str . contains ( \"S\" , case = False )) ], \"octupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"octupole\" ])) & ( survey_df . name . str . contains ( \"O\" , case = False )) ], }","title":"Module pyhdtoolkit.cpymadtools.latwiss"},{"location":"reference/pyhdtoolkit/cpymadtools/latwiss/#variables","text":"PLOT_PARAMS","title":"Variables"},{"location":"reference/pyhdtoolkit/cpymadtools/latwiss/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/latwiss/#plot_latwiss","text":"def plot_latwiss ( cpymad_instance : cpymad . madx . Madx , title : str , figsize : Tuple [ int , int ] = ( 16 , 10 ), savefig : str = None , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_sextupoles : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ) ) -> matplotlib . figure . Figure Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is heavily refactored code, but the original is from Guido Sterbini. WARNING: This will FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None title str title of your plot. None figsize Tuple[int, int] size of the figure, defaults to (16, 10). None savefig str will save the figure if this is not None, using the string value passed. None xlimits Tuple[float, float] will implement xlim (for the s coordinate) if this is not None, using the tuple passed. None plot_dipoles bool if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. None plot_quadrupoles bool if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. None plot_sextupoles bool if True, sextupole patches will be plotted on the layout subplot of the figure. Defaults to False. Sextupoles are plotted in yellow. None disp_ylim Tuple[float, float] vertical axis limits for the dispersion values. Defaults to (-10, 125). None beta_ylim Tuple[float, float] vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. None k0l_lim Tuple[float, float] vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). None k1l_lim Tuple[float, float] vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source def plot_latwiss ( cpymad_instance : Madx , title : str , figsize : Tuple [ int, int ] = ( 16 , 10 ), savefig : str = None , xlimits : Tuple [ float, float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_sextupoles : bool = False , disp_ylim : Tuple [ float, float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float, float ] = None , k0l_lim : Tuple [ float, float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float, float ] = ( - 0.08 , 0.08 ), ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is heavily refactored code, but the original is from Guido Sterbini. WARNING: This will FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_sextupoles (bool): if True, sextupole patches will be plotted on the layout subplot of the figure. Defaults to False. Sextupoles are plotted in yellow. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint : disable = too - many - arguments logger . debug ( \"Getting Twiss dataframe from cpymad\" ) twiss_df = cpymad_instance . table . twiss . dframe () figure = plt . figure ( figsize = figsize ) # Create a subplot for the lattice patches ( takes a third of figure ) logger . trace ( \"Setting up element patches subplots\" ) quadrupole_patches_axis = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) dipole_patches_axis = quadrupole_patches_axis . twinx () quadrupole_patches_axis . set_ylabel ( \"1/f=K1L [m$^{-1}$]\" , color = \"red\" ) # quadrupole in red quadrupole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"red\" ) dipole_patches_axis . set_ylabel ( \"$\\\\theta$=K0L [rad]\" , color = \"blue\" ) # dipoles in blue dipole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"blue\" ) quadrupole_patches_axis . set_ylim ( k1l_lim ) dipole_patches_axis . set_ylim ( k0l_lim ) dipole_patches_axis . grid ( False ) quadrupole_patches_axis . set_title ( title ) quadrupole_patches_axis . plot ( twiss_df . s , 0 * twiss_df . s , \"k\" ) # All elements can be defined as a 'multipole' , but dipoles can also be defined as # 'rbend' or 'sbend' , quadrupoles as 'quadrupoles' and sextupoles as 'sextupoles' logger . debug ( \"Extracting element-specific dataframes\" ) quadrupoles_df = twiss_df [ (twiss_df.keyword.isin([\"multipole\", \"quadrupole\" ] )) & ( twiss_df . name . str . contains ( \"Q\" , case = False )) ] dipoles_df = twiss_df [ (twiss_df.keyword.isin([\"multipole\", \"rbend\", \"sbend\" ] )) & ( twiss_df . name . str . contains ( \"B\" , case = False )) ] sextupoles_df = twiss_df [ (twiss_df.keyword.isin([\"multipole\", \"sextupole\" ] )) & ( twiss_df . name . str . contains ( \"S\" , case = False )) ] # Plotting dipole patches , beware 'sbend' and 'rbend' have an 'angle' value and not a 'k0l' if plot_dipoles : logger . debug ( \"Plotting dipole patches\" ) for _ , dipole in dipoles_df . iterrows () : if dipole . k0l != 0 : logger . trace ( \"Plotting dipole element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . k0l , v_offset = dipole . k0l / 2 , color = \"b\" , ) if dipole . angle != 0 : logger . trace ( \"Plotting 'sbend' / 'rbend' element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . angle , v_offset = dipole . angle / 2 , color = \"b\" , ) # Plotting the quadrupole patches if plot_quadrupoles : logger . debug ( \"Plotting quadrupole patches\" ) for _ , quad in quadrupoles_df . iterrows () : _plot_lattice_series ( quadrupole_patches_axis , quad , height = quad . k1l , v_offset = quad . k1l / 2 , color = \"r\" ) # Plotting the sextupole patches if plot_sextupoles : logger . debug ( \"Plotting sextupole patches\" ) for _ , sext in sextupoles_df . iterrows () : _plot_lattice_series ( dipole_patches_axis , sext , height = sext . k2l , v_offset = sext . k2l / 2 , color = \"y\" ) # Plotting beta functions on remaining two thirds of the figure logger . trace ( \"Setting up betatron functions subplot\" ) betatron_axis = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 2 , sharex = quadrupole_patches_axis ) betatron_axis . plot ( twiss_df . s , twiss_df . betx , label = \"$\\\\beta_x$\" , lw = 1.5 ) betatron_axis . plot ( twiss_df . s , twiss_df . bety , label = \"$\\\\beta_y$\" , lw = 1.5 ) betatron_axis . legend ( loc = 2 ) betatron_axis . set_ylabel ( \"$\\\\beta$-functions [m]\" ) if beta_ylim : logger . debug ( \"Setting ylim for betatron functions plot\" ) betatron_axis . set_ylim ( beta_ylim ) betatron_axis . set_xlabel ( \"s [m]\" ) # Plotting the dispersion logger . trace ( \"Setting up dispersion functions subplot\" ) dispertion_axis = betatron_axis . twinx () dispertion_axis . plot ( twiss_df . s , twiss_df . dx , color = \"brown\" , label = \"$D_x$\" , lw = 2 ) dispertion_axis . plot ( twiss_df . s , twiss_df . dy , ls = \"-.\" , color = \"sienna\" , label = \"$D_y$\" , lw = 2 ) dispertion_axis . legend ( loc = 1 ) dispertion_axis . set_ylabel ( \"Dispersions [m]\" , color = \"brown\" ) dispertion_axis . tick_params ( axis = \"y\" , labelcolor = \"brown\" ) dispertion_axis . grid ( False ) if disp_ylim : logger . debug ( \"Setting ylim for dispersion plot\" ) dispertion_axis . set_ylim ( disp_ylim ) if xlimits : logger . debug ( \"Setting xlim for longitudinal coordinate\" ) plt . xlim ( xlimits ) if savefig : logger . info ( f \"Saving latwiss plot as {savefig}\" ) plt . savefig ( savefig , format = \"pdf\" , dpi = 500 ) return figure","title":"plot_latwiss"},{"location":"reference/pyhdtoolkit/cpymadtools/latwiss/#plot_machine_survey","text":"def plot_machine_survey ( cpymad_instance : cpymad . madx . Madx , title : str = 'Machine Layout' , figsize : Tuple [ int , int ] = ( 16 , 11 ), savefig : str = None , show_elements : bool = False , high_orders : bool = False ) -> matplotlib . figure . Figure Provided with an active Cpymad class after having ran a script, will create a plot representing the machine geometry in 2D. Original code is from Guido Sterbini. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None title str title of your plot. None figsize Tuple[int, int] size of the figure, defaults to (16, 10). None savefig str will save the figure if this is not None, using the string value passed. None show_elements bool if True, will try to plot by differentiating elements. Experimental, defaults to False. None high_orders bool if True, plot sextupoles and octupoles when show_elements is True, otherwise only up to quadrupoles. Defaults to False. None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source def plot_machine_survey ( cpymad_instance : Madx , title : str = \"Machine Layout\" , figsize : Tuple [ int, int ] = ( 16 , 11 ), savefig : str = None , show_elements : bool = False , high_orders : bool = False , ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing the machine geometry in 2D. Original code is from Guido Sterbini. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. show_elements (bool): if True, will try to plot by differentiating elements. Experimental, defaults to False. high_orders (bool): if True, plot sextupoles and octupoles when show_elements is True, otherwise only up to quadrupoles. Defaults to False. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" logger . debug ( \"Getting machine survey from cpymad\" ) cpymad_instance . input ( \"survey;\" ) survey = cpymad_instance . table . survey . dframe () figure = plt . figure ( figsize = figsize ) if show_elements : logger . debug ( \"Plotting survey with elements differentiation\" ) element_dfs = _make_survey_groups ( survey ) plt . scatter ( element_dfs [ \"dipoles\" ] . z , element_dfs [ \"dipoles\" ] . x , marker = \".\" , c = element_dfs [ \"dipoles\" ] . s , cmap = \"copper\" , label = \"Dipoles\" , ) plt . scatter ( element_dfs [ \"quad_foc\" ] . z , element_dfs [ \"quad_foc\" ] . x , marker = \"o\" , color = \"blue\" , label = \"QF\" , ) plt . scatter ( element_dfs [ \"quad_defoc\" ] . z , element_dfs [ \"quad_defoc\" ] . x , marker = \"o\" , color = \"red\" , label = \"QD\" , ) if high_orders : logger . debug ( \"Plotting high order magnetic elements (up to octupoles)\" ) plt . scatter ( element_dfs [ \"sextupoles\" ] . z , element_dfs [ \"sextupoles\" ] . x , marker = \".\" , color = \"m\" , label = \"MS\" , ) plt . scatter ( element_dfs [ \"octupoles\" ] . z , element_dfs [ \"octupoles\" ] . x , marker = \".\" , color = \"cyan\" , label = \"MO\" , ) plt . legend ( loc = 2 ) else : logger . debug ( \"Plotting survey without elements differentiation\" ) plt . scatter ( survey . z , survey . x , c = survey . s ) plt . axis ( \"equal\" ) plt . colorbar (). set_label ( \"s [m]\" ) plt . xlabel ( \"z [m]\" ) plt . ylabel ( \"x [m]\" ) plt . title ( title ) if savefig : logger . info ( f \"Saving machine survey plot as {savefig}\" ) plt . savefig ( savefig , format = \"pdf\" , dpi = 500 ) return figure","title":"plot_machine_survey"},{"location":"reference/pyhdtoolkit/cpymadtools/matching/","text":"Module pyhdtoolkit.cpymadtools.matching Module cpymadtools.matching Created on 2020.02.03 View Source \"\"\" Module cpymadtools.matching --------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X matchings with a cpymad.madx.Madx object. \"\"\" from typing import Dict , Sequence , Tuple from cpymad.madx import Madx from loguru import logger # ----- Utlites ----- # def get_lhc_tune_and_chroma_knobs ( accelerator : str , beam : int = 1 , telescopic_squeeze : bool = False ) -> Tuple [ str , str , str , str ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get names of knobs needed to match tunes and chromaticities as a tuple of strings. Args: accelerator (str): Accelerator either 'LHC' (dQ[xy], dQp[xy] knobs) or 'HLLHC' (kqt[fd], ks[fd] knobs). beam (int): Beam to use, for the knob names. telescopic_squeeze (bool): if set to True, returns the knobs for Telescopic Squeeze configuration. Defaults to False. Returns: Tuple of strings with knobs for `(qx, qy, dqx, dqy)`. \"\"\" beam = 2 if beam == 4 else beam suffix = \"_sq\" if telescopic_squeeze else \"\" if accelerator . upper () not in ( \"LHC\" , \"HLLHC\" ): logger . error ( \"Invalid accelerator name, only 'LHC' and 'HLLHC' implemented\" ) raise NotImplementedError ( f \"Accelerator '{accelerator}' not implemented.\" ) return { \"LHC\" : ( f \"dQx.b{beam}{suffix}\" , f \"dQy.b{beam}{suffix}\" , f \"dQpx.b{beam}{suffix}\" , f \"dQpy.b{beam}{suffix}\" , ), \"HLLHC\" : ( f \"kqtf.b{beam}{suffix}\" , f \"kqtd.b{beam}{suffix}\" , f \"ksf.b{beam}{suffix}\" , f \"ksd.b{beam}{suffix}\" , ), }[ accelerator . upper ()] def match_tunes_and_chromaticities ( cpymad_instance : Madx , accelerator : str = None , sequence : str = None , q1_target : float = None , q2_target : float = None , dq1_target : float = None , dq2_target : float = None , varied_knobs : Sequence [ str ] = None , step : float = 1e-7 , calls : int = 100 , tolerance : float = 1e-21 , ) -> None : \"\"\" Provided with an active `cpymad` class after having ran a script, will run an additional matching command to reach the provided values for tunes and chromaticities. Tune matching is always performed. If chromaticity target values are given, then a matching is done for them, followed by an additionnal matching for both tunes and chromaticities. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. q1_target (float): horizontal tune to match to. q2_target (float): vertical tune to match to. dq1_target (float): horizontal chromaticity to match to. dq2_target (float): vertical chromaticity to match to. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default {accelerator.upper()} values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ - 1 ])) def match ( * args , ** kwargs ): \"\"\"Create matching commands for kwarg targets, varying the given args.\"\"\" logger . debug ( f \"Executing matching commands, using sequence '{sequence}'\" ) cpymad_instance . command . match ( chrom = True ) logger . trace ( f \"Targets are given as {kwargs}\" ) cpymad_instance . command . global_ ( sequence = sequence , ** kwargs ) for variable_name in args : logger . trace ( f \"Creating vary command for knob '{variable_name}'\" ) cpymad_instance . command . vary ( name = variable_name , step = step ) cpymad_instance . command . lmdif ( calls = calls , tolerance = tolerance ) cpymad_instance . command . endmatch () logger . trace ( \"Performing routine TWISS\" ) cpymad_instance . twiss () # prevents errors if the user forget to do so before querying tables if q1_target and q2_target and dq1_target and dq2_target : logger . info ( f \"Doing combined matching to Qx={q1_target}, Qy={q2_target}, \" f \"dq={dq1_target}, dqy={dq2_target} for sequence '{sequence}'\" ) logger . trace ( f \"Vary knobs sent are {varied_knobs}\" ) match ( * varied_knobs , q1 = q1_target , q2 = q2_target , dq1 = dq1_target , dq2 = dq2_target ) elif q1_target and q2_target : logger . info ( f \"Matching tunes to Qx={q1_target}, Qy={q2_target} for sequence '{sequence}'\" ) logger . trace ( f \"Vary knobs sent are {varied_knobs[:2]}\" ) match ( * varied_knobs [: 2 ], q1 = q1_target , q2 = q2_target ) # first two in varied_knobs are tune knobs def get_closest_tune_approach ( cpymad_instance : Madx , accelerator : str = None , sequence : str = None , varied_knobs : Sequence [ str ] = None , step : float = 1e-7 , calls : float = 100 , tolerance : float = 1e-21 , ) -> float : \"\"\" Provided with an active `cpymad` class after having ran a script, tries to match the tunes to their mid-fractional tunes. The difference between this mid-tune and the actual matched tune is the closest tune approach. NOTA BENE: This assumes your lattice has previously been matched to desired tunes and chromaticities, as it will determine the appropriate targets from the Madx instance's internal tables. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. Returns: The closest tune approach, in absolute value. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default {accelerator.upper()} values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ - 1 ])) logger . debug ( \"Saving knob values to restore after closest tune approach\" ) saved_knobs : Dict [ str , float ] = { knob : cpymad_instance . globals [ knob ] for knob in varied_knobs } logger . trace ( f \"Saved knobs are {saved_knobs}\" ) logger . debug ( \"Retrieving tunes and chromaticities from internal tables\" ) q1 , q2 = cpymad_instance . table . summ . q1 [ 0 ], cpymad_instance . table . summ . q2 [ 0 ] dq1 , dq2 = cpymad_instance . table . summ . dq1 [ 0 ], cpymad_instance . table . summ . dq2 [ 0 ] logger . trace ( f \"Retrieved values are q1 = {q1}, q2 = {q2}, dq1 = {dq1}, dq2 = {dq2}\" ) logger . debug ( \"Determining target tunes for closest approach\" ) middle_of_fractional_tunes = ( _fractional_tune ( q1 ) + _fractional_tune ( q2 )) / 2 qx_target = int ( q1 ) + middle_of_fractional_tunes qy_target = int ( q2 ) + middle_of_fractional_tunes logger . trace ( f \"Targeting tunes Qx = {qx_target} | Qy = {qy_target}\" ) logger . info ( \"Performing closest tune approach routine, matching should fail at DeltaQ = dqmin\" ) match_tunes_and_chromaticities ( cpymad_instance , accelerator , sequence , qx_target , qy_target , # dq1, # remove? # dq2, # remove? varied_knobs = varied_knobs , step = step , calls = calls , tolerance = tolerance , ) logger . debug ( \"Retrieving tune separation from internal tables\" ) dqmin = cpymad_instance . table . summ . q1 [ 0 ] - cpymad_instance . table . summ . q2 [ 0 ] - ( int ( q1 ) - int ( q2 )) logger . info ( \"Restoring saved knobs\" ) for knob , knob_value in saved_knobs . items (): cpymad_instance . globals [ knob ] = knob_value cpymad_instance . twiss () return abs ( dqmin ) # ----- Helpers ----- # def _fractional_tune ( tune : float ) -> float : \"\"\" Return only the fractional part of a tune value. Args: tune (float): tune value. Returns: The fractional part. \"\"\" return tune - int ( tune ) # ok since int truncates to lower integer Functions get_closest_tune_approach def get_closest_tune_approach ( cpymad_instance : cpymad . madx . Madx , accelerator : str = None , sequence : str = None , varied_knobs : Sequence [ str ] = None , step : float = 1e-07 , calls : float = 100 , tolerance : float = 1e-21 ) -> float Provided with an active cpymad class after having ran a script, tries to match the tunes to their mid-fractional tunes. The difference between this mid-tune and the actual matched tune is the closest tune approach. NOTA BENE: This assumes your lattice has previously been matched to desired tunes and chromaticities, as it will determine the appropriate targets from the Madx instance's internal tables. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None accelerator str name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. None sequence str name of the sequence you want to activate for the tune matching. None varied_knobs Sequence[str] the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. None step float step size to use when varying knobs. None calls int max number of varying calls to perform. None tolerance float tolerance for successfull matching. None Returns: Type Description None The closest tune approach, in absolute value. View Source def get_closest_tune_approach ( cpymad_instance : Madx , accelerator : str = None , sequence : str = None , varied_knobs : Sequence [ str ] = None , step : float = 1e-7 , calls : float = 100 , tolerance : float = 1e-21 , ) -> float : \"\"\" Provided with an active `cpymad` class after having ran a script, tries to match the tunes to their mid-fractional tunes. The difference between this mid-tune and the actual matched tune is the closest tune approach. NOTA BENE: This assumes your lattice has previously been matched to desired tunes and chromaticities, as it will determine the appropriate targets from the Madx instance's internal tables. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\" kqf \", \" ksd \", \" kqf \", \" kqd \"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. Returns: The closest tune approach, in absolute value. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default {accelerator.upper()} values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ -1 ] )) logger . debug ( \"Saving knob values to restore after closest tune approach\" ) saved_knobs : Dict [ str, float ] = { knob : cpymad_instance . globals [ knob ] for knob in varied_knobs } logger . trace ( f \"Saved knobs are {saved_knobs}\" ) logger . debug ( \"Retrieving tunes and chromaticities from internal tables\" ) q1 , q2 = cpymad_instance . table . summ . q1 [ 0 ] , cpymad_instance . table . summ . q2 [ 0 ] dq1 , dq2 = cpymad_instance . table . summ . dq1 [ 0 ] , cpymad_instance . table . summ . dq2 [ 0 ] logger . trace ( f \"Retrieved values are q1 = {q1}, q2 = {q2}, dq1 = {dq1}, dq2 = {dq2}\" ) logger . debug ( \"Determining target tunes for closest approach\" ) middle_of_fractional_tunes = ( _fractional_tune ( q1 ) + _fractional_tune ( q2 )) / 2 qx_target = int ( q1 ) + middle_of_fractional_tunes qy_target = int ( q2 ) + middle_of_fractional_tunes logger . trace ( f \"Targeting tunes Qx = {qx_target} | Qy = {qy_target}\" ) logger . info ( \"Performing closest tune approach routine, matching should fail at DeltaQ = dqmin\" ) match_tunes_and_chromaticities ( cpymad_instance , accelerator , sequence , qx_target , qy_target , # dq1 , # remove ? # dq2 , # remove ? varied_knobs = varied_knobs , step = step , calls = calls , tolerance = tolerance , ) logger . debug ( \"Retrieving tune separation from internal tables\" ) dqmin = cpymad_instance . table . summ . q1 [ 0 ] - cpymad_instance . table . summ . q2 [ 0 ] - ( int ( q1 ) - int ( q2 )) logger . info ( \"Restoring saved knobs\" ) for knob , knob_value in saved_knobs . items () : cpymad_instance . globals [ knob ] = knob_value cpymad_instance . twiss () return abs ( dqmin ) get_lhc_tune_and_chroma_knobs def get_lhc_tune_and_chroma_knobs ( accelerator : str , beam : int = 1 , telescopic_squeeze : bool = False ) -> Tuple [ str , str , str , str ] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get names of knobs needed to match tunes and chromaticities as a tuple of strings. Parameters: Name Type Description Default accelerator str Accelerator either 'LHC' (dQ[xy], dQp[xy] knobs) or 'HLLHC' (kqt[fd], ks[fd] knobs). None beam int Beam to use, for the knob names. None telescopic_squeeze bool if set to True, returns the knobs for Telescopic Squeeze configuration. Defaults to False. None Returns: Type Description None Tuple of strings with knobs for (qx, qy, dqx, dqy) . View Source def get_lhc_tune_and_chroma_knobs ( accelerator : str , beam : int = 1 , telescopic_squeeze : bool = False ) -> Tuple [ str, str, str, str ] : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get names of knobs needed to match tunes and chromaticities as a tuple of strings. Args: accelerator (str): Accelerator either 'LHC' (dQ[xy], dQp[xy] knobs) or 'HLLHC' (kqt[fd], ks[fd] knobs). beam (int): Beam to use, for the knob names. telescopic_squeeze (bool): if set to True, returns the knobs for Telescopic Squeeze configuration. Defaults to False. Returns: Tuple of strings with knobs for `(qx, qy, dqx, dqy)`. \"\"\" beam = 2 if beam == 4 else beam suffix = \"_sq\" if telescopic_squeeze else \"\" if accelerator . upper () not in ( \"LHC\" , \"HLLHC\" ) : logger . error ( \"Invalid accelerator name, only 'LHC' and 'HLLHC' implemented\" ) raise NotImplementedError ( f \"Accelerator '{accelerator}' not implemented.\" ) return { \"LHC\" : ( f \"dQx.b{beam}{suffix}\" , f \"dQy.b{beam}{suffix}\" , f \"dQpx.b{beam}{suffix}\" , f \"dQpy.b{beam}{suffix}\" , ), \"HLLHC\" : ( f \"kqtf.b{beam}{suffix}\" , f \"kqtd.b{beam}{suffix}\" , f \"ksf.b{beam}{suffix}\" , f \"ksd.b{beam}{suffix}\" , ), } [ accelerator.upper() ] match_tunes_and_chromaticities def match_tunes_and_chromaticities ( cpymad_instance : cpymad . madx . Madx , accelerator : str = None , sequence : str = None , q1_target : float = None , q2_target : float = None , dq1_target : float = None , dq2_target : float = None , varied_knobs : Sequence [ str ] = None , step : float = 1e-07 , calls : int = 100 , tolerance : float = 1e-21 ) -> None Provided with an active cpymad class after having ran a script, will run an additional matching command to reach the provided values for tunes and chromaticities. Tune matching is always performed. If chromaticity target values are given, then a matching is done for them, followed by an additionnal matching for both tunes and chromaticities. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None accelerator str name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. None sequence str name of the sequence you want to activate for the tune matching. None q1_target float horizontal tune to match to. None q2_target float vertical tune to match to. None dq1_target float horizontal chromaticity to match to. None dq2_target float vertical chromaticity to match to. None varied_knobs Sequence[str] the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. None step float step size to use when varying knobs. None calls int max number of varying calls to perform. None tolerance float tolerance for successfull matching. None View Source def match_tunes_and_chromaticities ( cpymad_instance : Madx , accelerator : str = None , sequence : str = None , q1_target : float = None , q2_target : float = None , dq1_target : float = None , dq2_target : float = None , varied_knobs : Sequence [ str ] = None , step : float = 1e-7 , calls : int = 100 , tolerance : float = 1e-21 , ) -> None : \"\"\" Provided with an active `cpymad` class after having ran a script, will run an additional matching command to reach the provided values for tunes and chromaticities. Tune matching is always performed. If chromaticity target values are given, then a matching is done for them, followed by an additionnal matching for both tunes and chromaticities. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. q1_target (float): horizontal tune to match to. q2_target (float): vertical tune to match to. dq1_target (float): horizontal chromaticity to match to. dq2_target (float): vertical chromaticity to match to. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\" kqf \", \" ksd \", \" kqf \", \" kqd \"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default {accelerator.upper()} values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ -1 ] )) def match ( * args , ** kwargs ) : \"\"\"Create matching commands for kwarg targets, varying the given args.\"\"\" logger . debug ( f \"Executing matching commands, using sequence '{sequence}'\" ) cpymad_instance . command . match ( chrom = True ) logger . trace ( f \"Targets are given as {kwargs}\" ) cpymad_instance . command . global_ ( sequence = sequence , ** kwargs ) for variable_name in args : logger . trace ( f \"Creating vary command for knob '{variable_name}'\" ) cpymad_instance . command . vary ( name = variable_name , step = step ) cpymad_instance . command . lmdif ( calls = calls , tolerance = tolerance ) cpymad_instance . command . endmatch () logger . trace ( \"Performing routine TWISS\" ) cpymad_instance . twiss () # prevents errors if the user forget to do so before querying tables if q1_target and q2_target and dq1_target and dq2_target : logger . info ( f \"Doing combined matching to Qx={q1_target}, Qy={q2_target}, \" f \"dq={dq1_target}, dqy={dq2_target} for sequence '{sequence}'\" ) logger . trace ( f \"Vary knobs sent are {varied_knobs}\" ) match ( * varied_knobs , q1 = q1_target , q2 = q2_target , dq1 = dq1_target , dq2 = dq2_target ) elif q1_target and q2_target : logger . info ( f \"Matching tunes to Qx={q1_target}, Qy={q2_target} for sequence '{sequence}'\" ) logger . trace ( f \"Vary knobs sent are {varied_knobs[:2]}\" ) match ( * varied_knobs [ :2 ] , q1 = q1_target , q2 = q2_target ) # first two in varied_knobs are tune knobs","title":"Matching"},{"location":"reference/pyhdtoolkit/cpymadtools/matching/#module-pyhdtoolkitcpymadtoolsmatching","text":"Module cpymadtools.matching Created on 2020.02.03 View Source \"\"\" Module cpymadtools.matching --------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X matchings with a cpymad.madx.Madx object. \"\"\" from typing import Dict , Sequence , Tuple from cpymad.madx import Madx from loguru import logger # ----- Utlites ----- # def get_lhc_tune_and_chroma_knobs ( accelerator : str , beam : int = 1 , telescopic_squeeze : bool = False ) -> Tuple [ str , str , str , str ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get names of knobs needed to match tunes and chromaticities as a tuple of strings. Args: accelerator (str): Accelerator either 'LHC' (dQ[xy], dQp[xy] knobs) or 'HLLHC' (kqt[fd], ks[fd] knobs). beam (int): Beam to use, for the knob names. telescopic_squeeze (bool): if set to True, returns the knobs for Telescopic Squeeze configuration. Defaults to False. Returns: Tuple of strings with knobs for `(qx, qy, dqx, dqy)`. \"\"\" beam = 2 if beam == 4 else beam suffix = \"_sq\" if telescopic_squeeze else \"\" if accelerator . upper () not in ( \"LHC\" , \"HLLHC\" ): logger . error ( \"Invalid accelerator name, only 'LHC' and 'HLLHC' implemented\" ) raise NotImplementedError ( f \"Accelerator '{accelerator}' not implemented.\" ) return { \"LHC\" : ( f \"dQx.b{beam}{suffix}\" , f \"dQy.b{beam}{suffix}\" , f \"dQpx.b{beam}{suffix}\" , f \"dQpy.b{beam}{suffix}\" , ), \"HLLHC\" : ( f \"kqtf.b{beam}{suffix}\" , f \"kqtd.b{beam}{suffix}\" , f \"ksf.b{beam}{suffix}\" , f \"ksd.b{beam}{suffix}\" , ), }[ accelerator . upper ()] def match_tunes_and_chromaticities ( cpymad_instance : Madx , accelerator : str = None , sequence : str = None , q1_target : float = None , q2_target : float = None , dq1_target : float = None , dq2_target : float = None , varied_knobs : Sequence [ str ] = None , step : float = 1e-7 , calls : int = 100 , tolerance : float = 1e-21 , ) -> None : \"\"\" Provided with an active `cpymad` class after having ran a script, will run an additional matching command to reach the provided values for tunes and chromaticities. Tune matching is always performed. If chromaticity target values are given, then a matching is done for them, followed by an additionnal matching for both tunes and chromaticities. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. q1_target (float): horizontal tune to match to. q2_target (float): vertical tune to match to. dq1_target (float): horizontal chromaticity to match to. dq2_target (float): vertical chromaticity to match to. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default {accelerator.upper()} values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ - 1 ])) def match ( * args , ** kwargs ): \"\"\"Create matching commands for kwarg targets, varying the given args.\"\"\" logger . debug ( f \"Executing matching commands, using sequence '{sequence}'\" ) cpymad_instance . command . match ( chrom = True ) logger . trace ( f \"Targets are given as {kwargs}\" ) cpymad_instance . command . global_ ( sequence = sequence , ** kwargs ) for variable_name in args : logger . trace ( f \"Creating vary command for knob '{variable_name}'\" ) cpymad_instance . command . vary ( name = variable_name , step = step ) cpymad_instance . command . lmdif ( calls = calls , tolerance = tolerance ) cpymad_instance . command . endmatch () logger . trace ( \"Performing routine TWISS\" ) cpymad_instance . twiss () # prevents errors if the user forget to do so before querying tables if q1_target and q2_target and dq1_target and dq2_target : logger . info ( f \"Doing combined matching to Qx={q1_target}, Qy={q2_target}, \" f \"dq={dq1_target}, dqy={dq2_target} for sequence '{sequence}'\" ) logger . trace ( f \"Vary knobs sent are {varied_knobs}\" ) match ( * varied_knobs , q1 = q1_target , q2 = q2_target , dq1 = dq1_target , dq2 = dq2_target ) elif q1_target and q2_target : logger . info ( f \"Matching tunes to Qx={q1_target}, Qy={q2_target} for sequence '{sequence}'\" ) logger . trace ( f \"Vary knobs sent are {varied_knobs[:2]}\" ) match ( * varied_knobs [: 2 ], q1 = q1_target , q2 = q2_target ) # first two in varied_knobs are tune knobs def get_closest_tune_approach ( cpymad_instance : Madx , accelerator : str = None , sequence : str = None , varied_knobs : Sequence [ str ] = None , step : float = 1e-7 , calls : float = 100 , tolerance : float = 1e-21 , ) -> float : \"\"\" Provided with an active `cpymad` class after having ran a script, tries to match the tunes to their mid-fractional tunes. The difference between this mid-tune and the actual matched tune is the closest tune approach. NOTA BENE: This assumes your lattice has previously been matched to desired tunes and chromaticities, as it will determine the appropriate targets from the Madx instance's internal tables. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. Returns: The closest tune approach, in absolute value. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default {accelerator.upper()} values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ - 1 ])) logger . debug ( \"Saving knob values to restore after closest tune approach\" ) saved_knobs : Dict [ str , float ] = { knob : cpymad_instance . globals [ knob ] for knob in varied_knobs } logger . trace ( f \"Saved knobs are {saved_knobs}\" ) logger . debug ( \"Retrieving tunes and chromaticities from internal tables\" ) q1 , q2 = cpymad_instance . table . summ . q1 [ 0 ], cpymad_instance . table . summ . q2 [ 0 ] dq1 , dq2 = cpymad_instance . table . summ . dq1 [ 0 ], cpymad_instance . table . summ . dq2 [ 0 ] logger . trace ( f \"Retrieved values are q1 = {q1}, q2 = {q2}, dq1 = {dq1}, dq2 = {dq2}\" ) logger . debug ( \"Determining target tunes for closest approach\" ) middle_of_fractional_tunes = ( _fractional_tune ( q1 ) + _fractional_tune ( q2 )) / 2 qx_target = int ( q1 ) + middle_of_fractional_tunes qy_target = int ( q2 ) + middle_of_fractional_tunes logger . trace ( f \"Targeting tunes Qx = {qx_target} | Qy = {qy_target}\" ) logger . info ( \"Performing closest tune approach routine, matching should fail at DeltaQ = dqmin\" ) match_tunes_and_chromaticities ( cpymad_instance , accelerator , sequence , qx_target , qy_target , # dq1, # remove? # dq2, # remove? varied_knobs = varied_knobs , step = step , calls = calls , tolerance = tolerance , ) logger . debug ( \"Retrieving tune separation from internal tables\" ) dqmin = cpymad_instance . table . summ . q1 [ 0 ] - cpymad_instance . table . summ . q2 [ 0 ] - ( int ( q1 ) - int ( q2 )) logger . info ( \"Restoring saved knobs\" ) for knob , knob_value in saved_knobs . items (): cpymad_instance . globals [ knob ] = knob_value cpymad_instance . twiss () return abs ( dqmin ) # ----- Helpers ----- # def _fractional_tune ( tune : float ) -> float : \"\"\" Return only the fractional part of a tune value. Args: tune (float): tune value. Returns: The fractional part. \"\"\" return tune - int ( tune ) # ok since int truncates to lower integer","title":"Module pyhdtoolkit.cpymadtools.matching"},{"location":"reference/pyhdtoolkit/cpymadtools/matching/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/matching/#get_closest_tune_approach","text":"def get_closest_tune_approach ( cpymad_instance : cpymad . madx . Madx , accelerator : str = None , sequence : str = None , varied_knobs : Sequence [ str ] = None , step : float = 1e-07 , calls : float = 100 , tolerance : float = 1e-21 ) -> float Provided with an active cpymad class after having ran a script, tries to match the tunes to their mid-fractional tunes. The difference between this mid-tune and the actual matched tune is the closest tune approach. NOTA BENE: This assumes your lattice has previously been matched to desired tunes and chromaticities, as it will determine the appropriate targets from the Madx instance's internal tables. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None accelerator str name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. None sequence str name of the sequence you want to activate for the tune matching. None varied_knobs Sequence[str] the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. None step float step size to use when varying knobs. None calls int max number of varying calls to perform. None tolerance float tolerance for successfull matching. None Returns: Type Description None The closest tune approach, in absolute value. View Source def get_closest_tune_approach ( cpymad_instance : Madx , accelerator : str = None , sequence : str = None , varied_knobs : Sequence [ str ] = None , step : float = 1e-7 , calls : float = 100 , tolerance : float = 1e-21 , ) -> float : \"\"\" Provided with an active `cpymad` class after having ran a script, tries to match the tunes to their mid-fractional tunes. The difference between this mid-tune and the actual matched tune is the closest tune approach. NOTA BENE: This assumes your lattice has previously been matched to desired tunes and chromaticities, as it will determine the appropriate targets from the Madx instance's internal tables. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\" kqf \", \" ksd \", \" kqf \", \" kqd \"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. Returns: The closest tune approach, in absolute value. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default {accelerator.upper()} values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ -1 ] )) logger . debug ( \"Saving knob values to restore after closest tune approach\" ) saved_knobs : Dict [ str, float ] = { knob : cpymad_instance . globals [ knob ] for knob in varied_knobs } logger . trace ( f \"Saved knobs are {saved_knobs}\" ) logger . debug ( \"Retrieving tunes and chromaticities from internal tables\" ) q1 , q2 = cpymad_instance . table . summ . q1 [ 0 ] , cpymad_instance . table . summ . q2 [ 0 ] dq1 , dq2 = cpymad_instance . table . summ . dq1 [ 0 ] , cpymad_instance . table . summ . dq2 [ 0 ] logger . trace ( f \"Retrieved values are q1 = {q1}, q2 = {q2}, dq1 = {dq1}, dq2 = {dq2}\" ) logger . debug ( \"Determining target tunes for closest approach\" ) middle_of_fractional_tunes = ( _fractional_tune ( q1 ) + _fractional_tune ( q2 )) / 2 qx_target = int ( q1 ) + middle_of_fractional_tunes qy_target = int ( q2 ) + middle_of_fractional_tunes logger . trace ( f \"Targeting tunes Qx = {qx_target} | Qy = {qy_target}\" ) logger . info ( \"Performing closest tune approach routine, matching should fail at DeltaQ = dqmin\" ) match_tunes_and_chromaticities ( cpymad_instance , accelerator , sequence , qx_target , qy_target , # dq1 , # remove ? # dq2 , # remove ? varied_knobs = varied_knobs , step = step , calls = calls , tolerance = tolerance , ) logger . debug ( \"Retrieving tune separation from internal tables\" ) dqmin = cpymad_instance . table . summ . q1 [ 0 ] - cpymad_instance . table . summ . q2 [ 0 ] - ( int ( q1 ) - int ( q2 )) logger . info ( \"Restoring saved knobs\" ) for knob , knob_value in saved_knobs . items () : cpymad_instance . globals [ knob ] = knob_value cpymad_instance . twiss () return abs ( dqmin )","title":"get_closest_tune_approach"},{"location":"reference/pyhdtoolkit/cpymadtools/matching/#get_lhc_tune_and_chroma_knobs","text":"def get_lhc_tune_and_chroma_knobs ( accelerator : str , beam : int = 1 , telescopic_squeeze : bool = False ) -> Tuple [ str , str , str , str ] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get names of knobs needed to match tunes and chromaticities as a tuple of strings. Parameters: Name Type Description Default accelerator str Accelerator either 'LHC' (dQ[xy], dQp[xy] knobs) or 'HLLHC' (kqt[fd], ks[fd] knobs). None beam int Beam to use, for the knob names. None telescopic_squeeze bool if set to True, returns the knobs for Telescopic Squeeze configuration. Defaults to False. None Returns: Type Description None Tuple of strings with knobs for (qx, qy, dqx, dqy) . View Source def get_lhc_tune_and_chroma_knobs ( accelerator : str , beam : int = 1 , telescopic_squeeze : bool = False ) -> Tuple [ str, str, str, str ] : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get names of knobs needed to match tunes and chromaticities as a tuple of strings. Args: accelerator (str): Accelerator either 'LHC' (dQ[xy], dQp[xy] knobs) or 'HLLHC' (kqt[fd], ks[fd] knobs). beam (int): Beam to use, for the knob names. telescopic_squeeze (bool): if set to True, returns the knobs for Telescopic Squeeze configuration. Defaults to False. Returns: Tuple of strings with knobs for `(qx, qy, dqx, dqy)`. \"\"\" beam = 2 if beam == 4 else beam suffix = \"_sq\" if telescopic_squeeze else \"\" if accelerator . upper () not in ( \"LHC\" , \"HLLHC\" ) : logger . error ( \"Invalid accelerator name, only 'LHC' and 'HLLHC' implemented\" ) raise NotImplementedError ( f \"Accelerator '{accelerator}' not implemented.\" ) return { \"LHC\" : ( f \"dQx.b{beam}{suffix}\" , f \"dQy.b{beam}{suffix}\" , f \"dQpx.b{beam}{suffix}\" , f \"dQpy.b{beam}{suffix}\" , ), \"HLLHC\" : ( f \"kqtf.b{beam}{suffix}\" , f \"kqtd.b{beam}{suffix}\" , f \"ksf.b{beam}{suffix}\" , f \"ksd.b{beam}{suffix}\" , ), } [ accelerator.upper() ]","title":"get_lhc_tune_and_chroma_knobs"},{"location":"reference/pyhdtoolkit/cpymadtools/matching/#match_tunes_and_chromaticities","text":"def match_tunes_and_chromaticities ( cpymad_instance : cpymad . madx . Madx , accelerator : str = None , sequence : str = None , q1_target : float = None , q2_target : float = None , dq1_target : float = None , dq2_target : float = None , varied_knobs : Sequence [ str ] = None , step : float = 1e-07 , calls : int = 100 , tolerance : float = 1e-21 ) -> None Provided with an active cpymad class after having ran a script, will run an additional matching command to reach the provided values for tunes and chromaticities. Tune matching is always performed. If chromaticity target values are given, then a matching is done for them, followed by an additionnal matching for both tunes and chromaticities. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None accelerator str name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. None sequence str name of the sequence you want to activate for the tune matching. None q1_target float horizontal tune to match to. None q2_target float vertical tune to match to. None dq1_target float horizontal chromaticity to match to. None dq2_target float vertical chromaticity to match to. None varied_knobs Sequence[str] the variables names to 'vary' in the MADX routine. An input could be [\"kqf\", \"ksd\", \"kqf\", \"kqd\"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. None step float step size to use when varying knobs. None calls int max number of varying calls to perform. None tolerance float tolerance for successfull matching. None View Source def match_tunes_and_chromaticities ( cpymad_instance : Madx , accelerator : str = None , sequence : str = None , q1_target : float = None , q2_target : float = None , dq1_target : float = None , dq2_target : float = None , varied_knobs : Sequence [ str ] = None , step : float = 1e-7 , calls : int = 100 , tolerance : float = 1e-21 , ) -> None : \"\"\" Provided with an active `cpymad` class after having ran a script, will run an additional matching command to reach the provided values for tunes and chromaticities. Tune matching is always performed. If chromaticity target values are given, then a matching is done for them, followed by an additionnal matching for both tunes and chromaticities. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. accelerator (str): name of the accelerator, used to determmine knobs if 'variables' not given. Automatic determination will only work for LHC and HLLHC. sequence (str): name of the sequence you want to activate for the tune matching. q1_target (float): horizontal tune to match to. q2_target (float): vertical tune to match to. dq1_target (float): horizontal chromaticity to match to. dq2_target (float): vertical chromaticity to match to. varied_knobs (Sequence[str]): the variables names to 'vary' in the MADX routine. An input could be [\" kqf \", \" ksd \", \" kqf \", \" kqd \"] as they are common names used for quadrupole and sextupole strengths (foc / defoc) in most examples. step (float): step size to use when varying knobs. calls (int): max number of varying calls to perform. tolerance (float): tolerance for successfull matching. \"\"\" if accelerator and not varied_knobs : logger . trace ( f \"Getting knobs from default {accelerator.upper()} values\" ) varied_knobs = get_lhc_tune_and_chroma_knobs ( accelerator = accelerator , beam = int ( sequence [ -1 ] )) def match ( * args , ** kwargs ) : \"\"\"Create matching commands for kwarg targets, varying the given args.\"\"\" logger . debug ( f \"Executing matching commands, using sequence '{sequence}'\" ) cpymad_instance . command . match ( chrom = True ) logger . trace ( f \"Targets are given as {kwargs}\" ) cpymad_instance . command . global_ ( sequence = sequence , ** kwargs ) for variable_name in args : logger . trace ( f \"Creating vary command for knob '{variable_name}'\" ) cpymad_instance . command . vary ( name = variable_name , step = step ) cpymad_instance . command . lmdif ( calls = calls , tolerance = tolerance ) cpymad_instance . command . endmatch () logger . trace ( \"Performing routine TWISS\" ) cpymad_instance . twiss () # prevents errors if the user forget to do so before querying tables if q1_target and q2_target and dq1_target and dq2_target : logger . info ( f \"Doing combined matching to Qx={q1_target}, Qy={q2_target}, \" f \"dq={dq1_target}, dqy={dq2_target} for sequence '{sequence}'\" ) logger . trace ( f \"Vary knobs sent are {varied_knobs}\" ) match ( * varied_knobs , q1 = q1_target , q2 = q2_target , dq1 = dq1_target , dq2 = dq2_target ) elif q1_target and q2_target : logger . info ( f \"Matching tunes to Qx={q1_target}, Qy={q2_target} for sequence '{sequence}'\" ) logger . trace ( f \"Vary knobs sent are {varied_knobs[:2]}\" ) match ( * varied_knobs [ :2 ] , q1 = q1_target , q2 = q2_target ) # first two in varied_knobs are tune knobs","title":"match_tunes_and_chromaticities"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/","text":"Module pyhdtoolkit.cpymadtools.orbit Module cpymadtools.orbit Created on 2020.02.03 View Source \"\"\" Module cpymadtools.orbit ------------------------ Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X orbit setup with a cpymad.madx.Madx object, mainly for LHC and HLLHC machines. \"\"\" from typing import Dict , List , Tuple from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.cpymadtools.constants import LHC_CROSSING_SCHEMES # ----- Utilities ----- # def lhc_orbit_variables () -> Tuple [ List [ str ], Dict [ str , str ]]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the variable names used for orbit setup in the (HL)LHC. Returns: A tuple with a list of all orbit variables, and a dict of additional variables, that in the default configurations have the same value as another variable. \"\"\" logger . trace ( \"Returning (HL)LHC orbit variables\" ) on_variables = ( \"crab1\" , \"crab5\" , # exists only in HL-LHC \"x1\" , \"sep1\" , \"o1\" , \"oh1\" , \"ov1\" , \"x2\" , \"sep2\" , \"o2\" , \"oe2\" , \"a2\" , \"oh2\" , \"ov2\" , \"x5\" , \"sep5\" , \"o5\" , \"oh5\" , \"ov5\" , \"phi_IR5\" , \"x8\" , \"sep8\" , \"o8\" , \"a8\" , \"sep8h\" , \"x8v\" , \"oh8\" , \"ov8\" , \"alice\" , \"sol_alice\" , \"lhcb\" , \"sol_atlas\" , \"sol_cms\" , ) variables = [ f \"on_{var}\" for var in on_variables ] + [ f \"phi_IR{ir:d}\" for ir in ( 1 , 2 , 5 , 8 )] special = { \"on_ssep1\" : \"on_sep1\" , \"on_xx1\" : \"on_x1\" , \"on_ssep5\" : \"on_sep5\" , \"on_xx5\" : \"on_x5\" , } return variables , special def setup_lhc_orbit ( cpymad_instance : Madx , scheme : str = \"flat\" , ** kwargs ) -> Dict [ str , float ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Automated orbit setup for (hl)lhc runs, for some default schemes. Assumed that at least sequence and optics files have been called. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. scheme (str): the default scheme to apply, as defined in `LHC_CROSSING_SCHEMES`. Accepted values are keys of `LHC_CROSSING_SCHEMES`. Defaults to 'flat' (every orbit variable to 0). Keyword Args: All standard crossing scheme variables (on_x1, phi_IR1, etc). Values given here override the values in the default scheme configurations. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" if scheme not in LHC_CROSSING_SCHEMES . keys (): logger . error ( f \"Invalid scheme parameter, should be one of {LHC_CROSSING_SCHEMES.keys()}\" ) raise ValueError ( \"Invalid scheme parameter given\" ) logger . debug ( \"Getting orbit variables\" ) variables , special = lhc_orbit_variables () scheme_dict = LHC_CROSSING_SCHEMES [ scheme ] final_scheme = {} for orbit_variable in variables : variable_value = kwargs . get ( orbit_variable , scheme_dict . get ( orbit_variable , 0 )) logger . trace ( f \"Setting orbit variable '{orbit_variable}' to {variable_value}\" ) # Sets value in MAD-X globals & returned dict, taken from scheme dict or kwargs if provided cpymad_instance . globals [ orbit_variable ] = final_scheme [ orbit_variable ] = variable_value for special_variable , copy_from in special . items (): special_variable_value = kwargs . get ( special_variable , cpymad_instance . globals [ copy_from ]) logger . trace ( f \"Setting special orbit variable '{special_variable}' to {special_variable_value}\" ) # Sets value in MAD-X globals & returned dict, taken from a given global or kwargs if provided cpymad_instance . globals [ special_variable ] = final_scheme [ special_variable ] = special_variable_value return final_scheme def get_current_orbit_setup ( cpymad_instance : Madx ) -> Dict [ str , float ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the current values for the (HL)LHC orbit variales. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" logger . debug ( \"Extracting orbit variables from global table\" ) variables , specials = lhc_orbit_variables () return { orbit_variable : cpymad_instance . globals [ orbit_variable ] for orbit_variable in variables + list ( specials . keys ()) } Variables LHC_CROSSING_SCHEMES Functions get_current_orbit_setup def get_current_orbit_setup ( cpymad_instance : cpymad . madx . Madx ) -> Dict [ str , float ] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the current values for the (HL)LHC orbit variales. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None Returns: Type Description None A dictionary of all orbit variables set, and their values as set in the MAD-X globals. View Source def get_current_orbit_setup ( cpymad_instance : Madx ) -> Dict [ str, float ] : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the current values for the (HL)LHC orbit variales. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" logger . debug ( \"Extracting orbit variables from global table\" ) variables , specials = lhc_orbit_variables () return { orbit_variable : cpymad_instance . globals [ orbit_variable ] for orbit_variable in variables + list ( specials . keys ()) } lhc_orbit_variables def lhc_orbit_variables ( ) -> Tuple [ List [ str ], Dict [ str , str ]] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the variable names used for orbit setup in the (HL)LHC. Returns: Type Description None A tuple with a list of all orbit variables, and a dict of additional variables, that in the default configurations have the same value as another variable. View Source def lhc_orbit_variables () -> Tuple [ List[str ] , Dict [ str, str ] ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the variable names used for orbit setup in the (HL)LHC. Returns: A tuple with a list of all orbit variables, and a dict of additional variables, that in the default configurations have the same value as another variable. \"\"\" logger . trace ( \"Returning (HL)LHC orbit variables\" ) on_variables = ( \"crab1\" , \"crab5\" , # exists only in HL - LHC \"x1\" , \"sep1\" , \"o1\" , \"oh1\" , \"ov1\" , \"x2\" , \"sep2\" , \"o2\" , \"oe2\" , \"a2\" , \"oh2\" , \"ov2\" , \"x5\" , \"sep5\" , \"o5\" , \"oh5\" , \"ov5\" , \"phi_IR5\" , \"x8\" , \"sep8\" , \"o8\" , \"a8\" , \"sep8h\" , \"x8v\" , \"oh8\" , \"ov8\" , \"alice\" , \"sol_alice\" , \"lhcb\" , \"sol_atlas\" , \"sol_cms\" , ) variables = [ f\"on_{var}\" for var in on_variables ] + [ f\"phi_IR{ir:d}\" for ir in (1, 2, 5, 8) ] special = { \"on_ssep1\" : \"on_sep1\" , \"on_xx1\" : \"on_x1\" , \"on_ssep5\" : \"on_sep5\" , \"on_xx5\" : \"on_x5\" , } return variables , special setup_lhc_orbit def setup_lhc_orbit ( cpymad_instance : cpymad . madx . Madx , scheme : str = 'flat' , ** kwargs ) -> Dict [ str , float ] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Automated orbit setup for (hl)lhc runs, for some default schemes. Assumed that at least sequence and optics files have been called. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. scheme (str): the default scheme to apply, as defined in LHC_CROSSING_SCHEMES . Accepted values are keys of LHC_CROSSING_SCHEMES . Defaults to 'flat' (every orbit variable to 0). Keyword Args: All standard crossing scheme variables (on_x1, phi_IR1, etc). Values given here override the values in the default scheme configurations. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. View Source def setup_lhc_orbit ( cpymad_instance : Madx , scheme : str = \"flat\" , ** kwargs ) -> Dict [ str, float ] : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Automated orbit setup for (hl)lhc runs, for some default schemes. Assumed that at least sequence and optics files have been called. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. scheme (str): the default scheme to apply, as defined in `LHC_CROSSING_SCHEMES`. Accepted values are keys of `LHC_CROSSING_SCHEMES`. Defaults to 'flat' (every orbit variable to 0). Keyword Args: All standard crossing scheme variables (on_x1, phi_IR1, etc). Values given here override the values in the default scheme configurations. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" if scheme not in LHC_CROSSING_SCHEMES . keys () : logger . error ( f \"Invalid scheme parameter, should be one of {LHC_CROSSING_SCHEMES.keys()}\" ) raise ValueError ( \"Invalid scheme parameter given\" ) logger . debug ( \"Getting orbit variables\" ) variables , special = lhc_orbit_variables () scheme_dict = LHC_CROSSING_SCHEMES [ scheme ] final_scheme = {} for orbit_variable in variables : variable_value = kwargs . get ( orbit_variable , scheme_dict . get ( orbit_variable , 0 )) logger . trace ( f \"Setting orbit variable '{orbit_variable}' to {variable_value}\" ) # Sets value in MAD - X globals & returned dict , taken from scheme dict or kwargs if provided cpymad_instance . globals [ orbit_variable ] = final_scheme [ orbit_variable ] = variable_value for special_variable , copy_from in special . items () : special_variable_value = kwargs . get ( special_variable , cpymad_instance . globals [ copy_from ] ) logger . trace ( f \"Setting special orbit variable '{special_variable}' to {special_variable_value}\" ) # Sets value in MAD - X globals & returned dict , taken from a given global or kwargs if provided cpymad_instance . globals [ special_variable ] = final_scheme [ special_variable ] = special_variable_value return final_scheme","title":"Orbit"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#module-pyhdtoolkitcpymadtoolsorbit","text":"Module cpymadtools.orbit Created on 2020.02.03 View Source \"\"\" Module cpymadtools.orbit ------------------------ Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X orbit setup with a cpymad.madx.Madx object, mainly for LHC and HLLHC machines. \"\"\" from typing import Dict , List , Tuple from cpymad.madx import Madx from loguru import logger from pyhdtoolkit.cpymadtools.constants import LHC_CROSSING_SCHEMES # ----- Utilities ----- # def lhc_orbit_variables () -> Tuple [ List [ str ], Dict [ str , str ]]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the variable names used for orbit setup in the (HL)LHC. Returns: A tuple with a list of all orbit variables, and a dict of additional variables, that in the default configurations have the same value as another variable. \"\"\" logger . trace ( \"Returning (HL)LHC orbit variables\" ) on_variables = ( \"crab1\" , \"crab5\" , # exists only in HL-LHC \"x1\" , \"sep1\" , \"o1\" , \"oh1\" , \"ov1\" , \"x2\" , \"sep2\" , \"o2\" , \"oe2\" , \"a2\" , \"oh2\" , \"ov2\" , \"x5\" , \"sep5\" , \"o5\" , \"oh5\" , \"ov5\" , \"phi_IR5\" , \"x8\" , \"sep8\" , \"o8\" , \"a8\" , \"sep8h\" , \"x8v\" , \"oh8\" , \"ov8\" , \"alice\" , \"sol_alice\" , \"lhcb\" , \"sol_atlas\" , \"sol_cms\" , ) variables = [ f \"on_{var}\" for var in on_variables ] + [ f \"phi_IR{ir:d}\" for ir in ( 1 , 2 , 5 , 8 )] special = { \"on_ssep1\" : \"on_sep1\" , \"on_xx1\" : \"on_x1\" , \"on_ssep5\" : \"on_sep5\" , \"on_xx5\" : \"on_x5\" , } return variables , special def setup_lhc_orbit ( cpymad_instance : Madx , scheme : str = \"flat\" , ** kwargs ) -> Dict [ str , float ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Automated orbit setup for (hl)lhc runs, for some default schemes. Assumed that at least sequence and optics files have been called. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. scheme (str): the default scheme to apply, as defined in `LHC_CROSSING_SCHEMES`. Accepted values are keys of `LHC_CROSSING_SCHEMES`. Defaults to 'flat' (every orbit variable to 0). Keyword Args: All standard crossing scheme variables (on_x1, phi_IR1, etc). Values given here override the values in the default scheme configurations. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" if scheme not in LHC_CROSSING_SCHEMES . keys (): logger . error ( f \"Invalid scheme parameter, should be one of {LHC_CROSSING_SCHEMES.keys()}\" ) raise ValueError ( \"Invalid scheme parameter given\" ) logger . debug ( \"Getting orbit variables\" ) variables , special = lhc_orbit_variables () scheme_dict = LHC_CROSSING_SCHEMES [ scheme ] final_scheme = {} for orbit_variable in variables : variable_value = kwargs . get ( orbit_variable , scheme_dict . get ( orbit_variable , 0 )) logger . trace ( f \"Setting orbit variable '{orbit_variable}' to {variable_value}\" ) # Sets value in MAD-X globals & returned dict, taken from scheme dict or kwargs if provided cpymad_instance . globals [ orbit_variable ] = final_scheme [ orbit_variable ] = variable_value for special_variable , copy_from in special . items (): special_variable_value = kwargs . get ( special_variable , cpymad_instance . globals [ copy_from ]) logger . trace ( f \"Setting special orbit variable '{special_variable}' to {special_variable_value}\" ) # Sets value in MAD-X globals & returned dict, taken from a given global or kwargs if provided cpymad_instance . globals [ special_variable ] = final_scheme [ special_variable ] = special_variable_value return final_scheme def get_current_orbit_setup ( cpymad_instance : Madx ) -> Dict [ str , float ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the current values for the (HL)LHC orbit variales. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" logger . debug ( \"Extracting orbit variables from global table\" ) variables , specials = lhc_orbit_variables () return { orbit_variable : cpymad_instance . globals [ orbit_variable ] for orbit_variable in variables + list ( specials . keys ()) }","title":"Module pyhdtoolkit.cpymadtools.orbit"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#variables","text":"LHC_CROSSING_SCHEMES","title":"Variables"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#get_current_orbit_setup","text":"def get_current_orbit_setup ( cpymad_instance : cpymad . madx . Madx ) -> Dict [ str , float ] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the current values for the (HL)LHC orbit variales. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None Returns: Type Description None A dictionary of all orbit variables set, and their values as set in the MAD-X globals. View Source def get_current_orbit_setup ( cpymad_instance : Madx ) -> Dict [ str, float ] : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the current values for the (HL)LHC orbit variales. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" logger . debug ( \"Extracting orbit variables from global table\" ) variables , specials = lhc_orbit_variables () return { orbit_variable : cpymad_instance . globals [ orbit_variable ] for orbit_variable in variables + list ( specials . keys ()) }","title":"get_current_orbit_setup"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#lhc_orbit_variables","text":"def lhc_orbit_variables ( ) -> Tuple [ List [ str ], Dict [ str , str ]] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the variable names used for orbit setup in the (HL)LHC. Returns: Type Description None A tuple with a list of all orbit variables, and a dict of additional variables, that in the default configurations have the same value as another variable. View Source def lhc_orbit_variables () -> Tuple [ List[str ] , Dict [ str, str ] ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Get the variable names used for orbit setup in the (HL)LHC. Returns: A tuple with a list of all orbit variables, and a dict of additional variables, that in the default configurations have the same value as another variable. \"\"\" logger . trace ( \"Returning (HL)LHC orbit variables\" ) on_variables = ( \"crab1\" , \"crab5\" , # exists only in HL - LHC \"x1\" , \"sep1\" , \"o1\" , \"oh1\" , \"ov1\" , \"x2\" , \"sep2\" , \"o2\" , \"oe2\" , \"a2\" , \"oh2\" , \"ov2\" , \"x5\" , \"sep5\" , \"o5\" , \"oh5\" , \"ov5\" , \"phi_IR5\" , \"x8\" , \"sep8\" , \"o8\" , \"a8\" , \"sep8h\" , \"x8v\" , \"oh8\" , \"ov8\" , \"alice\" , \"sol_alice\" , \"lhcb\" , \"sol_atlas\" , \"sol_cms\" , ) variables = [ f\"on_{var}\" for var in on_variables ] + [ f\"phi_IR{ir:d}\" for ir in (1, 2, 5, 8) ] special = { \"on_ssep1\" : \"on_sep1\" , \"on_xx1\" : \"on_x1\" , \"on_ssep5\" : \"on_sep5\" , \"on_xx5\" : \"on_x5\" , } return variables , special","title":"lhc_orbit_variables"},{"location":"reference/pyhdtoolkit/cpymadtools/orbit/#setup_lhc_orbit","text":"def setup_lhc_orbit ( cpymad_instance : cpymad . madx . Madx , scheme : str = 'flat' , ** kwargs ) -> Dict [ str , float ] INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Automated orbit setup for (hl)lhc runs, for some default schemes. Assumed that at least sequence and optics files have been called. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. scheme (str): the default scheme to apply, as defined in LHC_CROSSING_SCHEMES . Accepted values are keys of LHC_CROSSING_SCHEMES . Defaults to 'flat' (every orbit variable to 0). Keyword Args: All standard crossing scheme variables (on_x1, phi_IR1, etc). Values given here override the values in the default scheme configurations. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. View Source def setup_lhc_orbit ( cpymad_instance : Madx , scheme : str = \"flat\" , ** kwargs ) -> Dict [ str, float ] : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Automated orbit setup for (hl)lhc runs, for some default schemes. Assumed that at least sequence and optics files have been called. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. scheme (str): the default scheme to apply, as defined in `LHC_CROSSING_SCHEMES`. Accepted values are keys of `LHC_CROSSING_SCHEMES`. Defaults to 'flat' (every orbit variable to 0). Keyword Args: All standard crossing scheme variables (on_x1, phi_IR1, etc). Values given here override the values in the default scheme configurations. Returns: A dictionary of all orbit variables set, and their values as set in the MAD-X globals. \"\"\" if scheme not in LHC_CROSSING_SCHEMES . keys () : logger . error ( f \"Invalid scheme parameter, should be one of {LHC_CROSSING_SCHEMES.keys()}\" ) raise ValueError ( \"Invalid scheme parameter given\" ) logger . debug ( \"Getting orbit variables\" ) variables , special = lhc_orbit_variables () scheme_dict = LHC_CROSSING_SCHEMES [ scheme ] final_scheme = {} for orbit_variable in variables : variable_value = kwargs . get ( orbit_variable , scheme_dict . get ( orbit_variable , 0 )) logger . trace ( f \"Setting orbit variable '{orbit_variable}' to {variable_value}\" ) # Sets value in MAD - X globals & returned dict , taken from scheme dict or kwargs if provided cpymad_instance . globals [ orbit_variable ] = final_scheme [ orbit_variable ] = variable_value for special_variable , copy_from in special . items () : special_variable_value = kwargs . get ( special_variable , cpymad_instance . globals [ copy_from ] ) logger . trace ( f \"Setting special orbit variable '{special_variable}' to {special_variable_value}\" ) # Sets value in MAD - X globals & returned dict , taken from a given global or kwargs if provided cpymad_instance . globals [ special_variable ] = final_scheme [ special_variable ] = special_variable_value return final_scheme","title":"setup_lhc_orbit"},{"location":"reference/pyhdtoolkit/cpymadtools/parameters/","text":"Module pyhdtoolkit.cpymadtools.parameters Module cpymadtools.parameters Created on 2020.02.03 View Source \"\"\" Module cpymadtools.parameters ----------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to compute different beam and machine parameters. \"\"\" from typing import Dict import numpy as np from loguru import logger # ----- Utilities ----- # def beam_parameters ( pc_gev : float , en_x_m : float , en_y_m : float , deltap_p : float , verbose : bool = False , ) -> Dict [ str , float ]: \"\"\"Calculate beam parameters from provided values. Args: pc_gev (float): particle momentum. en_x_m (float): horizontal emittance, in meters. en_y_m (float): vertical emittance, in meters. deltap_p (float): momentum deviation. verbose (bool): bolean, whether to print out a summary of parameters or not. Returns: A dictionnary with the calculated values. \"\"\" e0_gev = 0.9382720813 e_tot_gev = np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) gamma_r = e_tot_gev / e0_gev beta_r = pc_gev / np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) parameters : Dict [ str , float ] = { \"pc_GeV\" : pc_gev , # Particle momentum [GeV] \"B_rho_Tm\" : 3.3356 * pc_gev , # Beam rigidity [T/m] \"E_0_GeV\" : e0_gev , # Rest mass energy [GeV] \"E_tot_GeV\" : e_tot_gev , # Total beam energy [GeV] \"E_kin_GeV\" : e_tot_gev - e0_gev , # Kinectic beam energy [GeV] \"gamma_r\" : gamma_r , # Relativistic gamma \"beta_r\" : beta_r , # Relativistic beta \"en_x_m\" : en_x_m , # Horizontal emittance [m] \"en_y_m\" : en_y_m , # Vertical emittance [m] \"eg_x_m\" : en_x_m / gamma_r / beta_r , # Horizontal geometrical emittance \"eg_y_m\" : en_y_m / gamma_r / beta_r , # Vertical geometrical emittance \"deltap_p\" : deltap_p , # Momentum deviation } if verbose : logger . trace ( \"Outputing computed parameter values\" ) print ( f \"\"\"Particle type: proton Beam momentum = { parameters [ \"pc_GeV\" ] : 2.3f } GeV/c Normalized x-emittance = { parameters [ \"en_x_m\" ] * 1e6 : 2.3f } mm mrad Normalized y-emittance = { parameters [ \"en_y_m\" ] * 1e6 : 2.3f } mm mrad Momentum deviation deltap/p = { parameters [ \"deltap_p\" ] } -> Beam total energy = { parameters [ \"E_tot_GeV\" ] : 2.3f } GeV -> Beam kinetic energy = { parameters [ \"E_kin_GeV\" ] : 2.3f } GeV -> Beam rigidity = { parameters [ \"B_rho_Tm\" ] : 2.3f } Tm -> Relativistic beta = { parameters [ \"beta_r\" ] : 2.5f } -> Relativistic gamma = { parameters [ \"gamma_r\" ] : 2.3f } -> Geometrical x emittance = { parameters [ \"eg_x_m\" ] * 1e6 : 2.3f } mm mrad -> Geometrical y emittance = { parameters [ \"eg_y_m\" ] * 1e6 : 2.3f } mm mrad \"\"\" ) return parameters Functions beam_parameters def beam_parameters ( pc_gev : float , en_x_m : float , en_y_m : float , deltap_p : float , verbose : bool = False ) -> Dict [ str , float ] Calculate beam parameters from provided values. Parameters: Name Type Description Default pc_gev float particle momentum. None en_x_m float horizontal emittance, in meters. None en_y_m float vertical emittance, in meters. None deltap_p float momentum deviation. None verbose bool bolean, whether to print out a summary of parameters or not. None Returns: Type Description None A dictionnary with the calculated values. View Source def beam_parameters ( pc_gev : float , en_x_m : float , en_y_m : float , deltap_p : float , verbose : bool = False , ) -> Dict [ str, float ] : \"\"\"Calculate beam parameters from provided values. Args: pc_gev (float): particle momentum. en_x_m (float): horizontal emittance, in meters. en_y_m (float): vertical emittance, in meters. deltap_p (float): momentum deviation. verbose (bool): bolean, whether to print out a summary of parameters or not. Returns: A dictionnary with the calculated values. \"\"\" e0_gev = 0.9382720813 e_tot_gev = np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) gamma_r = e_tot_gev / e0_gev beta_r = pc_gev / np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) parameters : Dict [ str, float ] = { \"pc_GeV\" : pc_gev , # Particle momentum [ GeV ] \"B_rho_Tm\" : 3.3356 * pc_gev , # Beam rigidity [ T/m ] \"E_0_GeV\" : e0_gev , # Rest mass energy [ GeV ] \"E_tot_GeV\" : e_tot_gev , # Total beam energy [ GeV ] \"E_kin_GeV\" : e_tot_gev - e0_gev , # Kinectic beam energy [ GeV ] \"gamma_r\" : gamma_r , # Relativistic gamma \"beta_r\" : beta_r , # Relativistic beta \"en_x_m\" : en_x_m , # Horizontal emittance [ m ] \"en_y_m\" : en_y_m , # Vertical emittance [ m ] \"eg_x_m\" : en_x_m / gamma_r / beta_r , # Horizontal geometrical emittance \"eg_y_m\" : en_y_m / gamma_r / beta_r , # Vertical geometrical emittance \"deltap_p\" : deltap_p , # Momentum deviation } if verbose : logger . trace ( \"Outputing computed parameter values\" ) print ( f \"\"\"Particle type: proton Beam momentum = {parameters[\" pc_GeV \"]:2.3f} GeV/c Normalized x-emittance = {parameters[\" en_x_m \"] * 1e6:2.3f} mm mrad Normalized y-emittance = {parameters[\" en_y_m \"] * 1e6:2.3f} mm mrad Momentum deviation deltap/p = {parameters[\" deltap_p \"]} -> Beam total energy = {parameters[\" E_tot_GeV \"]:2.3f} GeV -> Beam kinetic energy = {parameters[\" E_kin_GeV \"]:2.3f} GeV -> Beam rigidity = {parameters[\" B_rho_Tm \"]:2.3f} Tm -> Relativistic beta = {parameters[\" beta_r \"]:2.5f} -> Relativistic gamma = {parameters[\" gamma_r \"]:2.3f} -> Geometrical x emittance = {parameters[\" eg_x_m \"] * 1e6:2.3f} mm mrad -> Geometrical y emittance = {parameters[\" eg_y_m \"] * 1e6:2.3f} mm mrad \"\"\" ) return parameters","title":"Parameters"},{"location":"reference/pyhdtoolkit/cpymadtools/parameters/#module-pyhdtoolkitcpymadtoolsparameters","text":"Module cpymadtools.parameters Created on 2020.02.03 View Source \"\"\" Module cpymadtools.parameters ----------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to compute different beam and machine parameters. \"\"\" from typing import Dict import numpy as np from loguru import logger # ----- Utilities ----- # def beam_parameters ( pc_gev : float , en_x_m : float , en_y_m : float , deltap_p : float , verbose : bool = False , ) -> Dict [ str , float ]: \"\"\"Calculate beam parameters from provided values. Args: pc_gev (float): particle momentum. en_x_m (float): horizontal emittance, in meters. en_y_m (float): vertical emittance, in meters. deltap_p (float): momentum deviation. verbose (bool): bolean, whether to print out a summary of parameters or not. Returns: A dictionnary with the calculated values. \"\"\" e0_gev = 0.9382720813 e_tot_gev = np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) gamma_r = e_tot_gev / e0_gev beta_r = pc_gev / np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) parameters : Dict [ str , float ] = { \"pc_GeV\" : pc_gev , # Particle momentum [GeV] \"B_rho_Tm\" : 3.3356 * pc_gev , # Beam rigidity [T/m] \"E_0_GeV\" : e0_gev , # Rest mass energy [GeV] \"E_tot_GeV\" : e_tot_gev , # Total beam energy [GeV] \"E_kin_GeV\" : e_tot_gev - e0_gev , # Kinectic beam energy [GeV] \"gamma_r\" : gamma_r , # Relativistic gamma \"beta_r\" : beta_r , # Relativistic beta \"en_x_m\" : en_x_m , # Horizontal emittance [m] \"en_y_m\" : en_y_m , # Vertical emittance [m] \"eg_x_m\" : en_x_m / gamma_r / beta_r , # Horizontal geometrical emittance \"eg_y_m\" : en_y_m / gamma_r / beta_r , # Vertical geometrical emittance \"deltap_p\" : deltap_p , # Momentum deviation } if verbose : logger . trace ( \"Outputing computed parameter values\" ) print ( f \"\"\"Particle type: proton Beam momentum = { parameters [ \"pc_GeV\" ] : 2.3f } GeV/c Normalized x-emittance = { parameters [ \"en_x_m\" ] * 1e6 : 2.3f } mm mrad Normalized y-emittance = { parameters [ \"en_y_m\" ] * 1e6 : 2.3f } mm mrad Momentum deviation deltap/p = { parameters [ \"deltap_p\" ] } -> Beam total energy = { parameters [ \"E_tot_GeV\" ] : 2.3f } GeV -> Beam kinetic energy = { parameters [ \"E_kin_GeV\" ] : 2.3f } GeV -> Beam rigidity = { parameters [ \"B_rho_Tm\" ] : 2.3f } Tm -> Relativistic beta = { parameters [ \"beta_r\" ] : 2.5f } -> Relativistic gamma = { parameters [ \"gamma_r\" ] : 2.3f } -> Geometrical x emittance = { parameters [ \"eg_x_m\" ] * 1e6 : 2.3f } mm mrad -> Geometrical y emittance = { parameters [ \"eg_y_m\" ] * 1e6 : 2.3f } mm mrad \"\"\" ) return parameters","title":"Module pyhdtoolkit.cpymadtools.parameters"},{"location":"reference/pyhdtoolkit/cpymadtools/parameters/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/parameters/#beam_parameters","text":"def beam_parameters ( pc_gev : float , en_x_m : float , en_y_m : float , deltap_p : float , verbose : bool = False ) -> Dict [ str , float ] Calculate beam parameters from provided values. Parameters: Name Type Description Default pc_gev float particle momentum. None en_x_m float horizontal emittance, in meters. None en_y_m float vertical emittance, in meters. None deltap_p float momentum deviation. None verbose bool bolean, whether to print out a summary of parameters or not. None Returns: Type Description None A dictionnary with the calculated values. View Source def beam_parameters ( pc_gev : float , en_x_m : float , en_y_m : float , deltap_p : float , verbose : bool = False , ) -> Dict [ str, float ] : \"\"\"Calculate beam parameters from provided values. Args: pc_gev (float): particle momentum. en_x_m (float): horizontal emittance, in meters. en_y_m (float): vertical emittance, in meters. deltap_p (float): momentum deviation. verbose (bool): bolean, whether to print out a summary of parameters or not. Returns: A dictionnary with the calculated values. \"\"\" e0_gev = 0.9382720813 e_tot_gev = np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) gamma_r = e_tot_gev / e0_gev beta_r = pc_gev / np . sqrt ( pc_gev ** 2 + e0_gev ** 2 ) parameters : Dict [ str, float ] = { \"pc_GeV\" : pc_gev , # Particle momentum [ GeV ] \"B_rho_Tm\" : 3.3356 * pc_gev , # Beam rigidity [ T/m ] \"E_0_GeV\" : e0_gev , # Rest mass energy [ GeV ] \"E_tot_GeV\" : e_tot_gev , # Total beam energy [ GeV ] \"E_kin_GeV\" : e_tot_gev - e0_gev , # Kinectic beam energy [ GeV ] \"gamma_r\" : gamma_r , # Relativistic gamma \"beta_r\" : beta_r , # Relativistic beta \"en_x_m\" : en_x_m , # Horizontal emittance [ m ] \"en_y_m\" : en_y_m , # Vertical emittance [ m ] \"eg_x_m\" : en_x_m / gamma_r / beta_r , # Horizontal geometrical emittance \"eg_y_m\" : en_y_m / gamma_r / beta_r , # Vertical geometrical emittance \"deltap_p\" : deltap_p , # Momentum deviation } if verbose : logger . trace ( \"Outputing computed parameter values\" ) print ( f \"\"\"Particle type: proton Beam momentum = {parameters[\" pc_GeV \"]:2.3f} GeV/c Normalized x-emittance = {parameters[\" en_x_m \"] * 1e6:2.3f} mm mrad Normalized y-emittance = {parameters[\" en_y_m \"] * 1e6:2.3f} mm mrad Momentum deviation deltap/p = {parameters[\" deltap_p \"]} -> Beam total energy = {parameters[\" E_tot_GeV \"]:2.3f} GeV -> Beam kinetic energy = {parameters[\" E_kin_GeV \"]:2.3f} GeV -> Beam rigidity = {parameters[\" B_rho_Tm \"]:2.3f} Tm -> Relativistic beta = {parameters[\" beta_r \"]:2.5f} -> Relativistic gamma = {parameters[\" gamma_r \"]:2.3f} -> Geometrical x emittance = {parameters[\" eg_x_m \"] * 1e6:2.3f} mm mrad -> Geometrical y emittance = {parameters[\" eg_y_m \"] * 1e6:2.3f} mm mrad \"\"\" ) return parameters","title":"beam_parameters"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/","text":"Module pyhdtoolkit.cpymadtools.plotters Module cpymadtools.plotters Created on 2019.12.08 View Source \"\"\" Module cpymadtools.plotters --------------------------- Created on 2019.12.08 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions to plot different output results from a cpymad.madx.Madx object's simulation results. \"\"\" from pathlib import Path from typing import Dict , Tuple import matplotlib import matplotlib.pyplot as plt import numpy as np import pandas as pd from cpymad.madx import Madx from loguru import logger from matplotlib import colors as mcolors from pyhdtoolkit.optics.twiss import courant_snyder_transform from pyhdtoolkit.plotting.settings import PLOT_PARAMS plt . rcParams . update ( PLOT_PARAMS ) COLORS_DICT = dict ( mcolors . BASE_COLORS , ** mcolors . CSS4_COLORS ) BY_HSV = sorted ( ( tuple ( mcolors . rgb_to_hsv ( mcolors . to_rgba ( color )[: 3 ])), name ) for name , color in COLORS_DICT . items () ) SORTED_COLORS = [ name for hsv , name in BY_HSV ] class AperturePlotter : \"\"\" A class to plot the physical aperture of your machine. \"\"\" @staticmethod def plot_aperture ( cpymad_instance : Madx , beam_params : Dict [ str , float ], figsize : Tuple [ int , int ] = ( 13 , 20 ), xlimits : Tuple [ float , float ] = None , hplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. beam_params (Dict[str, float]): a beam_parameters dictionary obtained through cpymadtools.helpers.beam_parameters. figsize (str): size of the figure, defaults to (15, 15). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. hplane_ylim (Tuple[float, float]): the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). vplane_ylim (Tuple[float, float]): the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint: disable=too-many-arguments # We need to interpolate in order to get high resolution along the s direction logger . debug ( \"Running interpolation in cpymad\" ) cpymad_instance . input ( \"\"\" select, flag=interpolate, class=drift, slice=4, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=8, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; select, flag=interpolate, class=rbend, slice=10, range=#s/#e; twiss; \"\"\" ) logger . debug ( \"Getting Twiss dframe from cpymad\" ) twiss_hr : pd . DataFrame = cpymad_instance . table . twiss . dframe () twiss_hr [ \"betatronic_envelope_x\" ] = np . sqrt ( twiss_hr . betx . values * beam_params [ \"eg_y_m\" ]) twiss_hr [ \"betatronic_envelope_y\" ] = np . sqrt ( twiss_hr . bety . values * beam_params [ \"eg_y_m\" ]) twiss_hr [ \"dispersive_envelope_x\" ] = twiss_hr . dx . values * beam_params [ \"deltap_p\" ] twiss_hr [ \"dispersive_envelope_y\" ] = twiss_hr . dy . values * beam_params [ \"deltap_p\" ] twiss_hr [ \"envelope_x\" ] = np . sqrt ( twiss_hr . betatronic_envelope_x . values ** 2 + ( twiss_hr . dx . values * beam_params [ \"deltap_p\" ]) ** 2 ) twiss_hr [ \"envelope_y\" ] = np . sqrt ( twiss_hr . betatronic_envelope_y . values ** 2 + ( twiss_hr . dy . values * beam_params [ \"deltap_p\" ]) ** 2 ) machine = twiss_hr [ twiss_hr . apertype == \"ellipse\" ] figure = plt . figure ( figsize = figsize ) logger . debug ( \"Plotting the horizontal aperture\" ) axis1 = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) axis1 . plot ( twiss_hr . s , twiss_hr . envelope_x , color = \"b\" ) axis1 . plot ( twiss_hr . s , - twiss_hr . envelope_x , color = \"b\" ) axis1 . fill_between ( twiss_hr . s , twiss_hr . envelope_x , - twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_x , - 3 * twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( machine . s , machine . aper_1 , machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . fill_between ( machine . s , - machine . aper_1 , - machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . plot ( machine . s , machine . aper_1 , \"k.-\" ) axis1 . plot ( machine . s , - machine . aper_1 , \"k.-\" ) axis1 . set_xlim ( xlimits ) axis1 . set_ylim ( hplane_ylim ) axis1 . set_ylabel ( \"x [m]\" ) axis1 . set_xlabel ( \"s [m]\" ) axis1 . set_title ( f \"Horizontal aperture at { beam_params [ 'pc_GeV' ] } GeV/c\" ) logger . debug ( \"Plotting the vertical aperture\" ) axis2 = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis2 . plot ( twiss_hr . s , twiss_hr . envelope_y , color = \"r\" ) axis2 . plot ( twiss_hr . s , - twiss_hr . envelope_y , color = \"r\" ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( machine . s , machine . aper_2 , machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . fill_between ( machine . s , - machine . aper_2 , - machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . plot ( machine . s , machine . aper_2 , \"k.-\" ) axis2 . plot ( machine . s , - machine . aper_2 , \"k.-\" ) axis2 . set_ylim ( vplane_ylim ) axis2 . set_ylabel ( \"y [m]\" ) axis2 . set_xlabel ( \"s [m]\" ) axis2 . set_title ( f \"Vertical aperture at { beam_params [ 'pc_GeV' ] } GeV/c\" ) logger . debug ( \"Plotting the stay-clear envelope\" ) axis3 = plt . subplot2grid (( 3 , 3 ), ( 2 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis3 . plot ( machine . s , machine . aper_1 / machine . envelope_x , \".-b\" , label = \"Horizontal plane\" ) axis3 . plot ( machine . s , machine . aper_2 / machine . envelope_y , \".-r\" , label = \"Vertical plane\" ) axis3 . set_xlim ( xlimits ) axis3 . set_ylabel ( \"n1\" ) axis3 . set_xlabel ( \"s [m]\" ) axis3 . legend ( loc = \"best\" ) axis3 . set_title ( f \"Stay-clear envelope at { beam_params [ 'pc_GeV' ] } GeV/c\" ) if savefig : logger . info ( f \"Saving aperture plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure class DynamicAperturePlotter : \"\"\" A class to plot the dynamic aperture of your machine. \"\"\" @staticmethod def plot_dynamic_aperture ( vx_coords : np . ndarray , vy_coords : np . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure : \"\"\" Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Args: vx_coords (np.ndarray): numpy array of horizontal coordinates over turns. vy_coords (np.ndarray): numpy array of vertical coordinates over turns. n_particles (int): number of particles simulated. savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" figure = plt . figure ( figsize = ( 12 , 7 )) turn_lost = [] x_in_lost = [] for particle in range ( n_particles ): nb = len ( vx_coords [ particle ]) - max ( np . isnan ( vx_coords [ particle ]) . sum (), np . isnan ( vy_coords [ particle ]) . sum () ) turn_lost . append ( nb ) x_in_lost . append ( vx_coords [ particle ][ 0 ] ** 2 + vy_coords [ particle ][ 0 ] ** 2 ) turn_lost = np . array ( turn_lost ) x_in_lost = np . array ( x_in_lost ) plt . scatter ( turn_lost , x_in_lost * 1000 , linewidths = 0.7 , c = \"darkblue\" , marker = \".\" ) plt . title ( \"Amplitudes lost over turns\" , fontsize = 20 ) plt . xlabel ( \"Number of Turns Survived\" , fontsize = 17 ) plt . ylabel ( \"Initial amplitude [mm]\" , fontsize = 17 ) if savefig : logger . info ( f \"Saving dynamic aperture plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure class PhaseSpacePlotter : \"\"\" A class to plot Courant-Snyder coordinates phase space. \"\"\" @staticmethod def plot_courant_snyder_phase_space ( cpymad_instance : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ): logger . error ( f \"Plane should be either Horizontal or Vertical but ' { plane } ' was given\" ) raise ValueError ( \"Invalid plane value\" ) figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) # Getting the P matrix to compute Courant-Snyder coordinates logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = ( cpymad_instance . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . alfy [ 0 ] ) beta = ( cpymad_instance . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . bety [ 0 ] ) logger . debug ( f \"Plotting Courant-Snyder phase space for the { plane . lower () } plane\" ) for index , _ in enumerate ( u_coordinates ): logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle { index } \" ) u = np . array ([ u_coordinates [ index ], pu_coordinates [ index ]]) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0 , :] * 1e3 , u_bar [ 1 , :] * 1e3 , s = 0.1 , c = \"k\" ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$ \\\\ bar {x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$ \\\\ bar {y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving Courant-Snyder phase space plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure @staticmethod def plot_courant_snyder_phase_space_colored ( cpymad_instance : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156th color. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ): logger . error ( f \"Plane should be either Horizontal or Vertical but ' { plane } ' was given\" ) raise ValueError ( \"Invalid plane value\" ) # Getting a sufficiently long array of colors to use colors = int ( np . floor ( len ( u_coordinates ) / 100 )) * SORTED_COLORS while len ( colors ) > len ( u_coordinates ): colors . pop () figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = ( cpymad_instance . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . alfy [ 0 ] ) beta = ( cpymad_instance . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . bety [ 0 ] ) logger . debug ( f \"Plotting colored normalised phase space for the { plane . lower () } plane\" ) for index , _ in enumerate ( u_coordinates ): logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle { index } \" ) u = np . array ([ u_coordinates [ index ], pu_coordinates [ index ]]) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0 , :] * 1e3 , u_bar [ 1 , :] * 1e3 , s = 0.1 , c = colors [ index ]) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$ \\\\ bar {x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$ \\\\ bar {y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving colored Courant-Snyder phase space plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure class TuneDiagramPlotter : \"\"\" A class to plot a blank tune diagram with Farey sequences, as well as your working points. \"\"\" @staticmethod def farey_sequence ( order : int ) -> list : \"\"\" Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Args: order (int): the order up to which we want to calculate the sequence. Returns: The sequence as a list. \"\"\" seq = [[ 0 , 1 ]] a , b , c , d = 0 , 1 , 1 , order while c <= order : k = int (( order + b ) / d ) a , b , c , d = c , d , k * c - a , k * d - b seq . append ([ a , b ]) return seq @staticmethod def plot_blank_tune_diagram () -> matplotlib . figure . Figure : \"\"\" Plotting the tune diagram up to the 6th order. Original code from Rogelio Tom\u00e1s. Returns: The figure on which resonance lines from farey sequences are drawn. \"\"\" logger . debug ( \"Plotting resonance lines from Farey sequence, up to 5th order\" ) figure = plt . figure ( figsize = ( 13 , 13 )) plt . ylim (( 0 , 1 )) plt . xlim (( 0 , 1 )) x = np . linspace ( 0 , 1 , 1000 ) for i in range ( 1 , 6 ): farey_sequences = TuneDiagramPlotter . farey_sequence ( i ) for f in farey_sequences : h , k = f # Node h/k on the axes for sequence in farey_sequences : p , q = sequence a = float ( k * p ) # Resonance linea Qx + b*Qy = clinkedtop / q if a > 0 : b = float ( q - k * p ) c = float ( p * h ) plt . plot ( x , c / a - x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( x , c / a + x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , 1 - x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , 1 - x , \"b\" , alpha = 0.1 ) if q == k and p == 1 : # FN elements below 1/k break plt . title ( \"Tune Diagram\" , fontsize = 20 ) plt . axis ( \"square\" ) plt . xlim ([ 0 , 1 ]) plt . ylim ([ 0 , 1 ]) plt . xlabel ( \"$Q_ {x} }$\" , fontsize = 17 ) plt . ylabel ( \"$Q_ {y} $\" , fontsize = 17 ) return figure @staticmethod def plot_tune_diagram ( cpymad_instance : Madx , v_qx : np . ndarray = np . array ([ 0 ]), vxgood : np . ndarray = np . array ([ False ]), v_qy : np . ndarray = np . array ([ 0 ]), vygood : np . ndarray = np . array ([ False ]), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plots the evolution of particles' tunes on a Tune Diagram. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. v_qx (np.ndarray): horizontal tune value as a numpy array. vxgood (np.ndarray): ?? v_qy (np.ndarray): vertical tune value as a numpy array. vygood (np.ndarray): ?? savefig: will save the figure if this is not None, using the string value passed. Returns: The figure on which the diagram is drawn. \"\"\" figure = TuneDiagramPlotter . plot_blank_tune_diagram () logger . debug ( \"Getting Tunes from cpymad\" ) new_q1 : float = cpymad_instance . table . summ . dframe () . q1 [ 0 ] new_q2 : float = cpymad_instance . table . summ . dframe () . q2 [ 0 ] if vxgood . any () and vygood . any (): plt . plot ( v_qx [ vxgood * vygood ], v_qy [ vxgood * vygood ], \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif vxgood . any () and ~ vygood . any (): tp = np . ones ( len ( vxgood )) * ( new_q2 - np . floor ( new_q2 )) plt . plot ( v_qx [ vxgood ], tp [ vxgood ], \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif ~ vxgood . any () and vygood . any (): tp = np . ones ( len ( vygood )) * ( new_q1 - np . floor ( new_q1 )) plt . plot ( tp [ vygood ], v_qy [ vygood ], \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) if savefig : logger . info ( f \"Saving Tune diagram plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure Variables BY_HSV COLORS_DICT PLOT_PARAMS SORTED_COLORS Classes AperturePlotter class AperturePlotter ( / , * args , ** kwargs ) View Source class AperturePlotter : \"\"\" A class to plot the physical aperture of your machine. \"\"\" @staticmethod def plot_aperture ( cpymad_instance : Madx , beam_params : Dict [ str, float ] , figsize : Tuple [ int, int ] = ( 13 , 20 ), xlimits : Tuple [ float, float ] = None , hplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. beam_params (Dict[str, float]): a beam_parameters dictionary obtained through cpymadtools.helpers.beam_parameters. figsize (str): size of the figure, defaults to (15, 15). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. hplane_ylim (Tuple[float, float]): the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). vplane_ylim (Tuple[float, float]): the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint : disable = too - many - arguments # We need to interpolate in order to get high resolution along the s direction logger . debug ( \"Running interpolation in cpymad\" ) cpymad_instance . input ( \"\"\" select, flag=interpolate, class=drift, slice=4, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=8, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; select, flag=interpolate, class=rbend, slice=10, range=#s/#e; twiss; \"\"\" ) logger . debug ( \"Getting Twiss dframe from cpymad\" ) twiss_hr : pd . DataFrame = cpymad_instance . table . twiss . dframe () twiss_hr [ \"betatronic_envelope_x\" ] = np . sqrt ( twiss_hr . betx . values * beam_params [ \"eg_y_m\" ] ) twiss_hr [ \"betatronic_envelope_y\" ] = np . sqrt ( twiss_hr . bety . values * beam_params [ \"eg_y_m\" ] ) twiss_hr [ \"dispersive_envelope_x\" ] = twiss_hr . dx . values * beam_params [ \"deltap_p\" ] twiss_hr [ \"dispersive_envelope_y\" ] = twiss_hr . dy . values * beam_params [ \"deltap_p\" ] twiss_hr [ \"envelope_x\" ] = np . sqrt ( twiss_hr . betatronic_envelope_x . values ** 2 + ( twiss_hr . dx . values * beam_params [ \"deltap_p\" ] ) ** 2 ) twiss_hr [ \"envelope_y\" ] = np . sqrt ( twiss_hr . betatronic_envelope_y . values ** 2 + ( twiss_hr . dy . values * beam_params [ \"deltap_p\" ] ) ** 2 ) machine = twiss_hr [ twiss_hr.apertype == \"ellipse\" ] figure = plt . figure ( figsize = figsize ) logger . debug ( \"Plotting the horizontal aperture\" ) axis1 = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) axis1 . plot ( twiss_hr . s , twiss_hr . envelope_x , color = \"b\" ) axis1 . plot ( twiss_hr . s , - twiss_hr . envelope_x , color = \"b\" ) axis1 . fill_between ( twiss_hr . s , twiss_hr . envelope_x , - twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_x , - 3 * twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( machine . s , machine . aper_1 , machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . fill_between ( machine . s , - machine . aper_1 , - machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . plot ( machine . s , machine . aper_1 , \"k.-\" ) axis1 . plot ( machine . s , - machine . aper_1 , \"k.-\" ) axis1 . set_xlim ( xlimits ) axis1 . set_ylim ( hplane_ylim ) axis1 . set_ylabel ( \"x [m]\" ) axis1 . set_xlabel ( \"s [m]\" ) axis1 . set_title ( f \"Horizontal aperture at {beam_params['pc_GeV']} GeV/c\" ) logger . debug ( \"Plotting the vertical aperture\" ) axis2 = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis2 . plot ( twiss_hr . s , twiss_hr . envelope_y , color = \"r\" ) axis2 . plot ( twiss_hr . s , - twiss_hr . envelope_y , color = \"r\" ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( machine . s , machine . aper_2 , machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . fill_between ( machine . s , - machine . aper_2 , - machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . plot ( machine . s , machine . aper_2 , \"k.-\" ) axis2 . plot ( machine . s , - machine . aper_2 , \"k.-\" ) axis2 . set_ylim ( vplane_ylim ) axis2 . set_ylabel ( \"y [m]\" ) axis2 . set_xlabel ( \"s [m]\" ) axis2 . set_title ( f \"Vertical aperture at {beam_params['pc_GeV']} GeV/c\" ) logger . debug ( \"Plotting the stay-clear envelope\" ) axis3 = plt . subplot2grid (( 3 , 3 ), ( 2 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis3 . plot ( machine . s , machine . aper_1 / machine . envelope_x , \".-b\" , label = \"Horizontal plane\" ) axis3 . plot ( machine . s , machine . aper_2 / machine . envelope_y , \".-r\" , label = \"Vertical plane\" ) axis3 . set_xlim ( xlimits ) axis3 . set_ylabel ( \"n1\" ) axis3 . set_xlabel ( \"s [m]\" ) axis3 . legend ( loc = \"best\" ) axis3 . set_title ( f \"Stay-clear envelope at {beam_params['pc_GeV']} GeV/c\" ) if savefig : logger . info ( f \"Saving aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure Static methods plot_aperture def plot_aperture ( cpymad_instance : cpymad . madx . Madx , beam_params : Dict [ str , float ], figsize : Tuple [ int , int ] = ( 13 , 20 ), xlimits : Tuple [ float , float ] = None , hplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), savefig : str = None ) -> matplotlib . figure . Figure Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None beam_params Dict[str, float] a beam_parameters dictionary obtained through cpymadtools.helpers.beam_parameters. None figsize str size of the figure, defaults to (15, 15). None xlimits Tuple[float, float] will implement xlim (for the s coordinate) if this is not None, using the tuple passed. None hplane_ylim Tuple[float, float] the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). None vplane_ylim Tuple[float, float] the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). None savefig str will save the figure if this is not None, using the string value passed. None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_aperture ( cpymad_instance : Madx , beam_params : Dict [ str, float ] , figsize : Tuple [ int, int ] = ( 13 , 20 ), xlimits : Tuple [ float, float ] = None , hplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. beam_params (Dict[str, float]): a beam_parameters dictionary obtained through cpymadtools.helpers.beam_parameters. figsize (str): size of the figure, defaults to (15, 15). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. hplane_ylim (Tuple[float, float]): the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). vplane_ylim (Tuple[float, float]): the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint : disable = too - many - arguments # We need to interpolate in order to get high resolution along the s direction logger . debug ( \"Running interpolation in cpymad\" ) cpymad_instance . input ( \"\"\" select, flag=interpolate, class=drift, slice=4, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=8, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; select, flag=interpolate, class=rbend, slice=10, range=#s/#e; twiss; \"\"\" ) logger . debug ( \"Getting Twiss dframe from cpymad\" ) twiss_hr : pd . DataFrame = cpymad_instance . table . twiss . dframe () twiss_hr [ \"betatronic_envelope_x\" ] = np . sqrt ( twiss_hr . betx . values * beam_params [ \"eg_y_m\" ] ) twiss_hr [ \"betatronic_envelope_y\" ] = np . sqrt ( twiss_hr . bety . values * beam_params [ \"eg_y_m\" ] ) twiss_hr [ \"dispersive_envelope_x\" ] = twiss_hr . dx . values * beam_params [ \"deltap_p\" ] twiss_hr [ \"dispersive_envelope_y\" ] = twiss_hr . dy . values * beam_params [ \"deltap_p\" ] twiss_hr [ \"envelope_x\" ] = np . sqrt ( twiss_hr . betatronic_envelope_x . values ** 2 + ( twiss_hr . dx . values * beam_params [ \"deltap_p\" ] ) ** 2 ) twiss_hr [ \"envelope_y\" ] = np . sqrt ( twiss_hr . betatronic_envelope_y . values ** 2 + ( twiss_hr . dy . values * beam_params [ \"deltap_p\" ] ) ** 2 ) machine = twiss_hr [ twiss_hr.apertype == \"ellipse\" ] figure = plt . figure ( figsize = figsize ) logger . debug ( \"Plotting the horizontal aperture\" ) axis1 = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) axis1 . plot ( twiss_hr . s , twiss_hr . envelope_x , color = \"b\" ) axis1 . plot ( twiss_hr . s , - twiss_hr . envelope_x , color = \"b\" ) axis1 . fill_between ( twiss_hr . s , twiss_hr . envelope_x , - twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_x , - 3 * twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( machine . s , machine . aper_1 , machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . fill_between ( machine . s , - machine . aper_1 , - machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . plot ( machine . s , machine . aper_1 , \"k.-\" ) axis1 . plot ( machine . s , - machine . aper_1 , \"k.-\" ) axis1 . set_xlim ( xlimits ) axis1 . set_ylim ( hplane_ylim ) axis1 . set_ylabel ( \"x [m]\" ) axis1 . set_xlabel ( \"s [m]\" ) axis1 . set_title ( f \"Horizontal aperture at {beam_params['pc_GeV']} GeV/c\" ) logger . debug ( \"Plotting the vertical aperture\" ) axis2 = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis2 . plot ( twiss_hr . s , twiss_hr . envelope_y , color = \"r\" ) axis2 . plot ( twiss_hr . s , - twiss_hr . envelope_y , color = \"r\" ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( machine . s , machine . aper_2 , machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . fill_between ( machine . s , - machine . aper_2 , - machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . plot ( machine . s , machine . aper_2 , \"k.-\" ) axis2 . plot ( machine . s , - machine . aper_2 , \"k.-\" ) axis2 . set_ylim ( vplane_ylim ) axis2 . set_ylabel ( \"y [m]\" ) axis2 . set_xlabel ( \"s [m]\" ) axis2 . set_title ( f \"Vertical aperture at {beam_params['pc_GeV']} GeV/c\" ) logger . debug ( \"Plotting the stay-clear envelope\" ) axis3 = plt . subplot2grid (( 3 , 3 ), ( 2 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis3 . plot ( machine . s , machine . aper_1 / machine . envelope_x , \".-b\" , label = \"Horizontal plane\" ) axis3 . plot ( machine . s , machine . aper_2 / machine . envelope_y , \".-r\" , label = \"Vertical plane\" ) axis3 . set_xlim ( xlimits ) axis3 . set_ylabel ( \"n1\" ) axis3 . set_xlabel ( \"s [m]\" ) axis3 . legend ( loc = \"best\" ) axis3 . set_title ( f \"Stay-clear envelope at {beam_params['pc_GeV']} GeV/c\" ) if savefig : logger . info ( f \"Saving aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure DynamicAperturePlotter class DynamicAperturePlotter ( / , * args , ** kwargs ) View Source class DynamicAperturePlotter : \"\"\" A class to plot the dynamic aperture of your machine. \"\"\" @staticmethod def plot_dynamic_aperture ( vx_coords : np . ndarray , vy_coords : np . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure : \"\"\" Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Args: vx_coords (np.ndarray): numpy array of horizontal coordinates over turns. vy_coords (np.ndarray): numpy array of vertical coordinates over turns. n_particles (int): number of particles simulated. savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" figure = plt . figure ( figsize = ( 12 , 7 )) turn_lost = [] x_in_lost = [] for particle in range ( n_particles ) : nb = len ( vx_coords [ particle ] ) - max ( np . isnan ( vx_coords [ particle ] ). sum (), np . isnan ( vy_coords [ particle ] ). sum () ) turn_lost . append ( nb ) x_in_lost . append ( vx_coords [ particle ][ 0 ] ** 2 + vy_coords [ particle ][ 0 ] ** 2 ) turn_lost = np . array ( turn_lost ) x_in_lost = np . array ( x_in_lost ) plt . scatter ( turn_lost , x_in_lost * 1000 , linewidths = 0.7 , c = \"darkblue\" , marker = \".\" ) plt . title ( \"Amplitudes lost over turns\" , fontsize = 20 ) plt . xlabel ( \"Number of Turns Survived\" , fontsize = 17 ) plt . ylabel ( \"Initial amplitude [mm]\" , fontsize = 17 ) if savefig : logger . info ( f \"Saving dynamic aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure Static methods plot_dynamic_aperture def plot_dynamic_aperture ( vx_coords : numpy . ndarray , vy_coords : numpy . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Parameters: Name Type Description Default vx_coords np.ndarray numpy array of horizontal coordinates over turns. None vy_coords np.ndarray numpy array of vertical coordinates over turns. None n_particles int number of particles simulated. None savefig str will save the figure if this is not None, using the string value passed. None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_dynamic_aperture ( vx_coords : np . ndarray , vy_coords : np . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure : \"\"\" Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Args: vx_coords (np.ndarray): numpy array of horizontal coordinates over turns. vy_coords (np.ndarray): numpy array of vertical coordinates over turns. n_particles (int): number of particles simulated. savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" figure = plt . figure ( figsize = ( 12 , 7 )) turn_lost = [] x_in_lost = [] for particle in range ( n_particles ) : nb = len ( vx_coords [ particle ] ) - max ( np . isnan ( vx_coords [ particle ] ). sum (), np . isnan ( vy_coords [ particle ] ). sum () ) turn_lost . append ( nb ) x_in_lost . append ( vx_coords [ particle ][ 0 ] ** 2 + vy_coords [ particle ][ 0 ] ** 2 ) turn_lost = np . array ( turn_lost ) x_in_lost = np . array ( x_in_lost ) plt . scatter ( turn_lost , x_in_lost * 1000 , linewidths = 0.7 , c = \"darkblue\" , marker = \".\" ) plt . title ( \"Amplitudes lost over turns\" , fontsize = 20 ) plt . xlabel ( \"Number of Turns Survived\" , fontsize = 17 ) plt . ylabel ( \"Initial amplitude [mm]\" , fontsize = 17 ) if savefig : logger . info ( f \"Saving dynamic aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure PhaseSpacePlotter class PhaseSpacePlotter ( / , * args , ** kwargs ) View Source class PhaseSpacePlotter : \"\"\" A class to plot Courant-Snyder coordinates phase space. \"\"\" @staticmethod def plot_courant_snyder_phase_space ( cpymad_instance : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) # Getting the P matrix to compute Courant - Snyder coordinates logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = ( cpymad_instance . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . alfy [ 0 ] ) beta = ( cpymad_instance . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . bety [ 0 ] ) logger . debug ( f \"Plotting Courant-Snyder phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = \"k\" ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure @staticmethod def plot_courant_snyder_phase_space_colored ( cpymad_instance : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156th color. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) # Getting a sufficiently long array of colors to use colors = int ( np . floor ( len ( u_coordinates ) / 100 )) * SORTED_COLORS while len ( colors ) > len ( u_coordinates ) : colors . pop () figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = ( cpymad_instance . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . alfy [ 0 ] ) beta = ( cpymad_instance . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . bety [ 0 ] ) logger . debug ( f \"Plotting colored normalised phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = colors [ index ] ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving colored Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure Static methods plot_courant_snyder_phase_space def plot_courant_snyder_phase_space ( cpymad_instance : cpymad . madx . Madx , u_coordinates : numpy . ndarray , pu_coordinates : numpy . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = 'Horizontal' ) -> matplotlib . figure . Figure Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None u_coordinates np.ndarray numpy array of particles's coordinates for the given plane. None pu_coordinates np.ndarray numpy array of particles's momentum coordinates for the given plane. None savefig str will save the figure if this is not None, using the string value passed. None size Tuple[int, int] the wanted matplotlib figure size. Defaults to (16, 8). (16, 8) plane str the physical plane to plot. Defaults to 'Horizontal'. 'Horizontal' Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_courant_snyder_phase_space ( cpymad_instance : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) # Getting the P matrix to compute Courant - Snyder coordinates logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = ( cpymad_instance . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . alfy [ 0 ] ) beta = ( cpymad_instance . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . bety [ 0 ] ) logger . debug ( f \"Plotting Courant-Snyder phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = \"k\" ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure plot_courant_snyder_phase_space_colored def plot_courant_snyder_phase_space_colored ( cpymad_instance : cpymad . madx . Madx , u_coordinates : numpy . ndarray , pu_coordinates : numpy . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = 'Horizontal' ) -> matplotlib . figure . Figure Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156 th color. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None u_coordinates np.ndarray numpy array of particles's coordinates for the given plane. None pu_coordinates np.ndarray numpy array of particles's momentum coordinates for the given plane. None savefig str will save the figure if this is not None, using the string value passed. None size Tuple[int, int] the wanted matplotlib figure size. Defaults to (16, 8). (16, 8) plane str the physical plane to plot. Defaults to 'Horizontal'. 'Horizontal' Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_courant_snyder_phase_space_colored ( cpymad_instance : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156th color. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) # Getting a sufficiently long array of colors to use colors = int ( np . floor ( len ( u_coordinates ) / 100 )) * SORTED_COLORS while len ( colors ) > len ( u_coordinates ) : colors . pop () figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = ( cpymad_instance . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . alfy [ 0 ] ) beta = ( cpymad_instance . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . bety [ 0 ] ) logger . debug ( f \"Plotting colored normalised phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = colors [ index ] ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving colored Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure TuneDiagramPlotter class TuneDiagramPlotter ( / , * args , ** kwargs ) View Source class TuneDiagramPlotter : \"\"\" A class to plot a blank tune diagram with Farey sequences, as well as your working points. \"\"\" @staticmethod def farey_sequence ( order : int ) -> list : \"\"\" Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Args: order (int): the order up to which we want to calculate the sequence. Returns: The sequence as a list. \"\"\" seq = [ [0, 1 ] ] a , b , c , d = 0 , 1 , 1 , order while c <= order : k = int (( order + b ) / d ) a , b , c , d = c , d , k * c - a , k * d - b seq . append ( [ a, b ] ) return seq @staticmethod def plot_blank_tune_diagram () -> matplotlib . figure . Figure : \"\"\" Plotting the tune diagram up to the 6th order. Original code from Rogelio Tom\u00e1s. Returns: The figure on which resonance lines from farey sequences are drawn. \"\"\" logger . debug ( \"Plotting resonance lines from Farey sequence, up to 5th order\" ) figure = plt . figure ( figsize = ( 13 , 13 )) plt . ylim (( 0 , 1 )) plt . xlim (( 0 , 1 )) x = np . linspace ( 0 , 1 , 1000 ) for i in range ( 1 , 6 ) : farey_sequences = TuneDiagramPlotter . farey_sequence ( i ) for f in farey_sequences : h , k = f # Node h / k on the axes for sequence in farey_sequences : p , q = sequence a = float ( k * p ) # Resonance linea Qx + b * Qy = clinkedtop / q if a > 0 : b = float ( q - k * p ) c = float ( p * h ) plt . plot ( x , c / a - x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( x , c / a + x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , 1 - x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , 1 - x , \"b\" , alpha = 0.1 ) if q == k and p == 1 : # FN elements below 1 / k break plt . title ( \"Tune Diagram\" , fontsize = 20 ) plt . axis ( \"square\" ) plt . xlim ( [ 0, 1 ] ) plt . ylim ( [ 0, 1 ] ) plt . xlabel ( \"$Q_{x}}$\" , fontsize = 17 ) plt . ylabel ( \"$Q_{y}$\" , fontsize = 17 ) return figure @staticmethod def plot_tune_diagram ( cpymad_instance : Madx , v_qx : np . ndarray = np . array ( [ 0 ] ), vxgood : np . ndarray = np . array ( [ False ] ), v_qy : np . ndarray = np . array ( [ 0 ] ), vygood : np . ndarray = np . array ( [ False ] ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plots the evolution of particles' tunes on a Tune Diagram. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. v_qx (np.ndarray): horizontal tune value as a numpy array. vxgood (np.ndarray): ?? v_qy (np.ndarray): vertical tune value as a numpy array. vygood (np.ndarray): ?? savefig: will save the figure if this is not None, using the string value passed. Returns: The figure on which the diagram is drawn. \"\"\" figure = TuneDiagramPlotter . plot_blank_tune_diagram () logger . debug ( \"Getting Tunes from cpymad\" ) new_q1 : float = cpymad_instance . table . summ . dframe (). q1 [ 0 ] new_q2 : float = cpymad_instance . table . summ . dframe (). q2 [ 0 ] if vxgood . any () and vygood . any () : plt . plot ( v_qx [ vxgood * vygood ] , v_qy [ vxgood * vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif vxgood . any () and ~ vygood . any () : tp = np . ones ( len ( vxgood )) * ( new_q2 - np . floor ( new_q2 )) plt . plot ( v_qx [ vxgood ] , tp [ vxgood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif ~ vxgood . any () and vygood . any () : tp = np . ones ( len ( vygood )) * ( new_q1 - np . floor ( new_q1 )) plt . plot ( tp [ vygood ] , v_qy [ vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) if savefig : logger . info ( f \"Saving Tune diagram plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure Static methods farey_sequence def farey_sequence ( order : int ) -> list Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Parameters: Name Type Description Default order int the order up to which we want to calculate the sequence. None Returns: Type Description None The sequence as a list. View Source @staticmethod def farey_sequence ( order : int ) -> list : \"\"\" Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Args: order (int): the order up to which we want to calculate the sequence. Returns: The sequence as a list. \"\"\" seq = [ [0, 1 ] ] a , b , c , d = 0 , 1 , 1 , order while c <= order : k = int (( order + b ) / d ) a , b , c , d = c , d , k * c - a , k * d - b seq . append ( [ a, b ] ) return seq plot_blank_tune_diagram def plot_blank_tune_diagram ( ) -> matplotlib . figure . Figure Plotting the tune diagram up to the 6 th order. Original code from Rogelio Tom\u00e1s. Returns: Type Description None The figure on which resonance lines from farey sequences are drawn. View Source @staticmethod def plot_blank_tune_diagram () -> matplotlib . figure . Figure : \"\"\" Plotting the tune diagram up to the 6th order. Original code from Rogelio Tom\u00e1s. Returns: The figure on which resonance lines from farey sequences are drawn. \"\"\" logger . debug ( \"Plotting resonance lines from Farey sequence, up to 5th order\" ) figure = plt . figure ( figsize = ( 13 , 13 )) plt . ylim (( 0 , 1 )) plt . xlim (( 0 , 1 )) x = np . linspace ( 0 , 1 , 1000 ) for i in range ( 1 , 6 ) : farey_sequences = TuneDiagramPlotter . farey_sequence ( i ) for f in farey_sequences : h , k = f # Node h / k on the axes for sequence in farey_sequences : p , q = sequence a = float ( k * p ) # Resonance linea Qx + b * Qy = clinkedtop / q if a > 0 : b = float ( q - k * p ) c = float ( p * h ) plt . plot ( x , c / a - x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( x , c / a + x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , 1 - x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , 1 - x , \"b\" , alpha = 0.1 ) if q == k and p == 1 : # FN elements below 1 / k break plt . title ( \"Tune Diagram\" , fontsize = 20 ) plt . axis ( \"square\" ) plt . xlim ( [ 0, 1 ] ) plt . ylim ( [ 0, 1 ] ) plt . xlabel ( \"$Q_{x}}$\" , fontsize = 17 ) plt . ylabel ( \"$Q_{y}$\" , fontsize = 17 ) return figure plot_tune_diagram def plot_tune_diagram ( cpymad_instance : cpymad . madx . Madx , v_qx : numpy . ndarray = array ([ 0 ]), vxgood : numpy . ndarray = array ([ False ]), v_qy : numpy . ndarray = array ([ 0 ]), vygood : numpy . ndarray = array ([ False ]), savefig : str = None ) -> matplotlib . figure . Figure Plots the evolution of particles' tunes on a Tune Diagram. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None v_qx np.ndarray horizontal tune value as a numpy array. None vxgood np.ndarray ?? None v_qy np.ndarray vertical tune value as a numpy array. None vygood np.ndarray ?? None savefig None will save the figure if this is not None, using the string value passed. None Returns: Type Description None The figure on which the diagram is drawn. View Source @staticmethod def plot_tune_diagram ( cpymad_instance : Madx , v_qx : np . ndarray = np . array ( [ 0 ] ), vxgood : np . ndarray = np . array ( [ False ] ), v_qy : np . ndarray = np . array ( [ 0 ] ), vygood : np . ndarray = np . array ( [ False ] ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plots the evolution of particles' tunes on a Tune Diagram. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. v_qx (np.ndarray): horizontal tune value as a numpy array. vxgood (np.ndarray): ?? v_qy (np.ndarray): vertical tune value as a numpy array. vygood (np.ndarray): ?? savefig: will save the figure if this is not None, using the string value passed. Returns: The figure on which the diagram is drawn. \"\"\" figure = TuneDiagramPlotter . plot_blank_tune_diagram () logger . debug ( \"Getting Tunes from cpymad\" ) new_q1 : float = cpymad_instance . table . summ . dframe (). q1 [ 0 ] new_q2 : float = cpymad_instance . table . summ . dframe (). q2 [ 0 ] if vxgood . any () and vygood . any () : plt . plot ( v_qx [ vxgood * vygood ] , v_qy [ vxgood * vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif vxgood . any () and ~ vygood . any () : tp = np . ones ( len ( vxgood )) * ( new_q2 - np . floor ( new_q2 )) plt . plot ( v_qx [ vxgood ] , tp [ vxgood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif ~ vxgood . any () and vygood . any () : tp = np . ones ( len ( vygood )) * ( new_q1 - np . floor ( new_q1 )) plt . plot ( tp [ vygood ] , v_qy [ vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) if savefig : logger . info ( f \"Saving Tune diagram plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"Plotters"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#module-pyhdtoolkitcpymadtoolsplotters","text":"Module cpymadtools.plotters Created on 2019.12.08 View Source \"\"\" Module cpymadtools.plotters --------------------------- Created on 2019.12.08 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions to plot different output results from a cpymad.madx.Madx object's simulation results. \"\"\" from pathlib import Path from typing import Dict , Tuple import matplotlib import matplotlib.pyplot as plt import numpy as np import pandas as pd from cpymad.madx import Madx from loguru import logger from matplotlib import colors as mcolors from pyhdtoolkit.optics.twiss import courant_snyder_transform from pyhdtoolkit.plotting.settings import PLOT_PARAMS plt . rcParams . update ( PLOT_PARAMS ) COLORS_DICT = dict ( mcolors . BASE_COLORS , ** mcolors . CSS4_COLORS ) BY_HSV = sorted ( ( tuple ( mcolors . rgb_to_hsv ( mcolors . to_rgba ( color )[: 3 ])), name ) for name , color in COLORS_DICT . items () ) SORTED_COLORS = [ name for hsv , name in BY_HSV ] class AperturePlotter : \"\"\" A class to plot the physical aperture of your machine. \"\"\" @staticmethod def plot_aperture ( cpymad_instance : Madx , beam_params : Dict [ str , float ], figsize : Tuple [ int , int ] = ( 13 , 20 ), xlimits : Tuple [ float , float ] = None , hplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. beam_params (Dict[str, float]): a beam_parameters dictionary obtained through cpymadtools.helpers.beam_parameters. figsize (str): size of the figure, defaults to (15, 15). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. hplane_ylim (Tuple[float, float]): the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). vplane_ylim (Tuple[float, float]): the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint: disable=too-many-arguments # We need to interpolate in order to get high resolution along the s direction logger . debug ( \"Running interpolation in cpymad\" ) cpymad_instance . input ( \"\"\" select, flag=interpolate, class=drift, slice=4, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=8, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; select, flag=interpolate, class=rbend, slice=10, range=#s/#e; twiss; \"\"\" ) logger . debug ( \"Getting Twiss dframe from cpymad\" ) twiss_hr : pd . DataFrame = cpymad_instance . table . twiss . dframe () twiss_hr [ \"betatronic_envelope_x\" ] = np . sqrt ( twiss_hr . betx . values * beam_params [ \"eg_y_m\" ]) twiss_hr [ \"betatronic_envelope_y\" ] = np . sqrt ( twiss_hr . bety . values * beam_params [ \"eg_y_m\" ]) twiss_hr [ \"dispersive_envelope_x\" ] = twiss_hr . dx . values * beam_params [ \"deltap_p\" ] twiss_hr [ \"dispersive_envelope_y\" ] = twiss_hr . dy . values * beam_params [ \"deltap_p\" ] twiss_hr [ \"envelope_x\" ] = np . sqrt ( twiss_hr . betatronic_envelope_x . values ** 2 + ( twiss_hr . dx . values * beam_params [ \"deltap_p\" ]) ** 2 ) twiss_hr [ \"envelope_y\" ] = np . sqrt ( twiss_hr . betatronic_envelope_y . values ** 2 + ( twiss_hr . dy . values * beam_params [ \"deltap_p\" ]) ** 2 ) machine = twiss_hr [ twiss_hr . apertype == \"ellipse\" ] figure = plt . figure ( figsize = figsize ) logger . debug ( \"Plotting the horizontal aperture\" ) axis1 = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) axis1 . plot ( twiss_hr . s , twiss_hr . envelope_x , color = \"b\" ) axis1 . plot ( twiss_hr . s , - twiss_hr . envelope_x , color = \"b\" ) axis1 . fill_between ( twiss_hr . s , twiss_hr . envelope_x , - twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_x , - 3 * twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( machine . s , machine . aper_1 , machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . fill_between ( machine . s , - machine . aper_1 , - machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . plot ( machine . s , machine . aper_1 , \"k.-\" ) axis1 . plot ( machine . s , - machine . aper_1 , \"k.-\" ) axis1 . set_xlim ( xlimits ) axis1 . set_ylim ( hplane_ylim ) axis1 . set_ylabel ( \"x [m]\" ) axis1 . set_xlabel ( \"s [m]\" ) axis1 . set_title ( f \"Horizontal aperture at { beam_params [ 'pc_GeV' ] } GeV/c\" ) logger . debug ( \"Plotting the vertical aperture\" ) axis2 = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis2 . plot ( twiss_hr . s , twiss_hr . envelope_y , color = \"r\" ) axis2 . plot ( twiss_hr . s , - twiss_hr . envelope_y , color = \"r\" ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( machine . s , machine . aper_2 , machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . fill_between ( machine . s , - machine . aper_2 , - machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . plot ( machine . s , machine . aper_2 , \"k.-\" ) axis2 . plot ( machine . s , - machine . aper_2 , \"k.-\" ) axis2 . set_ylim ( vplane_ylim ) axis2 . set_ylabel ( \"y [m]\" ) axis2 . set_xlabel ( \"s [m]\" ) axis2 . set_title ( f \"Vertical aperture at { beam_params [ 'pc_GeV' ] } GeV/c\" ) logger . debug ( \"Plotting the stay-clear envelope\" ) axis3 = plt . subplot2grid (( 3 , 3 ), ( 2 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis3 . plot ( machine . s , machine . aper_1 / machine . envelope_x , \".-b\" , label = \"Horizontal plane\" ) axis3 . plot ( machine . s , machine . aper_2 / machine . envelope_y , \".-r\" , label = \"Vertical plane\" ) axis3 . set_xlim ( xlimits ) axis3 . set_ylabel ( \"n1\" ) axis3 . set_xlabel ( \"s [m]\" ) axis3 . legend ( loc = \"best\" ) axis3 . set_title ( f \"Stay-clear envelope at { beam_params [ 'pc_GeV' ] } GeV/c\" ) if savefig : logger . info ( f \"Saving aperture plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure class DynamicAperturePlotter : \"\"\" A class to plot the dynamic aperture of your machine. \"\"\" @staticmethod def plot_dynamic_aperture ( vx_coords : np . ndarray , vy_coords : np . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure : \"\"\" Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Args: vx_coords (np.ndarray): numpy array of horizontal coordinates over turns. vy_coords (np.ndarray): numpy array of vertical coordinates over turns. n_particles (int): number of particles simulated. savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" figure = plt . figure ( figsize = ( 12 , 7 )) turn_lost = [] x_in_lost = [] for particle in range ( n_particles ): nb = len ( vx_coords [ particle ]) - max ( np . isnan ( vx_coords [ particle ]) . sum (), np . isnan ( vy_coords [ particle ]) . sum () ) turn_lost . append ( nb ) x_in_lost . append ( vx_coords [ particle ][ 0 ] ** 2 + vy_coords [ particle ][ 0 ] ** 2 ) turn_lost = np . array ( turn_lost ) x_in_lost = np . array ( x_in_lost ) plt . scatter ( turn_lost , x_in_lost * 1000 , linewidths = 0.7 , c = \"darkblue\" , marker = \".\" ) plt . title ( \"Amplitudes lost over turns\" , fontsize = 20 ) plt . xlabel ( \"Number of Turns Survived\" , fontsize = 17 ) plt . ylabel ( \"Initial amplitude [mm]\" , fontsize = 17 ) if savefig : logger . info ( f \"Saving dynamic aperture plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure class PhaseSpacePlotter : \"\"\" A class to plot Courant-Snyder coordinates phase space. \"\"\" @staticmethod def plot_courant_snyder_phase_space ( cpymad_instance : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ): logger . error ( f \"Plane should be either Horizontal or Vertical but ' { plane } ' was given\" ) raise ValueError ( \"Invalid plane value\" ) figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) # Getting the P matrix to compute Courant-Snyder coordinates logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = ( cpymad_instance . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . alfy [ 0 ] ) beta = ( cpymad_instance . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . bety [ 0 ] ) logger . debug ( f \"Plotting Courant-Snyder phase space for the { plane . lower () } plane\" ) for index , _ in enumerate ( u_coordinates ): logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle { index } \" ) u = np . array ([ u_coordinates [ index ], pu_coordinates [ index ]]) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0 , :] * 1e3 , u_bar [ 1 , :] * 1e3 , s = 0.1 , c = \"k\" ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$ \\\\ bar {x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$ \\\\ bar {y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving Courant-Snyder phase space plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure @staticmethod def plot_courant_snyder_phase_space_colored ( cpymad_instance : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156th color. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ): logger . error ( f \"Plane should be either Horizontal or Vertical but ' { plane } ' was given\" ) raise ValueError ( \"Invalid plane value\" ) # Getting a sufficiently long array of colors to use colors = int ( np . floor ( len ( u_coordinates ) / 100 )) * SORTED_COLORS while len ( colors ) > len ( u_coordinates ): colors . pop () figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = ( cpymad_instance . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . alfy [ 0 ] ) beta = ( cpymad_instance . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . bety [ 0 ] ) logger . debug ( f \"Plotting colored normalised phase space for the { plane . lower () } plane\" ) for index , _ in enumerate ( u_coordinates ): logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle { index } \" ) u = np . array ([ u_coordinates [ index ], pu_coordinates [ index ]]) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0 , :] * 1e3 , u_bar [ 1 , :] * 1e3 , s = 0.1 , c = colors [ index ]) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$ \\\\ bar {x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$ \\\\ bar {y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$ \\\\ bar {py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving colored Courant-Snyder phase space plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure class TuneDiagramPlotter : \"\"\" A class to plot a blank tune diagram with Farey sequences, as well as your working points. \"\"\" @staticmethod def farey_sequence ( order : int ) -> list : \"\"\" Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Args: order (int): the order up to which we want to calculate the sequence. Returns: The sequence as a list. \"\"\" seq = [[ 0 , 1 ]] a , b , c , d = 0 , 1 , 1 , order while c <= order : k = int (( order + b ) / d ) a , b , c , d = c , d , k * c - a , k * d - b seq . append ([ a , b ]) return seq @staticmethod def plot_blank_tune_diagram () -> matplotlib . figure . Figure : \"\"\" Plotting the tune diagram up to the 6th order. Original code from Rogelio Tom\u00e1s. Returns: The figure on which resonance lines from farey sequences are drawn. \"\"\" logger . debug ( \"Plotting resonance lines from Farey sequence, up to 5th order\" ) figure = plt . figure ( figsize = ( 13 , 13 )) plt . ylim (( 0 , 1 )) plt . xlim (( 0 , 1 )) x = np . linspace ( 0 , 1 , 1000 ) for i in range ( 1 , 6 ): farey_sequences = TuneDiagramPlotter . farey_sequence ( i ) for f in farey_sequences : h , k = f # Node h/k on the axes for sequence in farey_sequences : p , q = sequence a = float ( k * p ) # Resonance linea Qx + b*Qy = clinkedtop / q if a > 0 : b = float ( q - k * p ) c = float ( p * h ) plt . plot ( x , c / a - x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( x , c / a + x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , 1 - x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , 1 - x , \"b\" , alpha = 0.1 ) if q == k and p == 1 : # FN elements below 1/k break plt . title ( \"Tune Diagram\" , fontsize = 20 ) plt . axis ( \"square\" ) plt . xlim ([ 0 , 1 ]) plt . ylim ([ 0 , 1 ]) plt . xlabel ( \"$Q_ {x} }$\" , fontsize = 17 ) plt . ylabel ( \"$Q_ {y} $\" , fontsize = 17 ) return figure @staticmethod def plot_tune_diagram ( cpymad_instance : Madx , v_qx : np . ndarray = np . array ([ 0 ]), vxgood : np . ndarray = np . array ([ False ]), v_qy : np . ndarray = np . array ([ 0 ]), vygood : np . ndarray = np . array ([ False ]), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plots the evolution of particles' tunes on a Tune Diagram. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. v_qx (np.ndarray): horizontal tune value as a numpy array. vxgood (np.ndarray): ?? v_qy (np.ndarray): vertical tune value as a numpy array. vygood (np.ndarray): ?? savefig: will save the figure if this is not None, using the string value passed. Returns: The figure on which the diagram is drawn. \"\"\" figure = TuneDiagramPlotter . plot_blank_tune_diagram () logger . debug ( \"Getting Tunes from cpymad\" ) new_q1 : float = cpymad_instance . table . summ . dframe () . q1 [ 0 ] new_q2 : float = cpymad_instance . table . summ . dframe () . q2 [ 0 ] if vxgood . any () and vygood . any (): plt . plot ( v_qx [ vxgood * vygood ], v_qy [ vxgood * vygood ], \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif vxgood . any () and ~ vygood . any (): tp = np . ones ( len ( vxgood )) * ( new_q2 - np . floor ( new_q2 )) plt . plot ( v_qx [ vxgood ], tp [ vxgood ], \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif ~ vxgood . any () and vygood . any (): tp = np . ones ( len ( vygood )) * ( new_q1 - np . floor ( new_q1 )) plt . plot ( tp [ vygood ], v_qy [ vygood ], \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) if savefig : logger . info ( f \"Saving Tune diagram plot at ' { Path ( savefig ) . absolute () } '\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"Module pyhdtoolkit.cpymadtools.plotters"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#variables","text":"BY_HSV COLORS_DICT PLOT_PARAMS SORTED_COLORS","title":"Variables"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#apertureplotter","text":"class AperturePlotter ( / , * args , ** kwargs ) View Source class AperturePlotter : \"\"\" A class to plot the physical aperture of your machine. \"\"\" @staticmethod def plot_aperture ( cpymad_instance : Madx , beam_params : Dict [ str, float ] , figsize : Tuple [ int, int ] = ( 13 , 20 ), xlimits : Tuple [ float, float ] = None , hplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. beam_params (Dict[str, float]): a beam_parameters dictionary obtained through cpymadtools.helpers.beam_parameters. figsize (str): size of the figure, defaults to (15, 15). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. hplane_ylim (Tuple[float, float]): the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). vplane_ylim (Tuple[float, float]): the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint : disable = too - many - arguments # We need to interpolate in order to get high resolution along the s direction logger . debug ( \"Running interpolation in cpymad\" ) cpymad_instance . input ( \"\"\" select, flag=interpolate, class=drift, slice=4, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=8, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; select, flag=interpolate, class=rbend, slice=10, range=#s/#e; twiss; \"\"\" ) logger . debug ( \"Getting Twiss dframe from cpymad\" ) twiss_hr : pd . DataFrame = cpymad_instance . table . twiss . dframe () twiss_hr [ \"betatronic_envelope_x\" ] = np . sqrt ( twiss_hr . betx . values * beam_params [ \"eg_y_m\" ] ) twiss_hr [ \"betatronic_envelope_y\" ] = np . sqrt ( twiss_hr . bety . values * beam_params [ \"eg_y_m\" ] ) twiss_hr [ \"dispersive_envelope_x\" ] = twiss_hr . dx . values * beam_params [ \"deltap_p\" ] twiss_hr [ \"dispersive_envelope_y\" ] = twiss_hr . dy . values * beam_params [ \"deltap_p\" ] twiss_hr [ \"envelope_x\" ] = np . sqrt ( twiss_hr . betatronic_envelope_x . values ** 2 + ( twiss_hr . dx . values * beam_params [ \"deltap_p\" ] ) ** 2 ) twiss_hr [ \"envelope_y\" ] = np . sqrt ( twiss_hr . betatronic_envelope_y . values ** 2 + ( twiss_hr . dy . values * beam_params [ \"deltap_p\" ] ) ** 2 ) machine = twiss_hr [ twiss_hr.apertype == \"ellipse\" ] figure = plt . figure ( figsize = figsize ) logger . debug ( \"Plotting the horizontal aperture\" ) axis1 = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) axis1 . plot ( twiss_hr . s , twiss_hr . envelope_x , color = \"b\" ) axis1 . plot ( twiss_hr . s , - twiss_hr . envelope_x , color = \"b\" ) axis1 . fill_between ( twiss_hr . s , twiss_hr . envelope_x , - twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_x , - 3 * twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( machine . s , machine . aper_1 , machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . fill_between ( machine . s , - machine . aper_1 , - machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . plot ( machine . s , machine . aper_1 , \"k.-\" ) axis1 . plot ( machine . s , - machine . aper_1 , \"k.-\" ) axis1 . set_xlim ( xlimits ) axis1 . set_ylim ( hplane_ylim ) axis1 . set_ylabel ( \"x [m]\" ) axis1 . set_xlabel ( \"s [m]\" ) axis1 . set_title ( f \"Horizontal aperture at {beam_params['pc_GeV']} GeV/c\" ) logger . debug ( \"Plotting the vertical aperture\" ) axis2 = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis2 . plot ( twiss_hr . s , twiss_hr . envelope_y , color = \"r\" ) axis2 . plot ( twiss_hr . s , - twiss_hr . envelope_y , color = \"r\" ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( machine . s , machine . aper_2 , machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . fill_between ( machine . s , - machine . aper_2 , - machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . plot ( machine . s , machine . aper_2 , \"k.-\" ) axis2 . plot ( machine . s , - machine . aper_2 , \"k.-\" ) axis2 . set_ylim ( vplane_ylim ) axis2 . set_ylabel ( \"y [m]\" ) axis2 . set_xlabel ( \"s [m]\" ) axis2 . set_title ( f \"Vertical aperture at {beam_params['pc_GeV']} GeV/c\" ) logger . debug ( \"Plotting the stay-clear envelope\" ) axis3 = plt . subplot2grid (( 3 , 3 ), ( 2 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis3 . plot ( machine . s , machine . aper_1 / machine . envelope_x , \".-b\" , label = \"Horizontal plane\" ) axis3 . plot ( machine . s , machine . aper_2 / machine . envelope_y , \".-r\" , label = \"Vertical plane\" ) axis3 . set_xlim ( xlimits ) axis3 . set_ylabel ( \"n1\" ) axis3 . set_xlabel ( \"s [m]\" ) axis3 . legend ( loc = \"best\" ) axis3 . set_title ( f \"Stay-clear envelope at {beam_params['pc_GeV']} GeV/c\" ) if savefig : logger . info ( f \"Saving aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"AperturePlotter"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#plot_aperture","text":"def plot_aperture ( cpymad_instance : cpymad . madx . Madx , beam_params : Dict [ str , float ], figsize : Tuple [ int , int ] = ( 13 , 20 ), xlimits : Tuple [ float , float ] = None , hplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float , float ] = ( - 0.12 , 0.12 ), savefig : str = None ) -> matplotlib . figure . Figure Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None beam_params Dict[str, float] a beam_parameters dictionary obtained through cpymadtools.helpers.beam_parameters. None figsize str size of the figure, defaults to (15, 15). None xlimits Tuple[float, float] will implement xlim (for the s coordinate) if this is not None, using the tuple passed. None hplane_ylim Tuple[float, float] the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). None vplane_ylim Tuple[float, float] the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). None savefig str will save the figure if this is not None, using the string value passed. None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_aperture ( cpymad_instance : Madx , beam_params : Dict [ str, float ] , figsize : Tuple [ int, int ] = ( 13 , 20 ), xlimits : Tuple [ float, float ] = None , hplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), vplane_ylim : Tuple [ float, float ] = ( - 0.12 , 0.12 ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plot the physical aperture of your machine, already defined into the provided cpymad.Madx object. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. beam_params (Dict[str, float]): a beam_parameters dictionary obtained through cpymadtools.helpers.beam_parameters. figsize (str): size of the figure, defaults to (15, 15). xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. hplane_ylim (Tuple[float, float]): the y limits for the horizontal plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). vplane_ylim (Tuple[float, float]): the y limits for the vertical plane plot (so that machine geometry doesn't make the plot look shrinked). Defaults to (-0.12, 0.12). savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint : disable = too - many - arguments # We need to interpolate in order to get high resolution along the s direction logger . debug ( \"Running interpolation in cpymad\" ) cpymad_instance . input ( \"\"\" select, flag=interpolate, class=drift, slice=4, range=#s/#e; select, flag=interpolate, class=quadrupole, slice=8, range=#s/#e; select, flag=interpolate, class=sbend, slice=10, range=#s/#e; select, flag=interpolate, class=rbend, slice=10, range=#s/#e; twiss; \"\"\" ) logger . debug ( \"Getting Twiss dframe from cpymad\" ) twiss_hr : pd . DataFrame = cpymad_instance . table . twiss . dframe () twiss_hr [ \"betatronic_envelope_x\" ] = np . sqrt ( twiss_hr . betx . values * beam_params [ \"eg_y_m\" ] ) twiss_hr [ \"betatronic_envelope_y\" ] = np . sqrt ( twiss_hr . bety . values * beam_params [ \"eg_y_m\" ] ) twiss_hr [ \"dispersive_envelope_x\" ] = twiss_hr . dx . values * beam_params [ \"deltap_p\" ] twiss_hr [ \"dispersive_envelope_y\" ] = twiss_hr . dy . values * beam_params [ \"deltap_p\" ] twiss_hr [ \"envelope_x\" ] = np . sqrt ( twiss_hr . betatronic_envelope_x . values ** 2 + ( twiss_hr . dx . values * beam_params [ \"deltap_p\" ] ) ** 2 ) twiss_hr [ \"envelope_y\" ] = np . sqrt ( twiss_hr . betatronic_envelope_y . values ** 2 + ( twiss_hr . dy . values * beam_params [ \"deltap_p\" ] ) ** 2 ) machine = twiss_hr [ twiss_hr.apertype == \"ellipse\" ] figure = plt . figure ( figsize = figsize ) logger . debug ( \"Plotting the horizontal aperture\" ) axis1 = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) axis1 . plot ( twiss_hr . s , twiss_hr . envelope_x , color = \"b\" ) axis1 . plot ( twiss_hr . s , - twiss_hr . envelope_x , color = \"b\" ) axis1 . fill_between ( twiss_hr . s , twiss_hr . envelope_x , - twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_x , - 3 * twiss_hr . envelope_x , color = \"b\" , alpha = 0.25 ) axis1 . fill_between ( machine . s , machine . aper_1 , machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . fill_between ( machine . s , - machine . aper_1 , - machine . aper_1 * 100 , color = \"k\" , alpha = 0.5 ) axis1 . plot ( machine . s , machine . aper_1 , \"k.-\" ) axis1 . plot ( machine . s , - machine . aper_1 , \"k.-\" ) axis1 . set_xlim ( xlimits ) axis1 . set_ylim ( hplane_ylim ) axis1 . set_ylabel ( \"x [m]\" ) axis1 . set_xlabel ( \"s [m]\" ) axis1 . set_title ( f \"Horizontal aperture at {beam_params['pc_GeV']} GeV/c\" ) logger . debug ( \"Plotting the vertical aperture\" ) axis2 = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis2 . plot ( twiss_hr . s , twiss_hr . envelope_y , color = \"r\" ) axis2 . plot ( twiss_hr . s , - twiss_hr . envelope_y , color = \"r\" ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , twiss_hr . envelope_y , - twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( twiss_hr . s , 3 * twiss_hr . envelope_y , - 3 * twiss_hr . envelope_y , color = \"r\" , alpha = 0.25 ) axis2 . fill_between ( machine . s , machine . aper_2 , machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . fill_between ( machine . s , - machine . aper_2 , - machine . aper_2 * 100 , color = \"k\" , alpha = 0.5 ) axis2 . plot ( machine . s , machine . aper_2 , \"k.-\" ) axis2 . plot ( machine . s , - machine . aper_2 , \"k.-\" ) axis2 . set_ylim ( vplane_ylim ) axis2 . set_ylabel ( \"y [m]\" ) axis2 . set_xlabel ( \"s [m]\" ) axis2 . set_title ( f \"Vertical aperture at {beam_params['pc_GeV']} GeV/c\" ) logger . debug ( \"Plotting the stay-clear envelope\" ) axis3 = plt . subplot2grid (( 3 , 3 ), ( 2 , 0 ), colspan = 3 , rowspan = 1 , sharex = axis1 ) axis3 . plot ( machine . s , machine . aper_1 / machine . envelope_x , \".-b\" , label = \"Horizontal plane\" ) axis3 . plot ( machine . s , machine . aper_2 / machine . envelope_y , \".-r\" , label = \"Vertical plane\" ) axis3 . set_xlim ( xlimits ) axis3 . set_ylabel ( \"n1\" ) axis3 . set_xlabel ( \"s [m]\" ) axis3 . legend ( loc = \"best\" ) axis3 . set_title ( f \"Stay-clear envelope at {beam_params['pc_GeV']} GeV/c\" ) if savefig : logger . info ( f \"Saving aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"plot_aperture"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#dynamicapertureplotter","text":"class DynamicAperturePlotter ( / , * args , ** kwargs ) View Source class DynamicAperturePlotter : \"\"\" A class to plot the dynamic aperture of your machine. \"\"\" @staticmethod def plot_dynamic_aperture ( vx_coords : np . ndarray , vy_coords : np . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure : \"\"\" Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Args: vx_coords (np.ndarray): numpy array of horizontal coordinates over turns. vy_coords (np.ndarray): numpy array of vertical coordinates over turns. n_particles (int): number of particles simulated. savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" figure = plt . figure ( figsize = ( 12 , 7 )) turn_lost = [] x_in_lost = [] for particle in range ( n_particles ) : nb = len ( vx_coords [ particle ] ) - max ( np . isnan ( vx_coords [ particle ] ). sum (), np . isnan ( vy_coords [ particle ] ). sum () ) turn_lost . append ( nb ) x_in_lost . append ( vx_coords [ particle ][ 0 ] ** 2 + vy_coords [ particle ][ 0 ] ** 2 ) turn_lost = np . array ( turn_lost ) x_in_lost = np . array ( x_in_lost ) plt . scatter ( turn_lost , x_in_lost * 1000 , linewidths = 0.7 , c = \"darkblue\" , marker = \".\" ) plt . title ( \"Amplitudes lost over turns\" , fontsize = 20 ) plt . xlabel ( \"Number of Turns Survived\" , fontsize = 17 ) plt . ylabel ( \"Initial amplitude [mm]\" , fontsize = 17 ) if savefig : logger . info ( f \"Saving dynamic aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"DynamicAperturePlotter"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#plot_dynamic_aperture","text":"def plot_dynamic_aperture ( vx_coords : numpy . ndarray , vy_coords : numpy . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Parameters: Name Type Description Default vx_coords np.ndarray numpy array of horizontal coordinates over turns. None vy_coords np.ndarray numpy array of vertical coordinates over turns. None n_particles int number of particles simulated. None savefig str will save the figure if this is not None, using the string value passed. None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_dynamic_aperture ( vx_coords : np . ndarray , vy_coords : np . ndarray , n_particles : int , savefig : str = None ) -> matplotlib . figure . Figure : \"\"\" Plots a visual aid for the dynamic aperture after a tracking. Initial amplitudes are on the vertical axis, and the turn at which they were lost is in the horizontal axis. Args: vx_coords (np.ndarray): numpy array of horizontal coordinates over turns. vy_coords (np.ndarray): numpy array of vertical coordinates over turns. n_particles (int): number of particles simulated. savefig (str): will save the figure if this is not None, using the string value passed. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" figure = plt . figure ( figsize = ( 12 , 7 )) turn_lost = [] x_in_lost = [] for particle in range ( n_particles ) : nb = len ( vx_coords [ particle ] ) - max ( np . isnan ( vx_coords [ particle ] ). sum (), np . isnan ( vy_coords [ particle ] ). sum () ) turn_lost . append ( nb ) x_in_lost . append ( vx_coords [ particle ][ 0 ] ** 2 + vy_coords [ particle ][ 0 ] ** 2 ) turn_lost = np . array ( turn_lost ) x_in_lost = np . array ( x_in_lost ) plt . scatter ( turn_lost , x_in_lost * 1000 , linewidths = 0.7 , c = \"darkblue\" , marker = \".\" ) plt . title ( \"Amplitudes lost over turns\" , fontsize = 20 ) plt . xlabel ( \"Number of Turns Survived\" , fontsize = 17 ) plt . ylabel ( \"Initial amplitude [mm]\" , fontsize = 17 ) if savefig : logger . info ( f \"Saving dynamic aperture plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"plot_dynamic_aperture"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#phasespaceplotter","text":"class PhaseSpacePlotter ( / , * args , ** kwargs ) View Source class PhaseSpacePlotter : \"\"\" A class to plot Courant-Snyder coordinates phase space. \"\"\" @staticmethod def plot_courant_snyder_phase_space ( cpymad_instance : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) # Getting the P matrix to compute Courant - Snyder coordinates logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = ( cpymad_instance . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . alfy [ 0 ] ) beta = ( cpymad_instance . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . bety [ 0 ] ) logger . debug ( f \"Plotting Courant-Snyder phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = \"k\" ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure @staticmethod def plot_courant_snyder_phase_space_colored ( cpymad_instance : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156th color. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) # Getting a sufficiently long array of colors to use colors = int ( np . floor ( len ( u_coordinates ) / 100 )) * SORTED_COLORS while len ( colors ) > len ( u_coordinates ) : colors . pop () figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = ( cpymad_instance . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . alfy [ 0 ] ) beta = ( cpymad_instance . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . bety [ 0 ] ) logger . debug ( f \"Plotting colored normalised phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = colors [ index ] ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving colored Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"PhaseSpacePlotter"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#plot_courant_snyder_phase_space","text":"def plot_courant_snyder_phase_space ( cpymad_instance : cpymad . madx . Madx , u_coordinates : numpy . ndarray , pu_coordinates : numpy . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = 'Horizontal' ) -> matplotlib . figure . Figure Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None u_coordinates np.ndarray numpy array of particles's coordinates for the given plane. None pu_coordinates np.ndarray numpy array of particles's momentum coordinates for the given plane. None savefig str will save the figure if this is not None, using the string value passed. None size Tuple[int, int] the wanted matplotlib figure size. Defaults to (16, 8). (16, 8) plane str the physical plane to plot. Defaults to 'Horizontal'. 'Horizontal' Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_courant_snyder_phase_space ( cpymad_instance : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) # Getting the P matrix to compute Courant - Snyder coordinates logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = ( cpymad_instance . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . alfy [ 0 ] ) beta = ( cpymad_instance . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . bety [ 0 ] ) logger . debug ( f \"Plotting Courant-Snyder phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = \"k\" ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"plot_courant_snyder_phase_space"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#plot_courant_snyder_phase_space_colored","text":"def plot_courant_snyder_phase_space_colored ( cpymad_instance : cpymad . madx . Madx , u_coordinates : numpy . ndarray , pu_coordinates : numpy . ndarray , savefig : str = None , size : Tuple [ int , int ] = ( 16 , 8 ), plane : str = 'Horizontal' ) -> matplotlib . figure . Figure Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156 th color. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None u_coordinates np.ndarray numpy array of particles's coordinates for the given plane. None pu_coordinates np.ndarray numpy array of particles's momentum coordinates for the given plane. None savefig str will save the figure if this is not None, using the string value passed. None size Tuple[int, int] the wanted matplotlib figure size. Defaults to (16, 8). (16, 8) plane str the physical plane to plot. Defaults to 'Horizontal'. 'Horizontal' Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source @staticmethod def plot_courant_snyder_phase_space_colored ( cpymad_instance : Madx , u_coordinates : np . ndarray , pu_coordinates : np . ndarray , savefig : str = None , size : Tuple [ int, int ] = ( 16 , 8 ), plane : str = \"Horizontal\" , ) -> matplotlib . figure . Figure : \"\"\" Plots the Courant-Snyder phase space of a particle distribution when provided by position and momentum coordinates for a specific plane. Each particle trajectory has its own color on the plot, within the limit of pyplot's 156 named colors. The sequence repeats after the 156th color. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. u_coordinates (np.ndarray): numpy array of particles's coordinates for the given plane. pu_coordinates (np.ndarray): numpy array of particles's momentum coordinates for the given plane. savefig (str): will save the figure if this is not None, using the string value passed. size (Tuple[int, int]): the wanted matplotlib figure size. Defaults to (16, 8). plane (str): the physical plane to plot. Defaults to 'Horizontal'. Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" if plane . upper () not in ( \"HORIZONTAL\" , \"VERTICAL\" ) : logger . error ( f \"Plane should be either Horizontal or Vertical but '{plane}' was given\" ) raise ValueError ( \"Invalid plane value\" ) # Getting a sufficiently long array of colors to use colors = int ( np . floor ( len ( u_coordinates ) / 100 )) * SORTED_COLORS while len ( colors ) > len ( u_coordinates ) : colors . pop () figure = plt . figure ( figsize = size ) plt . title ( \"Courant-Snyder Phase Space\" , fontsize = 20 ) logger . debug ( \"Getting Twiss functions from cpymad\" ) alpha = ( cpymad_instance . table . twiss . alfx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . alfy [ 0 ] ) beta = ( cpymad_instance . table . twiss . betx [ 0 ] if plane . upper () == \"HORIZONTAL\" else cpymad_instance . table . twiss . bety [ 0 ] ) logger . debug ( f \"Plotting colored normalised phase space for the {plane.lower()} plane\" ) for index , _ in enumerate ( u_coordinates ) : logger . trace ( f \"Getting and plotting Courant-Snyder coordinates for particle {index}\" ) u = np . array ( [ u_coordinates[index ] , pu_coordinates [ index ] ] ) u_bar = courant_snyder_transform ( u , alpha , beta ) plt . scatter ( u_bar [ 0, : ] * 1e3 , u_bar [ 1, : ] * 1e3 , s = 0.1 , c = colors [ index ] ) if plane . upper () == \"HORIZONTAL\" : plt . xlabel ( \"$\\\\bar{x} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{px} [mrad]$\" , fontsize = 17 ) else : plt . xlabel ( \"$\\\\bar{y} [mm]$\" , fontsize = 17 ) plt . ylabel ( \"$\\\\bar{py} [mrad]$\" , fontsize = 17 ) plt . axis ( \"Equal\" ) if savefig : logger . info ( f \"Saving colored Courant-Snyder phase space plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"plot_courant_snyder_phase_space_colored"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#tunediagramplotter","text":"class TuneDiagramPlotter ( / , * args , ** kwargs ) View Source class TuneDiagramPlotter : \"\"\" A class to plot a blank tune diagram with Farey sequences, as well as your working points. \"\"\" @staticmethod def farey_sequence ( order : int ) -> list : \"\"\" Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Args: order (int): the order up to which we want to calculate the sequence. Returns: The sequence as a list. \"\"\" seq = [ [0, 1 ] ] a , b , c , d = 0 , 1 , 1 , order while c <= order : k = int (( order + b ) / d ) a , b , c , d = c , d , k * c - a , k * d - b seq . append ( [ a, b ] ) return seq @staticmethod def plot_blank_tune_diagram () -> matplotlib . figure . Figure : \"\"\" Plotting the tune diagram up to the 6th order. Original code from Rogelio Tom\u00e1s. Returns: The figure on which resonance lines from farey sequences are drawn. \"\"\" logger . debug ( \"Plotting resonance lines from Farey sequence, up to 5th order\" ) figure = plt . figure ( figsize = ( 13 , 13 )) plt . ylim (( 0 , 1 )) plt . xlim (( 0 , 1 )) x = np . linspace ( 0 , 1 , 1000 ) for i in range ( 1 , 6 ) : farey_sequences = TuneDiagramPlotter . farey_sequence ( i ) for f in farey_sequences : h , k = f # Node h / k on the axes for sequence in farey_sequences : p , q = sequence a = float ( k * p ) # Resonance linea Qx + b * Qy = clinkedtop / q if a > 0 : b = float ( q - k * p ) c = float ( p * h ) plt . plot ( x , c / a - x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( x , c / a + x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , 1 - x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , 1 - x , \"b\" , alpha = 0.1 ) if q == k and p == 1 : # FN elements below 1 / k break plt . title ( \"Tune Diagram\" , fontsize = 20 ) plt . axis ( \"square\" ) plt . xlim ( [ 0, 1 ] ) plt . ylim ( [ 0, 1 ] ) plt . xlabel ( \"$Q_{x}}$\" , fontsize = 17 ) plt . ylabel ( \"$Q_{y}$\" , fontsize = 17 ) return figure @staticmethod def plot_tune_diagram ( cpymad_instance : Madx , v_qx : np . ndarray = np . array ( [ 0 ] ), vxgood : np . ndarray = np . array ( [ False ] ), v_qy : np . ndarray = np . array ( [ 0 ] ), vygood : np . ndarray = np . array ( [ False ] ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plots the evolution of particles' tunes on a Tune Diagram. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. v_qx (np.ndarray): horizontal tune value as a numpy array. vxgood (np.ndarray): ?? v_qy (np.ndarray): vertical tune value as a numpy array. vygood (np.ndarray): ?? savefig: will save the figure if this is not None, using the string value passed. Returns: The figure on which the diagram is drawn. \"\"\" figure = TuneDiagramPlotter . plot_blank_tune_diagram () logger . debug ( \"Getting Tunes from cpymad\" ) new_q1 : float = cpymad_instance . table . summ . dframe (). q1 [ 0 ] new_q2 : float = cpymad_instance . table . summ . dframe (). q2 [ 0 ] if vxgood . any () and vygood . any () : plt . plot ( v_qx [ vxgood * vygood ] , v_qy [ vxgood * vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif vxgood . any () and ~ vygood . any () : tp = np . ones ( len ( vxgood )) * ( new_q2 - np . floor ( new_q2 )) plt . plot ( v_qx [ vxgood ] , tp [ vxgood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif ~ vxgood . any () and vygood . any () : tp = np . ones ( len ( vygood )) * ( new_q1 - np . floor ( new_q1 )) plt . plot ( tp [ vygood ] , v_qy [ vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) if savefig : logger . info ( f \"Saving Tune diagram plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"TuneDiagramPlotter"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#farey_sequence","text":"def farey_sequence ( order : int ) -> list Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Parameters: Name Type Description Default order int the order up to which we want to calculate the sequence. None Returns: Type Description None The sequence as a list. View Source @staticmethod def farey_sequence ( order : int ) -> list : \"\"\" Returns the n-th farey_sequence sequence, ascending. Original code from Rogelio Tom\u00e1s. Args: order (int): the order up to which we want to calculate the sequence. Returns: The sequence as a list. \"\"\" seq = [ [0, 1 ] ] a , b , c , d = 0 , 1 , 1 , order while c <= order : k = int (( order + b ) / d ) a , b , c , d = c , d , k * c - a , k * d - b seq . append ( [ a, b ] ) return seq","title":"farey_sequence"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#plot_blank_tune_diagram","text":"def plot_blank_tune_diagram ( ) -> matplotlib . figure . Figure Plotting the tune diagram up to the 6 th order. Original code from Rogelio Tom\u00e1s. Returns: Type Description None The figure on which resonance lines from farey sequences are drawn. View Source @staticmethod def plot_blank_tune_diagram () -> matplotlib . figure . Figure : \"\"\" Plotting the tune diagram up to the 6th order. Original code from Rogelio Tom\u00e1s. Returns: The figure on which resonance lines from farey sequences are drawn. \"\"\" logger . debug ( \"Plotting resonance lines from Farey sequence, up to 5th order\" ) figure = plt . figure ( figsize = ( 13 , 13 )) plt . ylim (( 0 , 1 )) plt . xlim (( 0 , 1 )) x = np . linspace ( 0 , 1 , 1000 ) for i in range ( 1 , 6 ) : farey_sequences = TuneDiagramPlotter . farey_sequence ( i ) for f in farey_sequences : h , k = f # Node h / k on the axes for sequence in farey_sequences : p , q = sequence a = float ( k * p ) # Resonance linea Qx + b * Qy = clinkedtop / q if a > 0 : b = float ( q - k * p ) c = float ( p * h ) plt . plot ( x , c / a - x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( x , c / a + x * b / a , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , x , \"b\" , alpha = 0.1 ) plt . plot ( c / a - x * b / a , 1 - x , \"b\" , alpha = 0.1 ) plt . plot ( c / a + x * b / a , 1 - x , \"b\" , alpha = 0.1 ) if q == k and p == 1 : # FN elements below 1 / k break plt . title ( \"Tune Diagram\" , fontsize = 20 ) plt . axis ( \"square\" ) plt . xlim ( [ 0, 1 ] ) plt . ylim ( [ 0, 1 ] ) plt . xlabel ( \"$Q_{x}}$\" , fontsize = 17 ) plt . ylabel ( \"$Q_{y}$\" , fontsize = 17 ) return figure","title":"plot_blank_tune_diagram"},{"location":"reference/pyhdtoolkit/cpymadtools/plotters/#plot_tune_diagram","text":"def plot_tune_diagram ( cpymad_instance : cpymad . madx . Madx , v_qx : numpy . ndarray = array ([ 0 ]), vxgood : numpy . ndarray = array ([ False ]), v_qy : numpy . ndarray = array ([ 0 ]), vygood : numpy . ndarray = array ([ False ]), savefig : str = None ) -> matplotlib . figure . Figure Plots the evolution of particles' tunes on a Tune Diagram. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None v_qx np.ndarray horizontal tune value as a numpy array. None vxgood np.ndarray ?? None v_qy np.ndarray vertical tune value as a numpy array. None vygood np.ndarray ?? None savefig None will save the figure if this is not None, using the string value passed. None Returns: Type Description None The figure on which the diagram is drawn. View Source @staticmethod def plot_tune_diagram ( cpymad_instance : Madx , v_qx : np . ndarray = np . array ( [ 0 ] ), vxgood : np . ndarray = np . array ( [ False ] ), v_qy : np . ndarray = np . array ( [ 0 ] ), vygood : np . ndarray = np . array ( [ False ] ), savefig : str = None , ) -> matplotlib . figure . Figure : \"\"\" Plots the evolution of particles' tunes on a Tune Diagram. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. v_qx (np.ndarray): horizontal tune value as a numpy array. vxgood (np.ndarray): ?? v_qy (np.ndarray): vertical tune value as a numpy array. vygood (np.ndarray): ?? savefig: will save the figure if this is not None, using the string value passed. Returns: The figure on which the diagram is drawn. \"\"\" figure = TuneDiagramPlotter . plot_blank_tune_diagram () logger . debug ( \"Getting Tunes from cpymad\" ) new_q1 : float = cpymad_instance . table . summ . dframe (). q1 [ 0 ] new_q2 : float = cpymad_instance . table . summ . dframe (). q2 [ 0 ] if vxgood . any () and vygood . any () : plt . plot ( v_qx [ vxgood * vygood ] , v_qy [ vxgood * vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif vxgood . any () and ~ vygood . any () : tp = np . ones ( len ( vxgood )) * ( new_q2 - np . floor ( new_q2 )) plt . plot ( v_qx [ vxgood ] , tp [ vxgood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) elif ~ vxgood . any () and vygood . any () : tp = np . ones ( len ( vygood )) * ( new_q1 - np . floor ( new_q1 )) plt . plot ( tp [ vygood ] , v_qy [ vygood ] , \".r\" ) plt . plot ( new_q1 - np . floor ( new_q1 ), new_q2 - np . floor ( new_q2 ), \".g\" ) if savefig : logger . info ( f \"Saving Tune diagram plot at '{Path(savefig).absolute()}'\" ) plt . savefig ( Path ( savefig ), format = \"pdf\" , dpi = 500 ) return figure","title":"plot_tune_diagram"},{"location":"reference/pyhdtoolkit/cpymadtools/ptc/","text":"Module pyhdtoolkit.cpymadtools.ptc Module cpymadtools.ptc Created on 2020.02.03 View Source \"\"\" Module cpymadtools.ptc ---------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to harness MAD-X PTC functionality with a cpymad.madx.Madx object. \"\"\" from pathlib import Path from typing import Union import tfs from cpymad.madx import Madx from loguru import logger # ----- Utilities ----- # def get_amplitude_detuning ( cpymad_instance : Madx , order : int = 2 , file : Union [ Path , str ] = None ) -> tfs . TfsDataFrame : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Calculate amplitude detuning via PTC_NORMAL. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order (only 0, 1 or 2 implemented in PTC). Defaults to `2`. file (Union[Path, str]): path to output file. Default `None` Returns: A TfsDataframe with results. \"\"\" if order >= 3 : logger . error ( f \"Maximum amplitude detuning order in PTC is 2, but {order:d} was requested\" ) raise NotImplementedError ( \"PTC amplitude detuning is not implemented for order > 2\" ) logger . info ( \"Entering PTC to calculate amplitude detuning\" ) cpymad_instance . ptc_create_universe () # layout I got with mask (jdilly) # model=3 (Sixtrack code model: Delta-Matrix-Kick-Matrix) # method=4 (integration order), nst=3 (integration steps), exact=True (exact Hamiltonian) logger . trace ( \"Creating PTC layout\" ) cpymad_instance . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) # alternative layout: model=3, method=6, nst=3 # resplit=True (adaptive splitting of magnets), thin=0.0005 (splitting of quads), # xbend=0.0005 (splitting of dipoles) # madx.ptc_create_layout(model=3, method=6, nst=3, resplit=True, thin=0.0005, xbend=0.0005) logger . trace ( \"Incorporating MAD-X alignment errors\" ) cpymad_instance . ptc_align () # use madx alignment errors # madx.ptc_setswitch(fringe=True) # include fringe effects logger . trace ( \"Selecting tune orders\" ) cpymad_instance . select_ptc_normal ( q1 = \"0\" , q2 = \"0\" ) for ii in range ( 1 , order + 1 ): # These are d^iQ/ddp^i cpymad_instance . select_ptc_normal ( dq1 = f \"{ii:d}\" , dq2 = f \"{ii:d}\" ) # ANH = anharmonicities (ex, ey, deltap), works only with parameters as full strings # could be done nicer with permutations ... logger . trace ( \"Selecting anharmonicities\" ) if order >= 1 : # madx.select_ptc_normal('anhx=0, 0, 1') # dQx/ddp # madx.select_ptc_normal('anhy=0, 0, 1') # dQy/ddp cpymad_instance . select_ptc_normal ( \"anhx=1, 0, 0\" ) # dQx/dex cpymad_instance . select_ptc_normal ( \"anhx=0, 1, 0\" ) # dQx/dey cpymad_instance . select_ptc_normal ( \"anhy=1, 0, 0\" ) # dQy/dex cpymad_instance . select_ptc_normal ( \"anhy=0, 1, 0\" ) # dQy/dey if order >= 2 : # madx.select_ptc_normal('anhx=0, 0, 2') # d^2Qx/ddp^2 # madx.select_ptc_normal('anhy=0, 0, 2') # d^2Qy/ddp^2 cpymad_instance . select_ptc_normal ( \"anhx=2, 0, 0\" ) # d^2Qx/dex^2 cpymad_instance . select_ptc_normal ( \"anhx=1, 1, 0\" ) # d^2Qx/dexdey cpymad_instance . select_ptc_normal ( \"anhx=0, 2, 0\" ) # d^2Qx/dey^2 cpymad_instance . select_ptc_normal ( \"anhy=2, 0, 0\" ) # d^2Qy/dex^2 cpymad_instance . select_ptc_normal ( \"anhy=1, 1, 0\" ) # d^2Qy/dexdey cpymad_instance . select_ptc_normal ( \"anhy=0, 2, 0\" ) # d^2Qy/dey^2 # icase = phase-space dimensionality, no = order of map logger . debug ( \"Executing PTC Normal\" ) cpymad_instance . ptc_normal ( closed_orbit = True , normal = True , icase = 5 , no = 5 ) cpymad_instance . ptc_end () logger . debug ( \"Extracting results to TfsDataFrame\" ) dframe = tfs . TfsDataFrame ( cpymad_instance . table . normal_results . dframe ()) dframe . columns = dframe . columns . str . upper () dframe . NAME = dframe . NAME . str . upper () dframe . index = range ( len ( dframe . NAME )) # table has a weird index if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe def get_rdts ( cpymad_instance : Madx , order : int = 4 , file : Union [ Path , str ] = None ) -> tfs . TfsDataFrame : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Calculate the RDTs via PTC_TWISS. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order (only 0, 1 or 2 implemented in PTC). Defaults to `2`. file (Union[Path, str]): path to output file. Default `None` Returns: A TfsDataframe with results. \"\"\" logger . info ( f \"Entering PTC to calculate RDTs up to order {order}\" ) cpymad_instance . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) cpymad_instance . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) # madx.ptc_create_layout(model=3, method=6, nst=1) # from Michi logger . trace ( \"Incorporating MAD-X alignment errors\" ) cpymad_instance . ptc_align () # use madx alignment errors # madx.ptc_setswitch(fringe=True) # include fringe effects logger . debug ( \"Executing PTC Twiss\" ) cpymad_instance . ptc_twiss ( icase = 6 , no = order , normal = True , trackrdts = True ) cpymad_instance . ptc_end () logger . debug ( \"Extracting results to TfsDataFrame\" ) dframe = tfs . TfsDataFrame ( cpymad_instance . table . twissrdt . dframe ()) dframe . columns = dframe . columns . str . upper () dframe . NAME = dframe . NAME . str . upper () if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe Functions get_amplitude_detuning def get_amplitude_detuning ( cpymad_instance : cpymad . madx . Madx , order : int = 2 , file : Union [ pathlib . Path , str ] = None ) -> tfs . handler . TfsDataFrame INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Calculate amplitude detuning via PTC_NORMAL. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None order int maximum derivative order (only 0, 1 or 2 implemented in PTC). Defaults to 2 . 2 file Union[Path, str] path to output file. Default None None Returns: Type Description None A TfsDataframe with results. View Source def get_amplitude_detuning ( cpymad_instance : Madx , order : int = 2 , file : Union [ Path , str ] = None ) -> tfs . TfsDataFrame : \" \"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Calculate amplitude detuning via PTC_NORMAL. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order (only 0, 1 or 2 implemented in PTC). Defaults to `2`. file (Union[Path, str]): path to output file. Default `None` Returns: A TfsDataframe with results. \"\" \" if order >= 3 : logger . error ( f \"Maximum amplitude detuning order in PTC is 2, but {order:d} was requested\" ) raise NotImplementedError ( \"PTC amplitude detuning is not implemented for order > 2\" ) logger . info ( \"Entering PTC to calculate amplitude detuning\" ) cpymad_instance . ptc_create_universe () # layout I got with mask (jdilly) # model=3 (Sixtrack code model: Delta-Matrix-Kick-Matrix) # method=4 (integration order), nst=3 (integration steps), exact=True (exact Hamiltonian) logger . trace ( \"Creating PTC layout\" ) cpymad_instance . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) # alternative layout: model=3, method=6, nst=3 # resplit=True (adaptive splitting of magnets), thin=0.0005 (splitting of quads), # xbend=0.0005 (splitting of dipoles) # madx.ptc_create_layout(model=3, method=6, nst=3, resplit=True, thin=0.0005, xbend=0.0005) logger . trace ( \"Incorporating MAD-X alignment errors\" ) cpymad_instance . ptc_align () # use madx alignment errors # madx.ptc_setswitch(fringe=True) # include fringe effects logger . trace ( \"Selecting tune orders\" ) cpymad_instance . select_ptc_normal ( q1 = \"0\" , q2 = \"0\" ) for ii in range ( 1 , order + 1 ) : # These are d^iQ/ddp^i cpymad_instance . select_ptc_normal ( dq1 = f \"{ii:d}\" , dq2 = f \"{ii:d}\" ) # ANH = anharmonicities (ex, ey, deltap), works only with parameters as full strings # could be done nicer with permutations ... logger . trace ( \"Selecting anharmonicities\" ) if order >= 1 : # madx.select_ptc_normal('anhx=0, 0, 1') # dQx/ddp # madx.select_ptc_normal('anhy=0, 0, 1') # dQy/ddp cpymad_instance . select_ptc_normal ( \"anhx=1, 0, 0\" ) # dQx/dex cpymad_instance . select_ptc_normal ( \"anhx=0, 1, 0\" ) # dQx/dey cpymad_instance . select_ptc_normal ( \"anhy=1, 0, 0\" ) # dQy/dex cpymad_instance . select_ptc_normal ( \"anhy=0, 1, 0\" ) # dQy/dey if order >= 2 : # madx.select_ptc_normal('anhx=0, 0, 2') # d^2Qx/ddp^2 # madx.select_ptc_normal('anhy=0, 0, 2') # d^2Qy/ddp^2 cpymad_instance . select_ptc_normal ( \"anhx=2, 0, 0\" ) # d^2Qx/dex^2 cpymad_instance . select_ptc_normal ( \"anhx=1, 1, 0\" ) # d^2Qx/dexdey cpymad_instance . select_ptc_normal ( \"anhx=0, 2, 0\" ) # d^2Qx/dey^2 cpymad_instance . select_ptc_normal ( \"anhy=2, 0, 0\" ) # d^2Qy/dex^2 cpymad_instance . select_ptc_normal ( \"anhy=1, 1, 0\" ) # d^2Qy/dexdey cpymad_instance . select_ptc_normal ( \"anhy=0, 2, 0\" ) # d^2Qy/dey^2 # icase = phase-space dimensionality, no = order of map logger . debug ( \"Executing PTC Normal\" ) cpymad_instance . ptc_normal ( closed_orbit = True , normal = True , icase = 5 , no = 5 ) cpymad_instance . ptc_end () logger . debug ( \"Extracting results to TfsDataFrame\" ) dframe = tfs . TfsDataFrame ( cpymad_instance . table . normal_results . dframe ()) dframe . columns = dframe . columns . str . upper () dframe . NAME = dframe . NAME . str . upper () dframe . index = range ( len ( dframe . NAME )) # table has a weird index if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe get_rdts def get_rdts ( cpymad_instance : cpymad . madx . Madx , order : int = 4 , file : Union [ pathlib . Path , str ] = None ) -> tfs . handler . TfsDataFrame INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Calculate the RDTs via PTC_TWISS. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None order int maximum derivative order (only 0, 1 or 2 implemented in PTC). Defaults to 2 . 2 file Union[Path, str] path to output file. Default None None Returns: Type Description None A TfsDataframe with results. View Source def get_rdts ( cpymad_instance : Madx , order : int = 4 , file : Union [ Path , str ] = None ) -> tfs . TfsDataFrame : \" \"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Calculate the RDTs via PTC_TWISS. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order (only 0, 1 or 2 implemented in PTC). Defaults to `2`. file (Union[Path, str]): path to output file. Default `None` Returns: A TfsDataframe with results. \"\" \" logger . info ( f \"Entering PTC to calculate RDTs up to order {order}\" ) cpymad_instance . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) cpymad_instance . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) # madx.ptc_create_layout(model=3, method=6, nst=1) # from Michi logger . trace ( \"Incorporating MAD-X alignment errors\" ) cpymad_instance . ptc_align () # use madx alignment errors # madx.ptc_setswitch(fringe=True) # include fringe effects logger . debug ( \"Executing PTC Twiss\" ) cpymad_instance . ptc_twiss ( icase = 6 , no = order , normal = True , trackrdts = True ) cpymad_instance . ptc_end () logger . debug ( \"Extracting results to TfsDataFrame\" ) dframe = tfs . TfsDataFrame ( cpymad_instance . table . twissrdt . dframe ()) dframe . columns = dframe . columns . str . upper () dframe . NAME = dframe . NAME . str . upper () if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe","title":"Ptc"},{"location":"reference/pyhdtoolkit/cpymadtools/ptc/#module-pyhdtoolkitcpymadtoolsptc","text":"Module cpymadtools.ptc Created on 2020.02.03 View Source \"\"\" Module cpymadtools.ptc ---------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to harness MAD-X PTC functionality with a cpymad.madx.Madx object. \"\"\" from pathlib import Path from typing import Union import tfs from cpymad.madx import Madx from loguru import logger # ----- Utilities ----- # def get_amplitude_detuning ( cpymad_instance : Madx , order : int = 2 , file : Union [ Path , str ] = None ) -> tfs . TfsDataFrame : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Calculate amplitude detuning via PTC_NORMAL. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order (only 0, 1 or 2 implemented in PTC). Defaults to `2`. file (Union[Path, str]): path to output file. Default `None` Returns: A TfsDataframe with results. \"\"\" if order >= 3 : logger . error ( f \"Maximum amplitude detuning order in PTC is 2, but {order:d} was requested\" ) raise NotImplementedError ( \"PTC amplitude detuning is not implemented for order > 2\" ) logger . info ( \"Entering PTC to calculate amplitude detuning\" ) cpymad_instance . ptc_create_universe () # layout I got with mask (jdilly) # model=3 (Sixtrack code model: Delta-Matrix-Kick-Matrix) # method=4 (integration order), nst=3 (integration steps), exact=True (exact Hamiltonian) logger . trace ( \"Creating PTC layout\" ) cpymad_instance . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) # alternative layout: model=3, method=6, nst=3 # resplit=True (adaptive splitting of magnets), thin=0.0005 (splitting of quads), # xbend=0.0005 (splitting of dipoles) # madx.ptc_create_layout(model=3, method=6, nst=3, resplit=True, thin=0.0005, xbend=0.0005) logger . trace ( \"Incorporating MAD-X alignment errors\" ) cpymad_instance . ptc_align () # use madx alignment errors # madx.ptc_setswitch(fringe=True) # include fringe effects logger . trace ( \"Selecting tune orders\" ) cpymad_instance . select_ptc_normal ( q1 = \"0\" , q2 = \"0\" ) for ii in range ( 1 , order + 1 ): # These are d^iQ/ddp^i cpymad_instance . select_ptc_normal ( dq1 = f \"{ii:d}\" , dq2 = f \"{ii:d}\" ) # ANH = anharmonicities (ex, ey, deltap), works only with parameters as full strings # could be done nicer with permutations ... logger . trace ( \"Selecting anharmonicities\" ) if order >= 1 : # madx.select_ptc_normal('anhx=0, 0, 1') # dQx/ddp # madx.select_ptc_normal('anhy=0, 0, 1') # dQy/ddp cpymad_instance . select_ptc_normal ( \"anhx=1, 0, 0\" ) # dQx/dex cpymad_instance . select_ptc_normal ( \"anhx=0, 1, 0\" ) # dQx/dey cpymad_instance . select_ptc_normal ( \"anhy=1, 0, 0\" ) # dQy/dex cpymad_instance . select_ptc_normal ( \"anhy=0, 1, 0\" ) # dQy/dey if order >= 2 : # madx.select_ptc_normal('anhx=0, 0, 2') # d^2Qx/ddp^2 # madx.select_ptc_normal('anhy=0, 0, 2') # d^2Qy/ddp^2 cpymad_instance . select_ptc_normal ( \"anhx=2, 0, 0\" ) # d^2Qx/dex^2 cpymad_instance . select_ptc_normal ( \"anhx=1, 1, 0\" ) # d^2Qx/dexdey cpymad_instance . select_ptc_normal ( \"anhx=0, 2, 0\" ) # d^2Qx/dey^2 cpymad_instance . select_ptc_normal ( \"anhy=2, 0, 0\" ) # d^2Qy/dex^2 cpymad_instance . select_ptc_normal ( \"anhy=1, 1, 0\" ) # d^2Qy/dexdey cpymad_instance . select_ptc_normal ( \"anhy=0, 2, 0\" ) # d^2Qy/dey^2 # icase = phase-space dimensionality, no = order of map logger . debug ( \"Executing PTC Normal\" ) cpymad_instance . ptc_normal ( closed_orbit = True , normal = True , icase = 5 , no = 5 ) cpymad_instance . ptc_end () logger . debug ( \"Extracting results to TfsDataFrame\" ) dframe = tfs . TfsDataFrame ( cpymad_instance . table . normal_results . dframe ()) dframe . columns = dframe . columns . str . upper () dframe . NAME = dframe . NAME . str . upper () dframe . index = range ( len ( dframe . NAME )) # table has a weird index if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe def get_rdts ( cpymad_instance : Madx , order : int = 4 , file : Union [ Path , str ] = None ) -> tfs . TfsDataFrame : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Calculate the RDTs via PTC_TWISS. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order (only 0, 1 or 2 implemented in PTC). Defaults to `2`. file (Union[Path, str]): path to output file. Default `None` Returns: A TfsDataframe with results. \"\"\" logger . info ( f \"Entering PTC to calculate RDTs up to order {order}\" ) cpymad_instance . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) cpymad_instance . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) # madx.ptc_create_layout(model=3, method=6, nst=1) # from Michi logger . trace ( \"Incorporating MAD-X alignment errors\" ) cpymad_instance . ptc_align () # use madx alignment errors # madx.ptc_setswitch(fringe=True) # include fringe effects logger . debug ( \"Executing PTC Twiss\" ) cpymad_instance . ptc_twiss ( icase = 6 , no = order , normal = True , trackrdts = True ) cpymad_instance . ptc_end () logger . debug ( \"Extracting results to TfsDataFrame\" ) dframe = tfs . TfsDataFrame ( cpymad_instance . table . twissrdt . dframe ()) dframe . columns = dframe . columns . str . upper () dframe . NAME = dframe . NAME . str . upper () if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe","title":"Module pyhdtoolkit.cpymadtools.ptc"},{"location":"reference/pyhdtoolkit/cpymadtools/ptc/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/ptc/#get_amplitude_detuning","text":"def get_amplitude_detuning ( cpymad_instance : cpymad . madx . Madx , order : int = 2 , file : Union [ pathlib . Path , str ] = None ) -> tfs . handler . TfsDataFrame INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Calculate amplitude detuning via PTC_NORMAL. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None order int maximum derivative order (only 0, 1 or 2 implemented in PTC). Defaults to 2 . 2 file Union[Path, str] path to output file. Default None None Returns: Type Description None A TfsDataframe with results. View Source def get_amplitude_detuning ( cpymad_instance : Madx , order : int = 2 , file : Union [ Path , str ] = None ) -> tfs . TfsDataFrame : \" \"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Calculate amplitude detuning via PTC_NORMAL. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order (only 0, 1 or 2 implemented in PTC). Defaults to `2`. file (Union[Path, str]): path to output file. Default `None` Returns: A TfsDataframe with results. \"\" \" if order >= 3 : logger . error ( f \"Maximum amplitude detuning order in PTC is 2, but {order:d} was requested\" ) raise NotImplementedError ( \"PTC amplitude detuning is not implemented for order > 2\" ) logger . info ( \"Entering PTC to calculate amplitude detuning\" ) cpymad_instance . ptc_create_universe () # layout I got with mask (jdilly) # model=3 (Sixtrack code model: Delta-Matrix-Kick-Matrix) # method=4 (integration order), nst=3 (integration steps), exact=True (exact Hamiltonian) logger . trace ( \"Creating PTC layout\" ) cpymad_instance . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) # alternative layout: model=3, method=6, nst=3 # resplit=True (adaptive splitting of magnets), thin=0.0005 (splitting of quads), # xbend=0.0005 (splitting of dipoles) # madx.ptc_create_layout(model=3, method=6, nst=3, resplit=True, thin=0.0005, xbend=0.0005) logger . trace ( \"Incorporating MAD-X alignment errors\" ) cpymad_instance . ptc_align () # use madx alignment errors # madx.ptc_setswitch(fringe=True) # include fringe effects logger . trace ( \"Selecting tune orders\" ) cpymad_instance . select_ptc_normal ( q1 = \"0\" , q2 = \"0\" ) for ii in range ( 1 , order + 1 ) : # These are d^iQ/ddp^i cpymad_instance . select_ptc_normal ( dq1 = f \"{ii:d}\" , dq2 = f \"{ii:d}\" ) # ANH = anharmonicities (ex, ey, deltap), works only with parameters as full strings # could be done nicer with permutations ... logger . trace ( \"Selecting anharmonicities\" ) if order >= 1 : # madx.select_ptc_normal('anhx=0, 0, 1') # dQx/ddp # madx.select_ptc_normal('anhy=0, 0, 1') # dQy/ddp cpymad_instance . select_ptc_normal ( \"anhx=1, 0, 0\" ) # dQx/dex cpymad_instance . select_ptc_normal ( \"anhx=0, 1, 0\" ) # dQx/dey cpymad_instance . select_ptc_normal ( \"anhy=1, 0, 0\" ) # dQy/dex cpymad_instance . select_ptc_normal ( \"anhy=0, 1, 0\" ) # dQy/dey if order >= 2 : # madx.select_ptc_normal('anhx=0, 0, 2') # d^2Qx/ddp^2 # madx.select_ptc_normal('anhy=0, 0, 2') # d^2Qy/ddp^2 cpymad_instance . select_ptc_normal ( \"anhx=2, 0, 0\" ) # d^2Qx/dex^2 cpymad_instance . select_ptc_normal ( \"anhx=1, 1, 0\" ) # d^2Qx/dexdey cpymad_instance . select_ptc_normal ( \"anhx=0, 2, 0\" ) # d^2Qx/dey^2 cpymad_instance . select_ptc_normal ( \"anhy=2, 0, 0\" ) # d^2Qy/dex^2 cpymad_instance . select_ptc_normal ( \"anhy=1, 1, 0\" ) # d^2Qy/dexdey cpymad_instance . select_ptc_normal ( \"anhy=0, 2, 0\" ) # d^2Qy/dey^2 # icase = phase-space dimensionality, no = order of map logger . debug ( \"Executing PTC Normal\" ) cpymad_instance . ptc_normal ( closed_orbit = True , normal = True , icase = 5 , no = 5 ) cpymad_instance . ptc_end () logger . debug ( \"Extracting results to TfsDataFrame\" ) dframe = tfs . TfsDataFrame ( cpymad_instance . table . normal_results . dframe ()) dframe . columns = dframe . columns . str . upper () dframe . NAME = dframe . NAME . str . upper () dframe . index = range ( len ( dframe . NAME )) # table has a weird index if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe","title":"get_amplitude_detuning"},{"location":"reference/pyhdtoolkit/cpymadtools/ptc/#get_rdts","text":"def get_rdts ( cpymad_instance : cpymad . madx . Madx , order : int = 4 , file : Union [ pathlib . Path , str ] = None ) -> tfs . handler . TfsDataFrame INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Calculate the RDTs via PTC_TWISS. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None order int maximum derivative order (only 0, 1 or 2 implemented in PTC). Defaults to 2 . 2 file Union[Path, str] path to output file. Default None None Returns: Type Description None A TfsDataframe with results. View Source def get_rdts ( cpymad_instance : Madx , order : int = 4 , file : Union [ Path , str ] = None ) -> tfs . TfsDataFrame : \" \"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Calculate the RDTs via PTC_TWISS. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. order (int): maximum derivative order (only 0, 1 or 2 implemented in PTC). Defaults to `2`. file (Union[Path, str]): path to output file. Default `None` Returns: A TfsDataframe with results. \"\" \" logger . info ( f \"Entering PTC to calculate RDTs up to order {order}\" ) cpymad_instance . ptc_create_universe () logger . trace ( \"Creating PTC layout\" ) cpymad_instance . ptc_create_layout ( model = 3 , method = 4 , nst = 3 , exact = True ) # madx.ptc_create_layout(model=3, method=6, nst=1) # from Michi logger . trace ( \"Incorporating MAD-X alignment errors\" ) cpymad_instance . ptc_align () # use madx alignment errors # madx.ptc_setswitch(fringe=True) # include fringe effects logger . debug ( \"Executing PTC Twiss\" ) cpymad_instance . ptc_twiss ( icase = 6 , no = order , normal = True , trackrdts = True ) cpymad_instance . ptc_end () logger . debug ( \"Extracting results to TfsDataFrame\" ) dframe = tfs . TfsDataFrame ( cpymad_instance . table . twissrdt . dframe ()) dframe . columns = dframe . columns . str . upper () dframe . NAME = dframe . NAME . str . upper () if file : logger . debug ( f \"Exporting results to disk at '{Path(file).absolute()}'\" ) tfs . write ( file , dframe ) return dframe","title":"get_rdts"},{"location":"reference/pyhdtoolkit/cpymadtools/special/","text":"Module pyhdtoolkit.cpymadtools.special Module cpymadtools.special Created on 2020.02.03 View Source \"\"\" Module cpymadtools.special -------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X actions with a cpymad.madx.Madx object, that are very specific to LHC and HLLHC use cases. \"\"\" from typing import List from cpymad.madx import Madx from loguru import logger # ----- Utlites ----- # def power_landau_octupoles ( cpymad_instance : Madx , mo_current : float , beam : int , defective_arc : bool = False ) -> None : \"\"\" Power the Landau octupoles in the (HL)LHC. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. mo_current (float): MO powering in Amps. beam (int): beam to use. defective_arc: If set to `True`, the KOD in Arc 56 are powered for less Imax. \"\"\" try : brho = cpymad_instance . globals . nrj * 1e9 / cpymad_instance . globals . clight # clight is MAD-X constant except AttributeError as madx_error : logger . error ( \"The global MAD-X variable 'NRJ' should have been set in the optics files but is not defined.\" ) raise EnvironmentError ( \"No 'NRJ' variable found in scripts\" ) from madx_error logger . info ( f \"Powering Landau Octupoles, beam {beam} @ {cpymad_instance.globals.nrj} GeV with {mo_current} A.\" ) strength = mo_current / cpymad_instance . globals . Imax_MO * cpymad_instance . globals . Kmax_MO / brho beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ): for fd in \"FD\" : octupole = f \"KO{fd}.{arc}\" logger . trace ( f \"Powering element '{octupole}' at {strength} Amps\" ) cpymad_instance . globals [ octupole ] = strength if defective_arc and ( beam == 1 ): cpymad_instance . globals [ \"KOD.A56B1\" ] = strength * 4.65 / 6 # defective MO group def deactivate_lhc_arc_sextupoles ( cpymad_instance : Madx , beam : int ) -> None : \"\"\" Deactivate all arc sextupoles in the (HL)LHC. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. beam (int): beam to use. \"\"\" # KSF1 and KSD2 - Strong sextupoles of sectors 81/12/45/56 # KSF2 and KSD1 - Weak sextupoles of sectors 81/12/45/56 # Rest: Weak sextupoles in sectors 78/23/34/67 logger . info ( f \"Deactivating all arc sextupoles for beam {beam}.\" ) beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ): for fd in \"FD\" : for i in ( 1 , 2 ): sextupole = f \"KS{fd}{i:d}.{arc}\" logger . trace ( f \"De-powering element '{sextupole}'\" ) cpymad_instance . globals [ sextupole ] = 0.0 def apply_lhc_colinearity_knob ( cpymad_instance : Madx , colinearity_knob_value : float = 0 , ir : int = None ) -> None : \"\"\" Applies the LHC colinearity knob. If you don't know what this is, you should not be using this function. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. colinearity_knob_value (float): Units of the colinearity knob to apply. Defaults to 0 so users don't mess up local coupling by mistake. This should be a positive integer, normally between 1 and 10. ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. \"\"\" logger . info ( f \"Applying Colinearity knob with a unit setting of {colinearity_knob_value}\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) knob_variables = ( f \"kqsx3.r{ir:d}\" , f \"kqsx3.l{ir:d}\" ) # MQSX IP coupling correctors right_knob , left_knob = knob_variables cpymad_instance . globals [ right_knob ] = colinearity_knob_value * 1e-4 cpymad_instance . globals [ left_knob ] = - 1 * colinearity_knob_value * 1e-4 def apply_lhc_rigidity_waist_shift_knob ( cpymad_instance : Madx , rigidty_waist_shift_value : float = 0 , ir : int = None , side : str = \"left\" ) -> None : \"\"\" Applies the LHC rigidity waist shift knob, moving the waist left or right of IP. If you don't know what this is, you should not be using this function. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. rigidty_waist_shift_value (float): Units of the rigidity waist shift knob (positive values only). ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. side (str): Which side of the IP to move the waist to, determines a sign in the calculation. Defaults to 'left', which means s_waist < s_ip (and setting it to 'right' would move the waist to s_waist > s_ip). \"\"\" logger . info ( f \"Applying Rigidity Waist Shift knob with a unit setting of {rigidty_waist_shift_value}\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) knob_variables = ( f \"kqx.r{ir:d}\" , f \"kqx.l{ir:d}\" ) # Closest IP triplet right_knob , left_knob = knob_variables current_right_knob = cpymad_instance . globals [ right_knob ] current_left_knob = cpymad_instance . globals [ left_knob ] if side == \"left\" : cpymad_instance . globals [ right_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_right_knob cpymad_instance . globals [ left_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_left_knob elif side == \"right\" : cpymad_instance . globals [ right_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_right_knob cpymad_instance . globals [ left_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_left_knob else : logger . error ( f \"Given side '{side}' invalid, only 'left' and 'right' are accepted values.\" ) raise ValueError ( \"Invalid value for parameter 'side'.\" ) def apply_lhc_coupling_knob ( cpymad_instance : Madx , coupling_knob : float = 0 , beam : int = 1 , telescopic_squeeze : bool = False ) -> None : \"\"\" Applies the LHC coupling knob to reach the desired C- value. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. coupling_knob (float): Desired value for the Cminus, typically a few units of 1E-3. Defaults to 0 so users don't mess up coupling by mistake beam (int): beam to apply the knob to, defaults to beam 1. telescopic_squeeze (bool): if set to True, uses the knobs for Telescopic Squeeze configuration. Defaults to False. \"\"\" logger . info ( \"Applying coupling knob\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) suffix = \"_sq\" if telescopic_squeeze else \"\" knob_name = f \"CMRS.b{beam:d}{suffix}\" logger . trace ( f \"Knob '{knob_name}' is {cpymad_instance.globals[knob_name]} before implementation\" ) cpymad_instance . globals [ knob_name ] = coupling_knob def make_sixtrack_output ( cpymad_instance : Madx , energy : int ) -> None : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Prepare output for sixtrack run. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. \"\"\" logger . info ( \"Preparing outputs for SixTrack\" ) logger . debug ( \"Powering RF cavities\" ) cpymad_instance . globals [ \"VRF400\" ] = 8 if energy < 5000 else 16 # is 6 at injection for protons iirc? cpymad_instance . globals [ \"LAGRF400.B1\" ] = 0.5 # cavity phase difference in units of 2pi cpymad_instance . globals [ \"LAGRF400.B2\" ] = 0.0 logger . debug ( \"Executing TWISS and SIXTRACK commands\" ) cpymad_instance . twiss () # used by sixtrack cpymad_instance . sixtrack ( cavall = True , radius = 0.017 ) # this value is only ok for HL(LHC) magnet radius # ----- Helpers ----- # def _all_lhc_arcs ( beam : int ) -> List [ str ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Names of all LHC arcs for a given beam. Args: beam (int): beam to get names for. Returns: The list of names. \"\"\" return [ f \"A{i+1}{(i+1)%8+1}B{beam:d}\" for i in range ( 8 )] def _get_k_strings ( start : int = 0 , stop : int = 8 , orientation : str = \"both\" ) -> List [ str ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Returns the list of K-strings for various magnets and orders (K1L, K2SL etc strings). Args: start (int): the starting order, defaults to 0. stop (int): the order to go up to, defaults to 8. orientation (str): magnet orientation, can be 'straight', 'skew' or 'both'. Defaults to 'both'. Returns: The list of names as strings. \"\"\" if orientation not in ( \"straight\" , \"skew\" , \"both\" ,): logger . error ( f \"Orientation '{orientation}' is not accepted, should be one of 'straight', 'skew', 'both'.\" ) raise ValueError ( \"Invalid 'orientation' parameter\" ) if orientation == \"straight\" : orientation = ( \"\" ,) elif orientation == \"skew\" : orientation = ( \"S\" ,) else : # both orientation = ( \"\" , \"S\" ) return [ f \"K{i:d}{s:s}L\" for i in range ( start , stop ) for s in orientation ] Functions apply_lhc_colinearity_knob def apply_lhc_colinearity_knob ( cpymad_instance : cpymad . madx . Madx , colinearity_knob_value : float = 0 , ir : int = None ) -> None Applies the LHC colinearity knob. If you don't know what this is, you should not be using this function. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None colinearity_knob_value float Units of the colinearity knob to apply. Defaults to 0 so users don't mess up local coupling by mistake. This should be a positive integer, normally between 1 and 10. None ir int The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. None View Source def apply_lhc_colinearity_knob ( cpymad_instance : Madx , colinearity_knob_value : float = 0 , ir : int = None ) -> None : \"\"\" Applies the LHC colinearity knob. If you don't know what this is, you should not be using this function. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. colinearity_knob_value (float): Units of the colinearity knob to apply. Defaults to 0 so users don't mess up local coupling by mistake. This should be a positive integer, normally between 1 and 10. ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. \"\"\" logger . info ( f \"Applying Colinearity knob with a unit setting of {colinearity_knob_value}\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) knob_variables = ( f \"kqsx3.r{ir:d}\" , f \"kqsx3.l{ir:d}\" ) # MQSX IP coupling correctors right_knob , left_knob = knob_variables cpymad_instance . globals [ right_knob ] = colinearity_knob_value * 1e-4 cpymad_instance . globals [ left_knob ] = - 1 * colinearity_knob_value * 1e-4 apply_lhc_coupling_knob def apply_lhc_coupling_knob ( cpymad_instance : cpymad . madx . Madx , coupling_knob : float = 0 , beam : int = 1 , telescopic_squeeze : bool = False ) -> None Applies the LHC coupling knob to reach the desired C- value. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. coupling_knob (float): Desired value for the Cminus, typically a few units of 1E-3. Defaults to 0 so users don't mess up coupling by mistake beam (int): beam to apply the knob to, defaults to beam 1. telescopic_squeeze (bool): if set to True, uses the knobs for Telescopic Squeeze configuration. Defaults to False. View Source def apply_lhc_coupling_knob ( cpymad_instance : Madx , coupling_knob : float = 0 , beam : int = 1 , telescopic_squeeze : bool = False ) -> None : \"\"\" Applies the LHC coupling knob to reach the desired C- value. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. coupling_knob (float): Desired value for the Cminus, typically a few units of 1E-3. Defaults to 0 so users don't mess up coupling by mistake beam (int): beam to apply the knob to, defaults to beam 1. telescopic_squeeze (bool): if set to True, uses the knobs for Telescopic Squeeze configuration. Defaults to False. \"\"\" logger . info ( \"Applying coupling knob\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) suffix = \"_sq\" if telescopic_squeeze else \"\" knob_name = f \"CMRS.b{beam:d}{suffix}\" logger . trace ( f \"Knob '{knob_name}' is {cpymad_instance.globals[knob_name]} before implementation\" ) cpymad_instance . globals [ knob_name ] = coupling_knob apply_lhc_rigidity_waist_shift_knob def apply_lhc_rigidity_waist_shift_knob ( cpymad_instance : cpymad . madx . Madx , rigidty_waist_shift_value : float = 0 , ir : int = None , side : str = 'left' ) -> None Applies the LHC rigidity waist shift knob, moving the waist left or right of IP. If you don't know what this is, you should not be using this function. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None rigidty_waist_shift_value float Units of the rigidity waist shift knob (positive values only). None ir int The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. None side str Which side of the IP to move the waist to, determines a sign in the calculation. Defaults to 'left', which means s_waist < s_ip (and setting it to 'right' would move the waist to s_waist > s_ip). None View Source def apply_lhc_rigidity_waist_shift_knob ( cpymad_instance : Madx , rigidty_waist_shift_value : float = 0 , ir : int = None , side : str = \"left\" ) -> None : \"\"\" Applies the LHC rigidity waist shift knob, moving the waist left or right of IP. If you don't know what this is, you should not be using this function. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. rigidty_waist_shift_value (float): Units of the rigidity waist shift knob (positive values only). ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. side (str): Which side of the IP to move the waist to, determines a sign in the calculation. Defaults to 'left', which means s_waist < s_ip (and setting it to 'right' would move the waist to s_waist > s_ip). \"\"\" logger . info ( f \"Applying Rigidity Waist Shift knob with a unit setting of {rigidty_waist_shift_value}\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) knob_variables = ( f \"kqx.r{ir:d}\" , f \"kqx.l{ir:d}\" ) # Closest IP triplet right_knob , left_knob = knob_variables current_right_knob = cpymad_instance . globals [ right_knob ] current_left_knob = cpymad_instance . globals [ left_knob ] if side == \"left\" : cpymad_instance . globals [ right_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_right_knob cpymad_instance . globals [ left_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_left_knob elif side == \"right\" : cpymad_instance . globals [ right_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_right_knob cpymad_instance . globals [ left_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_left_knob else : logger . error ( f \"Given side '{side}' invalid, only 'left' and 'right' are accepted values.\" ) raise ValueError ( \"Invalid value for parameter 'side'.\" ) deactivate_lhc_arc_sextupoles def deactivate_lhc_arc_sextupoles ( cpymad_instance : cpymad . madx . Madx , beam : int ) -> None Deactivate all arc sextupoles in the (HL)LHC. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None beam int beam to use. None View Source def deactivate_lhc_arc_sextupoles ( cpymad_instance : Madx , beam : int ) -> None : \"\"\" Deactivate all arc sextupoles in the (HL)LHC. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. beam (int): beam to use. \"\"\" # KSF1 and KSD2 - Strong sextupoles of sectors 81 / 12 / 45 / 56 # KSF2 and KSD1 - Weak sextupoles of sectors 81 / 12 / 45 / 56 # Rest : Weak sextupoles in sectors 78 / 23 / 34 / 67 logger . info ( f \"Deactivating all arc sextupoles for beam {beam}.\" ) beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ) : for fd in \"FD\" : for i in ( 1 , 2 ) : sextupole = f \"KS{fd}{i:d}.{arc}\" logger . trace ( f \"De-powering element '{sextupole}'\" ) cpymad_instance . globals [ sextupole ] = 0.0 make_sixtrack_output def make_sixtrack_output ( cpymad_instance : cpymad . madx . Madx , energy : int ) -> None INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Prepare output for sixtrack run. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None energy float beam energy in GeV. None View Source def make_sixtrack_output ( cpymad_instance : Madx , energy : int ) -> None : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Prepare output for sixtrack run. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. \"\"\" logger . info ( \"Preparing outputs for SixTrack\" ) logger . debug ( \"Powering RF cavities\" ) cpymad_instance . globals [ \"VRF400\" ] = 8 if energy < 5000 else 16 # is 6 at injection for protons iirc ? cpymad_instance . globals [ \"LAGRF400.B1\" ] = 0.5 # cavity phase difference in units of 2 pi cpymad_instance . globals [ \"LAGRF400.B2\" ] = 0.0 logger . debug ( \"Executing TWISS and SIXTRACK commands\" ) cpymad_instance . twiss () # used by sixtrack cpymad_instance . sixtrack ( cavall = True , radius = 0.017 ) # this value is only ok for HL ( LHC ) magnet radius power_landau_octupoles def power_landau_octupoles ( cpymad_instance : cpymad . madx . Madx , mo_current : float , beam : int , defective_arc : bool = False ) -> None Power the Landau octupoles in the (HL)LHC. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None mo_current float MO powering in Amps. None beam int beam to use. None defective_arc None If set to True , the KOD in Arc 56 are powered for less Imax. None View Source def power_landau_octupoles ( cpymad_instance : Madx , mo_current : float , beam : int , defective_arc : bool = False ) -> None : \"\"\" Power the Landau octupoles in the (HL)LHC. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. mo_current (float): MO powering in Amps. beam (int): beam to use. defective_arc: If set to `True`, the KOD in Arc 56 are powered for less Imax. \"\"\" try : brho = cpymad_instance . globals . nrj * 1e9 / cpymad_instance . globals . clight # clight is MAD-X constant except AttributeError as madx_error : logger . error ( \"The global MAD-X variable 'NRJ' should have been set in the optics files but is not defined.\" ) raise EnvironmentError ( \"No 'NRJ' variable found in scripts\" ) from madx_error logger . info ( f \"Powering Landau Octupoles, beam {beam} @ {cpymad_instance.globals.nrj} GeV with {mo_current} A.\" ) strength = mo_current / cpymad_instance . globals . Imax_MO * cpymad_instance . globals . Kmax_MO / brho beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ): for fd in \"FD\" : octupole = f \"KO{fd}.{arc}\" logger . trace ( f \"Powering element '{octupole}' at {strength} Amps\" ) cpymad_instance . globals [ octupole ] = strength if defective_arc and ( beam == 1 ): cpymad_instance . globals [ \"KOD.A56B1\" ] = strength * 4.65 / 6 # defective MO group","title":"Special"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#module-pyhdtoolkitcpymadtoolsspecial","text":"Module cpymadtools.special Created on 2020.02.03 View Source \"\"\" Module cpymadtools.special -------------------------- Created on 2020.02.03 :author: Felix Soubelet (felix.soubelet@cern.ch) A module with functions to perform MAD-X actions with a cpymad.madx.Madx object, that are very specific to LHC and HLLHC use cases. \"\"\" from typing import List from cpymad.madx import Madx from loguru import logger # ----- Utlites ----- # def power_landau_octupoles ( cpymad_instance : Madx , mo_current : float , beam : int , defective_arc : bool = False ) -> None : \"\"\" Power the Landau octupoles in the (HL)LHC. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. mo_current (float): MO powering in Amps. beam (int): beam to use. defective_arc: If set to `True`, the KOD in Arc 56 are powered for less Imax. \"\"\" try : brho = cpymad_instance . globals . nrj * 1e9 / cpymad_instance . globals . clight # clight is MAD-X constant except AttributeError as madx_error : logger . error ( \"The global MAD-X variable 'NRJ' should have been set in the optics files but is not defined.\" ) raise EnvironmentError ( \"No 'NRJ' variable found in scripts\" ) from madx_error logger . info ( f \"Powering Landau Octupoles, beam {beam} @ {cpymad_instance.globals.nrj} GeV with {mo_current} A.\" ) strength = mo_current / cpymad_instance . globals . Imax_MO * cpymad_instance . globals . Kmax_MO / brho beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ): for fd in \"FD\" : octupole = f \"KO{fd}.{arc}\" logger . trace ( f \"Powering element '{octupole}' at {strength} Amps\" ) cpymad_instance . globals [ octupole ] = strength if defective_arc and ( beam == 1 ): cpymad_instance . globals [ \"KOD.A56B1\" ] = strength * 4.65 / 6 # defective MO group def deactivate_lhc_arc_sextupoles ( cpymad_instance : Madx , beam : int ) -> None : \"\"\" Deactivate all arc sextupoles in the (HL)LHC. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. beam (int): beam to use. \"\"\" # KSF1 and KSD2 - Strong sextupoles of sectors 81/12/45/56 # KSF2 and KSD1 - Weak sextupoles of sectors 81/12/45/56 # Rest: Weak sextupoles in sectors 78/23/34/67 logger . info ( f \"Deactivating all arc sextupoles for beam {beam}.\" ) beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ): for fd in \"FD\" : for i in ( 1 , 2 ): sextupole = f \"KS{fd}{i:d}.{arc}\" logger . trace ( f \"De-powering element '{sextupole}'\" ) cpymad_instance . globals [ sextupole ] = 0.0 def apply_lhc_colinearity_knob ( cpymad_instance : Madx , colinearity_knob_value : float = 0 , ir : int = None ) -> None : \"\"\" Applies the LHC colinearity knob. If you don't know what this is, you should not be using this function. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. colinearity_knob_value (float): Units of the colinearity knob to apply. Defaults to 0 so users don't mess up local coupling by mistake. This should be a positive integer, normally between 1 and 10. ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. \"\"\" logger . info ( f \"Applying Colinearity knob with a unit setting of {colinearity_knob_value}\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) knob_variables = ( f \"kqsx3.r{ir:d}\" , f \"kqsx3.l{ir:d}\" ) # MQSX IP coupling correctors right_knob , left_knob = knob_variables cpymad_instance . globals [ right_knob ] = colinearity_knob_value * 1e-4 cpymad_instance . globals [ left_knob ] = - 1 * colinearity_knob_value * 1e-4 def apply_lhc_rigidity_waist_shift_knob ( cpymad_instance : Madx , rigidty_waist_shift_value : float = 0 , ir : int = None , side : str = \"left\" ) -> None : \"\"\" Applies the LHC rigidity waist shift knob, moving the waist left or right of IP. If you don't know what this is, you should not be using this function. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. rigidty_waist_shift_value (float): Units of the rigidity waist shift knob (positive values only). ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. side (str): Which side of the IP to move the waist to, determines a sign in the calculation. Defaults to 'left', which means s_waist < s_ip (and setting it to 'right' would move the waist to s_waist > s_ip). \"\"\" logger . info ( f \"Applying Rigidity Waist Shift knob with a unit setting of {rigidty_waist_shift_value}\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) knob_variables = ( f \"kqx.r{ir:d}\" , f \"kqx.l{ir:d}\" ) # Closest IP triplet right_knob , left_knob = knob_variables current_right_knob = cpymad_instance . globals [ right_knob ] current_left_knob = cpymad_instance . globals [ left_knob ] if side == \"left\" : cpymad_instance . globals [ right_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_right_knob cpymad_instance . globals [ left_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_left_knob elif side == \"right\" : cpymad_instance . globals [ right_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_right_knob cpymad_instance . globals [ left_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_left_knob else : logger . error ( f \"Given side '{side}' invalid, only 'left' and 'right' are accepted values.\" ) raise ValueError ( \"Invalid value for parameter 'side'.\" ) def apply_lhc_coupling_knob ( cpymad_instance : Madx , coupling_knob : float = 0 , beam : int = 1 , telescopic_squeeze : bool = False ) -> None : \"\"\" Applies the LHC coupling knob to reach the desired C- value. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. coupling_knob (float): Desired value for the Cminus, typically a few units of 1E-3. Defaults to 0 so users don't mess up coupling by mistake beam (int): beam to apply the knob to, defaults to beam 1. telescopic_squeeze (bool): if set to True, uses the knobs for Telescopic Squeeze configuration. Defaults to False. \"\"\" logger . info ( \"Applying coupling knob\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) suffix = \"_sq\" if telescopic_squeeze else \"\" knob_name = f \"CMRS.b{beam:d}{suffix}\" logger . trace ( f \"Knob '{knob_name}' is {cpymad_instance.globals[knob_name]} before implementation\" ) cpymad_instance . globals [ knob_name ] = coupling_knob def make_sixtrack_output ( cpymad_instance : Madx , energy : int ) -> None : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Prepare output for sixtrack run. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. \"\"\" logger . info ( \"Preparing outputs for SixTrack\" ) logger . debug ( \"Powering RF cavities\" ) cpymad_instance . globals [ \"VRF400\" ] = 8 if energy < 5000 else 16 # is 6 at injection for protons iirc? cpymad_instance . globals [ \"LAGRF400.B1\" ] = 0.5 # cavity phase difference in units of 2pi cpymad_instance . globals [ \"LAGRF400.B2\" ] = 0.0 logger . debug ( \"Executing TWISS and SIXTRACK commands\" ) cpymad_instance . twiss () # used by sixtrack cpymad_instance . sixtrack ( cavall = True , radius = 0.017 ) # this value is only ok for HL(LHC) magnet radius # ----- Helpers ----- # def _all_lhc_arcs ( beam : int ) -> List [ str ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Names of all LHC arcs for a given beam. Args: beam (int): beam to get names for. Returns: The list of names. \"\"\" return [ f \"A{i+1}{(i+1)%8+1}B{beam:d}\" for i in range ( 8 )] def _get_k_strings ( start : int = 0 , stop : int = 8 , orientation : str = \"both\" ) -> List [ str ]: \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Returns the list of K-strings for various magnets and orders (K1L, K2SL etc strings). Args: start (int): the starting order, defaults to 0. stop (int): the order to go up to, defaults to 8. orientation (str): magnet orientation, can be 'straight', 'skew' or 'both'. Defaults to 'both'. Returns: The list of names as strings. \"\"\" if orientation not in ( \"straight\" , \"skew\" , \"both\" ,): logger . error ( f \"Orientation '{orientation}' is not accepted, should be one of 'straight', 'skew', 'both'.\" ) raise ValueError ( \"Invalid 'orientation' parameter\" ) if orientation == \"straight\" : orientation = ( \"\" ,) elif orientation == \"skew\" : orientation = ( \"S\" ,) else : # both orientation = ( \"\" , \"S\" ) return [ f \"K{i:d}{s:s}L\" for i in range ( start , stop ) for s in orientation ]","title":"Module pyhdtoolkit.cpymadtools.special"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#apply_lhc_colinearity_knob","text":"def apply_lhc_colinearity_knob ( cpymad_instance : cpymad . madx . Madx , colinearity_knob_value : float = 0 , ir : int = None ) -> None Applies the LHC colinearity knob. If you don't know what this is, you should not be using this function. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None colinearity_knob_value float Units of the colinearity knob to apply. Defaults to 0 so users don't mess up local coupling by mistake. This should be a positive integer, normally between 1 and 10. None ir int The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. None View Source def apply_lhc_colinearity_knob ( cpymad_instance : Madx , colinearity_knob_value : float = 0 , ir : int = None ) -> None : \"\"\" Applies the LHC colinearity knob. If you don't know what this is, you should not be using this function. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. colinearity_knob_value (float): Units of the colinearity knob to apply. Defaults to 0 so users don't mess up local coupling by mistake. This should be a positive integer, normally between 1 and 10. ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. \"\"\" logger . info ( f \"Applying Colinearity knob with a unit setting of {colinearity_knob_value}\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) knob_variables = ( f \"kqsx3.r{ir:d}\" , f \"kqsx3.l{ir:d}\" ) # MQSX IP coupling correctors right_knob , left_knob = knob_variables cpymad_instance . globals [ right_knob ] = colinearity_knob_value * 1e-4 cpymad_instance . globals [ left_knob ] = - 1 * colinearity_knob_value * 1e-4","title":"apply_lhc_colinearity_knob"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#apply_lhc_coupling_knob","text":"def apply_lhc_coupling_knob ( cpymad_instance : cpymad . madx . Madx , coupling_knob : float = 0 , beam : int = 1 , telescopic_squeeze : bool = False ) -> None Applies the LHC coupling knob to reach the desired C- value. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. coupling_knob (float): Desired value for the Cminus, typically a few units of 1E-3. Defaults to 0 so users don't mess up coupling by mistake beam (int): beam to apply the knob to, defaults to beam 1. telescopic_squeeze (bool): if set to True, uses the knobs for Telescopic Squeeze configuration. Defaults to False. View Source def apply_lhc_coupling_knob ( cpymad_instance : Madx , coupling_knob : float = 0 , beam : int = 1 , telescopic_squeeze : bool = False ) -> None : \"\"\" Applies the LHC coupling knob to reach the desired C- value. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. coupling_knob (float): Desired value for the Cminus, typically a few units of 1E-3. Defaults to 0 so users don't mess up coupling by mistake beam (int): beam to apply the knob to, defaults to beam 1. telescopic_squeeze (bool): if set to True, uses the knobs for Telescopic Squeeze configuration. Defaults to False. \"\"\" logger . info ( \"Applying coupling knob\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) suffix = \"_sq\" if telescopic_squeeze else \"\" knob_name = f \"CMRS.b{beam:d}{suffix}\" logger . trace ( f \"Knob '{knob_name}' is {cpymad_instance.globals[knob_name]} before implementation\" ) cpymad_instance . globals [ knob_name ] = coupling_knob","title":"apply_lhc_coupling_knob"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#apply_lhc_rigidity_waist_shift_knob","text":"def apply_lhc_rigidity_waist_shift_knob ( cpymad_instance : cpymad . madx . Madx , rigidty_waist_shift_value : float = 0 , ir : int = None , side : str = 'left' ) -> None Applies the LHC rigidity waist shift knob, moving the waist left or right of IP. If you don't know what this is, you should not be using this function. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None rigidty_waist_shift_value float Units of the rigidity waist shift knob (positive values only). None ir int The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. None side str Which side of the IP to move the waist to, determines a sign in the calculation. Defaults to 'left', which means s_waist < s_ip (and setting it to 'right' would move the waist to s_waist > s_ip). None View Source def apply_lhc_rigidity_waist_shift_knob ( cpymad_instance : Madx , rigidty_waist_shift_value : float = 0 , ir : int = None , side : str = \"left\" ) -> None : \"\"\" Applies the LHC rigidity waist shift knob, moving the waist left or right of IP. If you don't know what this is, you should not be using this function. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. rigidty_waist_shift_value (float): Units of the rigidity waist shift knob (positive values only). ir (int): The Interaction Region to apply the knob to, should be one of [1, 2, 5, 8]. Classically 1 or 5. side (str): Which side of the IP to move the waist to, determines a sign in the calculation. Defaults to 'left', which means s_waist < s_ip (and setting it to 'right' would move the waist to s_waist > s_ip). \"\"\" logger . info ( f \"Applying Rigidity Waist Shift knob with a unit setting of {rigidty_waist_shift_value}\" ) logger . warning ( \"You should re-match tunes & chromaticities after this\" ) knob_variables = ( f \"kqx.r{ir:d}\" , f \"kqx.l{ir:d}\" ) # Closest IP triplet right_knob , left_knob = knob_variables current_right_knob = cpymad_instance . globals [ right_knob ] current_left_knob = cpymad_instance . globals [ left_knob ] if side == \"left\" : cpymad_instance . globals [ right_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_right_knob cpymad_instance . globals [ left_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_left_knob elif side == \"right\" : cpymad_instance . globals [ right_knob ] = ( 1 + rigidty_waist_shift_value * 0.005 ) * current_right_knob cpymad_instance . globals [ left_knob ] = ( 1 - rigidty_waist_shift_value * 0.005 ) * current_left_knob else : logger . error ( f \"Given side '{side}' invalid, only 'left' and 'right' are accepted values.\" ) raise ValueError ( \"Invalid value for parameter 'side'.\" )","title":"apply_lhc_rigidity_waist_shift_knob"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#deactivate_lhc_arc_sextupoles","text":"def deactivate_lhc_arc_sextupoles ( cpymad_instance : cpymad . madx . Madx , beam : int ) -> None Deactivate all arc sextupoles in the (HL)LHC. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None beam int beam to use. None View Source def deactivate_lhc_arc_sextupoles ( cpymad_instance : Madx , beam : int ) -> None : \"\"\" Deactivate all arc sextupoles in the (HL)LHC. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. beam (int): beam to use. \"\"\" # KSF1 and KSD2 - Strong sextupoles of sectors 81 / 12 / 45 / 56 # KSF2 and KSD1 - Weak sextupoles of sectors 81 / 12 / 45 / 56 # Rest : Weak sextupoles in sectors 78 / 23 / 34 / 67 logger . info ( f \"Deactivating all arc sextupoles for beam {beam}.\" ) beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ) : for fd in \"FD\" : for i in ( 1 , 2 ) : sextupole = f \"KS{fd}{i:d}.{arc}\" logger . trace ( f \"De-powering element '{sextupole}'\" ) cpymad_instance . globals [ sextupole ] = 0.0","title":"deactivate_lhc_arc_sextupoles"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#make_sixtrack_output","text":"def make_sixtrack_output ( cpymad_instance : cpymad . madx . Madx , energy : int ) -> None INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Prepare output for sixtrack run. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None energy float beam energy in GeV. None View Source def make_sixtrack_output ( cpymad_instance : Madx , energy : int ) -> None : \"\"\" INITIAL IMPLEMENTATION CREDITS GO TO JOSCHUA DILLY (@JoschD). Prepare output for sixtrack run. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. energy (float): beam energy in GeV. \"\"\" logger . info ( \"Preparing outputs for SixTrack\" ) logger . debug ( \"Powering RF cavities\" ) cpymad_instance . globals [ \"VRF400\" ] = 8 if energy < 5000 else 16 # is 6 at injection for protons iirc ? cpymad_instance . globals [ \"LAGRF400.B1\" ] = 0.5 # cavity phase difference in units of 2 pi cpymad_instance . globals [ \"LAGRF400.B2\" ] = 0.0 logger . debug ( \"Executing TWISS and SIXTRACK commands\" ) cpymad_instance . twiss () # used by sixtrack cpymad_instance . sixtrack ( cavall = True , radius = 0.017 ) # this value is only ok for HL ( LHC ) magnet radius","title":"make_sixtrack_output"},{"location":"reference/pyhdtoolkit/cpymadtools/special/#power_landau_octupoles","text":"def power_landau_octupoles ( cpymad_instance : cpymad . madx . Madx , mo_current : float , beam : int , defective_arc : bool = False ) -> None Power the Landau octupoles in the (HL)LHC. Parameters: Name Type Description Default cpymad_instance cpymad.madx.Madx an instanciated cpymad Madx object. None mo_current float MO powering in Amps. None beam int beam to use. None defective_arc None If set to True , the KOD in Arc 56 are powered for less Imax. None View Source def power_landau_octupoles ( cpymad_instance : Madx , mo_current : float , beam : int , defective_arc : bool = False ) -> None : \"\"\" Power the Landau octupoles in the (HL)LHC. Args: cpymad_instance (cpymad.madx.Madx): an instanciated cpymad Madx object. mo_current (float): MO powering in Amps. beam (int): beam to use. defective_arc: If set to `True`, the KOD in Arc 56 are powered for less Imax. \"\"\" try : brho = cpymad_instance . globals . nrj * 1e9 / cpymad_instance . globals . clight # clight is MAD-X constant except AttributeError as madx_error : logger . error ( \"The global MAD-X variable 'NRJ' should have been set in the optics files but is not defined.\" ) raise EnvironmentError ( \"No 'NRJ' variable found in scripts\" ) from madx_error logger . info ( f \"Powering Landau Octupoles, beam {beam} @ {cpymad_instance.globals.nrj} GeV with {mo_current} A.\" ) strength = mo_current / cpymad_instance . globals . Imax_MO * cpymad_instance . globals . Kmax_MO / brho beam = 2 if beam == 4 else beam for arc in _all_lhc_arcs ( beam ): for fd in \"FD\" : octupole = f \"KO{fd}.{arc}\" logger . trace ( f \"Powering element '{octupole}' at {strength} Amps\" ) cpymad_instance . globals [ octupole ] = strength if defective_arc and ( beam == 1 ): cpymad_instance . globals [ \"KOD.A56B1\" ] = strength * 4.65 / 6 # defective MO group","title":"power_landau_octupoles"},{"location":"reference/pyhdtoolkit/maths/","text":"Module pyhdtoolkit.maths None None Sub-modules pyhdtoolkit.maths.nonconvex_phase_sync pyhdtoolkit.maths.stats_fitting","title":"Index"},{"location":"reference/pyhdtoolkit/maths/#module-pyhdtoolkitmaths","text":"None None","title":"Module pyhdtoolkit.maths"},{"location":"reference/pyhdtoolkit/maths/#sub-modules","text":"pyhdtoolkit.maths.nonconvex_phase_sync pyhdtoolkit.maths.stats_fitting","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/","text":"Module pyhdtoolkit.maths.nonconvex_phase_sync Module maths.nonconvex_phase_sync Created on 2020.01.13 View Source \"\"\" Module maths.nonconvex_phase_sync --------------------------------- Created on 2020.01.13 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 implementation of the Nonconvex Phase Synchronisation method found in the following paper (DOI: 10.1137/16M105808X, the algorithm reproduced is page 8). Methodology and Use Case ======================== We consider that from measurements, we can only obtain noisy relative phase advances mu_{i} - mu_{j} and want a converging solution to reconstruct the different individual mu_{1}, ...., mu_{n} values. From measurements, we construct a hermitian matrix C in the shape of: C_{ij} = z_{i} * bar(z_{j}) = exp(i * (mu_{i} - mu_{j})) A mock one with random values (500 by 500 as we have 500 BPMs per plane in the LHC) would be: c_matrix = np.exp(1j * np.random.rand(500, 500)) Considering 4 BPMs, the measurement matrix would be: M_matrix = [[mu_{1 -> 1}, mu_{1 -> 2}, mu_{1 -> 3}, mu_{1 -> 4}], [mu_{2 -> 1}, mu_{2 -> 2}, mu_{2 -> 3}, mu_{2 -> 4}], [mu_{3 -> 1}, mu_{3 -> 2}, mu_{3 -> 3}, mu_{3 -> 4}], [mu_{4 -> 1}, mu_{4 -> 2}, mu_{4 -> 3}, mu_{4 -> 4}]] Note two particular properties here: - Because our measurements are phase differences, the M_matrix will necessarily have zeros on its diagonal (mu_{k -> k} = 0). - By definition, since mu_{a -> b} = - mu_{b -> a}, M_matrix is symmetric. - Also note that for all computations, M_matrix needs to be initialised in radians! We can very simply get our C_matrix (see page 1 of referenced paper) with `numpy.exp` which, applied to a `numpy.ndarray` applies the exponential function element-wise. See reference at https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html Then follows: C_matrix = np.exp(1j * M_matrix) Note that M_matrix being symmetric, then c_matrix will be Hermitian. Note that M_matrix having zeros in its diagonal, c_matrix will have (1 + 0j) on its diagonal. With added noise to those values (noise should be included in M_matrix in the case of measurements), we can reconstruct a good estimator of the original values through the EVM method, provided in the class below. \"\"\" import numpy as np from loguru import logger class PhaseReconstructor: \"\"\" Class algorithm to reconstruct your phase values. Make sure to provide vectors as `numpy.ndarray` with shape (1, N), N being the dimension. \"\"\" __slots__ = { \"c_matrix\" : \"Hermitian square matrix from your measurements\" , \"c_matrix_eigenvalues\" : \"Eigenvalues of c_matrix\" , \"c_matrix_eigenvectors\" : \"Eigenvectors of c_matrix\" , \"space_dimension\" : \"Dimension of your measurement space\" , } def __init__ ( self , measurements_hermitian_matrix: np . ndarray ) -> None: \"\"\" Initialize your reconstructor object from measurements. Args: measurements_hermitian_matrix: a `numpy.ndarray` object built from measurements, see module docstring on how to build this matrix. \"\"\" logger . debug ( \"Checking that the provided matrix is Hermitian\" ) if np . allclose ( measurements_hermitian_matrix , np . conj ( measurements_hermitian_matrix ). T ): self . c_matrix: np . ndarray = measurements_hermitian_matrix self . c_matrix_eigenvalues: np . ndarray = np . linalg . eigvalsh ( self . c_matrix ) # Numpy gives the eigenvectors in column form, so transpose is needed there! self . c_matrix_eigenvectors: np . ndarray = np . linalg . eigh ( self . c_matrix )[- 1 ]. T self . space_dimension: int = self . c_matrix . shape [ 0 ] else: logger . exception ( \"Instantiating a PhaseReconstructor with a non hermitian matrix is \" \"not possible\" ) raise ValueError ( \"Provided matrix should be Hermitian\" ) @property def alpha ( self ) -> np . float64: \"\"\" This is a factor used to define the new reconstruction matrix. It is taken either as the operator norm of the hermitian noise matrix, or as the max value between 0 and the opposite of the min eigenvalue of c_matrix (chosen for implementation, since our noise is included in the measurements). See page 8 of the paper for reference. Returns: A real scalar value, because c_matrix is Hermitian and the eigenvalues of real symmetric or complex Hermitian matrices are always real (see G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pg. 222.) \"\"\" return np . float64 ( max ( 0 , np . amin ( self . c_matrix_eigenvalues ))) @property def leading_eigenvector ( self ) -> np . ndarray: \"\"\" Returns the leading eigenvector of `self.c_matrix`, which is the eigenvector corresponding to the max eigenvalue (in absolute value). Returns: A `numpy.ndarray` object, corresponding to said eigenvector. \"\"\" return self . c_matrix_eigenvectors [ np . where ( self . c_matrix_eigenvalues == np . amax ( np . absolute ( self . c_matrix_eigenvalues ))) ]. reshape (( 1 , self . space_dimension )) @property def reconstructor_matrix ( self ) -> np . ndarray: \"\"\" This is the reconstructor matrix built from `self.c_matrix` and the `alpha` property. It is the matrix denoted as \\\\widetilde{C} on page 8 of the reference paper. Returns: A `numpy.ndarray`, with same dimension as `self.c_matrix`. \"\"\" return self . c_matrix + self . alpha * np . identity ( self . c_matrix . shape [ 0 ]) def get_eigenvector_estimator ( self , eigenvector: np . ndarray ) -> np . ndarray: \"\"\" Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Args: eigenvector (np.ndarray): a numpy array representing the vector. Returns: A `numpy.ndarray` object of the same dimension as param `eigenvector`. \"\"\" try: return eigenvector / np . absolute ( eigenvector ) except RuntimeWarning: # In case of 0-division, we don't want `inf` values from numpy # Generate a random complex vector with same dimension (since `eigenvector` is of # dimension N * 1, `numpy.ndarray.size` method will give us N). # Remember to initialize a random real and imaginary part. logger . exception ( \"Encountered 0-division, trying normalization\" ) e_vect = np . random . randn ( eigenvector . size ) + 1 j * np . random . randn ( eigenvector . size ) while ( np . absolute ( e_vect @ eigenvector ) == 0 ): # Guarantee that we don't fall back to this edge case. e_vect = np . random . randn ( eigenvector . size ) + 1 j * np . random . randn ( eigenvector . size ) return ( e_vect @ eigenvector / np . absolute ( e_vect @ eigenvector )). reshape ( ( 1 , self . space_dimension ) ) def reconstruct_complex_phases_evm ( self ) -> np . ndarray: \"\"\" Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: The complex form of the result as a 'numpy.ndarray' instance. \"\"\" logger . debug ( \"Getting complex phase results\" ) return self . get_eigenvector_estimator ( self . leading_eigenvector ) @staticmethod def convert_complex_result_to_phase_values ( complex_estimator: np . ndarray , deg: bool = False ) -> np . ndarray: \"\"\" Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A `numpy.ndarray` with the real phase values of the result. \"\"\" logger . debug ( \"Casting complex phases to real values\" ) return np . apply_along_axis ( np . angle , axis = 0 , arr = complex_estimator , deg = deg ) Classes PhaseReconstructor class PhaseReconstructor ( measurements_hermitian_matrix : numpy . ndarray ) View Source class PhaseReconstructor : \" \"\" Class algorithm to reconstruct your phase values. Make sure to provide vectors as `numpy.ndarray` with shape (1, N), N being the dimension. \"\" \" __slots__ = { \"c_matrix\" : \"Hermitian square matrix from your measurements\" , \"c_matrix_eigenvalues\" : \"Eigenvalues of c_matrix\" , \"c_matrix_eigenvectors\" : \"Eigenvectors of c_matrix\" , \"space_dimension\" : \"Dimension of your measurement space\" , } def __init__ ( self , measurements_hermitian_matrix : np . ndarray ) -> None : \" \"\" Initialize your reconstructor object from measurements. Args: measurements_hermitian_matrix: a `numpy.ndarray` object built from measurements, see module docstring on how to build this matrix. \"\" \" logger . debug ( \"Checking that the provided matrix is Hermitian\" ) if np . allclose ( measurements_hermitian_matrix , np . conj ( measurements_hermitian_matrix ). T ) : self . c_matrix : np . ndarray = measurements_hermitian_matrix self . c_matrix_eigenvalues : np . ndarray = np . linalg . eigvalsh ( self . c_matrix ) # Numpy gives the eigenvectors in column form, so transpose is needed there! self . c_matrix_eigenvectors : np . ndarray = np . linalg . eigh ( self . c_matrix ) [ - 1 ] . T self . space_dimension : int = self . c_matrix . shape [ 0 ] else : logger . exception ( \"Instantiating a PhaseReconstructor with a non hermitian matrix is \" \"not possible\" ) raise ValueError ( \"Provided matrix should be Hermitian\" ) @property def alpha ( self ) -> np . float64 : \" \"\" This is a factor used to define the new reconstruction matrix. It is taken either as the operator norm of the hermitian noise matrix, or as the max value between 0 and the opposite of the min eigenvalue of c_matrix (chosen for implementation, since our noise is included in the measurements). See page 8 of the paper for reference. Returns: A real scalar value, because c_matrix is Hermitian and the eigenvalues of real symmetric or complex Hermitian matrices are always real (see G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pg. 222.) \"\" \" return np . float64 ( max ( 0 , np . amin ( self . c_matrix_eigenvalues ))) @property def leading_eigenvector ( self ) -> np . ndarray : \" \"\" Returns the leading eigenvector of `self.c_matrix`, which is the eigenvector corresponding to the max eigenvalue (in absolute value). Returns: A `numpy.ndarray` object, corresponding to said eigenvector. \"\" \" return self . c_matrix_eigenvectors [ np . where ( self . c_matrix_eigenvalues == np . amax ( np . absolute ( self . c_matrix_eigenvalues ))) ] . reshape (( 1 , self . space_dimension )) @property def reconstructor_matrix ( self ) -> np . ndarray : \" \"\" This is the reconstructor matrix built from `self.c_matrix` and the `alpha` property. It is the matrix denoted as \\\\ widetilde{C} on page 8 of the reference paper. Returns: A `numpy.ndarray`, with same dimension as `self.c_matrix`. \"\" \" return self . c_matrix + self . alpha * np . identity ( self . c_matrix . shape [ 0 ] ) def get_eigenvector_estimator ( self , eigenvector : np . ndarray ) -> np . ndarray : \" \"\" Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Args: eigenvector (np.ndarray): a numpy array representing the vector. Returns: A `numpy.ndarray` object of the same dimension as param `eigenvector`. \"\" \" try : return eigenvector / np . absolute ( eigenvector ) except RuntimeWarning : # In case of 0-division, we don't want `inf` values from numpy # Generate a random complex vector with same dimension (since `eigenvector` is of # dimension N * 1, `numpy.ndarray.size` method will give us N). # Remember to initialize a random real and imaginary part. logger . exception ( \"Encountered 0-division, trying normalization\" ) e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) while ( np . absolute ( e_vect @ eigenvector ) == 0 ) : # Guarantee that we don't fall back to this edge case. e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) return ( e_vect @ eigenvector / np . absolute ( e_vect @ eigenvector )). reshape ( ( 1 , self . space_dimension ) ) def reconstruct_complex_phases_evm ( self ) -> np . ndarray : \" \"\" Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: The complex form of the result as a 'numpy.ndarray' instance. \"\" \" logger . debug ( \"Getting complex phase results\" ) return self . get_eigenvector_estimator ( self . leading_eigenvector ) @staticmethod def convert_complex_result_to_phase_values ( complex_estimator : np . ndarray , deg : bool = False ) -> np . ndarray : \" \"\" Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A `numpy.ndarray` with the real phase values of the result. \"\" \" logger . debug ( \"Casting complex phases to real values\" ) return np . apply_along_axis ( np . angle , axis = 0 , arr = complex_estimator , deg = deg ) Static methods convert_complex_result_to_phase_values def convert_complex_result_to_phase_values ( complex_estimator : numpy . ndarray , deg : bool = False ) -> numpy . ndarray Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A numpy.ndarray with the real phase values of the result. View Source @staticmethod def convert_complex_result_to_phase_values ( complex_estimator : np . ndarray , deg : bool = False ) -> np . ndarray : \"\"\" Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A `numpy.ndarray` with the real phase values of the result. \"\"\" logger . debug ( \"Casting complex phases to real values\" ) return np . apply_along_axis ( np . angle , axis = 0 , arr = complex_estimator , deg = deg ) Instance variables alpha This is a factor used to define the new reconstruction matrix. It is taken either as the operator norm of the hermitian noise matrix, or as the max value between 0 and the opposite of the min eigenvalue of c_matrix (chosen for implementation, since our noise is included in the measurements). See page 8 of the paper for reference. c_matrix c_matrix_eigenvalues c_matrix_eigenvectors leading_eigenvector Returns the leading eigenvector of self.c_matrix , which is the eigenvector corresponding to the max eigenvalue (in absolute value). reconstructor_matrix This is the reconstructor matrix built from self.c_matrix and the alpha property. It is the matrix denoted as \\widetilde{C} on page 8 of the reference paper. space_dimension Methods get_eigenvector_estimator def get_eigenvector_estimator ( self , eigenvector : numpy . ndarray ) -> numpy . ndarray Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Parameters: Name Type Description Default eigenvector np.ndarray a numpy array representing the vector. None Returns: Type Description None A numpy.ndarray object of the same dimension as param eigenvector . View Source def get_eigenvector_estimator ( self , eigenvector : np . ndarray ) -> np . ndarray : \" \"\" Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Args: eigenvector (np.ndarray): a numpy array representing the vector. Returns: A `numpy.ndarray` object of the same dimension as param `eigenvector`. \"\" \" try : return eigenvector / np . absolute ( eigenvector ) except RuntimeWarning : # In case of 0-division, we don't want `inf` values from numpy # Generate a random complex vector with same dimension (since `eigenvector` is of # dimension N * 1, `numpy.ndarray.size` method will give us N). # Remember to initialize a random real and imaginary part. logger . exception ( \"Encountered 0-division, trying normalization\" ) e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) while ( np . absolute ( e_vect @ eigenvector ) == 0 ) : # Guarantee that we don't fall back to this edge case. e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) return ( e_vect @ eigenvector / np . absolute ( e_vect @ eigenvector )). reshape ( ( 1 , self . space_dimension ) ) reconstruct_complex_phases_evm def reconstruct_complex_phases_evm ( self ) -> numpy . ndarray Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: Type Description None The complex form of the result as a 'numpy.ndarray' instance. View Source def reconstruct_complex_phases_evm ( self ) -> np . ndarray : \"\"\" Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: The complex form of the result as a 'numpy.ndarray' instance. \"\"\" logger . debug ( \"Getting complex phase results\" ) return self . get_eigenvector_estimator ( self . leading_eigenvector )","title":"Nonconvex Phase Sync"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#module-pyhdtoolkitmathsnonconvex_phase_sync","text":"Module maths.nonconvex_phase_sync Created on 2020.01.13 View Source \"\"\" Module maths.nonconvex_phase_sync --------------------------------- Created on 2020.01.13 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 implementation of the Nonconvex Phase Synchronisation method found in the following paper (DOI: 10.1137/16M105808X, the algorithm reproduced is page 8). Methodology and Use Case ======================== We consider that from measurements, we can only obtain noisy relative phase advances mu_{i} - mu_{j} and want a converging solution to reconstruct the different individual mu_{1}, ...., mu_{n} values. From measurements, we construct a hermitian matrix C in the shape of: C_{ij} = z_{i} * bar(z_{j}) = exp(i * (mu_{i} - mu_{j})) A mock one with random values (500 by 500 as we have 500 BPMs per plane in the LHC) would be: c_matrix = np.exp(1j * np.random.rand(500, 500)) Considering 4 BPMs, the measurement matrix would be: M_matrix = [[mu_{1 -> 1}, mu_{1 -> 2}, mu_{1 -> 3}, mu_{1 -> 4}], [mu_{2 -> 1}, mu_{2 -> 2}, mu_{2 -> 3}, mu_{2 -> 4}], [mu_{3 -> 1}, mu_{3 -> 2}, mu_{3 -> 3}, mu_{3 -> 4}], [mu_{4 -> 1}, mu_{4 -> 2}, mu_{4 -> 3}, mu_{4 -> 4}]] Note two particular properties here: - Because our measurements are phase differences, the M_matrix will necessarily have zeros on its diagonal (mu_{k -> k} = 0). - By definition, since mu_{a -> b} = - mu_{b -> a}, M_matrix is symmetric. - Also note that for all computations, M_matrix needs to be initialised in radians! We can very simply get our C_matrix (see page 1 of referenced paper) with `numpy.exp` which, applied to a `numpy.ndarray` applies the exponential function element-wise. See reference at https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html Then follows: C_matrix = np.exp(1j * M_matrix) Note that M_matrix being symmetric, then c_matrix will be Hermitian. Note that M_matrix having zeros in its diagonal, c_matrix will have (1 + 0j) on its diagonal. With added noise to those values (noise should be included in M_matrix in the case of measurements), we can reconstruct a good estimator of the original values through the EVM method, provided in the class below. \"\"\" import numpy as np from loguru import logger class PhaseReconstructor: \"\"\" Class algorithm to reconstruct your phase values. Make sure to provide vectors as `numpy.ndarray` with shape (1, N), N being the dimension. \"\"\" __slots__ = { \"c_matrix\" : \"Hermitian square matrix from your measurements\" , \"c_matrix_eigenvalues\" : \"Eigenvalues of c_matrix\" , \"c_matrix_eigenvectors\" : \"Eigenvectors of c_matrix\" , \"space_dimension\" : \"Dimension of your measurement space\" , } def __init__ ( self , measurements_hermitian_matrix: np . ndarray ) -> None: \"\"\" Initialize your reconstructor object from measurements. Args: measurements_hermitian_matrix: a `numpy.ndarray` object built from measurements, see module docstring on how to build this matrix. \"\"\" logger . debug ( \"Checking that the provided matrix is Hermitian\" ) if np . allclose ( measurements_hermitian_matrix , np . conj ( measurements_hermitian_matrix ). T ): self . c_matrix: np . ndarray = measurements_hermitian_matrix self . c_matrix_eigenvalues: np . ndarray = np . linalg . eigvalsh ( self . c_matrix ) # Numpy gives the eigenvectors in column form, so transpose is needed there! self . c_matrix_eigenvectors: np . ndarray = np . linalg . eigh ( self . c_matrix )[- 1 ]. T self . space_dimension: int = self . c_matrix . shape [ 0 ] else: logger . exception ( \"Instantiating a PhaseReconstructor with a non hermitian matrix is \" \"not possible\" ) raise ValueError ( \"Provided matrix should be Hermitian\" ) @property def alpha ( self ) -> np . float64: \"\"\" This is a factor used to define the new reconstruction matrix. It is taken either as the operator norm of the hermitian noise matrix, or as the max value between 0 and the opposite of the min eigenvalue of c_matrix (chosen for implementation, since our noise is included in the measurements). See page 8 of the paper for reference. Returns: A real scalar value, because c_matrix is Hermitian and the eigenvalues of real symmetric or complex Hermitian matrices are always real (see G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pg. 222.) \"\"\" return np . float64 ( max ( 0 , np . amin ( self . c_matrix_eigenvalues ))) @property def leading_eigenvector ( self ) -> np . ndarray: \"\"\" Returns the leading eigenvector of `self.c_matrix`, which is the eigenvector corresponding to the max eigenvalue (in absolute value). Returns: A `numpy.ndarray` object, corresponding to said eigenvector. \"\"\" return self . c_matrix_eigenvectors [ np . where ( self . c_matrix_eigenvalues == np . amax ( np . absolute ( self . c_matrix_eigenvalues ))) ]. reshape (( 1 , self . space_dimension )) @property def reconstructor_matrix ( self ) -> np . ndarray: \"\"\" This is the reconstructor matrix built from `self.c_matrix` and the `alpha` property. It is the matrix denoted as \\\\widetilde{C} on page 8 of the reference paper. Returns: A `numpy.ndarray`, with same dimension as `self.c_matrix`. \"\"\" return self . c_matrix + self . alpha * np . identity ( self . c_matrix . shape [ 0 ]) def get_eigenvector_estimator ( self , eigenvector: np . ndarray ) -> np . ndarray: \"\"\" Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Args: eigenvector (np.ndarray): a numpy array representing the vector. Returns: A `numpy.ndarray` object of the same dimension as param `eigenvector`. \"\"\" try: return eigenvector / np . absolute ( eigenvector ) except RuntimeWarning: # In case of 0-division, we don't want `inf` values from numpy # Generate a random complex vector with same dimension (since `eigenvector` is of # dimension N * 1, `numpy.ndarray.size` method will give us N). # Remember to initialize a random real and imaginary part. logger . exception ( \"Encountered 0-division, trying normalization\" ) e_vect = np . random . randn ( eigenvector . size ) + 1 j * np . random . randn ( eigenvector . size ) while ( np . absolute ( e_vect @ eigenvector ) == 0 ): # Guarantee that we don't fall back to this edge case. e_vect = np . random . randn ( eigenvector . size ) + 1 j * np . random . randn ( eigenvector . size ) return ( e_vect @ eigenvector / np . absolute ( e_vect @ eigenvector )). reshape ( ( 1 , self . space_dimension ) ) def reconstruct_complex_phases_evm ( self ) -> np . ndarray: \"\"\" Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: The complex form of the result as a 'numpy.ndarray' instance. \"\"\" logger . debug ( \"Getting complex phase results\" ) return self . get_eigenvector_estimator ( self . leading_eigenvector ) @staticmethod def convert_complex_result_to_phase_values ( complex_estimator: np . ndarray , deg: bool = False ) -> np . ndarray: \"\"\" Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A `numpy.ndarray` with the real phase values of the result. \"\"\" logger . debug ( \"Casting complex phases to real values\" ) return np . apply_along_axis ( np . angle , axis = 0 , arr = complex_estimator , deg = deg )","title":"Module pyhdtoolkit.maths.nonconvex_phase_sync"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#phasereconstructor","text":"class PhaseReconstructor ( measurements_hermitian_matrix : numpy . ndarray ) View Source class PhaseReconstructor : \" \"\" Class algorithm to reconstruct your phase values. Make sure to provide vectors as `numpy.ndarray` with shape (1, N), N being the dimension. \"\" \" __slots__ = { \"c_matrix\" : \"Hermitian square matrix from your measurements\" , \"c_matrix_eigenvalues\" : \"Eigenvalues of c_matrix\" , \"c_matrix_eigenvectors\" : \"Eigenvectors of c_matrix\" , \"space_dimension\" : \"Dimension of your measurement space\" , } def __init__ ( self , measurements_hermitian_matrix : np . ndarray ) -> None : \" \"\" Initialize your reconstructor object from measurements. Args: measurements_hermitian_matrix: a `numpy.ndarray` object built from measurements, see module docstring on how to build this matrix. \"\" \" logger . debug ( \"Checking that the provided matrix is Hermitian\" ) if np . allclose ( measurements_hermitian_matrix , np . conj ( measurements_hermitian_matrix ). T ) : self . c_matrix : np . ndarray = measurements_hermitian_matrix self . c_matrix_eigenvalues : np . ndarray = np . linalg . eigvalsh ( self . c_matrix ) # Numpy gives the eigenvectors in column form, so transpose is needed there! self . c_matrix_eigenvectors : np . ndarray = np . linalg . eigh ( self . c_matrix ) [ - 1 ] . T self . space_dimension : int = self . c_matrix . shape [ 0 ] else : logger . exception ( \"Instantiating a PhaseReconstructor with a non hermitian matrix is \" \"not possible\" ) raise ValueError ( \"Provided matrix should be Hermitian\" ) @property def alpha ( self ) -> np . float64 : \" \"\" This is a factor used to define the new reconstruction matrix. It is taken either as the operator norm of the hermitian noise matrix, or as the max value between 0 and the opposite of the min eigenvalue of c_matrix (chosen for implementation, since our noise is included in the measurements). See page 8 of the paper for reference. Returns: A real scalar value, because c_matrix is Hermitian and the eigenvalues of real symmetric or complex Hermitian matrices are always real (see G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pg. 222.) \"\" \" return np . float64 ( max ( 0 , np . amin ( self . c_matrix_eigenvalues ))) @property def leading_eigenvector ( self ) -> np . ndarray : \" \"\" Returns the leading eigenvector of `self.c_matrix`, which is the eigenvector corresponding to the max eigenvalue (in absolute value). Returns: A `numpy.ndarray` object, corresponding to said eigenvector. \"\" \" return self . c_matrix_eigenvectors [ np . where ( self . c_matrix_eigenvalues == np . amax ( np . absolute ( self . c_matrix_eigenvalues ))) ] . reshape (( 1 , self . space_dimension )) @property def reconstructor_matrix ( self ) -> np . ndarray : \" \"\" This is the reconstructor matrix built from `self.c_matrix` and the `alpha` property. It is the matrix denoted as \\\\ widetilde{C} on page 8 of the reference paper. Returns: A `numpy.ndarray`, with same dimension as `self.c_matrix`. \"\" \" return self . c_matrix + self . alpha * np . identity ( self . c_matrix . shape [ 0 ] ) def get_eigenvector_estimator ( self , eigenvector : np . ndarray ) -> np . ndarray : \" \"\" Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Args: eigenvector (np.ndarray): a numpy array representing the vector. Returns: A `numpy.ndarray` object of the same dimension as param `eigenvector`. \"\" \" try : return eigenvector / np . absolute ( eigenvector ) except RuntimeWarning : # In case of 0-division, we don't want `inf` values from numpy # Generate a random complex vector with same dimension (since `eigenvector` is of # dimension N * 1, `numpy.ndarray.size` method will give us N). # Remember to initialize a random real and imaginary part. logger . exception ( \"Encountered 0-division, trying normalization\" ) e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) while ( np . absolute ( e_vect @ eigenvector ) == 0 ) : # Guarantee that we don't fall back to this edge case. e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) return ( e_vect @ eigenvector / np . absolute ( e_vect @ eigenvector )). reshape ( ( 1 , self . space_dimension ) ) def reconstruct_complex_phases_evm ( self ) -> np . ndarray : \" \"\" Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: The complex form of the result as a 'numpy.ndarray' instance. \"\" \" logger . debug ( \"Getting complex phase results\" ) return self . get_eigenvector_estimator ( self . leading_eigenvector ) @staticmethod def convert_complex_result_to_phase_values ( complex_estimator : np . ndarray , deg : bool = False ) -> np . ndarray : \" \"\" Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A `numpy.ndarray` with the real phase values of the result. \"\" \" logger . debug ( \"Casting complex phases to real values\" ) return np . apply_along_axis ( np . angle , axis = 0 , arr = complex_estimator , deg = deg )","title":"PhaseReconstructor"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#convert_complex_result_to_phase_values","text":"def convert_complex_result_to_phase_values ( complex_estimator : numpy . ndarray , deg : bool = False ) -> numpy . ndarray Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A numpy.ndarray with the real phase values of the result. View Source @staticmethod def convert_complex_result_to_phase_values ( complex_estimator : np . ndarray , deg : bool = False ) -> np . ndarray : \"\"\" Casts back the complex form of your result to real phase values. Args: complex_estimator (np.ndarray): your result's complex form as a numpy array. deg (bool): if this is set to True, the result is cast to degrees (from radians) before being returned. Defaults to False. Returns: A `numpy.ndarray` with the real phase values of the result. \"\"\" logger . debug ( \"Casting complex phases to real values\" ) return np . apply_along_axis ( np . angle , axis = 0 , arr = complex_estimator , deg = deg )","title":"convert_complex_result_to_phase_values"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#instance-variables","text":"alpha This is a factor used to define the new reconstruction matrix. It is taken either as the operator norm of the hermitian noise matrix, or as the max value between 0 and the opposite of the min eigenvalue of c_matrix (chosen for implementation, since our noise is included in the measurements). See page 8 of the paper for reference. c_matrix c_matrix_eigenvalues c_matrix_eigenvectors leading_eigenvector Returns the leading eigenvector of self.c_matrix , which is the eigenvector corresponding to the max eigenvalue (in absolute value). reconstructor_matrix This is the reconstructor matrix built from self.c_matrix and the alpha property. It is the matrix denoted as \\widetilde{C} on page 8 of the reference paper. space_dimension","title":"Instance variables"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#methods","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#get_eigenvector_estimator","text":"def get_eigenvector_estimator ( self , eigenvector : numpy . ndarray ) -> numpy . ndarray Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Parameters: Name Type Description Default eigenvector np.ndarray a numpy array representing the vector. None Returns: Type Description None A numpy.ndarray object of the same dimension as param eigenvector . View Source def get_eigenvector_estimator ( self , eigenvector : np . ndarray ) -> np . ndarray : \" \"\" Return the eigenvector estimator of a given eigenvector (id est the component-wise projection of said eigenvector onto \u2102\u02c6{n}_{1}, see reference paper at page 7 for implementation. Args: eigenvector (np.ndarray): a numpy array representing the vector. Returns: A `numpy.ndarray` object of the same dimension as param `eigenvector`. \"\" \" try : return eigenvector / np . absolute ( eigenvector ) except RuntimeWarning : # In case of 0-division, we don't want `inf` values from numpy # Generate a random complex vector with same dimension (since `eigenvector` is of # dimension N * 1, `numpy.ndarray.size` method will give us N). # Remember to initialize a random real and imaginary part. logger . exception ( \"Encountered 0-division, trying normalization\" ) e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) while ( np . absolute ( e_vect @ eigenvector ) == 0 ) : # Guarantee that we don't fall back to this edge case. e_vect = np . random . randn ( eigenvector . size ) + 1j * np . random . randn ( eigenvector . size ) return ( e_vect @ eigenvector / np . absolute ( e_vect @ eigenvector )). reshape ( ( 1 , self . space_dimension ) )","title":"get_eigenvector_estimator"},{"location":"reference/pyhdtoolkit/maths/nonconvex_phase_sync/#reconstruct_complex_phases_evm","text":"def reconstruct_complex_phases_evm ( self ) -> numpy . ndarray Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: Type Description None The complex form of the result as a 'numpy.ndarray' instance. View Source def reconstruct_complex_phases_evm ( self ) -> np . ndarray : \"\"\" Reconstruct simplest estimator fom the eigenvector method. The result is in complex form, and will be radians once cast back to real form. Returns: The complex form of the result as a 'numpy.ndarray' instance. \"\"\" logger . debug ( \"Getting complex phase results\" ) return self . get_eigenvector_estimator ( self . leading_eigenvector )","title":"reconstruct_complex_phases_evm"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/","text":"Module pyhdtoolkit.maths.stats_fitting Module maths.stats_fitting Created on 2020.02.06 View Source \"\"\" Module maths.stats_fitting -------------------------- Created on 2020.02.06 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 module implementing methods to find the best fit of statistical distributions to data. \"\"\" import warnings from typing import Dict , Tuple , Union import matplotlib import numpy as np import pandas as pd import scipy.stats as st from loguru import logger # Distributions to check # DISTRIBUTIONS : Dict [ st . rv_continuous , str ] = { st . chi : \"Chi\" , st . chi2 : \"Chi-Square\" , st . expon : \"Exponential\" , st . laplace : \"Laplace\" , st . lognorm : \"LogNorm\" , st . norm : \"Normal\" , } def set_distributions_dict ( dist_dict : Dict [ st . rv_continuous , str ]) -> None : \"\"\" Sets DISTRIBUTIONS as the provided dict. This is useful to define only the ones you want to try out. Args: dist_dict Dict[st.rv_continuous, str]: dictionnary with the wanted distributions, in the format of DISTRIBUTIONS, aka scipy.stats generator objects as keys, and a string representation of their name as value. Returns: Nothing, but modifies the global DISTRIBUTIONS dict called by other functions. \"\"\" # pylint: disable=global-statement logger . debug ( \"Setting tested distributions\" ) global DISTRIBUTIONS DISTRIBUTIONS = dist_dict def best_fit_distribution ( data : Union [ pd . Series , np . ndarray ], bins : int = 200 , ax : matplotlib . axes . Axes = None ) -> Tuple [ st . rv_continuous , Tuple [ float , ... ]]: \"\"\" Model data by finding best fit candidate distribution among those in DISTRIBUTIONS. Args: data (Union[pd.Series, np.ndarray]): a pandas Series or numpy array with your distribution data. bins (int): the number of bins to decompose your data in before performing fittings. ax (matplotlib.axes.Axes): the matplotlib axis object on which to plot the pdf of tried functions. This should be provided as the ax on which you plotted your distribution. Returns: A tuple containing the scipy.stats generator corresponding to the best fit candidate, and the parameters to give said generator to get this fit. \"\"\" # pylint: disable=too-many-locals logger . debug ( f \"Getting histogram of original data, in { bins } bins\" ) y , x = np . histogram ( data , bins = bins , density = True ) x = ( x + np . roll ( x , - 1 ))[: - 1 ] / 2.0 logger . debug ( \"Creating initial guess\" ) best_distribution = st . norm best_params = ( 0.0 , 1.0 ) best_sse = np . inf # Estimate distribution parameters from data for distribution , distname in DISTRIBUTIONS . items (): try : with warnings . catch_warnings (): # Ignore warnings from data that can't be fit warnings . filterwarnings ( \"ignore\" ) logger . debug ( f \"Trying to fit distribution ' { distname } '\" ) params = distribution . fit ( data ) * args , loc , scale = params logger . debug ( f \"Calculating PDF goodness of fit and error for distribution ' { distname } '\" ) pdf = distribution . pdf ( x , loc = loc , scale = scale , * args ) sse = np . sum ( np . power ( y - pdf , 2.0 )) try : if ax : logger . debug ( f \"Plotting fitted PDF for distribution ' { distname } '\" ) pd . Series ( pdf , x ) . plot ( ax = ax , label = f \" { distname } fit\" , alpha = 1 , lw = 2 ) except Exception : logger . exception ( f \"Plotting distribution ' { distname } ' failed\" ) logger . debug ( f \"Identifying if distribution ' { distname } ' is a better fit than previous tries\" ) if best_sse > sse > 0 : best_distribution = distribution best_params = params best_sse = sse except Exception : logger . exception ( f \"Trying to fit distribution ' { distname } ' failed and aborted\" ) logger . info ( f \"Found a best fit: ' { DISTRIBUTIONS [ best_distribution ] } ' distribution\" ) return best_distribution , best_params def make_pdf ( distribution : st . rv_continuous , params : Tuple [ float , ... ], size : int = 25_000 ) -> pd . Series : \"\"\" Generate a pandas Series for the distributions's Probability Distribution Function. This Series will have axis values as index, and PDF values as values. Args: distribution (st.rv_continuous): a scipy.stats generator object params (Tuple[float, ...]): the parameters for this generator given back by the fit. size (int): the number of points to evaluate. Returns: A pandas Series object with the PDF as values, corresponding axis values as index. \"\"\" # Separate parts of parameters * args , loc , scale = params logger . debug ( \"Getting sane start and end points of distribution\" ) start = ( distribution . ppf ( 0.01 , * args , loc = loc , scale = scale ) if args else distribution . ppf ( 0.01 , loc = loc , scale = scale ) ) end = ( distribution . ppf ( 0.99 , * args , loc = loc , scale = scale ) if args else distribution . ppf ( 0.99 , loc = loc , scale = scale ) ) logger . debug ( \"Building PDF\" ) x = np . linspace ( start , end , size ) y = distribution . pdf ( x , loc = loc , scale = scale , * args ) return pd . Series ( y , x ) Variables DISTRIBUTIONS Functions best_fit_distribution def best_fit_distribution ( data : Union [ pandas . core . series . Series , numpy . ndarray ], bins : int = 200 , ax : matplotlib . axes . _axes . Axes = None ) -> Tuple [ scipy . stats . _distn_infrastructure . rv_continuous , Tuple [ float , ... ]] Model data by finding best fit candidate distribution among those in DISTRIBUTIONS. Parameters: Name Type Description Default data Union[pd.Series, np.ndarray] a pandas Series or numpy array with your distribution data. None bins int the number of bins to decompose your data in before performing fittings. None ax matplotlib.axes.Axes the matplotlib axis object on which to plot the pdf of tried functions. This should be provided as the ax on which you plotted your distribution. None Returns: Type Description None A tuple containing the scipy.stats generator corresponding to the best fit candidate, and the parameters to give said generator to get this fit. View Source def best_fit_distribution ( data : Union [ pd . Series , np . ndarray ], bins : int = 200 , ax : matplotlib . axes . Axes = None ) -> Tuple [ st . rv_continuous , Tuple [ float , ...]] : \"\"\" Model data by finding best fit candidate distribution among those in DISTRIBUTIONS. Args: data (Union[pd.Series, np.ndarray]): a pandas Series or numpy array with your distribution data. bins (int): the number of bins to decompose your data in before performing fittings. ax (matplotlib.axes.Axes): the matplotlib axis object on which to plot the pdf of tried functions. This should be provided as the ax on which you plotted your distribution. Returns: A tuple containing the scipy.stats generator corresponding to the best fit candidate, and the parameters to give said generator to get this fit. \"\"\" # pylint : disable = too - many - locals logger . debug ( f \"Getting histogram of original data, in {bins} bins\" ) y , x = np . histogram ( data , bins = bins , density = True ) x = ( x + np . roll ( x , - 1 ))[:- 1 ] / 2.0 logger . debug ( \"Creating initial guess\" ) best_distribution = st . norm best_params = ( 0.0 , 1.0 ) best_sse = np . inf # Estimate distribution parameters from data for distribution , distname in DISTRIBUTIONS . items () : try : with warnings . catch_warnings () : # Ignore warnings from data that can't be fit warnings.filterwarnings(\"ignore\") logger.debug(f\"Trying to fit distribution ' { distname } '\") params = distribution.fit(data) *args, loc, scale = params logger.debug(f\"Calculating PDF goodness of fit and error for distribution ' { distname } '\") pdf = distribution.pdf(x, loc=loc, scale=scale, *args) sse = np.sum(np.power(y - pdf, 2.0)) try: if ax: logger.debug(f\"Plotting fitted PDF for distribution ' { distname } '\") pd.Series(pdf, x).plot(ax=ax, label=f\"{distname} fit\", alpha=1, lw=2) except Exception: logger.exception(f\"Plotting distribution ' { distname } ' failed\") logger.debug(f\"Identifying if distribution ' { distname } ' is a better fit than previous tries\") if best_sse > sse > 0: best_distribution = distribution best_params = params best_sse = sse except Exception: logger.exception(f\"Trying to fit distribution ' { distname } ' failed and aborted\") logger.info(f\"Found a best fit: ' { DISTRIBUTIONS [ best_distribution ]} ' distribution \" ) return best_distribution , best_params make_pdf def make_pdf ( distribution : scipy . stats . _distn_infrastructure . rv_continuous , params : Tuple [ float , ... ], size : int = 25000 ) -> pandas . core . series . Series Generate a pandas Series for the distributions's Probability Distribution Function. This Series will have axis values as index, and PDF values as values. Parameters: Name Type Description Default distribution st.rv_continuous a scipy.stats generator object None params Tuple[float, ...] the parameters for this generator given back by the fit. None size int the number of points to evaluate. None Returns: Type Description None A pandas Series object with the PDF as values, corresponding axis values as index. View Source def make_pdf ( distribution : st . rv_continuous , params : Tuple [ float , ...], size : int = 25 _000 ) -> pd . Series : \"\"\" Generate a pandas Series for the distributions's Probability Distribution Function. This Series will have axis values as index, and PDF values as values. Args: distribution (st.rv_continuous): a scipy.stats generator object params (Tuple[float, ...]): the parameters for this generator given back by the fit. size (int): the number of points to evaluate. Returns: A pandas Series object with the PDF as values, corresponding axis values as index. \"\"\" # Separate parts of parameters * args , loc , scale = params logger . debug ( \"Getting sane start and end points of distribution\" ) start = ( distribution . ppf ( 0 . 01 , * args , loc = loc , scale = scale ) if args else distribution . ppf ( 0 . 01 , loc = loc , scale = scale ) ) end = ( distribution . ppf ( 0 . 99 , * args , loc = loc , scale = scale ) if args else distribution . ppf ( 0 . 99 , loc = loc , scale = scale ) ) logger . debug ( \"Building PDF\" ) x = np . linspace ( start , end , size ) y = distribution . pdf ( x , loc = loc , scale = scale , * args ) return pd . Series ( y , x ) set_distributions_dict def set_distributions_dict ( dist_dict : Dict [ scipy . stats . _distn_infrastructure . rv_continuous , str ] ) -> None Sets DISTRIBUTIONS as the provided dict. This is useful to define only the ones you want to try out. Parameters: Name Type Description Default dist_dict Dict[st.rv_continuous, str] None dictionnary with the wanted distributions, in the format of DISTRIBUTIONS, aka scipy.stats generator objects as keys, and a string representation of their name as value. None Returns: Type Description None Nothing, but modifies the global DISTRIBUTIONS dict called by other functions. View Source def set_distributions_dict ( dist_dict : Dict [ st . rv_continuous , str ]) -> None : \"\"\" Sets DISTRIBUTIONS as the provided dict. This is useful to define only the ones you want to try out. Args: dist_dict Dict[st.rv_continuous, str]: dictionnary with the wanted distributions, in the format of DISTRIBUTIONS, aka scipy.stats generator objects as keys, and a string representation of their name as value. Returns: Nothing, but modifies the global DISTRIBUTIONS dict called by other functions. \"\"\" # pylint : disable = global - statement logger . debug ( \"Setting tested distributions\" ) global DISTRIBUTIONS DISTRIBUTIONS = dist_dict","title":"Stats Fitting"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/#module-pyhdtoolkitmathsstats_fitting","text":"Module maths.stats_fitting Created on 2020.02.06 View Source \"\"\" Module maths.stats_fitting -------------------------- Created on 2020.02.06 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 module implementing methods to find the best fit of statistical distributions to data. \"\"\" import warnings from typing import Dict , Tuple , Union import matplotlib import numpy as np import pandas as pd import scipy.stats as st from loguru import logger # Distributions to check # DISTRIBUTIONS : Dict [ st . rv_continuous , str ] = { st . chi : \"Chi\" , st . chi2 : \"Chi-Square\" , st . expon : \"Exponential\" , st . laplace : \"Laplace\" , st . lognorm : \"LogNorm\" , st . norm : \"Normal\" , } def set_distributions_dict ( dist_dict : Dict [ st . rv_continuous , str ]) -> None : \"\"\" Sets DISTRIBUTIONS as the provided dict. This is useful to define only the ones you want to try out. Args: dist_dict Dict[st.rv_continuous, str]: dictionnary with the wanted distributions, in the format of DISTRIBUTIONS, aka scipy.stats generator objects as keys, and a string representation of their name as value. Returns: Nothing, but modifies the global DISTRIBUTIONS dict called by other functions. \"\"\" # pylint: disable=global-statement logger . debug ( \"Setting tested distributions\" ) global DISTRIBUTIONS DISTRIBUTIONS = dist_dict def best_fit_distribution ( data : Union [ pd . Series , np . ndarray ], bins : int = 200 , ax : matplotlib . axes . Axes = None ) -> Tuple [ st . rv_continuous , Tuple [ float , ... ]]: \"\"\" Model data by finding best fit candidate distribution among those in DISTRIBUTIONS. Args: data (Union[pd.Series, np.ndarray]): a pandas Series or numpy array with your distribution data. bins (int): the number of bins to decompose your data in before performing fittings. ax (matplotlib.axes.Axes): the matplotlib axis object on which to plot the pdf of tried functions. This should be provided as the ax on which you plotted your distribution. Returns: A tuple containing the scipy.stats generator corresponding to the best fit candidate, and the parameters to give said generator to get this fit. \"\"\" # pylint: disable=too-many-locals logger . debug ( f \"Getting histogram of original data, in { bins } bins\" ) y , x = np . histogram ( data , bins = bins , density = True ) x = ( x + np . roll ( x , - 1 ))[: - 1 ] / 2.0 logger . debug ( \"Creating initial guess\" ) best_distribution = st . norm best_params = ( 0.0 , 1.0 ) best_sse = np . inf # Estimate distribution parameters from data for distribution , distname in DISTRIBUTIONS . items (): try : with warnings . catch_warnings (): # Ignore warnings from data that can't be fit warnings . filterwarnings ( \"ignore\" ) logger . debug ( f \"Trying to fit distribution ' { distname } '\" ) params = distribution . fit ( data ) * args , loc , scale = params logger . debug ( f \"Calculating PDF goodness of fit and error for distribution ' { distname } '\" ) pdf = distribution . pdf ( x , loc = loc , scale = scale , * args ) sse = np . sum ( np . power ( y - pdf , 2.0 )) try : if ax : logger . debug ( f \"Plotting fitted PDF for distribution ' { distname } '\" ) pd . Series ( pdf , x ) . plot ( ax = ax , label = f \" { distname } fit\" , alpha = 1 , lw = 2 ) except Exception : logger . exception ( f \"Plotting distribution ' { distname } ' failed\" ) logger . debug ( f \"Identifying if distribution ' { distname } ' is a better fit than previous tries\" ) if best_sse > sse > 0 : best_distribution = distribution best_params = params best_sse = sse except Exception : logger . exception ( f \"Trying to fit distribution ' { distname } ' failed and aborted\" ) logger . info ( f \"Found a best fit: ' { DISTRIBUTIONS [ best_distribution ] } ' distribution\" ) return best_distribution , best_params def make_pdf ( distribution : st . rv_continuous , params : Tuple [ float , ... ], size : int = 25_000 ) -> pd . Series : \"\"\" Generate a pandas Series for the distributions's Probability Distribution Function. This Series will have axis values as index, and PDF values as values. Args: distribution (st.rv_continuous): a scipy.stats generator object params (Tuple[float, ...]): the parameters for this generator given back by the fit. size (int): the number of points to evaluate. Returns: A pandas Series object with the PDF as values, corresponding axis values as index. \"\"\" # Separate parts of parameters * args , loc , scale = params logger . debug ( \"Getting sane start and end points of distribution\" ) start = ( distribution . ppf ( 0.01 , * args , loc = loc , scale = scale ) if args else distribution . ppf ( 0.01 , loc = loc , scale = scale ) ) end = ( distribution . ppf ( 0.99 , * args , loc = loc , scale = scale ) if args else distribution . ppf ( 0.99 , loc = loc , scale = scale ) ) logger . debug ( \"Building PDF\" ) x = np . linspace ( start , end , size ) y = distribution . pdf ( x , loc = loc , scale = scale , * args ) return pd . Series ( y , x )","title":"Module pyhdtoolkit.maths.stats_fitting"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/#variables","text":"DISTRIBUTIONS","title":"Variables"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/#best_fit_distribution","text":"def best_fit_distribution ( data : Union [ pandas . core . series . Series , numpy . ndarray ], bins : int = 200 , ax : matplotlib . axes . _axes . Axes = None ) -> Tuple [ scipy . stats . _distn_infrastructure . rv_continuous , Tuple [ float , ... ]] Model data by finding best fit candidate distribution among those in DISTRIBUTIONS. Parameters: Name Type Description Default data Union[pd.Series, np.ndarray] a pandas Series or numpy array with your distribution data. None bins int the number of bins to decompose your data in before performing fittings. None ax matplotlib.axes.Axes the matplotlib axis object on which to plot the pdf of tried functions. This should be provided as the ax on which you plotted your distribution. None Returns: Type Description None A tuple containing the scipy.stats generator corresponding to the best fit candidate, and the parameters to give said generator to get this fit. View Source def best_fit_distribution ( data : Union [ pd . Series , np . ndarray ], bins : int = 200 , ax : matplotlib . axes . Axes = None ) -> Tuple [ st . rv_continuous , Tuple [ float , ...]] : \"\"\" Model data by finding best fit candidate distribution among those in DISTRIBUTIONS. Args: data (Union[pd.Series, np.ndarray]): a pandas Series or numpy array with your distribution data. bins (int): the number of bins to decompose your data in before performing fittings. ax (matplotlib.axes.Axes): the matplotlib axis object on which to plot the pdf of tried functions. This should be provided as the ax on which you plotted your distribution. Returns: A tuple containing the scipy.stats generator corresponding to the best fit candidate, and the parameters to give said generator to get this fit. \"\"\" # pylint : disable = too - many - locals logger . debug ( f \"Getting histogram of original data, in {bins} bins\" ) y , x = np . histogram ( data , bins = bins , density = True ) x = ( x + np . roll ( x , - 1 ))[:- 1 ] / 2.0 logger . debug ( \"Creating initial guess\" ) best_distribution = st . norm best_params = ( 0.0 , 1.0 ) best_sse = np . inf # Estimate distribution parameters from data for distribution , distname in DISTRIBUTIONS . items () : try : with warnings . catch_warnings () : # Ignore warnings from data that can't be fit warnings.filterwarnings(\"ignore\") logger.debug(f\"Trying to fit distribution ' { distname } '\") params = distribution.fit(data) *args, loc, scale = params logger.debug(f\"Calculating PDF goodness of fit and error for distribution ' { distname } '\") pdf = distribution.pdf(x, loc=loc, scale=scale, *args) sse = np.sum(np.power(y - pdf, 2.0)) try: if ax: logger.debug(f\"Plotting fitted PDF for distribution ' { distname } '\") pd.Series(pdf, x).plot(ax=ax, label=f\"{distname} fit\", alpha=1, lw=2) except Exception: logger.exception(f\"Plotting distribution ' { distname } ' failed\") logger.debug(f\"Identifying if distribution ' { distname } ' is a better fit than previous tries\") if best_sse > sse > 0: best_distribution = distribution best_params = params best_sse = sse except Exception: logger.exception(f\"Trying to fit distribution ' { distname } ' failed and aborted\") logger.info(f\"Found a best fit: ' { DISTRIBUTIONS [ best_distribution ]} ' distribution \" ) return best_distribution , best_params","title":"best_fit_distribution"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/#make_pdf","text":"def make_pdf ( distribution : scipy . stats . _distn_infrastructure . rv_continuous , params : Tuple [ float , ... ], size : int = 25000 ) -> pandas . core . series . Series Generate a pandas Series for the distributions's Probability Distribution Function. This Series will have axis values as index, and PDF values as values. Parameters: Name Type Description Default distribution st.rv_continuous a scipy.stats generator object None params Tuple[float, ...] the parameters for this generator given back by the fit. None size int the number of points to evaluate. None Returns: Type Description None A pandas Series object with the PDF as values, corresponding axis values as index. View Source def make_pdf ( distribution : st . rv_continuous , params : Tuple [ float , ...], size : int = 25 _000 ) -> pd . Series : \"\"\" Generate a pandas Series for the distributions's Probability Distribution Function. This Series will have axis values as index, and PDF values as values. Args: distribution (st.rv_continuous): a scipy.stats generator object params (Tuple[float, ...]): the parameters for this generator given back by the fit. size (int): the number of points to evaluate. Returns: A pandas Series object with the PDF as values, corresponding axis values as index. \"\"\" # Separate parts of parameters * args , loc , scale = params logger . debug ( \"Getting sane start and end points of distribution\" ) start = ( distribution . ppf ( 0 . 01 , * args , loc = loc , scale = scale ) if args else distribution . ppf ( 0 . 01 , loc = loc , scale = scale ) ) end = ( distribution . ppf ( 0 . 99 , * args , loc = loc , scale = scale ) if args else distribution . ppf ( 0 . 99 , loc = loc , scale = scale ) ) logger . debug ( \"Building PDF\" ) x = np . linspace ( start , end , size ) y = distribution . pdf ( x , loc = loc , scale = scale , * args ) return pd . Series ( y , x )","title":"make_pdf"},{"location":"reference/pyhdtoolkit/maths/stats_fitting/#set_distributions_dict","text":"def set_distributions_dict ( dist_dict : Dict [ scipy . stats . _distn_infrastructure . rv_continuous , str ] ) -> None Sets DISTRIBUTIONS as the provided dict. This is useful to define only the ones you want to try out. Parameters: Name Type Description Default dist_dict Dict[st.rv_continuous, str] None dictionnary with the wanted distributions, in the format of DISTRIBUTIONS, aka scipy.stats generator objects as keys, and a string representation of their name as value. None Returns: Type Description None Nothing, but modifies the global DISTRIBUTIONS dict called by other functions. View Source def set_distributions_dict ( dist_dict : Dict [ st . rv_continuous , str ]) -> None : \"\"\" Sets DISTRIBUTIONS as the provided dict. This is useful to define only the ones you want to try out. Args: dist_dict Dict[st.rv_continuous, str]: dictionnary with the wanted distributions, in the format of DISTRIBUTIONS, aka scipy.stats generator objects as keys, and a string representation of their name as value. Returns: Nothing, but modifies the global DISTRIBUTIONS dict called by other functions. \"\"\" # pylint : disable = global - statement logger . debug ( \"Setting tested distributions\" ) global DISTRIBUTIONS DISTRIBUTIONS = dist_dict","title":"set_distributions_dict"},{"location":"reference/pyhdtoolkit/optics/","text":"Module pyhdtoolkit.optics optics package ~ ~ ~ ~ ~~ These are miscellaneous utilities to perform optics calculation from simulation outputs. View Source \"\"\" optics package ~~~~~~~~~~~~~~ These are miscellaneous utilities to perform optics calculation from simulation outputs. :copyright: (c) 2020 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .beam import Beam Sub-modules pyhdtoolkit.optics.beam pyhdtoolkit.optics.twiss","title":"Index"},{"location":"reference/pyhdtoolkit/optics/#module-pyhdtoolkitoptics","text":"optics package ~ ~ ~ ~ ~~ These are miscellaneous utilities to perform optics calculation from simulation outputs. View Source \"\"\" optics package ~~~~~~~~~~~~~~ These are miscellaneous utilities to perform optics calculation from simulation outputs. :copyright: (c) 2020 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .beam import Beam","title":"Module pyhdtoolkit.optics"},{"location":"reference/pyhdtoolkit/optics/#sub-modules","text":"pyhdtoolkit.optics.beam pyhdtoolkit.optics.twiss","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/optics/beam/","text":"Module pyhdtoolkit.optics.beam Module optics.beam Created on 2020.11.11 View Source \"\"\" Module optics.beam ------------------ Created on 2020.11.11 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 module implementing various functionality for simple beam parameter calculations. \"\"\" import numpy as np from scipy import constants class Beam : \"\"\" Class to encompass functionality. \"\"\" def __init__ ( self , energy : float , emittance : float , m0 : float = constants . physical_constants [ \"proton mass energy equivalent in MeV\" ][ 0 ], ) -> None : \"\"\" Args: energy (float): energy of the particles in your beam, in [GeV]. emittance (float): beam emittance, in [m]. m0 (float): rest mass of the beam's particles in MeV. Defaults to that of a proton. \"\"\" self . energy = energy self . emittance = emittance self . rest_mass = m0 @property def gamma_rel ( self ) -> float : \"\"\"Relativistic gamma.\"\"\" return ( 1e3 * self . energy + self . rest_mass ) / self . rest_mass @property def beta_rel ( self ) -> float : \"\"\"Relativistic beta.\"\"\" return np . sqrt ( 1 + 1 / ( self . gamma_rel ** 2 )) @property def brho ( self ) -> float : \"\"\"Beam rigidity [T/m].\"\"\" return ( 1 / 0.3 ) * self . beta_rel * self . energy / constants . c @property def normalized_emittance ( self ) -> float : \"\"\" Normalized emittance [m]. \"\"\" return self . emittance * self . beta_rel * self . gamma_rel @property def rms_emittance ( self ) -> float : \"\"\" Rms emittance [m]. \"\"\" return self . emittance / ( self . beta_rel * self . gamma_rel ) def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = constants . c ) -> float : \"\"\" Revolution frequency. Args: circumference (float): the machine circumference in [m]. Defaults to that of the LHC. speed (float): the particles' speed in the machine, in [m/s]. Defaults to c. \"\"\" return self . beta_rel * speed / circumference def eta ( self , alpha_p : float ) -> float : \"\"\" Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Args: alpha_p (float): momentum compaction factor. \"\"\" return ( 1 / ( self . gamma_rel ** 2 )) - alpha_p @staticmethod def gamma_transition ( alpha_p : float ) -> float : \"\"\" Relativistic gamma corresponding to the transition energy. Args: alpha_p (float): momentum compaction factor. \"\"\" return np . sqrt ( 1 / alpha_p ) Classes Beam class Beam ( energy : float , emittance : float , m0 : float = 938.27208816 ) View Source class Beam : \"\"\" Class to encompass functionality. \"\"\" def __init__ ( self , energy : float , emittance : float , m0 : float = constants . physical_constants [ \"proton mass energy equivalent in MeV\" ][ 0 ] , ) -> None : \"\"\" Args: energy (float): energy of the particles in your beam, in [GeV]. emittance (float): beam emittance, in [m]. m0 (float): rest mass of the beam's particles in MeV. Defaults to that of a proton. \"\"\" self . energy = energy self . emittance = emittance self . rest_mass = m0 @property def gamma_rel ( self ) -> float : \"\"\"Relativistic gamma.\"\"\" return ( 1e3 * self . energy + self . rest_mass ) / self . rest_mass @property def beta_rel ( self ) -> float : \"\"\"Relativistic beta.\"\"\" return np . sqrt ( 1 + 1 / ( self . gamma_rel ** 2 )) @property def brho ( self ) -> float : \"\"\"Beam rigidity [T/m].\"\"\" return ( 1 / 0.3 ) * self . beta_rel * self . energy / constants . c @property def normalized_emittance ( self ) -> float : \"\"\" Normalized emittance [m]. \"\"\" return self . emittance * self . beta_rel * self . gamma_rel @property def rms_emittance ( self ) -> float : \"\"\" Rms emittance [m]. \"\"\" return self . emittance / ( self . beta_rel * self . gamma_rel ) def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = constants . c ) -> float : \"\"\" Revolution frequency. Args: circumference (float): the machine circumference in [m]. Defaults to that of the LHC. speed (float): the particles' speed in the machine, in [m/s]. Defaults to c. \"\"\" return self . beta_rel * speed / circumference def eta ( self , alpha_p : float ) -> float : \"\"\" Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Args: alpha_p (float): momentum compaction factor. \"\"\" return ( 1 / ( self . gamma_rel ** 2 )) - alpha_p @staticmethod def gamma_transition ( alpha_p : float ) -> float : \"\"\" Relativistic gamma corresponding to the transition energy. Args: alpha_p (float): momentum compaction factor. \"\"\" return np . sqrt ( 1 / alpha_p ) Static methods gamma_transition def gamma_transition ( alpha_p : float ) -> float Relativistic gamma corresponding to the transition energy. Parameters: Name Type Description Default alpha_p float momentum compaction factor. None View Source @staticmethod def gamma_transition ( alpha_p : float ) -> float : \"\"\" Relativistic gamma corresponding to the transition energy. Args: alpha_p (float): momentum compaction factor. \"\"\" return np . sqrt ( 1 / alpha_p ) Instance variables beta_rel Relativistic beta. brho Beam rigidity [T/m]. gamma_rel Relativistic gamma. normalized_emittance Normalized emittance [m]. rms_emittance Rms emittance [m]. Methods eta def eta ( self , alpha_p : float ) -> float Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Parameters: Name Type Description Default alpha_p float momentum compaction factor. None View Source def eta ( self , alpha_p : float ) -> float : \"\"\" Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Args: alpha_p (float): momentum compaction factor. \"\"\" return ( 1 / ( self . gamma_rel ** 2 )) - alpha_p revolution_frequency def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = 299792458.0 ) -> float Revolution frequency. Parameters: Name Type Description Default circumference float the machine circumference in [m]. Defaults to that of the LHC. that of the LHC speed float the particles' speed in the machine, in [m/s]. Defaults to c. c View Source def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = constants . c ) -> float : \"\"\" Revolution frequency. Args: circumference (float): the machine circumference in [m]. Defaults to that of the LHC. speed (float): the particles' speed in the machine, in [m/s]. Defaults to c. \"\"\" return self . beta_rel * speed / circumference","title":"Beam"},{"location":"reference/pyhdtoolkit/optics/beam/#module-pyhdtoolkitopticsbeam","text":"Module optics.beam Created on 2020.11.11 View Source \"\"\" Module optics.beam ------------------ Created on 2020.11.11 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 module implementing various functionality for simple beam parameter calculations. \"\"\" import numpy as np from scipy import constants class Beam : \"\"\" Class to encompass functionality. \"\"\" def __init__ ( self , energy : float , emittance : float , m0 : float = constants . physical_constants [ \"proton mass energy equivalent in MeV\" ][ 0 ], ) -> None : \"\"\" Args: energy (float): energy of the particles in your beam, in [GeV]. emittance (float): beam emittance, in [m]. m0 (float): rest mass of the beam's particles in MeV. Defaults to that of a proton. \"\"\" self . energy = energy self . emittance = emittance self . rest_mass = m0 @property def gamma_rel ( self ) -> float : \"\"\"Relativistic gamma.\"\"\" return ( 1e3 * self . energy + self . rest_mass ) / self . rest_mass @property def beta_rel ( self ) -> float : \"\"\"Relativistic beta.\"\"\" return np . sqrt ( 1 + 1 / ( self . gamma_rel ** 2 )) @property def brho ( self ) -> float : \"\"\"Beam rigidity [T/m].\"\"\" return ( 1 / 0.3 ) * self . beta_rel * self . energy / constants . c @property def normalized_emittance ( self ) -> float : \"\"\" Normalized emittance [m]. \"\"\" return self . emittance * self . beta_rel * self . gamma_rel @property def rms_emittance ( self ) -> float : \"\"\" Rms emittance [m]. \"\"\" return self . emittance / ( self . beta_rel * self . gamma_rel ) def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = constants . c ) -> float : \"\"\" Revolution frequency. Args: circumference (float): the machine circumference in [m]. Defaults to that of the LHC. speed (float): the particles' speed in the machine, in [m/s]. Defaults to c. \"\"\" return self . beta_rel * speed / circumference def eta ( self , alpha_p : float ) -> float : \"\"\" Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Args: alpha_p (float): momentum compaction factor. \"\"\" return ( 1 / ( self . gamma_rel ** 2 )) - alpha_p @staticmethod def gamma_transition ( alpha_p : float ) -> float : \"\"\" Relativistic gamma corresponding to the transition energy. Args: alpha_p (float): momentum compaction factor. \"\"\" return np . sqrt ( 1 / alpha_p )","title":"Module pyhdtoolkit.optics.beam"},{"location":"reference/pyhdtoolkit/optics/beam/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/optics/beam/#beam","text":"class Beam ( energy : float , emittance : float , m0 : float = 938.27208816 ) View Source class Beam : \"\"\" Class to encompass functionality. \"\"\" def __init__ ( self , energy : float , emittance : float , m0 : float = constants . physical_constants [ \"proton mass energy equivalent in MeV\" ][ 0 ] , ) -> None : \"\"\" Args: energy (float): energy of the particles in your beam, in [GeV]. emittance (float): beam emittance, in [m]. m0 (float): rest mass of the beam's particles in MeV. Defaults to that of a proton. \"\"\" self . energy = energy self . emittance = emittance self . rest_mass = m0 @property def gamma_rel ( self ) -> float : \"\"\"Relativistic gamma.\"\"\" return ( 1e3 * self . energy + self . rest_mass ) / self . rest_mass @property def beta_rel ( self ) -> float : \"\"\"Relativistic beta.\"\"\" return np . sqrt ( 1 + 1 / ( self . gamma_rel ** 2 )) @property def brho ( self ) -> float : \"\"\"Beam rigidity [T/m].\"\"\" return ( 1 / 0.3 ) * self . beta_rel * self . energy / constants . c @property def normalized_emittance ( self ) -> float : \"\"\" Normalized emittance [m]. \"\"\" return self . emittance * self . beta_rel * self . gamma_rel @property def rms_emittance ( self ) -> float : \"\"\" Rms emittance [m]. \"\"\" return self . emittance / ( self . beta_rel * self . gamma_rel ) def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = constants . c ) -> float : \"\"\" Revolution frequency. Args: circumference (float): the machine circumference in [m]. Defaults to that of the LHC. speed (float): the particles' speed in the machine, in [m/s]. Defaults to c. \"\"\" return self . beta_rel * speed / circumference def eta ( self , alpha_p : float ) -> float : \"\"\" Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Args: alpha_p (float): momentum compaction factor. \"\"\" return ( 1 / ( self . gamma_rel ** 2 )) - alpha_p @staticmethod def gamma_transition ( alpha_p : float ) -> float : \"\"\" Relativistic gamma corresponding to the transition energy. Args: alpha_p (float): momentum compaction factor. \"\"\" return np . sqrt ( 1 / alpha_p )","title":"Beam"},{"location":"reference/pyhdtoolkit/optics/beam/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/optics/beam/#gamma_transition","text":"def gamma_transition ( alpha_p : float ) -> float Relativistic gamma corresponding to the transition energy. Parameters: Name Type Description Default alpha_p float momentum compaction factor. None View Source @staticmethod def gamma_transition ( alpha_p : float ) -> float : \"\"\" Relativistic gamma corresponding to the transition energy. Args: alpha_p (float): momentum compaction factor. \"\"\" return np . sqrt ( 1 / alpha_p )","title":"gamma_transition"},{"location":"reference/pyhdtoolkit/optics/beam/#instance-variables","text":"beta_rel Relativistic beta. brho Beam rigidity [T/m]. gamma_rel Relativistic gamma. normalized_emittance Normalized emittance [m]. rms_emittance Rms emittance [m].","title":"Instance variables"},{"location":"reference/pyhdtoolkit/optics/beam/#methods","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/optics/beam/#eta","text":"def eta ( self , alpha_p : float ) -> float Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Parameters: Name Type Description Default alpha_p float momentum compaction factor. None View Source def eta ( self , alpha_p : float ) -> float : \"\"\" Slip factor parameter eta: eta = 0 at transition energy (eta < 0 above transition). Args: alpha_p (float): momentum compaction factor. \"\"\" return ( 1 / ( self . gamma_rel ** 2 )) - alpha_p","title":"eta"},{"location":"reference/pyhdtoolkit/optics/beam/#revolution_frequency","text":"def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = 299792458.0 ) -> float Revolution frequency. Parameters: Name Type Description Default circumference float the machine circumference in [m]. Defaults to that of the LHC. that of the LHC speed float the particles' speed in the machine, in [m/s]. Defaults to c. c View Source def revolution_frequency ( self , circumference : float = 26658.8832 , speed : float = constants . c ) -> float : \"\"\" Revolution frequency. Args: circumference (float): the machine circumference in [m]. Defaults to that of the LHC. speed (float): the particles' speed in the machine, in [m/s]. Defaults to c. \"\"\" return self . beta_rel * speed / circumference","title":"revolution_frequency"},{"location":"reference/pyhdtoolkit/optics/twiss/","text":"Module pyhdtoolkit.optics.twiss Module optics.twiss Created on 2020.09.07 View Source \"\"\" Module optics.twiss ------------------- Created on 2020.09.07 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 module implementing various functionality for optics calculations from / to twiss parameters. \"\"\" import numba import numpy as np @numba . njit () def courant_snyder_transform ( u_vector : np . ndarray , alpha : float , beta : float ) -> np . ndarray : \"\"\" Perform the Courant-Snyder transform on rergular (nonchaotic) phase-space coordinatess. Specifically, if considering the horizontal plane and noting U = (x, px) the phase-space vector, it returns U_bar = (x_bar, px_bar) according to the transform: U_bar = P * U, where P = [1/sqrt(beta_x) 0 ] [alpha_x/sqrt(beta_x) sqrt(beta_x)] Args: u_vector (np.ndarray): two-dimentional array of phase-space (spatial and momenta) coordinates, either horizontal or vertical. alpha (float): alpha twiss parameter in the appropriate plane. beta (float): beta twiss parameter in the appropriate plane. Returns: The normal phase-space coordinates from the Courant-Snyder transform. \"\"\" p_matrix = np . array ([[ 1 / np . sqrt ( beta ), 0 ], [ alpha / np . sqrt ( beta ), np . sqrt ( beta )]]) return p_matrix @ u_vector Functions courant_snyder_transform def courant_snyder_transform ( u_vector : numpy . ndarray , alpha : float , beta : float ) -> numpy . ndarray Perform the Courant-Snyder transform on rergular (nonchaotic) phase-space coordinatess. Specifically, if considering the horizontal plane and noting U = (x, px) the phase-space vector, it returns U_bar = (x_bar, px_bar) according to the transform: U_bar = P * U, where P = [1/sqrt(beta_x) 0 ] [alpha_x/sqrt(beta_x) sqrt(beta_x)] Parameters: Name Type Description Default u_vector np.ndarray two-dimentional array of phase-space (spatial and momenta) coordinates, either horizontal or vertical. None alpha float alpha twiss parameter in the appropriate plane. None beta float beta twiss parameter in the appropriate plane. None Returns: Type Description None The normal phase-space coordinates from the Courant-Snyder transform. View Source @numba . njit () def courant_snyder_transform ( u_vector : np . ndarray , alpha : float , beta : float ) -> np . ndarray : \"\"\" Perform the Courant-Snyder transform on rergular (nonchaotic) phase-space coordinatess. Specifically, if considering the horizontal plane and noting U = (x, px) the phase-space vector, it returns U_bar = (x_bar, px_bar) according to the transform: U_bar = P * U, where P = [1/sqrt(beta_x) 0 ] [alpha_x/sqrt(beta_x) sqrt(beta_x)] Args: u_vector (np.ndarray): two-dimentional array of phase-space (spatial and momenta) coordinates, either horizontal or vertical. alpha (float): alpha twiss parameter in the appropriate plane. beta (float): beta twiss parameter in the appropriate plane. Returns: The normal phase-space coordinates from the Courant-Snyder transform. \"\"\" p_matrix = np . array ( [ [1 / np.sqrt(beta), 0 ] , [ alpha / np.sqrt(beta), np.sqrt(beta) ] ] ) return p_matrix @ u_vector","title":"Twiss"},{"location":"reference/pyhdtoolkit/optics/twiss/#module-pyhdtoolkitopticstwiss","text":"Module optics.twiss Created on 2020.09.07 View Source \"\"\" Module optics.twiss ------------------- Created on 2020.09.07 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 module implementing various functionality for optics calculations from / to twiss parameters. \"\"\" import numba import numpy as np @numba . njit () def courant_snyder_transform ( u_vector : np . ndarray , alpha : float , beta : float ) -> np . ndarray : \"\"\" Perform the Courant-Snyder transform on rergular (nonchaotic) phase-space coordinatess. Specifically, if considering the horizontal plane and noting U = (x, px) the phase-space vector, it returns U_bar = (x_bar, px_bar) according to the transform: U_bar = P * U, where P = [1/sqrt(beta_x) 0 ] [alpha_x/sqrt(beta_x) sqrt(beta_x)] Args: u_vector (np.ndarray): two-dimentional array of phase-space (spatial and momenta) coordinates, either horizontal or vertical. alpha (float): alpha twiss parameter in the appropriate plane. beta (float): beta twiss parameter in the appropriate plane. Returns: The normal phase-space coordinates from the Courant-Snyder transform. \"\"\" p_matrix = np . array ([[ 1 / np . sqrt ( beta ), 0 ], [ alpha / np . sqrt ( beta ), np . sqrt ( beta )]]) return p_matrix @ u_vector","title":"Module pyhdtoolkit.optics.twiss"},{"location":"reference/pyhdtoolkit/optics/twiss/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/optics/twiss/#courant_snyder_transform","text":"def courant_snyder_transform ( u_vector : numpy . ndarray , alpha : float , beta : float ) -> numpy . ndarray Perform the Courant-Snyder transform on rergular (nonchaotic) phase-space coordinatess. Specifically, if considering the horizontal plane and noting U = (x, px) the phase-space vector, it returns U_bar = (x_bar, px_bar) according to the transform: U_bar = P * U, where P = [1/sqrt(beta_x) 0 ] [alpha_x/sqrt(beta_x) sqrt(beta_x)] Parameters: Name Type Description Default u_vector np.ndarray two-dimentional array of phase-space (spatial and momenta) coordinates, either horizontal or vertical. None alpha float alpha twiss parameter in the appropriate plane. None beta float beta twiss parameter in the appropriate plane. None Returns: Type Description None The normal phase-space coordinates from the Courant-Snyder transform. View Source @numba . njit () def courant_snyder_transform ( u_vector : np . ndarray , alpha : float , beta : float ) -> np . ndarray : \"\"\" Perform the Courant-Snyder transform on rergular (nonchaotic) phase-space coordinatess. Specifically, if considering the horizontal plane and noting U = (x, px) the phase-space vector, it returns U_bar = (x_bar, px_bar) according to the transform: U_bar = P * U, where P = [1/sqrt(beta_x) 0 ] [alpha_x/sqrt(beta_x) sqrt(beta_x)] Args: u_vector (np.ndarray): two-dimentional array of phase-space (spatial and momenta) coordinates, either horizontal or vertical. alpha (float): alpha twiss parameter in the appropriate plane. beta (float): beta twiss parameter in the appropriate plane. Returns: The normal phase-space coordinates from the Courant-Snyder transform. \"\"\" p_matrix = np . array ( [ [1 / np.sqrt(beta), 0 ] , [ alpha / np.sqrt(beta), np.sqrt(beta) ] ] ) return p_matrix @ u_vector","title":"courant_snyder_transform"},{"location":"reference/pyhdtoolkit/plotting/","text":"Module pyhdtoolkit.plotting plotting package ~ ~ ~ ~ ~ ~ ~ These are miscellaneous utilities to integrate to my plots. View Source \"\"\" plotting package ~~~~~~~~~~~~~~~~~~~ These are miscellaneous utilities to integrate to my plots. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .helpers import AnnotationsPlotter from .settings import PLOT_PARAMS Sub-modules pyhdtoolkit.plotting.helpers pyhdtoolkit.plotting.settings Variables python3 PLOT_PARAMS","title":"Index"},{"location":"reference/pyhdtoolkit/plotting/#module-pyhdtoolkitplotting","text":"plotting package ~ ~ ~ ~ ~ ~ ~ These are miscellaneous utilities to integrate to my plots. View Source \"\"\" plotting package ~~~~~~~~~~~~~~~~~~~ These are miscellaneous utilities to integrate to my plots. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .helpers import AnnotationsPlotter from .settings import PLOT_PARAMS","title":"Module pyhdtoolkit.plotting"},{"location":"reference/pyhdtoolkit/plotting/#sub-modules","text":"pyhdtoolkit.plotting.helpers pyhdtoolkit.plotting.settings","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/plotting/#variables","text":"python3 PLOT_PARAMS","title":"Variables"},{"location":"reference/pyhdtoolkit/plotting/helpers/","text":"Module pyhdtoolkit.plotting.helpers Module plotting.helpers Created on 2019.06.15 View Source \"\"\" Module plotting.helpers ----------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions for more descriptive plots. \"\"\" from typing import Tuple import matplotlib.axes class AnnotationsPlotter : \"\"\" A class to encapsulate all useful plotting additional tidbits. \"\"\" @staticmethod def set_arrow_label ( axis : matplotlib . axes . Axes , label : str , arrow_position : Tuple [ float , float ], label_position : Tuple [ float , float ], color : str = \"k\" , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ) -> matplotlib . text . Annotation : \"\"\" Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes `Axes` instance. Original code from Guido Sterbini. Args: axis (matplotlib.axes.Axes): a matplotlib axis to plot on. label (str): label text to print on the axis. arrow_position (Tuple[float, float]): where on the plot to point the tip of the arrow. label_position (Tuple[float, float]): where on the plot the text label (and thus start of the arrow) is. color (str): color parameter for your arrow and label. Defaults to 'k'. arrow_arc_rad (float): angle value defining the upwards / downwards shape of and bending of the arrow. fontsize (int): text size in the box Returns: A matploblit text annotation object. \"\"\" return axis . annotate ( label , xy = arrow_position , xycoords = \"data\" , xytext = label_position , textcoords = \"data\" , size = fontsize , color = color , va = \"center\" , ha = \"center\" , bbox = dict ( boxstyle = \"round4\" , fc = \"w\" , color = color , lw = 2 ), arrowprops = dict ( arrowstyle = \"-|>\" , connectionstyle = \"arc3,rad=\" + str ( arrow_arc_rad ), fc = \"w\" , color = color , lw = 2 , ), ) Classes AnnotationsPlotter class AnnotationsPlotter ( / , * args , ** kwargs ) View Source class AnnotationsPlotter : \" \"\" A class to encapsulate all useful plotting additional tidbits. \"\" \" @staticmethod def set _arrow_label ( axis : matplotlib . axes . Axes , label : str , arrow_position : Tuple [ float , float ] , label_position : Tuple [ float , float ] , color : str = \"k\" , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ) -> matplotlib . text . Annotation : \" \"\" Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes `Axes` instance. Original code from Guido Sterbini. Args: axis (matplotlib.axes.Axes): a matplotlib axis to plot on. label (str): label text to print on the axis. arrow_position (Tuple[float, float]): where on the plot to point the tip of the arrow. label_position (Tuple[float, float]): where on the plot the text label (and thus start of the arrow) is. color (str): color parameter for your arrow and label. Defaults to 'k'. arrow_arc_rad (float): angle value defining the upwards / downwards shape of and bending of the arrow. fontsize (int): text size in the box Returns: A matploblit text annotation object. \"\" \" return axis . annotate ( label , xy = arrow_position , xycoords = \"data\" , xytext = label_position , textcoords = \"data\" , size = fontsize , color = color , va = \"center\" , ha = \"center\" , bbox = dict ( boxstyle = \"round4\" , fc = \"w\" , color = color , lw = 2 ), arrowprops = dict ( arrowstyle = \"-|>\" , connectionstyle = \"arc3,rad=\" + str ( arrow_arc_rad ), fc = \"w\" , color = color , lw = 2 , ), ) Static methods set_arrow_label def set_arrow_label ( axis : matplotlib . axes . _axes . Axes , label : str , arrow_position : Tuple [ float , float ], label_position : Tuple [ float , float ], color : str = 'k' , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 ) -> matplotlib . text . Annotation Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes Axes instance. Original code from Guido Sterbini. Parameters: Name Type Description Default axis matplotlib.axes.Axes a matplotlib axis to plot on. None label str label text to print on the axis. None arrow_position Tuple[float, float] where on the plot to point the tip of the arrow. None label_position Tuple[float, float] where on the plot the text label (and thus start of the arrow) is. None color str color parameter for your arrow and label. Defaults to 'k'. 'k' arrow_arc_rad float angle value defining the upwards / downwards shape of and bending of the arrow. None fontsize int text size in the box None Returns: Type Description None A matploblit text annotation object. View Source @staticmethod def set _arrow_label ( axis : matplotlib . axes . Axes , label : str , arrow_position : Tuple [ float , float ] , label_position : Tuple [ float , float ] , color : str = \"k\" , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ) -> matplotlib . text . Annotation : \" \"\" Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes `Axes` instance. Original code from Guido Sterbini. Args: axis (matplotlib.axes.Axes): a matplotlib axis to plot on. label (str): label text to print on the axis. arrow_position (Tuple[float, float]): where on the plot to point the tip of the arrow. label_position (Tuple[float, float]): where on the plot the text label (and thus start of the arrow) is. color (str): color parameter for your arrow and label. Defaults to 'k'. arrow_arc_rad (float): angle value defining the upwards / downwards shape of and bending of the arrow. fontsize (int): text size in the box Returns: A matploblit text annotation object. \"\" \" return axis . annotate ( label , xy = arrow_position , xycoords = \"data\" , xytext = label_position , textcoords = \"data\" , size = fontsize , color = color , va = \"center\" , ha = \"center\" , bbox = dict ( boxstyle = \"round4\" , fc = \"w\" , color = color , lw = 2 ), arrowprops = dict ( arrowstyle = \"-|>\" , connectionstyle = \"arc3,rad=\" + str ( arrow_arc_rad ), fc = \"w\" , color = color , lw = 2 , ), )","title":"Helpers"},{"location":"reference/pyhdtoolkit/plotting/helpers/#module-pyhdtoolkitplottinghelpers","text":"Module plotting.helpers Created on 2019.06.15 View Source \"\"\" Module plotting.helpers ----------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions for more descriptive plots. \"\"\" from typing import Tuple import matplotlib.axes class AnnotationsPlotter : \"\"\" A class to encapsulate all useful plotting additional tidbits. \"\"\" @staticmethod def set_arrow_label ( axis : matplotlib . axes . Axes , label : str , arrow_position : Tuple [ float , float ], label_position : Tuple [ float , float ], color : str = \"k\" , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ) -> matplotlib . text . Annotation : \"\"\" Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes `Axes` instance. Original code from Guido Sterbini. Args: axis (matplotlib.axes.Axes): a matplotlib axis to plot on. label (str): label text to print on the axis. arrow_position (Tuple[float, float]): where on the plot to point the tip of the arrow. label_position (Tuple[float, float]): where on the plot the text label (and thus start of the arrow) is. color (str): color parameter for your arrow and label. Defaults to 'k'. arrow_arc_rad (float): angle value defining the upwards / downwards shape of and bending of the arrow. fontsize (int): text size in the box Returns: A matploblit text annotation object. \"\"\" return axis . annotate ( label , xy = arrow_position , xycoords = \"data\" , xytext = label_position , textcoords = \"data\" , size = fontsize , color = color , va = \"center\" , ha = \"center\" , bbox = dict ( boxstyle = \"round4\" , fc = \"w\" , color = color , lw = 2 ), arrowprops = dict ( arrowstyle = \"-|>\" , connectionstyle = \"arc3,rad=\" + str ( arrow_arc_rad ), fc = \"w\" , color = color , lw = 2 , ), )","title":"Module pyhdtoolkit.plotting.helpers"},{"location":"reference/pyhdtoolkit/plotting/helpers/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/plotting/helpers/#annotationsplotter","text":"class AnnotationsPlotter ( / , * args , ** kwargs ) View Source class AnnotationsPlotter : \" \"\" A class to encapsulate all useful plotting additional tidbits. \"\" \" @staticmethod def set _arrow_label ( axis : matplotlib . axes . Axes , label : str , arrow_position : Tuple [ float , float ] , label_position : Tuple [ float , float ] , color : str = \"k\" , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ) -> matplotlib . text . Annotation : \" \"\" Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes `Axes` instance. Original code from Guido Sterbini. Args: axis (matplotlib.axes.Axes): a matplotlib axis to plot on. label (str): label text to print on the axis. arrow_position (Tuple[float, float]): where on the plot to point the tip of the arrow. label_position (Tuple[float, float]): where on the plot the text label (and thus start of the arrow) is. color (str): color parameter for your arrow and label. Defaults to 'k'. arrow_arc_rad (float): angle value defining the upwards / downwards shape of and bending of the arrow. fontsize (int): text size in the box Returns: A matploblit text annotation object. \"\" \" return axis . annotate ( label , xy = arrow_position , xycoords = \"data\" , xytext = label_position , textcoords = \"data\" , size = fontsize , color = color , va = \"center\" , ha = \"center\" , bbox = dict ( boxstyle = \"round4\" , fc = \"w\" , color = color , lw = 2 ), arrowprops = dict ( arrowstyle = \"-|>\" , connectionstyle = \"arc3,rad=\" + str ( arrow_arc_rad ), fc = \"w\" , color = color , lw = 2 , ), )","title":"AnnotationsPlotter"},{"location":"reference/pyhdtoolkit/plotting/helpers/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/plotting/helpers/#set_arrow_label","text":"def set_arrow_label ( axis : matplotlib . axes . _axes . Axes , label : str , arrow_position : Tuple [ float , float ], label_position : Tuple [ float , float ], color : str = 'k' , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 ) -> matplotlib . text . Annotation Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes Axes instance. Original code from Guido Sterbini. Parameters: Name Type Description Default axis matplotlib.axes.Axes a matplotlib axis to plot on. None label str label text to print on the axis. None arrow_position Tuple[float, float] where on the plot to point the tip of the arrow. None label_position Tuple[float, float] where on the plot the text label (and thus start of the arrow) is. None color str color parameter for your arrow and label. Defaults to 'k'. 'k' arrow_arc_rad float angle value defining the upwards / downwards shape of and bending of the arrow. None fontsize int text size in the box None Returns: Type Description None A matploblit text annotation object. View Source @staticmethod def set _arrow_label ( axis : matplotlib . axes . Axes , label : str , arrow_position : Tuple [ float , float ] , label_position : Tuple [ float , float ] , color : str = \"k\" , arrow_arc_rad : float = - 0.2 , fontsize : int = 20 , ) -> matplotlib . text . Annotation : \" \"\" Add a label box with text and an arrow from the box to a specified position to an existing provided matplotlib.axes `Axes` instance. Original code from Guido Sterbini. Args: axis (matplotlib.axes.Axes): a matplotlib axis to plot on. label (str): label text to print on the axis. arrow_position (Tuple[float, float]): where on the plot to point the tip of the arrow. label_position (Tuple[float, float]): where on the plot the text label (and thus start of the arrow) is. color (str): color parameter for your arrow and label. Defaults to 'k'. arrow_arc_rad (float): angle value defining the upwards / downwards shape of and bending of the arrow. fontsize (int): text size in the box Returns: A matploblit text annotation object. \"\" \" return axis . annotate ( label , xy = arrow_position , xycoords = \"data\" , xytext = label_position , textcoords = \"data\" , size = fontsize , color = color , va = \"center\" , ha = \"center\" , bbox = dict ( boxstyle = \"round4\" , fc = \"w\" , color = color , lw = 2 ), arrowprops = dict ( arrowstyle = \"-|>\" , connectionstyle = \"arc3,rad=\" + str ( arrow_arc_rad ), fc = \"w\" , color = color , lw = 2 , ), )","title":"set_arrow_label"},{"location":"reference/pyhdtoolkit/plotting/settings/","text":"Module pyhdtoolkit.plotting.settings Module plotting.settings Created on 2019.12.08 View Source \"\"\" Module plotting.settings ------------------------ Created on 2019.12.08 :author: Felix Soubelet (felix.soubelet@cern.ch) Some settings for better matplotlib.pyplot plots. Work in progress. \"\"\" from typing import Dict , Union # Set those with matplotlib.pyplot.rcParams.update(PLOT_PARAMS). # Will ALWAYS be overwritten by later on definition PLOT_PARAMS : Dict [ str , Union [ float , bool , str , tuple ]] = { # ------ Axes ------ # \"axes.linewidth\" : 0.8 , # Linewidth of axes edges \"axes.grid\" : False , # Do not display grid \"axes.labelsize\" : 25 , # Fontsize of the x and y axis labels \"axes.titlesize\" : 27 , # Fontsize of the axes title # ------ Date Forrmats ------ # \"date.autoformatter.year\" : \"%Y\" , # AutoDateFormatter setting for years display \"date.autoformatter.month\" : \"%Y-%m\" , # AutoDateFormatter setting for months display \"date.autoformatter.day\" : \"%Y-%m- %d \" , # AutoDateFormatter setting for days display \"date.autoformatter.hour\" : \"%m- %d %H\" , # AutoDateFormatter setting for seconds display \"date.autoformatter.minute\" : \" %d %H:%M\" , # AutoDateFormatter setting for minutes display \"date.autoformatter.second\" : \"%H:%M:%S\" , # AutoDateFormatter setting for seconds display \"date.autoformatter.microsecond\" : \"%M:%S. %f \" , # AutoDateFormatter setting for microseconds # ------ General Figure ------ # \"figure.autolayout\" : True , # Adjust subplot params to fit the figure (tight_layout) \"figure.dpi\" : 300 , # Figure dots per inch \"figure.figsize\" : ( 18 , 11 ), # Size of the figure \"figure.max_open_warning\" : 10 , # Max number of figures to open before warning \"figure.titlesize\" : 30 , # Size of the figure title # ------ Fonts ------ # \"font.family\" : \"sans-serif\" , # Font family # \"font.sans-serif\": \"Helvetica\", # Sans-Serif font to use \"font.style\" : \"normal\" , # Style to apply to text font # ------- Legend ------ # \"legend.fancybox\" : True , # Use rounded box for legend background \"legend.fontsize\" : 22 , # Legend text font size \"legend.loc\" : \"best\" , # Default legend location # ------ Lines ------ # \"lines.linewidth\" : 1 , # Line width, in points \"lines.markersize\" : 5 , # Marker size, in points \"lines.antialiased\" : True , # Apply anti-aliasing to lines display # ------ Patches ------ # \"patch.linewidth\" : 1 , # Width of patches edge lines \"patch.antialiased\" : True , # Apply anti-aliasing to patches display # ------ Paths ------ # \"path.simplify\" : True , # Reduce file size by removing \"invisible\" points # ------ Saving ------ # \"savefig.dpi\" : 300 , # Saved figure dots per inch \"savefig.format\" : \"pdf\" , # Saved figure file format \"savefig.bbox\" : \"tight\" , # Careful: incompatible with pipe-based animation backends # ------ Text ------ # \"text.antialiased\" : True , # Apply anti-aliasing to text elements \"text.color\" : \"black\" , # Default text color \"text.usetex\" : False , # Do not use LaTeX for text handling (I don't have a local installation) # ------ Ticks ------ # \"xtick.labelsize\" : 20 , # Fontsize of the x axis tick labels \"ytick.labelsize\" : 20 , # Fontsize of the y axis tick labels } Variables PLOT_PARAMS","title":"Settings"},{"location":"reference/pyhdtoolkit/plotting/settings/#module-pyhdtoolkitplottingsettings","text":"Module plotting.settings Created on 2019.12.08 View Source \"\"\" Module plotting.settings ------------------------ Created on 2019.12.08 :author: Felix Soubelet (felix.soubelet@cern.ch) Some settings for better matplotlib.pyplot plots. Work in progress. \"\"\" from typing import Dict , Union # Set those with matplotlib.pyplot.rcParams.update(PLOT_PARAMS). # Will ALWAYS be overwritten by later on definition PLOT_PARAMS : Dict [ str , Union [ float , bool , str , tuple ]] = { # ------ Axes ------ # \"axes.linewidth\" : 0.8 , # Linewidth of axes edges \"axes.grid\" : False , # Do not display grid \"axes.labelsize\" : 25 , # Fontsize of the x and y axis labels \"axes.titlesize\" : 27 , # Fontsize of the axes title # ------ Date Forrmats ------ # \"date.autoformatter.year\" : \"%Y\" , # AutoDateFormatter setting for years display \"date.autoformatter.month\" : \"%Y-%m\" , # AutoDateFormatter setting for months display \"date.autoformatter.day\" : \"%Y-%m- %d \" , # AutoDateFormatter setting for days display \"date.autoformatter.hour\" : \"%m- %d %H\" , # AutoDateFormatter setting for seconds display \"date.autoformatter.minute\" : \" %d %H:%M\" , # AutoDateFormatter setting for minutes display \"date.autoformatter.second\" : \"%H:%M:%S\" , # AutoDateFormatter setting for seconds display \"date.autoformatter.microsecond\" : \"%M:%S. %f \" , # AutoDateFormatter setting for microseconds # ------ General Figure ------ # \"figure.autolayout\" : True , # Adjust subplot params to fit the figure (tight_layout) \"figure.dpi\" : 300 , # Figure dots per inch \"figure.figsize\" : ( 18 , 11 ), # Size of the figure \"figure.max_open_warning\" : 10 , # Max number of figures to open before warning \"figure.titlesize\" : 30 , # Size of the figure title # ------ Fonts ------ # \"font.family\" : \"sans-serif\" , # Font family # \"font.sans-serif\": \"Helvetica\", # Sans-Serif font to use \"font.style\" : \"normal\" , # Style to apply to text font # ------- Legend ------ # \"legend.fancybox\" : True , # Use rounded box for legend background \"legend.fontsize\" : 22 , # Legend text font size \"legend.loc\" : \"best\" , # Default legend location # ------ Lines ------ # \"lines.linewidth\" : 1 , # Line width, in points \"lines.markersize\" : 5 , # Marker size, in points \"lines.antialiased\" : True , # Apply anti-aliasing to lines display # ------ Patches ------ # \"patch.linewidth\" : 1 , # Width of patches edge lines \"patch.antialiased\" : True , # Apply anti-aliasing to patches display # ------ Paths ------ # \"path.simplify\" : True , # Reduce file size by removing \"invisible\" points # ------ Saving ------ # \"savefig.dpi\" : 300 , # Saved figure dots per inch \"savefig.format\" : \"pdf\" , # Saved figure file format \"savefig.bbox\" : \"tight\" , # Careful: incompatible with pipe-based animation backends # ------ Text ------ # \"text.antialiased\" : True , # Apply anti-aliasing to text elements \"text.color\" : \"black\" , # Default text color \"text.usetex\" : False , # Do not use LaTeX for text handling (I don't have a local installation) # ------ Ticks ------ # \"xtick.labelsize\" : 20 , # Fontsize of the x axis tick labels \"ytick.labelsize\" : 20 , # Fontsize of the y axis tick labels }","title":"Module pyhdtoolkit.plotting.settings"},{"location":"reference/pyhdtoolkit/plotting/settings/#variables","text":"PLOT_PARAMS","title":"Variables"},{"location":"reference/pyhdtoolkit/scripts/","text":"Module pyhdtoolkit.scripts scripts module ~ ~ ~ ~ ~ Various scripts to run, that will help manipulate data or run different flow of programs and simulations. This is helpful in my work. View Source \"\"\" scripts module ~~~~~~~~~~~~~~~ Various scripts to run, that will help manipulate data or run different flow of programs and simulations. This is helpful in my work. :copyright: (c) 2019-2020 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" Sub-modules pyhdtoolkit.scripts.ac_dipole pyhdtoolkit.scripts.triplet_errors","title":"Index"},{"location":"reference/pyhdtoolkit/scripts/#module-pyhdtoolkitscripts","text":"scripts module ~ ~ ~ ~ ~ Various scripts to run, that will help manipulate data or run different flow of programs and simulations. This is helpful in my work. View Source \"\"\" scripts module ~~~~~~~~~~~~~~~ Various scripts to run, that will help manipulate data or run different flow of programs and simulations. This is helpful in my work. :copyright: (c) 2019-2020 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\"","title":"Module pyhdtoolkit.scripts"},{"location":"reference/pyhdtoolkit/scripts/#sub-modules","text":"pyhdtoolkit.scripts.ac_dipole pyhdtoolkit.scripts.triplet_errors","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/","text":"Module pyhdtoolkit.scripts.ac_dipole None None Sub-modules pyhdtoolkit.scripts.ac_dipole.sext_ac_dipole_tracking","title":"Index"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/#module-pyhdtoolkitscriptsac_dipole","text":"None None","title":"Module pyhdtoolkit.scripts.ac_dipole"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/#sub-modules","text":"pyhdtoolkit.scripts.ac_dipole.sext_ac_dipole_tracking","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/","text":"Module pyhdtoolkit.scripts.ac_dipole.sext_ac_dipole_tracking Script scripts.ac_dipole.sext_ac_dipole_tracking Created on 2020.02.27 View Source \"\"\" Script scripts.ac_dipole.sext_ac_dipole_tracking ------------------------------------------------ Created on 2020.02.27 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 utility to launch a series of MAD-X simlations with the proper parameters, call the appropriate python scripts on the outputs and organise the results. Made to be ran with the OMC conda environment, and ran directly for the commandline. A pair of examples ================== Running with kicks in the horizontal plane only, for two sigma values: python path/to/sext_ac_dipole_tracking.py --planes horizontal --mask /path/to/kick/mask.mask \\ --type kick --sigmas 5 10 Running with free oscillations in both planes succesively, for many offset values: python path/to/sext_ac_dipole_tracking.py --planes horizontal vertical \\ --mask /path/to/offset/mask.mask --type amp -- sigmas 5 10 15 20 \"\"\" import argparse import shutil import sys from pathlib import Path from typing import Dict , List , Union from loguru import logger from pyhdtoolkit.utils.cmdline import CommandLine from pyhdtoolkit.utils.contexts import timeit from pyhdtoolkit.utils.defaults import LOGURU_FORMAT , OMC_PYTHON , TBT_CONVERTER_SCRIPT class ACDipoleGrid : \"\"\" Algorithm as a class to run the simulations and organize the outputs. \"\"\" __slots__ = { \"grid_output_dir\" : \"PosixPath object to the directory for all outputs\" , \"mask_files_dir\" : \"PosixPath object to the directory for simulations' masks\" , \"outputdata_dir\" : \"PosixPath object to the directory for simulation data\" , \"trackfiles_dir\" : \"PosixPath object to the directory for trackone files\" , \"trackfiles_planes\" : \"PosixPath objects to the planes' trackone files\" , \"run_planes\" : \"List of planes for which to run simulations\" , \"sigmas\" : \"List of amplitudes for AC dipole to kick to (in bunch sigma)\" , \"template_file\" : \"PosixPath object to the location of the mask template\" , \"template_str\" : \"Text in the template_file\" , } def __init__ ( self ) -> None : self . grid_output_dir : Path = Path ( \"grid_outputs\" ) self . mask_files_dir : Path = self . grid_output_dir / \"mask_files\" self . outputdata_dir : Path = self . grid_output_dir / \"outputdata_dirs\" self . trackfiles_dir : Path = self . grid_output_dir / \"trackfiles\" self . trackfiles_planes : Dict [ str , Path ] = { \"horizontal\" : self . grid_output_dir / \"trackfiles\" / \"X\" , \"vertical\" : self . grid_output_dir / \"trackfiles\" / \"Y\" , } self . run_planes : List [ str ] = _parse_arguments () . planes self . sigmas : List [ float ] = sorted ( _parse_arguments () . sigmas ) self . template_file : Path = Path ( _parse_arguments () . template ) self . template_str : str = self . template_file . read_text () def _check_input_sanity ( self ) -> None : \"\"\" Makes sure there are no duplicates in the provided sigma values, because that will mess up a long time after launch and you will cry. \"\"\" if len ( self . sigmas ) != len ( set ( self . sigmas )): logger . error ( \"There is a duplicate in the sigma values, which would cause a failure later.\" ) sys . exit () def _create_output_dirs ( self ) -> None : \"\"\" Will create the proper output dirs if they don't exist already. \"\"\" if not self . grid_output_dir . is_dir (): logger . debug ( f \"Creating directory '{self.grid_output_dir}'\" ) self . grid_output_dir . mkdir () if not self . mask_files_dir . is_dir (): logger . debug ( f \"Creating directory '{self.mask_files_dir}'\" ) self . mask_files_dir . mkdir () if not self . outputdata_dir . is_dir (): logger . debug ( f \"Creating directory '{self.outputdata_dir}'\" ) self . outputdata_dir . mkdir () if not self . trackfiles_dir . is_dir (): logger . debug ( f \"Creating directory '{self.trackfiles_dir}'\" ) self . trackfiles_dir . mkdir () if not self . trackfiles_planes [ \"horizontal\" ] . is_dir (): logger . debug ( f \"Creating directory '{self.trackfiles_planes['horizontal']}'\" ) self . trackfiles_planes [ \"horizontal\" ] . mkdir () if not self . trackfiles_planes [ \"vertical\" ] . is_dir (): logger . debug ( f \"Creating directory '{self.trackfiles_planes['vertical']}'\" ) self . trackfiles_planes [ \"vertical\" ] . mkdir () else : logger . error ( \"Output directories already present, you may want to move those.\" ) sys . exit () def track_forced_oscillations_for_plane ( self , kick_plane : str = None ) -> None : \"\"\" Run MAD-X simulations with AC dipole tracking for the given plane, and handle outputs. Args: kick_plane: the name of the plane on which to apply a kick, either 'horizontal' or 'vertical'. \"\"\" if kick_plane not in ( \"horizontal\" , \"vertical\" ): logger . error ( f \"Plane parameter {kick_plane} is not a valid value\" ) raise ValueError ( \"Plane parameter should be one of 'horizontal' or 'vertical'\" ) with timeit ( lambda spanned : logger . info ( f \"Tracked all amplitudes for {kick_plane} kicks in {spanned:.4f} seconds\" ) ): for kick_in_sigma in self . sigmas : print ( \"\" ) # Set the wanted kick value (in sigmas) in the given plane, and a small initial # offset in the other (small offset so that harpy doesn't cry and we get tune). # Do NOT kick both planes: cross-terms influence the detuning. plane_letter = \"x\" if kick_plane == \"horizontal\" else \"y\" replace_dict = ( { \" %(SIGMAX_VALUE)s \" : kick_in_sigma , \" %(SIGMAY_VALUE)s \" : 0 , \" %(AMPLX_VALUE)s \" : 0 , \" %(AMPLY_VALUE)s \" : 0.5 , } if kick_plane == \"horizontal\" else { \" %(SIGMAX_VALUE)s \" : 0 , \" %(SIGMAY_VALUE)s \" : kick_in_sigma , \" %(AMPLX_VALUE)s \" : 0.5 , \" %(AMPLY_VALUE)s \" : 0 , } ) filename_to_write = Path ( f \"sext_ac_dipole_tracking_{kick_in_sigma}_sigma_{plane_letter}_kick\" ) mask_file = create_script_file ( self . template_str , values_replacing_dict = replace_dict , filename = Path ( str ( filename_to_write )), ) run_madx_mask ( mask_file ) _move_mask_file_after_running ( mask_file_path = mask_file , mask_files_dir = self . mask_files_dir ) _rename_madx_outputs ( kick_in_sigma = kick_in_sigma , outputdata_dir = self . outputdata_dir , plane = plane_letter , ) _convert_trackone_to_sdds () _move_trackone_sdds ( kick_in_sigma = kick_in_sigma , trackfiles_dir = self . trackfiles_planes [ kick_plane ], plane = plane_letter , ) def track_free_oscillations_for_plane ( self , kick_plane : str = None ) -> None : \"\"\" Run MAD-X simulations with amplitude offset tracking for the given plane, and handle outputs. Args: kick_plane: the name of the plane on which to apply an offset, either 'horizontal' or 'vertical'. \"\"\" if kick_plane not in ( \"horizontal\" , \"vertical\" ): logger . error ( f \"Plane parameter {kick_plane} is not a valid value\" ) raise ValueError ( \"Plane parameter should be one of 'horizontal' or 'vertical'\" ) with timeit ( lambda spanned : logger . info ( f \"Tracked all amplitudes for {kick_plane} offsets in {spanned:.4f} seconds\" ) ): for kick_in_sigma in self . sigmas : print ( \"\" ) plane_letter = \"x\" if kick_plane == \"horizontal\" else \"y\" action_var_value = kick_in_sigma amplitudes_dict = ( { \" %(AMPLX_VALUE)s \" : action_var_value , \" %(AMPLY_VALUE)s \" : 0.5 } if kick_plane == \"horizontal\" else { \" %(AMPLX_VALUE)s \" : 0.5 , \" %(AMPLY_VALUE)s \" : action_var_value } ) filename_to_write = Path ( f \"initial_amplitude_tracking_{kick_in_sigma}_sigma_{plane_letter}_kick\" ) mask_file = create_script_file ( self . template_str , values_replacing_dict = amplitudes_dict , filename = Path ( str ( filename_to_write )), ) run_madx_mask ( mask_file ) _move_mask_file_after_running ( mask_file_path = mask_file , mask_files_dir = self . mask_files_dir ) _rename_madx_outputs ( kick_in_sigma = kick_in_sigma , outputdata_dir = self . outputdata_dir , plane = plane_letter , ) _convert_trackone_to_sdds () _move_trackone_sdds ( kick_in_sigma = kick_in_sigma , trackfiles_dir = self . trackfiles_planes [ kick_plane ], plane = plane_letter , ) @logger.catch def main () -> None : \"\"\" Run the whole process: create a class instance, simulate for horizontal and vertical kicks, exit. \"\"\" command_line_args = _parse_arguments () _set_logger_level ( command_line_args . log_level ) simulations = ACDipoleGrid () simulations . _check_input_sanity () simulations . _create_output_dirs () sim_type = command_line_args . simulation_type try : if sim_type == \"kick\" : logger . info ( f \"Planes to kick then track on are: {simulations.run_planes}\" ) logger . info ( f \"Kick values to compute are (in bunch sigmas): {simulations.sigmas}\" ) for plane in simulations . run_planes : simulations . track_forced_oscillations_for_plane ( kick_plane = plane ) elif sim_type == \"amp\" : logger . info ( f \"Planes to offset then track on are: {simulations.run_planes}\" ) logger . info ( f \"Registered initial tracking amplitudes (in bunch sigmas): {simulations.sigmas}\" ) for plane in simulations . run_planes : simulations . track_free_oscillations_for_plane ( kick_plane = plane ) else : logger . error ( f \"Simulation type {sim_type} is not a valid value\" ) raise ValueError ( \"Simulation type should be one of 'kick' or 'amp'\" ) except KeyboardInterrupt : logger . info ( \"Manual interruption, ending processes\" ) _cleanup_madx_residuals () logger . warning ( \"The 'grid_outputs' folder was left untouched, check for unexpected MADX residuals\" ) # ---------------------- Public Utilities ---------------------- # def run_madx_mask ( mask_file : Path ) -> None : \"\"\" Run madx on the provided file. Args: mask_file (Path): Path object with the mask file location. \"\"\" logger . debug ( f \"Running madx on script: '{mask_file.absolute()}'\" ) exit_code , std_out = CommandLine . run ( f \"madx {mask_file.absolute()}\" ) if exit_code != 0 : # Dump madx log in case of failure so we can see where it went wrong. logger . warning ( f \"MAD-X command self-killed with exit code: {exit_code}\" ) log_dump = Path ( f \"failed_madx_returnedcode_{exit_code}.log\" ) with log_dump . open ( \"w\" ) as logfile : logfile . write ( std_out . decode ()) # Default 'utf-8' encoding, depends on your system. logger . warning ( f \"The standard output has been dumped to file 'failed_command_{exit_code}.logfile'\" ) def create_script_file ( template_as_str : str , values_replacing_dict : Dict [ str , float ], filename : Path , ) -> Path : \"\"\" Create new script file from template with the appropriate values. Args: template_as_str (str): string content of your template mask file. values_replacing_dict (Dict[str, float]): keys to find and values to replace them with in the template. filename (Path): Path object for the file in which to write the script. \"\"\" string_mask = _create_script_string ( template_as_str , values_replacing_dict ) return _write_script_to_file ( string_mask , filename ) # ---------------------- Private Utilities ---------------------- # def _convert_trackone_to_sdds () -> None : \"\"\" Run the omc3 tbt_converter script on trackone output of MAD-X. Will also cleanup the `converter` and 'stats' files left by tbt_converter afterwards. \"\"\" if not Path ( \"trackone\" ) . is_file (): logger . error ( \"Tried to call 'tbt_converter' without a 'trackone' file present, aborting\" ) sys . exit () logger . debug ( f \"Running '{TBT_CONVERTER_SCRIPT}' on 'trackone' file\" ) CommandLine . run ( f \"{OMC_PYTHON.absolute()} {TBT_CONVERTER_SCRIPT.absolute()} \" \"--files trackone --outputdir . --tbt_datatype trackone\" ) logger . debug ( \"Removing trackone file 'trackone'\" ) Path ( \"trackone\" ) . unlink () logger . debug ( \"Removing outputs of 'tbt_converter'\" ) if Path ( \"stats.txt\" ) . exists (): Path ( \"stats.txt\" ) . unlink () for tbt_output_file in list ( Path ( \".\" ) . glob ( \"converter_*\" )): tbt_output_file . unlink () def _create_script_string ( template_as_string : str , values_replacing_dict : Dict [ str , float ]) -> str : \"\"\" For each key in the provided dict, will replace it in the template scripts with the corresponding dict value. Args: template_as_string (str): the string content of your template mask file. values_replacing_dict (Dict[str, float]): pairs of key, value to find and replace in the template string. Returns: The new script string. \"\"\" script_string : str = template_as_string for key , value in values_replacing_dict . items (): script_string = script_string . replace ( str ( key ), str ( value )) return script_string def _move_mask_file_after_running ( mask_file_path : Path , mask_files_dir : Path ) -> None : \"\"\" Move the mask file after being done running it with MAD-X. Args: mask_file_path (Path): Path object with the file location. mask_files_dir (Path): Path object with the directory to move mask to' location. \"\"\" logger . debug ( f \"Moving mask file '{mask_file_path}' to directory '{mask_files_dir}'\" ) mask_file_path . rename ( f \"{mask_files_dir}/{mask_file_path}\" ) def _move_trackone_sdds ( kick_in_sigma : Union [ str , float ], trackfiles_dir : Path , plane : str ) -> None : \"\"\" Call after running omc3's `tbt_converter` on the `trackone` output by MAD-X, will move the resulting `trackone.sdds` file to the `trackfiles_dir`, with a naming reflecting the ac dipole kick strength. Args: kick_in_sigma (Union[str, float]): the AC dipole kick value (in sigma) for which you ran your simulation. trackfiles_dir (Path): PosixPath to the folder in which to store all sdds trackone files. plane (str): the plane on which ac dipole provided the kick, should be `x` or `y`. \"\"\" if str ( plane ) not in ( \"x\" , \"y\" ): logger . error ( f \"Plane parameter {plane} is not a valid value\" ) raise ValueError ( \"Plane parameter should be one of 'x' or 'y'\" ) logger . debug ( f \"Moving trackone sdds file to directory '{trackfiles_dir}'\" ) track_sdds_file = Path ( \"trackone.sdds\" ) if not track_sdds_file . is_file (): logger . error ( \"Conversion to trackone sdds file must have failed, check the omc3 script\" ) sys . exit () track_sdds_file . rename ( f \"{trackfiles_dir}/trackone_{kick_in_sigma}_sigma_{plane}.sdds\" ) def _parse_arguments () -> argparse . Namespace : \"\"\" Simple argument parser to make life easier in the command-line. Returns a NameSpace with arguments as attributes. \"\"\" parser = argparse . ArgumentParser ( description = \"Running MAD-X AC dipole trackings for you.\" ) parser . add_argument ( \"-s\" , \"--sigmas\" , dest = \"sigmas\" , nargs = \"+\" , default = [ 1 , 2 ], type = float , help = \"Different amplitude values (in bunch sigma) for the AC dipole kicks.\" \"Defaults to [1, 2].\" , ) parser . add_argument ( \"-p\" , \"--planes\" , dest = \"planes\" , nargs = \"+\" , default = [ \"horizontal\" ], type = str , help = \"Planes for which to kick, possible values are 'horizontal' and 'vertical',\" \"Defaults to 'horizontal'.\" , ) parser . add_argument ( \"-m\" , \"--mask\" , dest = \"template\" , default = \"/afs/cern.ch/work/f/fesoubel/public/MADX_scripts/AC_dipole_tracking/ac_kick_tracking_template.mask\" , type = str , help = \"Location of your MAD-X template mask file to use, defaults to \" \"'/afs/cern.ch/work/f/fesoubel/public/MADX_scripts/AC_dipole_tracking/ac_kick_track_template.mask'.\" , ) parser . add_argument ( \"-t\" , \"--type\" , dest = \"simulation_type\" , default = \"kick\" , type = str , help = \"Type of simulations to run, either 'kick' for AC dipole kick or 'amp' for free \" \"oscillations. Defaults to 'kick'.\" , ) parser . add_argument ( \"-l\" , \"--logs\" , dest = \"log_level\" , default = \"info\" , type = str , help = \"The base console logging level. Can be 'debug', 'info', 'warning' and 'error'.\" \"Defaults to 'info'.\" , ) return parser . parse_args () def _rename_madx_outputs ( kick_in_sigma : Union [ str , float ], outputdata_dir : Path , plane : str ) -> None : \"\"\" Call after running MAD-X on your mask, will move the 'Outpudata' created by MAD-X to the proper place. Args: kick_in_sigma (Union[str, float]): the AC dipole kick value (in sigma) for which you ran your simulation. outputdata_dir (Path): PosixPath to the folder in which to store all successive `Outputdata`'s location. plane (str): the plane on which ac dipole provided the kick, should be `x` or `y`. \"\"\" if str ( plane ) not in ( \"x\" , \"y\" ): raise ValueError ( f \"Plane parameter should be one of 'x', 'y' but {plane} was provided.\" ) madx_outputs = Path ( \"Outputdata\" ) logger . debug ( f \"Moving MAD-X outputs to directory '{outputdata_dir}'\" ) madx_outputs . rename ( f \"{outputdata_dir}/Outputdata_{kick_in_sigma}_sigma_{plane}\" ) def _set_logger_level ( log_level : str = \"info\" ) -> None : \"\"\" Sets the logger level to the one provided at the commandline. Default loguru handler will have DEBUG level and ID 0. We need to first remove this default handler and add ours with the wanted level. Args: log_level (str): the default logging level to print out. \"\"\" logger . remove ( 0 ) logger . add ( sys . stderr , format = LOGURU_FORMAT , level = log_level . upper ()) def _write_script_to_file ( script_as_string : str , filename : Union [ str , Path ]) -> Path : \"\"\" Create a new file with the provided script, and return the location. Args: script_as_string (str): the script string to write to file. filename (Union[str, Path]): the file name to use. Returns: The `pathlib.Path` object to the created file. \"\"\" file_path = Path ( str ( filename ) + \".mask\" ) logger . debug ( f \"Creating new mask file '{file_path}'\" ) with file_path . open ( \"w\" ) as script : script . write ( script_as_string ) return file_path def _cleanup_madx_residuals () -> None : \"\"\" Will look for specific madx artifacts and remove them. Meant to be called in case of interuption. \"\"\" expected_residuals : Dict [ str , List [ str ]] = { \"symlinks\" : [ \"db5\" , \"slhc\" , \"fidel\" , \"wise\" , \"optics2016\" , \"optics2017\" , \"optics2018\" , \"scripts\" ,], \"directories\" : [ \"temp\" , \"Outputdata\" ], \"files\" : [ \"fort.18\" ], } logger . debug ( \"Cleaning up expected MADX residuals\" ) for residual_type , residual_values in expected_residuals . items (): logger . trace ( f \"Cleaning up MADX residual {residual_type}\" ) for residual in residual_values : if Path ( residual ) . is_symlink () or Path ( residual ) . is_file (): Path ( residual ) . unlink () elif Path ( residual ) . is_dir (): shutil . rmtree ( Path ( residual )) logger . debug ( \"Cleaning up potential residual mask files\" ) for suspect_mask in list ( Path ( \".\" ) . glob ( \"*.mask\" )): if ( suspect_mask . stem . startswith ( \"initial_\" ) or suspect_mask . stem . startswith ( \"sext_\" ) ) and suspect_mask . stem . endswith ( \"_kick\" ): suspect_mask . unlink () if __name__ == \"__main__\" : main () Variables LOGURU_FORMAT Functions create_script_file def create_script_file ( template_as_str : str , values_replacing_dict : Dict [ str , float ], filename : pathlib . Path ) -> pathlib . Path Create new script file from template with the appropriate values. Parameters: Name Type Description Default template_as_str str string content of your template mask file. None values_replacing_dict Dict[str, float] keys to find and values to replace them with in the template. None filename Path Path object for the file in which to write the script. None View Source def create_script_file ( template_as_str : str , values_replacing_dict : Dict [ str , float ], filename : Path , ) -> Path : \"\"\" Create new script file from template with the appropriate values. Args: template_as_str (str): string content of your template mask file. values_replacing_dict (Dict[str, float]): keys to find and values to replace them with in the template. filename (Path): Path object for the file in which to write the script. \"\"\" string_mask = _create_script_string ( template_as_str , values_replacing_dict ) return _write_script_to_file ( string_mask , filename ) main def main ( ) -> None Run the whole process: create a class instance, simulate for horizontal and vertical kicks, exit. View Source @logger . catch def main () -> None : \"\"\" Run the whole process: create a class instance, simulate for horizontal and vertical kicks, exit. \"\"\" command_line_args = _parse_arguments () _set_logger_level ( command_line_args . log_level ) simulations = ACDipoleGrid () simulations . _check_input_sanity () simulations . _create_output_dirs () sim_type = command_line_args . simulation_type try : if sim_type == \"kick\" : logger . info ( f \"Planes to kick then track on are: {simulations.run_planes}\" ) logger . info ( f \"Kick values to compute are (in bunch sigmas): {simulations.sigmas}\" ) for plane in simulations . run_planes : simulations . track_forced_oscillations_for_plane ( kick_plane = plane ) elif sim_type == \"amp\" : logger . info ( f \"Planes to offset then track on are: {simulations.run_planes}\" ) logger . info ( f \"Registered initial tracking amplitudes (in bunch sigmas): {simulations.sigmas}\" ) for plane in simulations . run_planes : simulations . track_free_oscillations_for_plane ( kick_plane = plane ) else : logger . error ( f \"Simulation type {sim_type} is not a valid value\" ) raise ValueError ( \"Simulation type should be one of 'kick' or 'amp'\" ) except KeyboardInterrupt : logger . info ( \"Manual interruption, ending processes\" ) _cleanup_madx_residuals () logger . warning ( \"The 'grid_outputs' folder was left untouched, check for unexpected MADX residuals\" ) run_madx_mask def run_madx_mask ( mask_file : pathlib . Path ) -> None Run madx on the provided file. Parameters: Name Type Description Default mask_file Path Path object with the mask file location. None View Source def run_madx_mask ( mask_file : Path ) -> None : \"\"\" Run madx on the provided file. Args: mask_file (Path): Path object with the mask file location. \"\"\" logger . debug ( f \"Running madx on script: '{mask_file.absolute()}'\" ) exit_code , std_out = CommandLine . run ( f \"madx {mask_file.absolute()}\" ) if exit_code != 0 : # Dump madx log in case of failure so we can see where it went wrong . logger . warning ( f \"MAD-X command self-killed with exit code: {exit_code}\" ) log_dump = Path ( f \"failed_madx_returnedcode_{exit_code}.log\" ) with log_dump . open ( \"w\" ) as logfile : logfile . write ( std_out . decode ()) # Default 'utf-8' encoding , depends on your system . logger . warning ( f \"The standard output has been dumped to file 'failed_command_{exit_code}.logfile'\" ) Classes ACDipoleGrid class ACDipoleGrid ( ) View Source class ACDipoleGrid : \"\"\" Algorithm as a class to run the simulations and organize the outputs. \"\"\" __slots__ = { \"grid_output_dir\" : \"PosixPath object to the directory for all outputs\" , \"mask_files_dir\" : \"PosixPath object to the directory for simulations' masks\" , \"outputdata_dir\" : \"PosixPath object to the directory for simulation data\" , \"trackfiles_dir\" : \"PosixPath object to the directory for trackone files\" , \"trackfiles_planes\" : \"PosixPath objects to the planes' trackone files\" , \"run_planes\" : \"List of planes for which to run simulations\" , \"sigmas\" : \"List of amplitudes for AC dipole to kick to (in bunch sigma)\" , \"template_file\" : \"PosixPath object to the location of the mask template\" , \"template_str\" : \"Text in the template_file\" , } def __init__ ( self ) -> None : self . grid_output_dir : Path = Path ( \"grid_outputs\" ) self . mask_files_dir : Path = self . grid_output_dir / \"mask_files\" self . outputdata_dir : Path = self . grid_output_dir / \"outputdata_dirs\" self . trackfiles_dir : Path = self . grid_output_dir / \"trackfiles\" self . trackfiles_planes : Dict [ str, Path ] = { \"horizontal\" : self . grid_output_dir / \"trackfiles\" / \"X\" , \"vertical\" : self . grid_output_dir / \"trackfiles\" / \"Y\" , } self . run_planes : List [ str ] = _parse_arguments (). planes self . sigmas : List [ float ] = sorted ( _parse_arguments (). sigmas ) self . template_file : Path = Path ( _parse_arguments (). template ) self . template_str : str = self . template_file . read_text () def _check_input_sanity ( self ) -> None : \"\"\" Makes sure there are no duplicates in the provided sigma values, because that will mess up a long time after launch and you will cry. \"\"\" if len ( self . sigmas ) != len ( set ( self . sigmas )) : logger . error ( \"There is a duplicate in the sigma values, which would cause a failure later.\" ) sys . exit () def _create_output_dirs ( self ) -> None : \"\"\" Will create the proper output dirs if they don't exist already. \"\"\" if not self . grid_output_dir . is_dir () : logger . debug ( f \"Creating directory '{self.grid_output_dir}'\" ) self . grid_output_dir . mkdir () if not self . mask_files_dir . is_dir () : logger . debug ( f \"Creating directory '{self.mask_files_dir}'\" ) self . mask_files_dir . mkdir () if not self . outputdata_dir . is_dir () : logger . debug ( f \"Creating directory '{self.outputdata_dir}'\" ) self . outputdata_dir . mkdir () if not self . trackfiles_dir . is_dir () : logger . debug ( f \"Creating directory '{self.trackfiles_dir}'\" ) self . trackfiles_dir . mkdir () if not self . trackfiles_planes [ \"horizontal\" ] . is_dir () : logger . debug ( f \"Creating directory '{self.trackfiles_planes['horizontal']}'\" ) self . trackfiles_planes [ \"horizontal\" ] . mkdir () if not self . trackfiles_planes [ \"vertical\" ] . is_dir () : logger . debug ( f \"Creating directory '{self.trackfiles_planes['vertical']}'\" ) self . trackfiles_planes [ \"vertical\" ] . mkdir () else : logger . error ( \"Output directories already present, you may want to move those.\" ) sys . exit () def track_forced_oscillations_for_plane ( self , kick_plane : str = None ) -> None : \"\"\" Run MAD-X simulations with AC dipole tracking for the given plane, and handle outputs. Args: kick_plane: the name of the plane on which to apply a kick, either 'horizontal' or 'vertical'. \"\"\" if kick_plane not in ( \"horizontal\" , \"vertical\" ) : logger . error ( f \"Plane parameter {kick_plane} is not a valid value\" ) raise ValueError ( \"Plane parameter should be one of 'horizontal' or 'vertical'\" ) with timeit ( lambda spanned : logger . info ( f \"Tracked all amplitudes for {kick_plane} kicks in {spanned:.4f} seconds\" ) ) : for kick_in_sigma in self . sigmas : print ( \"\" ) # Set the wanted kick value ( in sigmas ) in the given plane , and a small initial # offset in the other ( small offset so that harpy doesn 't cry and we get tune). # Do NOT kick both planes: cross-terms influence the detuning. plane_letter = \"x\" if kick_plane == \"horizontal\" else \"y\" replace_dict = ( { \"%(SIGMAX_VALUE)s\": kick_in_sigma, \"%(SIGMAY_VALUE)s\": 0, \"%(AMPLX_VALUE)s\": 0, \"%(AMPLY_VALUE)s\": 0.5, } if kick_plane == \"horizontal\" else { \"%(SIGMAX_VALUE)s\": 0, \"%(SIGMAY_VALUE)s\": kick_in_sigma, \"%(AMPLX_VALUE)s\": 0.5, \"%(AMPLY_VALUE)s\": 0, } ) filename_to_write = Path(f\"sext_ac_dipole_tracking_{kick_in_sigma}_sigma_{plane_letter}_kick\") mask_file = create_script_file( self.template_str, values_replacing_dict=replace_dict, filename=Path(str(filename_to_write)), ) run_madx_mask(mask_file) _move_mask_file_after_running(mask_file_path=mask_file, mask_files_dir=self.mask_files_dir) _rename_madx_outputs( kick_in_sigma=kick_in_sigma, outputdata_dir=self.outputdata_dir, plane=plane_letter, ) _convert_trackone_to_sdds() _move_trackone_sdds( kick_in_sigma=kick_in_sigma, trackfiles_dir=self.trackfiles_planes[kick_plane], plane=plane_letter, ) def track_free_oscillations_for_plane(self, kick_plane: str = None) -> None: \"\"\" Run MAD-X simulations with amplitude offset tracking for the given plane, and handle outputs. Args: kick_plane: the name of the plane on which to apply an offset, either ' horizontal ' or ' vertical '. \"\"\" if kick_plane not in (\"horizontal\", \"vertical\"): logger.error(f\"Plane parameter {kick_plane} is not a valid value\") raise ValueError(\"Plane parameter should be one of ' horizontal ' or ' vertical ' \") with timeit( lambda spanned: logger.info( f\" Tracked all amplitudes for { kick_plane } offsets in { spanned : .4 f } seconds \" ) ): for kick_in_sigma in self.sigmas: print(\"\") plane_letter = \" x \" if kick_plane == \" horizontal \" else \" y \" action_var_value = kick_in_sigma amplitudes_dict = ( {\" % ( AMPLX_VALUE ) s \": action_var_value, \" % ( AMPLY_VALUE ) s \": 0.5} if kick_plane == \" horizontal \" else {\" % ( AMPLX_VALUE ) s \": 0.5, \" % ( AMPLY_VALUE ) s \": action_var_value} ) filename_to_write = Path( f\" initial_amplitude_tracking_ { kick_in_sigma } _sigma_ { plane_letter } _kick \" ) mask_file = create_script_file ( self . template_str , values_replacing_dict = amplitudes_dict , filename = Path ( str ( filename_to_write )), ) run_madx_mask ( mask_file ) _move_mask_file_after_running ( mask_file_path = mask_file , mask_files_dir = self . mask_files_dir ) _rename_madx_outputs ( kick_in_sigma = kick_in_sigma , outputdata_dir = self . outputdata_dir , plane = plane_letter , ) _convert_trackone_to_sdds () _move_trackone_sdds ( kick_in_sigma = kick_in_sigma , trackfiles_dir = self . trackfiles_planes [ kick_plane ] , plane = plane_letter , ) Instance variables grid_output_dir mask_files_dir outputdata_dir run_planes sigmas template_file template_str trackfiles_dir trackfiles_planes Methods track_forced_oscillations_for_plane def track_forced_oscillations_for_plane ( self , kick_plane : str = None ) -> None Run MAD-X simulations with AC dipole tracking for the given plane, and handle outputs. Parameters: Name Type Description Default kick_plane None the name of the plane on which to apply a kick, either 'horizontal' or 'vertical'. None View Source def track_forced_oscillations_for_plane ( self , kick_plane : str = None ) -> None : \"\"\" Run MAD-X simulations with AC dipole tracking for the given plane, and handle outputs. Args: kick_plane: the name of the plane on which to apply a kick, either 'horizontal' or 'vertical'. \"\"\" if kick_plane not in ( \"horizontal\" , \"vertical\" ) : logger . error ( f \"Plane parameter {kick_plane} is not a valid value\" ) raise ValueError ( \"Plane parameter should be one of 'horizontal' or 'vertical'\" ) with timeit ( lambda spanned : logger . info ( f \"Tracked all amplitudes for {kick_plane} kicks in {spanned:.4f} seconds\" ) ) : for kick_in_sigma in self . sigmas : print ( \"\" ) # Set the wanted kick value ( in sigmas ) in the given plane , and a small initial # offset in the other ( small offset so that harpy doesn ' t cry and we get tune ). # Do NOT kick both planes : cross - terms influence the detuning . plane_letter = \"x\" if kick_plane == \"horizontal\" else \"y\" replace_dict = ( { \"%(SIGMAX_VALUE)s\" : kick_in_sigma , \"%(SIGMAY_VALUE)s\" : 0 , \"%(AMPLX_VALUE)s\" : 0 , \"%(AMPLY_VALUE)s\" : 0.5 , } if kick_plane == \"horizontal\" else { \"%(SIGMAX_VALUE)s\" : 0 , \"%(SIGMAY_VALUE)s\" : kick_in_sigma , \"%(AMPLX_VALUE)s\" : 0.5 , \"%(AMPLY_VALUE)s\" : 0 , } ) filename_to_write = Path ( f \"sext_ac_dipole_tracking_{kick_in_sigma}_sigma_{plane_letter}_kick\" ) mask_file = create_script_file ( self . template_str , values_replacing_dict = replace_dict , filename = Path ( str ( filename_to_write )), ) run_madx_mask ( mask_file ) _move_mask_file_after_running ( mask_file_path = mask_file , mask_files_dir = self . mask_files_dir ) _rename_madx_outputs ( kick_in_sigma = kick_in_sigma , outputdata_dir = self . outputdata_dir , plane = plane_letter , ) _convert_trackone_to_sdds () _move_trackone_sdds ( kick_in_sigma = kick_in_sigma , trackfiles_dir = self . trackfiles_planes [ kick_plane ] , plane = plane_letter , ) track_free_oscillations_for_plane def track_free_oscillations_for_plane ( self , kick_plane : str = None ) -> None Run MAD-X simulations with amplitude offset tracking for the given plane, and handle outputs. Parameters: Name Type Description Default kick_plane None the name of the plane on which to apply an offset, either 'horizontal' or 'vertical'. None View Source def track_free_oscillations_for_plane ( self , kick_plane : str = None ) -> None : \"\"\" Run MAD-X simulations with amplitude offset tracking for the given plane, and handle outputs. Args: kick_plane: the name of the plane on which to apply an offset, either 'horizontal' or 'vertical'. \"\"\" if kick_plane not in ( \"horizontal\" , \"vertical\" ) : logger . error ( f \"Plane parameter {kick_plane} is not a valid value\" ) raise ValueError ( \"Plane parameter should be one of 'horizontal' or 'vertical'\" ) with timeit ( lambda spanned : logger . info ( f \"Tracked all amplitudes for {kick_plane} offsets in {spanned:.4f} seconds\" ) ) : for kick_in_sigma in self . sigmas : print ( \"\" ) plane_letter = \"x\" if kick_plane == \"horizontal\" else \"y\" action_var_value = kick_in_sigma amplitudes_dict = ( { \"%(AMPLX_VALUE)s\" : action_var_value , \"%(AMPLY_VALUE)s\" : 0.5 } if kick_plane == \"horizontal\" else { \"%(AMPLX_VALUE)s\" : 0.5 , \"%(AMPLY_VALUE)s\" : action_var_value } ) filename_to_write = Path ( f \"initial_amplitude_tracking_{kick_in_sigma}_sigma_{plane_letter}_kick\" ) mask_file = create_script_file ( self . template_str , values_replacing_dict = amplitudes_dict , filename = Path ( str ( filename_to_write )), ) run_madx_mask ( mask_file ) _move_mask_file_after_running ( mask_file_path = mask_file , mask_files_dir = self . mask_files_dir ) _rename_madx_outputs ( kick_in_sigma = kick_in_sigma , outputdata_dir = self . outputdata_dir , plane = plane_letter , ) _convert_trackone_to_sdds () _move_trackone_sdds ( kick_in_sigma = kick_in_sigma , trackfiles_dir = self . trackfiles_planes [ kick_plane ] , plane = plane_letter , )","title":"Sext Ac Dipole Tracking"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/#module-pyhdtoolkitscriptsac_dipolesext_ac_dipole_tracking","text":"Script scripts.ac_dipole.sext_ac_dipole_tracking Created on 2020.02.27 View Source \"\"\" Script scripts.ac_dipole.sext_ac_dipole_tracking ------------------------------------------------ Created on 2020.02.27 :author: Felix Soubelet (felix.soubelet@cern.ch) This is a Python3 utility to launch a series of MAD-X simlations with the proper parameters, call the appropriate python scripts on the outputs and organise the results. Made to be ran with the OMC conda environment, and ran directly for the commandline. A pair of examples ================== Running with kicks in the horizontal plane only, for two sigma values: python path/to/sext_ac_dipole_tracking.py --planes horizontal --mask /path/to/kick/mask.mask \\ --type kick --sigmas 5 10 Running with free oscillations in both planes succesively, for many offset values: python path/to/sext_ac_dipole_tracking.py --planes horizontal vertical \\ --mask /path/to/offset/mask.mask --type amp -- sigmas 5 10 15 20 \"\"\" import argparse import shutil import sys from pathlib import Path from typing import Dict , List , Union from loguru import logger from pyhdtoolkit.utils.cmdline import CommandLine from pyhdtoolkit.utils.contexts import timeit from pyhdtoolkit.utils.defaults import LOGURU_FORMAT , OMC_PYTHON , TBT_CONVERTER_SCRIPT class ACDipoleGrid : \"\"\" Algorithm as a class to run the simulations and organize the outputs. \"\"\" __slots__ = { \"grid_output_dir\" : \"PosixPath object to the directory for all outputs\" , \"mask_files_dir\" : \"PosixPath object to the directory for simulations' masks\" , \"outputdata_dir\" : \"PosixPath object to the directory for simulation data\" , \"trackfiles_dir\" : \"PosixPath object to the directory for trackone files\" , \"trackfiles_planes\" : \"PosixPath objects to the planes' trackone files\" , \"run_planes\" : \"List of planes for which to run simulations\" , \"sigmas\" : \"List of amplitudes for AC dipole to kick to (in bunch sigma)\" , \"template_file\" : \"PosixPath object to the location of the mask template\" , \"template_str\" : \"Text in the template_file\" , } def __init__ ( self ) -> None : self . grid_output_dir : Path = Path ( \"grid_outputs\" ) self . mask_files_dir : Path = self . grid_output_dir / \"mask_files\" self . outputdata_dir : Path = self . grid_output_dir / \"outputdata_dirs\" self . trackfiles_dir : Path = self . grid_output_dir / \"trackfiles\" self . trackfiles_planes : Dict [ str , Path ] = { \"horizontal\" : self . grid_output_dir / \"trackfiles\" / \"X\" , \"vertical\" : self . grid_output_dir / \"trackfiles\" / \"Y\" , } self . run_planes : List [ str ] = _parse_arguments () . planes self . sigmas : List [ float ] = sorted ( _parse_arguments () . sigmas ) self . template_file : Path = Path ( _parse_arguments () . template ) self . template_str : str = self . template_file . read_text () def _check_input_sanity ( self ) -> None : \"\"\" Makes sure there are no duplicates in the provided sigma values, because that will mess up a long time after launch and you will cry. \"\"\" if len ( self . sigmas ) != len ( set ( self . sigmas )): logger . error ( \"There is a duplicate in the sigma values, which would cause a failure later.\" ) sys . exit () def _create_output_dirs ( self ) -> None : \"\"\" Will create the proper output dirs if they don't exist already. \"\"\" if not self . grid_output_dir . is_dir (): logger . debug ( f \"Creating directory '{self.grid_output_dir}'\" ) self . grid_output_dir . mkdir () if not self . mask_files_dir . is_dir (): logger . debug ( f \"Creating directory '{self.mask_files_dir}'\" ) self . mask_files_dir . mkdir () if not self . outputdata_dir . is_dir (): logger . debug ( f \"Creating directory '{self.outputdata_dir}'\" ) self . outputdata_dir . mkdir () if not self . trackfiles_dir . is_dir (): logger . debug ( f \"Creating directory '{self.trackfiles_dir}'\" ) self . trackfiles_dir . mkdir () if not self . trackfiles_planes [ \"horizontal\" ] . is_dir (): logger . debug ( f \"Creating directory '{self.trackfiles_planes['horizontal']}'\" ) self . trackfiles_planes [ \"horizontal\" ] . mkdir () if not self . trackfiles_planes [ \"vertical\" ] . is_dir (): logger . debug ( f \"Creating directory '{self.trackfiles_planes['vertical']}'\" ) self . trackfiles_planes [ \"vertical\" ] . mkdir () else : logger . error ( \"Output directories already present, you may want to move those.\" ) sys . exit () def track_forced_oscillations_for_plane ( self , kick_plane : str = None ) -> None : \"\"\" Run MAD-X simulations with AC dipole tracking for the given plane, and handle outputs. Args: kick_plane: the name of the plane on which to apply a kick, either 'horizontal' or 'vertical'. \"\"\" if kick_plane not in ( \"horizontal\" , \"vertical\" ): logger . error ( f \"Plane parameter {kick_plane} is not a valid value\" ) raise ValueError ( \"Plane parameter should be one of 'horizontal' or 'vertical'\" ) with timeit ( lambda spanned : logger . info ( f \"Tracked all amplitudes for {kick_plane} kicks in {spanned:.4f} seconds\" ) ): for kick_in_sigma in self . sigmas : print ( \"\" ) # Set the wanted kick value (in sigmas) in the given plane, and a small initial # offset in the other (small offset so that harpy doesn't cry and we get tune). # Do NOT kick both planes: cross-terms influence the detuning. plane_letter = \"x\" if kick_plane == \"horizontal\" else \"y\" replace_dict = ( { \" %(SIGMAX_VALUE)s \" : kick_in_sigma , \" %(SIGMAY_VALUE)s \" : 0 , \" %(AMPLX_VALUE)s \" : 0 , \" %(AMPLY_VALUE)s \" : 0.5 , } if kick_plane == \"horizontal\" else { \" %(SIGMAX_VALUE)s \" : 0 , \" %(SIGMAY_VALUE)s \" : kick_in_sigma , \" %(AMPLX_VALUE)s \" : 0.5 , \" %(AMPLY_VALUE)s \" : 0 , } ) filename_to_write = Path ( f \"sext_ac_dipole_tracking_{kick_in_sigma}_sigma_{plane_letter}_kick\" ) mask_file = create_script_file ( self . template_str , values_replacing_dict = replace_dict , filename = Path ( str ( filename_to_write )), ) run_madx_mask ( mask_file ) _move_mask_file_after_running ( mask_file_path = mask_file , mask_files_dir = self . mask_files_dir ) _rename_madx_outputs ( kick_in_sigma = kick_in_sigma , outputdata_dir = self . outputdata_dir , plane = plane_letter , ) _convert_trackone_to_sdds () _move_trackone_sdds ( kick_in_sigma = kick_in_sigma , trackfiles_dir = self . trackfiles_planes [ kick_plane ], plane = plane_letter , ) def track_free_oscillations_for_plane ( self , kick_plane : str = None ) -> None : \"\"\" Run MAD-X simulations with amplitude offset tracking for the given plane, and handle outputs. Args: kick_plane: the name of the plane on which to apply an offset, either 'horizontal' or 'vertical'. \"\"\" if kick_plane not in ( \"horizontal\" , \"vertical\" ): logger . error ( f \"Plane parameter {kick_plane} is not a valid value\" ) raise ValueError ( \"Plane parameter should be one of 'horizontal' or 'vertical'\" ) with timeit ( lambda spanned : logger . info ( f \"Tracked all amplitudes for {kick_plane} offsets in {spanned:.4f} seconds\" ) ): for kick_in_sigma in self . sigmas : print ( \"\" ) plane_letter = \"x\" if kick_plane == \"horizontal\" else \"y\" action_var_value = kick_in_sigma amplitudes_dict = ( { \" %(AMPLX_VALUE)s \" : action_var_value , \" %(AMPLY_VALUE)s \" : 0.5 } if kick_plane == \"horizontal\" else { \" %(AMPLX_VALUE)s \" : 0.5 , \" %(AMPLY_VALUE)s \" : action_var_value } ) filename_to_write = Path ( f \"initial_amplitude_tracking_{kick_in_sigma}_sigma_{plane_letter}_kick\" ) mask_file = create_script_file ( self . template_str , values_replacing_dict = amplitudes_dict , filename = Path ( str ( filename_to_write )), ) run_madx_mask ( mask_file ) _move_mask_file_after_running ( mask_file_path = mask_file , mask_files_dir = self . mask_files_dir ) _rename_madx_outputs ( kick_in_sigma = kick_in_sigma , outputdata_dir = self . outputdata_dir , plane = plane_letter , ) _convert_trackone_to_sdds () _move_trackone_sdds ( kick_in_sigma = kick_in_sigma , trackfiles_dir = self . trackfiles_planes [ kick_plane ], plane = plane_letter , ) @logger.catch def main () -> None : \"\"\" Run the whole process: create a class instance, simulate for horizontal and vertical kicks, exit. \"\"\" command_line_args = _parse_arguments () _set_logger_level ( command_line_args . log_level ) simulations = ACDipoleGrid () simulations . _check_input_sanity () simulations . _create_output_dirs () sim_type = command_line_args . simulation_type try : if sim_type == \"kick\" : logger . info ( f \"Planes to kick then track on are: {simulations.run_planes}\" ) logger . info ( f \"Kick values to compute are (in bunch sigmas): {simulations.sigmas}\" ) for plane in simulations . run_planes : simulations . track_forced_oscillations_for_plane ( kick_plane = plane ) elif sim_type == \"amp\" : logger . info ( f \"Planes to offset then track on are: {simulations.run_planes}\" ) logger . info ( f \"Registered initial tracking amplitudes (in bunch sigmas): {simulations.sigmas}\" ) for plane in simulations . run_planes : simulations . track_free_oscillations_for_plane ( kick_plane = plane ) else : logger . error ( f \"Simulation type {sim_type} is not a valid value\" ) raise ValueError ( \"Simulation type should be one of 'kick' or 'amp'\" ) except KeyboardInterrupt : logger . info ( \"Manual interruption, ending processes\" ) _cleanup_madx_residuals () logger . warning ( \"The 'grid_outputs' folder was left untouched, check for unexpected MADX residuals\" ) # ---------------------- Public Utilities ---------------------- # def run_madx_mask ( mask_file : Path ) -> None : \"\"\" Run madx on the provided file. Args: mask_file (Path): Path object with the mask file location. \"\"\" logger . debug ( f \"Running madx on script: '{mask_file.absolute()}'\" ) exit_code , std_out = CommandLine . run ( f \"madx {mask_file.absolute()}\" ) if exit_code != 0 : # Dump madx log in case of failure so we can see where it went wrong. logger . warning ( f \"MAD-X command self-killed with exit code: {exit_code}\" ) log_dump = Path ( f \"failed_madx_returnedcode_{exit_code}.log\" ) with log_dump . open ( \"w\" ) as logfile : logfile . write ( std_out . decode ()) # Default 'utf-8' encoding, depends on your system. logger . warning ( f \"The standard output has been dumped to file 'failed_command_{exit_code}.logfile'\" ) def create_script_file ( template_as_str : str , values_replacing_dict : Dict [ str , float ], filename : Path , ) -> Path : \"\"\" Create new script file from template with the appropriate values. Args: template_as_str (str): string content of your template mask file. values_replacing_dict (Dict[str, float]): keys to find and values to replace them with in the template. filename (Path): Path object for the file in which to write the script. \"\"\" string_mask = _create_script_string ( template_as_str , values_replacing_dict ) return _write_script_to_file ( string_mask , filename ) # ---------------------- Private Utilities ---------------------- # def _convert_trackone_to_sdds () -> None : \"\"\" Run the omc3 tbt_converter script on trackone output of MAD-X. Will also cleanup the `converter` and 'stats' files left by tbt_converter afterwards. \"\"\" if not Path ( \"trackone\" ) . is_file (): logger . error ( \"Tried to call 'tbt_converter' without a 'trackone' file present, aborting\" ) sys . exit () logger . debug ( f \"Running '{TBT_CONVERTER_SCRIPT}' on 'trackone' file\" ) CommandLine . run ( f \"{OMC_PYTHON.absolute()} {TBT_CONVERTER_SCRIPT.absolute()} \" \"--files trackone --outputdir . --tbt_datatype trackone\" ) logger . debug ( \"Removing trackone file 'trackone'\" ) Path ( \"trackone\" ) . unlink () logger . debug ( \"Removing outputs of 'tbt_converter'\" ) if Path ( \"stats.txt\" ) . exists (): Path ( \"stats.txt\" ) . unlink () for tbt_output_file in list ( Path ( \".\" ) . glob ( \"converter_*\" )): tbt_output_file . unlink () def _create_script_string ( template_as_string : str , values_replacing_dict : Dict [ str , float ]) -> str : \"\"\" For each key in the provided dict, will replace it in the template scripts with the corresponding dict value. Args: template_as_string (str): the string content of your template mask file. values_replacing_dict (Dict[str, float]): pairs of key, value to find and replace in the template string. Returns: The new script string. \"\"\" script_string : str = template_as_string for key , value in values_replacing_dict . items (): script_string = script_string . replace ( str ( key ), str ( value )) return script_string def _move_mask_file_after_running ( mask_file_path : Path , mask_files_dir : Path ) -> None : \"\"\" Move the mask file after being done running it with MAD-X. Args: mask_file_path (Path): Path object with the file location. mask_files_dir (Path): Path object with the directory to move mask to' location. \"\"\" logger . debug ( f \"Moving mask file '{mask_file_path}' to directory '{mask_files_dir}'\" ) mask_file_path . rename ( f \"{mask_files_dir}/{mask_file_path}\" ) def _move_trackone_sdds ( kick_in_sigma : Union [ str , float ], trackfiles_dir : Path , plane : str ) -> None : \"\"\" Call after running omc3's `tbt_converter` on the `trackone` output by MAD-X, will move the resulting `trackone.sdds` file to the `trackfiles_dir`, with a naming reflecting the ac dipole kick strength. Args: kick_in_sigma (Union[str, float]): the AC dipole kick value (in sigma) for which you ran your simulation. trackfiles_dir (Path): PosixPath to the folder in which to store all sdds trackone files. plane (str): the plane on which ac dipole provided the kick, should be `x` or `y`. \"\"\" if str ( plane ) not in ( \"x\" , \"y\" ): logger . error ( f \"Plane parameter {plane} is not a valid value\" ) raise ValueError ( \"Plane parameter should be one of 'x' or 'y'\" ) logger . debug ( f \"Moving trackone sdds file to directory '{trackfiles_dir}'\" ) track_sdds_file = Path ( \"trackone.sdds\" ) if not track_sdds_file . is_file (): logger . error ( \"Conversion to trackone sdds file must have failed, check the omc3 script\" ) sys . exit () track_sdds_file . rename ( f \"{trackfiles_dir}/trackone_{kick_in_sigma}_sigma_{plane}.sdds\" ) def _parse_arguments () -> argparse . Namespace : \"\"\" Simple argument parser to make life easier in the command-line. Returns a NameSpace with arguments as attributes. \"\"\" parser = argparse . ArgumentParser ( description = \"Running MAD-X AC dipole trackings for you.\" ) parser . add_argument ( \"-s\" , \"--sigmas\" , dest = \"sigmas\" , nargs = \"+\" , default = [ 1 , 2 ], type = float , help = \"Different amplitude values (in bunch sigma) for the AC dipole kicks.\" \"Defaults to [1, 2].\" , ) parser . add_argument ( \"-p\" , \"--planes\" , dest = \"planes\" , nargs = \"+\" , default = [ \"horizontal\" ], type = str , help = \"Planes for which to kick, possible values are 'horizontal' and 'vertical',\" \"Defaults to 'horizontal'.\" , ) parser . add_argument ( \"-m\" , \"--mask\" , dest = \"template\" , default = \"/afs/cern.ch/work/f/fesoubel/public/MADX_scripts/AC_dipole_tracking/ac_kick_tracking_template.mask\" , type = str , help = \"Location of your MAD-X template mask file to use, defaults to \" \"'/afs/cern.ch/work/f/fesoubel/public/MADX_scripts/AC_dipole_tracking/ac_kick_track_template.mask'.\" , ) parser . add_argument ( \"-t\" , \"--type\" , dest = \"simulation_type\" , default = \"kick\" , type = str , help = \"Type of simulations to run, either 'kick' for AC dipole kick or 'amp' for free \" \"oscillations. Defaults to 'kick'.\" , ) parser . add_argument ( \"-l\" , \"--logs\" , dest = \"log_level\" , default = \"info\" , type = str , help = \"The base console logging level. Can be 'debug', 'info', 'warning' and 'error'.\" \"Defaults to 'info'.\" , ) return parser . parse_args () def _rename_madx_outputs ( kick_in_sigma : Union [ str , float ], outputdata_dir : Path , plane : str ) -> None : \"\"\" Call after running MAD-X on your mask, will move the 'Outpudata' created by MAD-X to the proper place. Args: kick_in_sigma (Union[str, float]): the AC dipole kick value (in sigma) for which you ran your simulation. outputdata_dir (Path): PosixPath to the folder in which to store all successive `Outputdata`'s location. plane (str): the plane on which ac dipole provided the kick, should be `x` or `y`. \"\"\" if str ( plane ) not in ( \"x\" , \"y\" ): raise ValueError ( f \"Plane parameter should be one of 'x', 'y' but {plane} was provided.\" ) madx_outputs = Path ( \"Outputdata\" ) logger . debug ( f \"Moving MAD-X outputs to directory '{outputdata_dir}'\" ) madx_outputs . rename ( f \"{outputdata_dir}/Outputdata_{kick_in_sigma}_sigma_{plane}\" ) def _set_logger_level ( log_level : str = \"info\" ) -> None : \"\"\" Sets the logger level to the one provided at the commandline. Default loguru handler will have DEBUG level and ID 0. We need to first remove this default handler and add ours with the wanted level. Args: log_level (str): the default logging level to print out. \"\"\" logger . remove ( 0 ) logger . add ( sys . stderr , format = LOGURU_FORMAT , level = log_level . upper ()) def _write_script_to_file ( script_as_string : str , filename : Union [ str , Path ]) -> Path : \"\"\" Create a new file with the provided script, and return the location. Args: script_as_string (str): the script string to write to file. filename (Union[str, Path]): the file name to use. Returns: The `pathlib.Path` object to the created file. \"\"\" file_path = Path ( str ( filename ) + \".mask\" ) logger . debug ( f \"Creating new mask file '{file_path}'\" ) with file_path . open ( \"w\" ) as script : script . write ( script_as_string ) return file_path def _cleanup_madx_residuals () -> None : \"\"\" Will look for specific madx artifacts and remove them. Meant to be called in case of interuption. \"\"\" expected_residuals : Dict [ str , List [ str ]] = { \"symlinks\" : [ \"db5\" , \"slhc\" , \"fidel\" , \"wise\" , \"optics2016\" , \"optics2017\" , \"optics2018\" , \"scripts\" ,], \"directories\" : [ \"temp\" , \"Outputdata\" ], \"files\" : [ \"fort.18\" ], } logger . debug ( \"Cleaning up expected MADX residuals\" ) for residual_type , residual_values in expected_residuals . items (): logger . trace ( f \"Cleaning up MADX residual {residual_type}\" ) for residual in residual_values : if Path ( residual ) . is_symlink () or Path ( residual ) . is_file (): Path ( residual ) . unlink () elif Path ( residual ) . is_dir (): shutil . rmtree ( Path ( residual )) logger . debug ( \"Cleaning up potential residual mask files\" ) for suspect_mask in list ( Path ( \".\" ) . glob ( \"*.mask\" )): if ( suspect_mask . stem . startswith ( \"initial_\" ) or suspect_mask . stem . startswith ( \"sext_\" ) ) and suspect_mask . stem . endswith ( \"_kick\" ): suspect_mask . unlink () if __name__ == \"__main__\" : main ()","title":"Module pyhdtoolkit.scripts.ac_dipole.sext_ac_dipole_tracking"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/#variables","text":"LOGURU_FORMAT","title":"Variables"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/#create_script_file","text":"def create_script_file ( template_as_str : str , values_replacing_dict : Dict [ str , float ], filename : pathlib . Path ) -> pathlib . Path Create new script file from template with the appropriate values. Parameters: Name Type Description Default template_as_str str string content of your template mask file. None values_replacing_dict Dict[str, float] keys to find and values to replace them with in the template. None filename Path Path object for the file in which to write the script. None View Source def create_script_file ( template_as_str : str , values_replacing_dict : Dict [ str , float ], filename : Path , ) -> Path : \"\"\" Create new script file from template with the appropriate values. Args: template_as_str (str): string content of your template mask file. values_replacing_dict (Dict[str, float]): keys to find and values to replace them with in the template. filename (Path): Path object for the file in which to write the script. \"\"\" string_mask = _create_script_string ( template_as_str , values_replacing_dict ) return _write_script_to_file ( string_mask , filename )","title":"create_script_file"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/#main","text":"def main ( ) -> None Run the whole process: create a class instance, simulate for horizontal and vertical kicks, exit. View Source @logger . catch def main () -> None : \"\"\" Run the whole process: create a class instance, simulate for horizontal and vertical kicks, exit. \"\"\" command_line_args = _parse_arguments () _set_logger_level ( command_line_args . log_level ) simulations = ACDipoleGrid () simulations . _check_input_sanity () simulations . _create_output_dirs () sim_type = command_line_args . simulation_type try : if sim_type == \"kick\" : logger . info ( f \"Planes to kick then track on are: {simulations.run_planes}\" ) logger . info ( f \"Kick values to compute are (in bunch sigmas): {simulations.sigmas}\" ) for plane in simulations . run_planes : simulations . track_forced_oscillations_for_plane ( kick_plane = plane ) elif sim_type == \"amp\" : logger . info ( f \"Planes to offset then track on are: {simulations.run_planes}\" ) logger . info ( f \"Registered initial tracking amplitudes (in bunch sigmas): {simulations.sigmas}\" ) for plane in simulations . run_planes : simulations . track_free_oscillations_for_plane ( kick_plane = plane ) else : logger . error ( f \"Simulation type {sim_type} is not a valid value\" ) raise ValueError ( \"Simulation type should be one of 'kick' or 'amp'\" ) except KeyboardInterrupt : logger . info ( \"Manual interruption, ending processes\" ) _cleanup_madx_residuals () logger . warning ( \"The 'grid_outputs' folder was left untouched, check for unexpected MADX residuals\" )","title":"main"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/#run_madx_mask","text":"def run_madx_mask ( mask_file : pathlib . Path ) -> None Run madx on the provided file. Parameters: Name Type Description Default mask_file Path Path object with the mask file location. None View Source def run_madx_mask ( mask_file : Path ) -> None : \"\"\" Run madx on the provided file. Args: mask_file (Path): Path object with the mask file location. \"\"\" logger . debug ( f \"Running madx on script: '{mask_file.absolute()}'\" ) exit_code , std_out = CommandLine . run ( f \"madx {mask_file.absolute()}\" ) if exit_code != 0 : # Dump madx log in case of failure so we can see where it went wrong . logger . warning ( f \"MAD-X command self-killed with exit code: {exit_code}\" ) log_dump = Path ( f \"failed_madx_returnedcode_{exit_code}.log\" ) with log_dump . open ( \"w\" ) as logfile : logfile . write ( std_out . decode ()) # Default 'utf-8' encoding , depends on your system . logger . warning ( f \"The standard output has been dumped to file 'failed_command_{exit_code}.logfile'\" )","title":"run_madx_mask"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/#acdipolegrid","text":"class ACDipoleGrid ( ) View Source class ACDipoleGrid : \"\"\" Algorithm as a class to run the simulations and organize the outputs. \"\"\" __slots__ = { \"grid_output_dir\" : \"PosixPath object to the directory for all outputs\" , \"mask_files_dir\" : \"PosixPath object to the directory for simulations' masks\" , \"outputdata_dir\" : \"PosixPath object to the directory for simulation data\" , \"trackfiles_dir\" : \"PosixPath object to the directory for trackone files\" , \"trackfiles_planes\" : \"PosixPath objects to the planes' trackone files\" , \"run_planes\" : \"List of planes for which to run simulations\" , \"sigmas\" : \"List of amplitudes for AC dipole to kick to (in bunch sigma)\" , \"template_file\" : \"PosixPath object to the location of the mask template\" , \"template_str\" : \"Text in the template_file\" , } def __init__ ( self ) -> None : self . grid_output_dir : Path = Path ( \"grid_outputs\" ) self . mask_files_dir : Path = self . grid_output_dir / \"mask_files\" self . outputdata_dir : Path = self . grid_output_dir / \"outputdata_dirs\" self . trackfiles_dir : Path = self . grid_output_dir / \"trackfiles\" self . trackfiles_planes : Dict [ str, Path ] = { \"horizontal\" : self . grid_output_dir / \"trackfiles\" / \"X\" , \"vertical\" : self . grid_output_dir / \"trackfiles\" / \"Y\" , } self . run_planes : List [ str ] = _parse_arguments (). planes self . sigmas : List [ float ] = sorted ( _parse_arguments (). sigmas ) self . template_file : Path = Path ( _parse_arguments (). template ) self . template_str : str = self . template_file . read_text () def _check_input_sanity ( self ) -> None : \"\"\" Makes sure there are no duplicates in the provided sigma values, because that will mess up a long time after launch and you will cry. \"\"\" if len ( self . sigmas ) != len ( set ( self . sigmas )) : logger . error ( \"There is a duplicate in the sigma values, which would cause a failure later.\" ) sys . exit () def _create_output_dirs ( self ) -> None : \"\"\" Will create the proper output dirs if they don't exist already. \"\"\" if not self . grid_output_dir . is_dir () : logger . debug ( f \"Creating directory '{self.grid_output_dir}'\" ) self . grid_output_dir . mkdir () if not self . mask_files_dir . is_dir () : logger . debug ( f \"Creating directory '{self.mask_files_dir}'\" ) self . mask_files_dir . mkdir () if not self . outputdata_dir . is_dir () : logger . debug ( f \"Creating directory '{self.outputdata_dir}'\" ) self . outputdata_dir . mkdir () if not self . trackfiles_dir . is_dir () : logger . debug ( f \"Creating directory '{self.trackfiles_dir}'\" ) self . trackfiles_dir . mkdir () if not self . trackfiles_planes [ \"horizontal\" ] . is_dir () : logger . debug ( f \"Creating directory '{self.trackfiles_planes['horizontal']}'\" ) self . trackfiles_planes [ \"horizontal\" ] . mkdir () if not self . trackfiles_planes [ \"vertical\" ] . is_dir () : logger . debug ( f \"Creating directory '{self.trackfiles_planes['vertical']}'\" ) self . trackfiles_planes [ \"vertical\" ] . mkdir () else : logger . error ( \"Output directories already present, you may want to move those.\" ) sys . exit () def track_forced_oscillations_for_plane ( self , kick_plane : str = None ) -> None : \"\"\" Run MAD-X simulations with AC dipole tracking for the given plane, and handle outputs. Args: kick_plane: the name of the plane on which to apply a kick, either 'horizontal' or 'vertical'. \"\"\" if kick_plane not in ( \"horizontal\" , \"vertical\" ) : logger . error ( f \"Plane parameter {kick_plane} is not a valid value\" ) raise ValueError ( \"Plane parameter should be one of 'horizontal' or 'vertical'\" ) with timeit ( lambda spanned : logger . info ( f \"Tracked all amplitudes for {kick_plane} kicks in {spanned:.4f} seconds\" ) ) : for kick_in_sigma in self . sigmas : print ( \"\" ) # Set the wanted kick value ( in sigmas ) in the given plane , and a small initial # offset in the other ( small offset so that harpy doesn 't cry and we get tune). # Do NOT kick both planes: cross-terms influence the detuning. plane_letter = \"x\" if kick_plane == \"horizontal\" else \"y\" replace_dict = ( { \"%(SIGMAX_VALUE)s\": kick_in_sigma, \"%(SIGMAY_VALUE)s\": 0, \"%(AMPLX_VALUE)s\": 0, \"%(AMPLY_VALUE)s\": 0.5, } if kick_plane == \"horizontal\" else { \"%(SIGMAX_VALUE)s\": 0, \"%(SIGMAY_VALUE)s\": kick_in_sigma, \"%(AMPLX_VALUE)s\": 0.5, \"%(AMPLY_VALUE)s\": 0, } ) filename_to_write = Path(f\"sext_ac_dipole_tracking_{kick_in_sigma}_sigma_{plane_letter}_kick\") mask_file = create_script_file( self.template_str, values_replacing_dict=replace_dict, filename=Path(str(filename_to_write)), ) run_madx_mask(mask_file) _move_mask_file_after_running(mask_file_path=mask_file, mask_files_dir=self.mask_files_dir) _rename_madx_outputs( kick_in_sigma=kick_in_sigma, outputdata_dir=self.outputdata_dir, plane=plane_letter, ) _convert_trackone_to_sdds() _move_trackone_sdds( kick_in_sigma=kick_in_sigma, trackfiles_dir=self.trackfiles_planes[kick_plane], plane=plane_letter, ) def track_free_oscillations_for_plane(self, kick_plane: str = None) -> None: \"\"\" Run MAD-X simulations with amplitude offset tracking for the given plane, and handle outputs. Args: kick_plane: the name of the plane on which to apply an offset, either ' horizontal ' or ' vertical '. \"\"\" if kick_plane not in (\"horizontal\", \"vertical\"): logger.error(f\"Plane parameter {kick_plane} is not a valid value\") raise ValueError(\"Plane parameter should be one of ' horizontal ' or ' vertical ' \") with timeit( lambda spanned: logger.info( f\" Tracked all amplitudes for { kick_plane } offsets in { spanned : .4 f } seconds \" ) ): for kick_in_sigma in self.sigmas: print(\"\") plane_letter = \" x \" if kick_plane == \" horizontal \" else \" y \" action_var_value = kick_in_sigma amplitudes_dict = ( {\" % ( AMPLX_VALUE ) s \": action_var_value, \" % ( AMPLY_VALUE ) s \": 0.5} if kick_plane == \" horizontal \" else {\" % ( AMPLX_VALUE ) s \": 0.5, \" % ( AMPLY_VALUE ) s \": action_var_value} ) filename_to_write = Path( f\" initial_amplitude_tracking_ { kick_in_sigma } _sigma_ { plane_letter } _kick \" ) mask_file = create_script_file ( self . template_str , values_replacing_dict = amplitudes_dict , filename = Path ( str ( filename_to_write )), ) run_madx_mask ( mask_file ) _move_mask_file_after_running ( mask_file_path = mask_file , mask_files_dir = self . mask_files_dir ) _rename_madx_outputs ( kick_in_sigma = kick_in_sigma , outputdata_dir = self . outputdata_dir , plane = plane_letter , ) _convert_trackone_to_sdds () _move_trackone_sdds ( kick_in_sigma = kick_in_sigma , trackfiles_dir = self . trackfiles_planes [ kick_plane ] , plane = plane_letter , )","title":"ACDipoleGrid"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/#instance-variables","text":"grid_output_dir mask_files_dir outputdata_dir run_planes sigmas template_file template_str trackfiles_dir trackfiles_planes","title":"Instance variables"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/#methods","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/#track_forced_oscillations_for_plane","text":"def track_forced_oscillations_for_plane ( self , kick_plane : str = None ) -> None Run MAD-X simulations with AC dipole tracking for the given plane, and handle outputs. Parameters: Name Type Description Default kick_plane None the name of the plane on which to apply a kick, either 'horizontal' or 'vertical'. None View Source def track_forced_oscillations_for_plane ( self , kick_plane : str = None ) -> None : \"\"\" Run MAD-X simulations with AC dipole tracking for the given plane, and handle outputs. Args: kick_plane: the name of the plane on which to apply a kick, either 'horizontal' or 'vertical'. \"\"\" if kick_plane not in ( \"horizontal\" , \"vertical\" ) : logger . error ( f \"Plane parameter {kick_plane} is not a valid value\" ) raise ValueError ( \"Plane parameter should be one of 'horizontal' or 'vertical'\" ) with timeit ( lambda spanned : logger . info ( f \"Tracked all amplitudes for {kick_plane} kicks in {spanned:.4f} seconds\" ) ) : for kick_in_sigma in self . sigmas : print ( \"\" ) # Set the wanted kick value ( in sigmas ) in the given plane , and a small initial # offset in the other ( small offset so that harpy doesn ' t cry and we get tune ). # Do NOT kick both planes : cross - terms influence the detuning . plane_letter = \"x\" if kick_plane == \"horizontal\" else \"y\" replace_dict = ( { \"%(SIGMAX_VALUE)s\" : kick_in_sigma , \"%(SIGMAY_VALUE)s\" : 0 , \"%(AMPLX_VALUE)s\" : 0 , \"%(AMPLY_VALUE)s\" : 0.5 , } if kick_plane == \"horizontal\" else { \"%(SIGMAX_VALUE)s\" : 0 , \"%(SIGMAY_VALUE)s\" : kick_in_sigma , \"%(AMPLX_VALUE)s\" : 0.5 , \"%(AMPLY_VALUE)s\" : 0 , } ) filename_to_write = Path ( f \"sext_ac_dipole_tracking_{kick_in_sigma}_sigma_{plane_letter}_kick\" ) mask_file = create_script_file ( self . template_str , values_replacing_dict = replace_dict , filename = Path ( str ( filename_to_write )), ) run_madx_mask ( mask_file ) _move_mask_file_after_running ( mask_file_path = mask_file , mask_files_dir = self . mask_files_dir ) _rename_madx_outputs ( kick_in_sigma = kick_in_sigma , outputdata_dir = self . outputdata_dir , plane = plane_letter , ) _convert_trackone_to_sdds () _move_trackone_sdds ( kick_in_sigma = kick_in_sigma , trackfiles_dir = self . trackfiles_planes [ kick_plane ] , plane = plane_letter , )","title":"track_forced_oscillations_for_plane"},{"location":"reference/pyhdtoolkit/scripts/ac_dipole/sext_ac_dipole_tracking/#track_free_oscillations_for_plane","text":"def track_free_oscillations_for_plane ( self , kick_plane : str = None ) -> None Run MAD-X simulations with amplitude offset tracking for the given plane, and handle outputs. Parameters: Name Type Description Default kick_plane None the name of the plane on which to apply an offset, either 'horizontal' or 'vertical'. None View Source def track_free_oscillations_for_plane ( self , kick_plane : str = None ) -> None : \"\"\" Run MAD-X simulations with amplitude offset tracking for the given plane, and handle outputs. Args: kick_plane: the name of the plane on which to apply an offset, either 'horizontal' or 'vertical'. \"\"\" if kick_plane not in ( \"horizontal\" , \"vertical\" ) : logger . error ( f \"Plane parameter {kick_plane} is not a valid value\" ) raise ValueError ( \"Plane parameter should be one of 'horizontal' or 'vertical'\" ) with timeit ( lambda spanned : logger . info ( f \"Tracked all amplitudes for {kick_plane} offsets in {spanned:.4f} seconds\" ) ) : for kick_in_sigma in self . sigmas : print ( \"\" ) plane_letter = \"x\" if kick_plane == \"horizontal\" else \"y\" action_var_value = kick_in_sigma amplitudes_dict = ( { \"%(AMPLX_VALUE)s\" : action_var_value , \"%(AMPLY_VALUE)s\" : 0.5 } if kick_plane == \"horizontal\" else { \"%(AMPLX_VALUE)s\" : 0.5 , \"%(AMPLY_VALUE)s\" : action_var_value } ) filename_to_write = Path ( f \"initial_amplitude_tracking_{kick_in_sigma}_sigma_{plane_letter}_kick\" ) mask_file = create_script_file ( self . template_str , values_replacing_dict = amplitudes_dict , filename = Path ( str ( filename_to_write )), ) run_madx_mask ( mask_file ) _move_mask_file_after_running ( mask_file_path = mask_file , mask_files_dir = self . mask_files_dir ) _rename_madx_outputs ( kick_in_sigma = kick_in_sigma , outputdata_dir = self . outputdata_dir , plane = plane_letter , ) _convert_trackone_to_sdds () _move_trackone_sdds ( kick_in_sigma = kick_in_sigma , trackfiles_dir = self . trackfiles_planes [ kick_plane ] , plane = plane_letter , )","title":"track_free_oscillations_for_plane"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/","text":"Module pyhdtoolkit.scripts.triplet_errors None None Sub-modules pyhdtoolkit.scripts.triplet_errors.algo pyhdtoolkit.scripts.triplet_errors.data_classes pyhdtoolkit.scripts.triplet_errors.plotting_functions","title":"Index"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/#module-pyhdtoolkitscriptstriplet_errors","text":"None None","title":"Module pyhdtoolkit.scripts.triplet_errors"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/#sub-modules","text":"pyhdtoolkit.scripts.triplet_errors.algo pyhdtoolkit.scripts.triplet_errors.data_classes pyhdtoolkit.scripts.triplet_errors.plotting_functions","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/algo/","text":"Module pyhdtoolkit.scripts.triplet_errors.algo Script scripts.triplets_errors.algo Created on 2019.06.15 View Source \"\"\" Script scripts.triplets_errors.algo ---------------------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) Command-line utility script, which will launch a series of MAD-X simulations, perform analysis of the outputs and hand out a plot. Arguments should be given as options at launch in the command-line. See README for instructions. \"\"\" import argparse import sys from copy import deepcopy from typing import List import cpymad import numpy as np import pandas as pd from loguru import logger from rich.progress import track from pyhdtoolkit.cpymadtools.generators import LatticeGenerator from pyhdtoolkit.scripts.triplet_errors.data_classes import BetaBeatValues , StdevValues from pyhdtoolkit.scripts.triplet_errors.plotting_functions import ( plot_bbing_max_errorbar , plot_bbing_with_ips_errorbar , ) from pyhdtoolkit.utils.contexts import timeit from pyhdtoolkit.utils.defaults import LOGURU_FORMAT class GridCompute : \"\"\" Algorithm as a class to run the simulations and analyze the outputs. Will prompt error values for confirmation, run MAD-X simulations through a `cpymad.madx.Madx` object, get beta-beating values from the outputs and return the appropriate structures. \"\"\" __slots__ = { \"reference_mad\" : \"cpymad Madx object to run the nominal configuration\" , \"errors_mad\" : \"cpymad Madx object to run errored simulations\" , \"rms_betabeatings\" : \"BetaBeatValues class to hold rms beta-beatings from simulations\" , \"standard_deviations\" : \"StdevValues class to hold standard deviations from simulations\" , \"lost_seeds_tf\" : \"List of field error values leading to loss of closed orbit\" , \"lost_seeds_miss\" : \"List of misalignment values leading to loss of closed orbit\" , \"nominal_twiss\" : \"Twiss dataframe from the nominal simulation\" , } def __init__ ( self ) -> None : \"\"\" Initializing will take some time since the reference script is being ran, to store the reference dframe. Unless you go into PTC it should be a matter of seconds. \"\"\" self . reference_mad = cpymad . madx . Madx ( stdout = False ) self . errors_mad = cpymad . madx . Madx ( stdout = False ) self . rms_betabeatings = BetaBeatValues () self . standard_deviations = StdevValues () self . lost_seeds_tf : List [ int ] = [] self . lost_seeds_miss : List [ int ] = [] self . nominal_twiss = self . _get_nominal_twiss () def _get_nominal_twiss ( self ) -> pd . DataFrame : \"\"\" Run a MAD-X simulation without errors, and extract the nominal Twiss from the results. This will be stored in the `nominal_twiss` instance attribute. Returns: Nothing, directly updates the instance's `nominal_twiss` attribute inplace. \"\"\" logger . info ( \"Running simulation for reference nominal run\" ) ref_script = LatticeGenerator . generate_tripleterrors_study_reference () self . reference_mad . input ( ref_script ) logger . debug ( \"Extracting reference Twiss dframe from cpymad\" ) return deepcopy ( self . reference_mad . table . twiss . dframe ()) def run_tf_errors ( self , error_values : List [ float ], n_seeds : int ) -> None : \"\"\" Run simulations for field errors, compute the values from the outputs, and store the final results in the class's data structures. Args: error_values (List[float]): the different error values to run simulations for n_seeds (int): number of simulations to run for each error values. Returns: Nothing, directly updates the instance's `rms_betabeatings` and `standard_deviations` attributes. \"\"\" with timeit ( lambda spanned : logger . info ( f \"Simulated field errors in: { spanned : .4f } seconds\" )): for error in error_values : logger . debug ( f \"Running simulation for Relative Field Error: { error } E-4\" ) temp_data = BetaBeatValues () for _ in track ( range ( n_seeds ), description = \"Simulating Field Errors Seeds\" , transient = True ): # Getting beta-beatings & appending to temporary BetaBeatValues tferrors_twiss : pd . DataFrame = self . _track_tf_error ( error ) betabeatings : pd . DataFrame = _get_betabeatings ( self . nominal_twiss , tferrors_twiss ) temp_data . update_tf_from_cpymad ( betabeatings ) # Append computed seeds' RMS for this error value in `rms_betabeatings` attribute. self . rms_betabeatings . update_tf_from_seeds ( temp_data ) # Getting stdev of all values for the N computed seeds self . standard_deviations . update_tf ( temp_data ) # Getting the lost seeds if any self . lost_seeds_tf . append ( n_seeds - len ( temp_data . tferror_bbx )) def _track_tf_error ( self , error : float ) -> pd . DataFrame : \"\"\" Run tferror tracking for a given seed, which is randomly assigned at function call. Args: error (float): the error value to input in the madx script. Returns: The twiss dframe from cpymad. \"\"\" seed = str ( np . random . randint ( 1e6 , 5e6 )) tferror_script = LatticeGenerator . generate_tripleterrors_study_tferror_job ( seed , str ( error )) self . errors_mad . input ( tferror_script ) return self . errors_mad . table . twiss . dframe () def run_miss_errors ( self , error_values : List [ float ], n_seeds : int ) -> None : \"\"\" Run the simulations for misalignment errors, compute the values from the outputs, and store the final results in the class's data structures. Args: error_values (List[float]): the different error values to run simulations for. n_seeds (int): number of simulations to run for each error values. Returns: Nothing, directly updates the instance's `rms_betabeatings` and `standard_deviations` attributes. \"\"\" with timeit ( lambda spanned : logger . info ( f \"Simulated misalignment errors in: { spanned : .4f } seconds\" )): for error in error_values : logger . debug ( f \"Running for Longitudinal Misalignment Error: { float ( error ) } mm\" ) temp_data = ( BetaBeatValues () ) # this will hold the beta-beats for all seeds with this error value. for _ in track ( range ( n_seeds ), description = \"Simulating Misalignment Seeds\" , transient = True ): # Getting beta-beatings & appending to temporary BetaBeatValues mserrors_twiss : pd . DataFrame = self . _track_miss_error ( error ) betabeatings : pd . DataFrame = _get_betabeatings ( self . nominal_twiss , mserrors_twiss ) temp_data . update_miss_from_cpymad ( betabeatings ) # Append computed seeds' RMS for this error value in `rms_betabeatings` attribute. self . rms_betabeatings . update_miss_from_seeds ( temp_data ) # Getting stdev of all values for the N computed seeds self . standard_deviations . update_miss ( temp_data ) # Getting the lost seeds if any self . lost_seeds_miss . append ( n_seeds - len ( temp_data . misserror_bbx )) def _track_miss_error ( self , error : float ) -> pd . DataFrame : \"\"\" Run misserror tracking for a given seed, which is randomly assigned at function call. Args: error (float): the error value to input in the madx script. Returns: The twiss dframe from cpymad. \"\"\" seed = str ( np . random . randint ( 1e6 , 5e6 )) mserror_script = LatticeGenerator . generate_tripleterrors_study_mserror_job ( seed , str ( error )) self . errors_mad . input ( mserror_script ) return self . errors_mad . table . twiss . dframe () def _get_betabeatings ( nominal_twiss : pd . DataFrame , errors_twiss : pd . DataFrame ) -> pd . DataFrame : \"\"\" Simple function to get beta-beatings from a `cpymad.madx.Madx`'s Twiss output. Args: nominal_twiss (pd.DataFrame): a twiss.dframe() results from a reference scenario. errors_twiss (pd.DataFrame): a twiss.dframe() results from the perturbed scenario. Returns: A `pd.DataFrame` with the beta-beat values, in percentage. \"\"\" betabeat = pd . DataFrame () betabeat [ \"NAME\" ] = nominal_twiss . name betabeat [ \"s\" ] = nominal_twiss . s betabeat [ \"BETX\" ] = 100 * ( errors_twiss . betx - nominal_twiss . betx ) / nominal_twiss . betx betabeat [ \"BETY\" ] = 100 * ( errors_twiss . bety - nominal_twiss . bety ) / nominal_twiss . bety return betabeat def _parse_arguments () -> argparse . Namespace : \"\"\" Simple argument parser to make life easier in the command-line. Returns a NameSpace with arguments as attributes. \"\"\" parser = argparse . ArgumentParser ( description = \"Running the beta-beating script.\" ) parser . add_argument ( \"-e\" , \"--errors\" , dest = \"errors\" , nargs = \"+\" , default = [ 1 , 3 , 5 ], type = int , help = \"Error values to simulate\" , ) parser . add_argument ( \"-s\" , \"--seeds\" , dest = \"seeds\" , default = 50 , type = int , help = \"Number of seeds to simulate per error.\" , ) parser . add_argument ( \"-p\" , \"--plotbetas\" , dest = \"plotbetas\" , default = False , help = \"Option for plotting betas at each error.\" , ) parser . add_argument ( \"-l\" , \"--logs\" , dest = \"log_level\" , default = \"info\" , type = str , help = \"The base console logging level. Can be 'debug', 'info', 'warning' and 'error'.\" \"Defaults to 'info'.\" , ) return parser . parse_args () def _set_logger_level ( log_level : str = \"info\" ) -> None : \"\"\" Sets the logger level to the one provided at the commandline. Default loguru handler will have DEBUG level and ID 0. We need to first remove this default handler and add ours with the wanted level. Args: log_level (str): string, the default logging level to print out. \"\"\" logger . remove ( 0 ) logger . add ( sys . stderr , format = LOGURU_FORMAT , level = log_level . upper ()) @logger . catch def main () -> None : \"\"\" Run the whole process. Will prompt for error grid values for confirmation. Instantiates a GridCompute object and runs for each type of errors. The results are stored in the class itself, to be accessed for plotting. \"\"\" command_line_args = _parse_arguments () _set_logger_level ( command_line_args . log_level ) simulations = GridCompute () logger . info ( f \"Here are the error values that will be ran: { command_line_args . errors } \" ) # Running simulations simulations . run_tf_errors ( command_line_args . errors , command_line_args . seeds ) simulations . run_miss_errors ( command_line_args . errors , command_line_args . seeds ) # Getting the results in dataframes and exporting to csv logger . info ( \"Exporting results to csv\" ) bbing_df : pd . DataFrame = simulations . rms_betabeatings . to_pandas () std_df : pd . DataFrame = simulations . standard_deviations . to_pandas () bbing_df . to_csv ( \"beta_beatings.csv\" , index = False ) std_df . to_csv ( \"standard_deviations.csv\" , index = False ) # Plotting the results plot_bbing_max_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Horizontal\" , figname = \"miss_vs_tf_max_hor.png\" , ) plot_bbing_max_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Vertical\" , figname = \"miss_vs_tf_max_ver.png\" , ) plot_bbing_with_ips_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Horizontal\" , figname = \"miss_vs_tf_ips_hor.png\" , ) plot_bbing_with_ips_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Vertical\" , figname = \"miss_vs_tf_ips_ver.png\" , ) if __name__ == \"__main__\" : main () Variables LOGURU_FORMAT Functions main def main ( ) -> None Run the whole process. Will prompt for error grid values for confirmation. Instantiates a GridCompute object and runs for each type of errors. The results are stored in the class itself, to be accessed for plotting. View Source @ logger . catch def main () -> None : \"\"\" Run the whole process. Will prompt for error grid values for confirmation. Instantiates a GridCompute object and runs for each type of errors. The results are stored in the class itself, to be accessed for plotting. \"\"\" command_line_args = _parse_arguments () _set_logger_level ( command_line_args . log_level ) simulations = GridCompute () logger . info ( f \"Here are the error values that will be ran: {command_line_args.errors}\" ) # Running simulations simulations . run_tf_errors ( command_line_args . errors , command_line_args . seeds ) simulations . run_miss_errors ( command_line_args . errors , command_line_args . seeds ) # Getting the results in dataframes and exporting to csv logger . info ( \"Exporting results to csv\" ) bbing_df : pd . DataFrame = simulations . rms_betabeatings . to_pandas () std_df : pd . DataFrame = simulations . standard_deviations . to_pandas () bbing_df . to_csv ( \"beta_beatings.csv\" , index = False ) std_df . to_csv ( \"standard_deviations.csv\" , index = False ) # Plotting the results plot_bbing_max_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Horizontal\" , figname = \"miss_vs_tf_max_hor.png\" , ) plot_bbing_max_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Vertical\" , figname = \"miss_vs_tf_max_ver.png\" , ) plot_bbing_with_ips_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Horizontal\" , figname = \"miss_vs_tf_ips_hor.png\" , ) plot_bbing_with_ips_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Vertical\" , figname = \"miss_vs_tf_ips_ver.png\" , ) Classes GridCompute class GridCompute ( ) View Source class GridCompute : \" \"\" Algorithm as a class to run the simulations and analyze the outputs. Will prompt error values for confirmation, run MAD-X simulations through a `cpymad.madx.Madx` object, get beta-beating values from the outputs and return the appropriate structures. \"\" \" __slots__ = { \"reference_mad\" : \"cpymad Madx object to run the nominal configuration\" , \"errors_mad\" : \"cpymad Madx object to run errored simulations\" , \"rms_betabeatings\" : \"BetaBeatValues class to hold rms beta-beatings from simulations\" , \"standard_deviations\" : \"StdevValues class to hold standard deviations from simulations\" , \"lost_seeds_tf\" : \"List of field error values leading to loss of closed orbit\" , \"lost_seeds_miss\" : \"List of misalignment values leading to loss of closed orbit\" , \"nominal_twiss\" : \"Twiss dataframe from the nominal simulation\" , } def __init__ ( self ) -> None : \" \"\" Initializing will take some time since the reference script is being ran, to store the reference dframe. Unless you go into PTC it should be a matter of seconds. \"\" \" self . reference_mad = cpymad . madx . Madx ( stdout = False ) self . errors_mad = cpymad . madx . Madx ( stdout = False ) self . rms_betabeatings = BetaBeatValues () self . standard_deviations = StdevValues () self . lost_seeds_tf : List [ int ] = [] self . lost_seeds_miss : List [ int ] = [] self . nominal_twiss = self . _get_nominal_twiss () def _get_nominal_twiss ( self ) -> pd . DataFrame : \" \"\" Run a MAD-X simulation without errors, and extract the nominal Twiss from the results. This will be stored in the `nominal_twiss` instance attribute. Returns: Nothing, directly updates the instance's `nominal_twiss` attribute inplace. \"\" \" logger . info ( \"Running simulation for reference nominal run\" ) ref_script = LatticeGenerator . generate_tripleterrors_study_reference () self . reference_mad . input ( ref_script ) logger . debug ( \"Extracting reference Twiss dframe from cpymad\" ) return deepcopy ( self . reference_mad . table . twiss . dframe ()) def run_tf_errors ( self , error_values : List [ float ] , n_seeds : int ) -> None : \" \"\" Run simulations for field errors, compute the values from the outputs, and store the final results in the class's data structures. Args: error_values (List[float]): the different error values to run simulations for n_seeds (int): number of simulations to run for each error values. Returns: Nothing, directly updates the instance's `rms_betabeatings` and `standard_deviations` attributes. \"\" \" with timeit ( lambda spanned : logger . info ( f \"Simulated field errors in: {spanned:.4f} seconds\" )) : for error in error_values : logger . debug ( f \"Running simulation for Relative Field Error: {error}E-4\" ) temp_data = BetaBeatValues () for _ in track ( range ( n_seeds ), description = \"Simulating Field Errors Seeds\" , transient = True ) : # Getting beta-beatings & appending to temporary BetaBeatValues tferrors_twiss : pd . DataFrame = self . _track_tf_error ( error ) betabeatings : pd . DataFrame = _get_betabeatings ( self . nominal_twiss , tferrors_twiss ) temp_data . update_tf_from_cpymad ( betabeatings ) # Append computed seeds' RMS for this error value in `rms_betabeatings` attribute. self . rms_betabeatings . update_tf_from_seeds ( temp_data ) # Getting stdev of all values for the N computed seeds self . standard_deviations . update_tf ( temp_data ) # Getting the lost seeds if any self . lost_seeds_tf . append ( n_seeds - len ( temp_data . tferror_bbx )) def _track_tf_error ( self , error : float ) -> pd . DataFrame : \" \"\" Run tferror tracking for a given seed, which is randomly assigned at function call. Args: error (float): the error value to input in the madx script. Returns: The twiss dframe from cpymad. \"\" \" seed = str ( np . random . randint ( 1e6 , 5e6 )) tferror_script = LatticeGenerator . generate_tripleterrors_study_tferror_job ( seed , str ( error )) self . errors_mad . input ( tferror_script ) return self . errors_mad . table . twiss . dframe () def run_miss_errors ( self , error_values : List [ float ] , n_seeds : int ) -> None : \" \"\" Run the simulations for misalignment errors, compute the values from the outputs, and store the final results in the class's data structures. Args: error_values (List[float]): the different error values to run simulations for. n_seeds (int): number of simulations to run for each error values. Returns: Nothing, directly updates the instance's `rms_betabeatings` and `standard_deviations` attributes. \"\" \" with timeit ( lambda spanned : logger . info ( f \"Simulated misalignment errors in: {spanned:.4f} seconds\" )) : for error in error_values : logger . debug ( f \"Running for Longitudinal Misalignment Error: {float(error)}mm\" ) temp_data = ( BetaBeatValues () ) # this will hold the beta-beats for all seeds with this error value. for _ in track ( range ( n_seeds ), description = \"Simulating Misalignment Seeds\" , transient = True ) : # Getting beta-beatings & appending to temporary BetaBeatValues mserrors_twiss : pd . DataFrame = self . _track_miss_error ( error ) betabeatings : pd . DataFrame = _get_betabeatings ( self . nominal_twiss , mserrors_twiss ) temp_data . update_miss_from_cpymad ( betabeatings ) # Append computed seeds' RMS for this error value in `rms_betabeatings` attribute. self . rms_betabeatings . update_miss_from_seeds ( temp_data ) # Getting stdev of all values for the N computed seeds self . standard_deviations . update_miss ( temp_data ) # Getting the lost seeds if any self . lost_seeds_miss . append ( n_seeds - len ( temp_data . misserror_bbx )) def _track_miss_error ( self , error : float ) -> pd . DataFrame : \" \"\" Run misserror tracking for a given seed, which is randomly assigned at function call. Args: error (float): the error value to input in the madx script. Returns: The twiss dframe from cpymad. \"\" \" seed = str ( np . random . randint ( 1e6 , 5e6 )) mserror_script = LatticeGenerator . generate_tripleterrors_study_mserror_job ( seed , str ( error )) self . errors_mad . input ( mserror_script ) return self . errors_mad . table . twiss . dframe () Instance variables errors_mad lost_seeds_miss lost_seeds_tf nominal_twiss reference_mad rms_betabeatings standard_deviations Methods run_miss_errors def run_miss_errors ( self , error_values : List [ float ], n_seeds : int ) -> None Run the simulations for misalignment errors, compute the values from the outputs, and store the final results in the class's data structures. Parameters: Name Type Description Default error_values List[float] the different error values to run simulations for. None n_seeds int number of simulations to run for each error values. None Returns: Type Description None Nothing, directly updates the instance's rms_betabeatings and standard_deviations attributes. View Source def run_miss_errors ( self , error_values : List [ float ] , n_seeds : int ) -> None : \" \"\" Run the simulations for misalignment errors, compute the values from the outputs, and store the final results in the class's data structures. Args: error_values (List[float]): the different error values to run simulations for. n_seeds (int): number of simulations to run for each error values. Returns: Nothing, directly updates the instance's `rms_betabeatings` and `standard_deviations` attributes. \"\" \" with timeit ( lambda spanned : logger . info ( f \"Simulated misalignment errors in: {spanned:.4f} seconds\" )) : for error in error_values : logger . debug ( f \"Running for Longitudinal Misalignment Error: {float(error)}mm\" ) temp_data = ( BetaBeatValues () ) # this will hold the beta-beats for all seeds with this error value. for _ in track ( range ( n_seeds ), description = \"Simulating Misalignment Seeds\" , transient = True ) : # Getting beta-beatings & appending to temporary BetaBeatValues mserrors_twiss : pd . DataFrame = self . _track_miss_error ( error ) betabeatings : pd . DataFrame = _get_betabeatings ( self . nominal_twiss , mserrors_twiss ) temp_data . update_miss_from_cpymad ( betabeatings ) # Append computed seeds' RMS for this error value in `rms_betabeatings` attribute. self . rms_betabeatings . update_miss_from_seeds ( temp_data ) # Getting stdev of all values for the N computed seeds self . standard_deviations . update_miss ( temp_data ) # Getting the lost seeds if any self . lost_seeds_miss . append ( n_seeds - len ( temp_data . misserror_bbx )) run_tf_errors def run_tf_errors ( self , error_values : List [ float ], n_seeds : int ) -> None Run simulations for field errors, compute the values from the outputs, and store the final results in the class's data structures. Parameters: Name Type Description Default error_values List[float] the different error values to run simulations for None n_seeds int number of simulations to run for each error values. None Returns: Type Description None Nothing, directly updates the instance's rms_betabeatings and standard_deviations attributes. View Source def run_tf_errors ( self , error_values : List [ float ] , n_seeds : int ) -> None : \" \"\" Run simulations for field errors, compute the values from the outputs, and store the final results in the class's data structures. Args: error_values (List[float]): the different error values to run simulations for n_seeds (int): number of simulations to run for each error values. Returns: Nothing, directly updates the instance's `rms_betabeatings` and `standard_deviations` attributes. \"\" \" with timeit ( lambda spanned : logger . info ( f \"Simulated field errors in: {spanned:.4f} seconds\" )) : for error in error_values : logger . debug ( f \"Running simulation for Relative Field Error: {error}E-4\" ) temp_data = BetaBeatValues () for _ in track ( range ( n_seeds ), description = \"Simulating Field Errors Seeds\" , transient = True ) : # Getting beta-beatings & appending to temporary BetaBeatValues tferrors_twiss : pd . DataFrame = self . _track_tf_error ( error ) betabeatings : pd . DataFrame = _get_betabeatings ( self . nominal_twiss , tferrors_twiss ) temp_data . update_tf_from_cpymad ( betabeatings ) # Append computed seeds' RMS for this error value in `rms_betabeatings` attribute. self . rms_betabeatings . update_tf_from_seeds ( temp_data ) # Getting stdev of all values for the N computed seeds self . standard_deviations . update_tf ( temp_data ) # Getting the lost seeds if any self . lost_seeds_tf . append ( n_seeds - len ( temp_data . tferror_bbx ))","title":"Algo"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/algo/#module-pyhdtoolkitscriptstriplet_errorsalgo","text":"Script scripts.triplets_errors.algo Created on 2019.06.15 View Source \"\"\" Script scripts.triplets_errors.algo ---------------------------------- Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) Command-line utility script, which will launch a series of MAD-X simulations, perform analysis of the outputs and hand out a plot. Arguments should be given as options at launch in the command-line. See README for instructions. \"\"\" import argparse import sys from copy import deepcopy from typing import List import cpymad import numpy as np import pandas as pd from loguru import logger from rich.progress import track from pyhdtoolkit.cpymadtools.generators import LatticeGenerator from pyhdtoolkit.scripts.triplet_errors.data_classes import BetaBeatValues , StdevValues from pyhdtoolkit.scripts.triplet_errors.plotting_functions import ( plot_bbing_max_errorbar , plot_bbing_with_ips_errorbar , ) from pyhdtoolkit.utils.contexts import timeit from pyhdtoolkit.utils.defaults import LOGURU_FORMAT class GridCompute : \"\"\" Algorithm as a class to run the simulations and analyze the outputs. Will prompt error values for confirmation, run MAD-X simulations through a `cpymad.madx.Madx` object, get beta-beating values from the outputs and return the appropriate structures. \"\"\" __slots__ = { \"reference_mad\" : \"cpymad Madx object to run the nominal configuration\" , \"errors_mad\" : \"cpymad Madx object to run errored simulations\" , \"rms_betabeatings\" : \"BetaBeatValues class to hold rms beta-beatings from simulations\" , \"standard_deviations\" : \"StdevValues class to hold standard deviations from simulations\" , \"lost_seeds_tf\" : \"List of field error values leading to loss of closed orbit\" , \"lost_seeds_miss\" : \"List of misalignment values leading to loss of closed orbit\" , \"nominal_twiss\" : \"Twiss dataframe from the nominal simulation\" , } def __init__ ( self ) -> None : \"\"\" Initializing will take some time since the reference script is being ran, to store the reference dframe. Unless you go into PTC it should be a matter of seconds. \"\"\" self . reference_mad = cpymad . madx . Madx ( stdout = False ) self . errors_mad = cpymad . madx . Madx ( stdout = False ) self . rms_betabeatings = BetaBeatValues () self . standard_deviations = StdevValues () self . lost_seeds_tf : List [ int ] = [] self . lost_seeds_miss : List [ int ] = [] self . nominal_twiss = self . _get_nominal_twiss () def _get_nominal_twiss ( self ) -> pd . DataFrame : \"\"\" Run a MAD-X simulation without errors, and extract the nominal Twiss from the results. This will be stored in the `nominal_twiss` instance attribute. Returns: Nothing, directly updates the instance's `nominal_twiss` attribute inplace. \"\"\" logger . info ( \"Running simulation for reference nominal run\" ) ref_script = LatticeGenerator . generate_tripleterrors_study_reference () self . reference_mad . input ( ref_script ) logger . debug ( \"Extracting reference Twiss dframe from cpymad\" ) return deepcopy ( self . reference_mad . table . twiss . dframe ()) def run_tf_errors ( self , error_values : List [ float ], n_seeds : int ) -> None : \"\"\" Run simulations for field errors, compute the values from the outputs, and store the final results in the class's data structures. Args: error_values (List[float]): the different error values to run simulations for n_seeds (int): number of simulations to run for each error values. Returns: Nothing, directly updates the instance's `rms_betabeatings` and `standard_deviations` attributes. \"\"\" with timeit ( lambda spanned : logger . info ( f \"Simulated field errors in: { spanned : .4f } seconds\" )): for error in error_values : logger . debug ( f \"Running simulation for Relative Field Error: { error } E-4\" ) temp_data = BetaBeatValues () for _ in track ( range ( n_seeds ), description = \"Simulating Field Errors Seeds\" , transient = True ): # Getting beta-beatings & appending to temporary BetaBeatValues tferrors_twiss : pd . DataFrame = self . _track_tf_error ( error ) betabeatings : pd . DataFrame = _get_betabeatings ( self . nominal_twiss , tferrors_twiss ) temp_data . update_tf_from_cpymad ( betabeatings ) # Append computed seeds' RMS for this error value in `rms_betabeatings` attribute. self . rms_betabeatings . update_tf_from_seeds ( temp_data ) # Getting stdev of all values for the N computed seeds self . standard_deviations . update_tf ( temp_data ) # Getting the lost seeds if any self . lost_seeds_tf . append ( n_seeds - len ( temp_data . tferror_bbx )) def _track_tf_error ( self , error : float ) -> pd . DataFrame : \"\"\" Run tferror tracking for a given seed, which is randomly assigned at function call. Args: error (float): the error value to input in the madx script. Returns: The twiss dframe from cpymad. \"\"\" seed = str ( np . random . randint ( 1e6 , 5e6 )) tferror_script = LatticeGenerator . generate_tripleterrors_study_tferror_job ( seed , str ( error )) self . errors_mad . input ( tferror_script ) return self . errors_mad . table . twiss . dframe () def run_miss_errors ( self , error_values : List [ float ], n_seeds : int ) -> None : \"\"\" Run the simulations for misalignment errors, compute the values from the outputs, and store the final results in the class's data structures. Args: error_values (List[float]): the different error values to run simulations for. n_seeds (int): number of simulations to run for each error values. Returns: Nothing, directly updates the instance's `rms_betabeatings` and `standard_deviations` attributes. \"\"\" with timeit ( lambda spanned : logger . info ( f \"Simulated misalignment errors in: { spanned : .4f } seconds\" )): for error in error_values : logger . debug ( f \"Running for Longitudinal Misalignment Error: { float ( error ) } mm\" ) temp_data = ( BetaBeatValues () ) # this will hold the beta-beats for all seeds with this error value. for _ in track ( range ( n_seeds ), description = \"Simulating Misalignment Seeds\" , transient = True ): # Getting beta-beatings & appending to temporary BetaBeatValues mserrors_twiss : pd . DataFrame = self . _track_miss_error ( error ) betabeatings : pd . DataFrame = _get_betabeatings ( self . nominal_twiss , mserrors_twiss ) temp_data . update_miss_from_cpymad ( betabeatings ) # Append computed seeds' RMS for this error value in `rms_betabeatings` attribute. self . rms_betabeatings . update_miss_from_seeds ( temp_data ) # Getting stdev of all values for the N computed seeds self . standard_deviations . update_miss ( temp_data ) # Getting the lost seeds if any self . lost_seeds_miss . append ( n_seeds - len ( temp_data . misserror_bbx )) def _track_miss_error ( self , error : float ) -> pd . DataFrame : \"\"\" Run misserror tracking for a given seed, which is randomly assigned at function call. Args: error (float): the error value to input in the madx script. Returns: The twiss dframe from cpymad. \"\"\" seed = str ( np . random . randint ( 1e6 , 5e6 )) mserror_script = LatticeGenerator . generate_tripleterrors_study_mserror_job ( seed , str ( error )) self . errors_mad . input ( mserror_script ) return self . errors_mad . table . twiss . dframe () def _get_betabeatings ( nominal_twiss : pd . DataFrame , errors_twiss : pd . DataFrame ) -> pd . DataFrame : \"\"\" Simple function to get beta-beatings from a `cpymad.madx.Madx`'s Twiss output. Args: nominal_twiss (pd.DataFrame): a twiss.dframe() results from a reference scenario. errors_twiss (pd.DataFrame): a twiss.dframe() results from the perturbed scenario. Returns: A `pd.DataFrame` with the beta-beat values, in percentage. \"\"\" betabeat = pd . DataFrame () betabeat [ \"NAME\" ] = nominal_twiss . name betabeat [ \"s\" ] = nominal_twiss . s betabeat [ \"BETX\" ] = 100 * ( errors_twiss . betx - nominal_twiss . betx ) / nominal_twiss . betx betabeat [ \"BETY\" ] = 100 * ( errors_twiss . bety - nominal_twiss . bety ) / nominal_twiss . bety return betabeat def _parse_arguments () -> argparse . Namespace : \"\"\" Simple argument parser to make life easier in the command-line. Returns a NameSpace with arguments as attributes. \"\"\" parser = argparse . ArgumentParser ( description = \"Running the beta-beating script.\" ) parser . add_argument ( \"-e\" , \"--errors\" , dest = \"errors\" , nargs = \"+\" , default = [ 1 , 3 , 5 ], type = int , help = \"Error values to simulate\" , ) parser . add_argument ( \"-s\" , \"--seeds\" , dest = \"seeds\" , default = 50 , type = int , help = \"Number of seeds to simulate per error.\" , ) parser . add_argument ( \"-p\" , \"--plotbetas\" , dest = \"plotbetas\" , default = False , help = \"Option for plotting betas at each error.\" , ) parser . add_argument ( \"-l\" , \"--logs\" , dest = \"log_level\" , default = \"info\" , type = str , help = \"The base console logging level. Can be 'debug', 'info', 'warning' and 'error'.\" \"Defaults to 'info'.\" , ) return parser . parse_args () def _set_logger_level ( log_level : str = \"info\" ) -> None : \"\"\" Sets the logger level to the one provided at the commandline. Default loguru handler will have DEBUG level and ID 0. We need to first remove this default handler and add ours with the wanted level. Args: log_level (str): string, the default logging level to print out. \"\"\" logger . remove ( 0 ) logger . add ( sys . stderr , format = LOGURU_FORMAT , level = log_level . upper ()) @logger . catch def main () -> None : \"\"\" Run the whole process. Will prompt for error grid values for confirmation. Instantiates a GridCompute object and runs for each type of errors. The results are stored in the class itself, to be accessed for plotting. \"\"\" command_line_args = _parse_arguments () _set_logger_level ( command_line_args . log_level ) simulations = GridCompute () logger . info ( f \"Here are the error values that will be ran: { command_line_args . errors } \" ) # Running simulations simulations . run_tf_errors ( command_line_args . errors , command_line_args . seeds ) simulations . run_miss_errors ( command_line_args . errors , command_line_args . seeds ) # Getting the results in dataframes and exporting to csv logger . info ( \"Exporting results to csv\" ) bbing_df : pd . DataFrame = simulations . rms_betabeatings . to_pandas () std_df : pd . DataFrame = simulations . standard_deviations . to_pandas () bbing_df . to_csv ( \"beta_beatings.csv\" , index = False ) std_df . to_csv ( \"standard_deviations.csv\" , index = False ) # Plotting the results plot_bbing_max_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Horizontal\" , figname = \"miss_vs_tf_max_hor.png\" , ) plot_bbing_max_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Vertical\" , figname = \"miss_vs_tf_max_ver.png\" , ) plot_bbing_with_ips_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Horizontal\" , figname = \"miss_vs_tf_ips_hor.png\" , ) plot_bbing_with_ips_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Vertical\" , figname = \"miss_vs_tf_ips_ver.png\" , ) if __name__ == \"__main__\" : main ()","title":"Module pyhdtoolkit.scripts.triplet_errors.algo"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/algo/#variables","text":"LOGURU_FORMAT","title":"Variables"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/algo/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/algo/#main","text":"def main ( ) -> None Run the whole process. Will prompt for error grid values for confirmation. Instantiates a GridCompute object and runs for each type of errors. The results are stored in the class itself, to be accessed for plotting. View Source @ logger . catch def main () -> None : \"\"\" Run the whole process. Will prompt for error grid values for confirmation. Instantiates a GridCompute object and runs for each type of errors. The results are stored in the class itself, to be accessed for plotting. \"\"\" command_line_args = _parse_arguments () _set_logger_level ( command_line_args . log_level ) simulations = GridCompute () logger . info ( f \"Here are the error values that will be ran: {command_line_args.errors}\" ) # Running simulations simulations . run_tf_errors ( command_line_args . errors , command_line_args . seeds ) simulations . run_miss_errors ( command_line_args . errors , command_line_args . seeds ) # Getting the results in dataframes and exporting to csv logger . info ( \"Exporting results to csv\" ) bbing_df : pd . DataFrame = simulations . rms_betabeatings . to_pandas () std_df : pd . DataFrame = simulations . standard_deviations . to_pandas () bbing_df . to_csv ( \"beta_beatings.csv\" , index = False ) std_df . to_csv ( \"standard_deviations.csv\" , index = False ) # Plotting the results plot_bbing_max_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Horizontal\" , figname = \"miss_vs_tf_max_hor.png\" , ) plot_bbing_max_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Vertical\" , figname = \"miss_vs_tf_max_ver.png\" , ) plot_bbing_with_ips_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Horizontal\" , figname = \"miss_vs_tf_ips_hor.png\" , ) plot_bbing_with_ips_errorbar ( command_line_args . errors , beta_beatings_df = bbing_df , stdev_df = std_df , plane = \"Vertical\" , figname = \"miss_vs_tf_ips_ver.png\" , )","title":"main"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/algo/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/algo/#gridcompute","text":"class GridCompute ( ) View Source class GridCompute : \" \"\" Algorithm as a class to run the simulations and analyze the outputs. Will prompt error values for confirmation, run MAD-X simulations through a `cpymad.madx.Madx` object, get beta-beating values from the outputs and return the appropriate structures. \"\" \" __slots__ = { \"reference_mad\" : \"cpymad Madx object to run the nominal configuration\" , \"errors_mad\" : \"cpymad Madx object to run errored simulations\" , \"rms_betabeatings\" : \"BetaBeatValues class to hold rms beta-beatings from simulations\" , \"standard_deviations\" : \"StdevValues class to hold standard deviations from simulations\" , \"lost_seeds_tf\" : \"List of field error values leading to loss of closed orbit\" , \"lost_seeds_miss\" : \"List of misalignment values leading to loss of closed orbit\" , \"nominal_twiss\" : \"Twiss dataframe from the nominal simulation\" , } def __init__ ( self ) -> None : \" \"\" Initializing will take some time since the reference script is being ran, to store the reference dframe. Unless you go into PTC it should be a matter of seconds. \"\" \" self . reference_mad = cpymad . madx . Madx ( stdout = False ) self . errors_mad = cpymad . madx . Madx ( stdout = False ) self . rms_betabeatings = BetaBeatValues () self . standard_deviations = StdevValues () self . lost_seeds_tf : List [ int ] = [] self . lost_seeds_miss : List [ int ] = [] self . nominal_twiss = self . _get_nominal_twiss () def _get_nominal_twiss ( self ) -> pd . DataFrame : \" \"\" Run a MAD-X simulation without errors, and extract the nominal Twiss from the results. This will be stored in the `nominal_twiss` instance attribute. Returns: Nothing, directly updates the instance's `nominal_twiss` attribute inplace. \"\" \" logger . info ( \"Running simulation for reference nominal run\" ) ref_script = LatticeGenerator . generate_tripleterrors_study_reference () self . reference_mad . input ( ref_script ) logger . debug ( \"Extracting reference Twiss dframe from cpymad\" ) return deepcopy ( self . reference_mad . table . twiss . dframe ()) def run_tf_errors ( self , error_values : List [ float ] , n_seeds : int ) -> None : \" \"\" Run simulations for field errors, compute the values from the outputs, and store the final results in the class's data structures. Args: error_values (List[float]): the different error values to run simulations for n_seeds (int): number of simulations to run for each error values. Returns: Nothing, directly updates the instance's `rms_betabeatings` and `standard_deviations` attributes. \"\" \" with timeit ( lambda spanned : logger . info ( f \"Simulated field errors in: {spanned:.4f} seconds\" )) : for error in error_values : logger . debug ( f \"Running simulation for Relative Field Error: {error}E-4\" ) temp_data = BetaBeatValues () for _ in track ( range ( n_seeds ), description = \"Simulating Field Errors Seeds\" , transient = True ) : # Getting beta-beatings & appending to temporary BetaBeatValues tferrors_twiss : pd . DataFrame = self . _track_tf_error ( error ) betabeatings : pd . DataFrame = _get_betabeatings ( self . nominal_twiss , tferrors_twiss ) temp_data . update_tf_from_cpymad ( betabeatings ) # Append computed seeds' RMS for this error value in `rms_betabeatings` attribute. self . rms_betabeatings . update_tf_from_seeds ( temp_data ) # Getting stdev of all values for the N computed seeds self . standard_deviations . update_tf ( temp_data ) # Getting the lost seeds if any self . lost_seeds_tf . append ( n_seeds - len ( temp_data . tferror_bbx )) def _track_tf_error ( self , error : float ) -> pd . DataFrame : \" \"\" Run tferror tracking for a given seed, which is randomly assigned at function call. Args: error (float): the error value to input in the madx script. Returns: The twiss dframe from cpymad. \"\" \" seed = str ( np . random . randint ( 1e6 , 5e6 )) tferror_script = LatticeGenerator . generate_tripleterrors_study_tferror_job ( seed , str ( error )) self . errors_mad . input ( tferror_script ) return self . errors_mad . table . twiss . dframe () def run_miss_errors ( self , error_values : List [ float ] , n_seeds : int ) -> None : \" \"\" Run the simulations for misalignment errors, compute the values from the outputs, and store the final results in the class's data structures. Args: error_values (List[float]): the different error values to run simulations for. n_seeds (int): number of simulations to run for each error values. Returns: Nothing, directly updates the instance's `rms_betabeatings` and `standard_deviations` attributes. \"\" \" with timeit ( lambda spanned : logger . info ( f \"Simulated misalignment errors in: {spanned:.4f} seconds\" )) : for error in error_values : logger . debug ( f \"Running for Longitudinal Misalignment Error: {float(error)}mm\" ) temp_data = ( BetaBeatValues () ) # this will hold the beta-beats for all seeds with this error value. for _ in track ( range ( n_seeds ), description = \"Simulating Misalignment Seeds\" , transient = True ) : # Getting beta-beatings & appending to temporary BetaBeatValues mserrors_twiss : pd . DataFrame = self . _track_miss_error ( error ) betabeatings : pd . DataFrame = _get_betabeatings ( self . nominal_twiss , mserrors_twiss ) temp_data . update_miss_from_cpymad ( betabeatings ) # Append computed seeds' RMS for this error value in `rms_betabeatings` attribute. self . rms_betabeatings . update_miss_from_seeds ( temp_data ) # Getting stdev of all values for the N computed seeds self . standard_deviations . update_miss ( temp_data ) # Getting the lost seeds if any self . lost_seeds_miss . append ( n_seeds - len ( temp_data . misserror_bbx )) def _track_miss_error ( self , error : float ) -> pd . DataFrame : \" \"\" Run misserror tracking for a given seed, which is randomly assigned at function call. Args: error (float): the error value to input in the madx script. Returns: The twiss dframe from cpymad. \"\" \" seed = str ( np . random . randint ( 1e6 , 5e6 )) mserror_script = LatticeGenerator . generate_tripleterrors_study_mserror_job ( seed , str ( error )) self . errors_mad . input ( mserror_script ) return self . errors_mad . table . twiss . dframe ()","title":"GridCompute"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/algo/#instance-variables","text":"errors_mad lost_seeds_miss lost_seeds_tf nominal_twiss reference_mad rms_betabeatings standard_deviations","title":"Instance variables"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/algo/#methods","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/algo/#run_miss_errors","text":"def run_miss_errors ( self , error_values : List [ float ], n_seeds : int ) -> None Run the simulations for misalignment errors, compute the values from the outputs, and store the final results in the class's data structures. Parameters: Name Type Description Default error_values List[float] the different error values to run simulations for. None n_seeds int number of simulations to run for each error values. None Returns: Type Description None Nothing, directly updates the instance's rms_betabeatings and standard_deviations attributes. View Source def run_miss_errors ( self , error_values : List [ float ] , n_seeds : int ) -> None : \" \"\" Run the simulations for misalignment errors, compute the values from the outputs, and store the final results in the class's data structures. Args: error_values (List[float]): the different error values to run simulations for. n_seeds (int): number of simulations to run for each error values. Returns: Nothing, directly updates the instance's `rms_betabeatings` and `standard_deviations` attributes. \"\" \" with timeit ( lambda spanned : logger . info ( f \"Simulated misalignment errors in: {spanned:.4f} seconds\" )) : for error in error_values : logger . debug ( f \"Running for Longitudinal Misalignment Error: {float(error)}mm\" ) temp_data = ( BetaBeatValues () ) # this will hold the beta-beats for all seeds with this error value. for _ in track ( range ( n_seeds ), description = \"Simulating Misalignment Seeds\" , transient = True ) : # Getting beta-beatings & appending to temporary BetaBeatValues mserrors_twiss : pd . DataFrame = self . _track_miss_error ( error ) betabeatings : pd . DataFrame = _get_betabeatings ( self . nominal_twiss , mserrors_twiss ) temp_data . update_miss_from_cpymad ( betabeatings ) # Append computed seeds' RMS for this error value in `rms_betabeatings` attribute. self . rms_betabeatings . update_miss_from_seeds ( temp_data ) # Getting stdev of all values for the N computed seeds self . standard_deviations . update_miss ( temp_data ) # Getting the lost seeds if any self . lost_seeds_miss . append ( n_seeds - len ( temp_data . misserror_bbx ))","title":"run_miss_errors"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/algo/#run_tf_errors","text":"def run_tf_errors ( self , error_values : List [ float ], n_seeds : int ) -> None Run simulations for field errors, compute the values from the outputs, and store the final results in the class's data structures. Parameters: Name Type Description Default error_values List[float] the different error values to run simulations for None n_seeds int number of simulations to run for each error values. None Returns: Type Description None Nothing, directly updates the instance's rms_betabeatings and standard_deviations attributes. View Source def run_tf_errors ( self , error_values : List [ float ] , n_seeds : int ) -> None : \" \"\" Run simulations for field errors, compute the values from the outputs, and store the final results in the class's data structures. Args: error_values (List[float]): the different error values to run simulations for n_seeds (int): number of simulations to run for each error values. Returns: Nothing, directly updates the instance's `rms_betabeatings` and `standard_deviations` attributes. \"\" \" with timeit ( lambda spanned : logger . info ( f \"Simulated field errors in: {spanned:.4f} seconds\" )) : for error in error_values : logger . debug ( f \"Running simulation for Relative Field Error: {error}E-4\" ) temp_data = BetaBeatValues () for _ in track ( range ( n_seeds ), description = \"Simulating Field Errors Seeds\" , transient = True ) : # Getting beta-beatings & appending to temporary BetaBeatValues tferrors_twiss : pd . DataFrame = self . _track_tf_error ( error ) betabeatings : pd . DataFrame = _get_betabeatings ( self . nominal_twiss , tferrors_twiss ) temp_data . update_tf_from_cpymad ( betabeatings ) # Append computed seeds' RMS for this error value in `rms_betabeatings` attribute. self . rms_betabeatings . update_tf_from_seeds ( temp_data ) # Getting stdev of all values for the N computed seeds self . standard_deviations . update_tf ( temp_data ) # Getting the lost seeds if any self . lost_seeds_tf . append ( n_seeds - len ( temp_data . tferror_bbx ))","title":"run_tf_errors"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/","text":"Module pyhdtoolkit.scripts.triplet_errors.data_classes Module scripts.triplet_errors.data_classes Created on 2019.06.15 View Source \"\"\" Module scripts.triplet_errors.data_classes ------------------------------------------ Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A few classes that will be useful to store values calculated from the results of the GridCompute Algorithm. \"\"\" from typing import List import numpy as np import pandas as pd from loguru import logger from pydantic import BaseModel class BetaBeatValues ( BaseModel ): \"\"\" Simple class to store and transfer beta-beating values. Class attributes are as follows: \"tferror_bbx\": \"Horizontal beta-beating values from field errors\", \"tferror_bby\": \"Vertical beta-beating values from field errors\", \"ip1_tferror_bbx\": \"Horizontal beta-beating values from field errors at IP1\", \"ip1_tferror_bby\": \"Vertical beta-beating values from field errors at IP1\", \"ip5_tferror_bbx\": \"Horizontal beta-beating values from field errors at IP5\", \"ip5_tferror_bby\": \"Vertical beta-beating values from field errors at IP5\", \"max_tferror_bbx\": \"Maximal horizontal beta-beating values from field errors\", \"max_tferror_bby\": \"Maximal vertical beta-beating values from field errors\", \"misserror_bbx\": \"Horizontal beta-beating values from misalignment errors\", \"misserror_bby\": \"Horizontal beta-beating values from misalignment errors\", \"ip1_misserror_bbx\": \"Horizontal beta-beating values from misalignment errors at IP1\", \"ip1_misserror_bby\": \"Vertical beta-beating values from misalignment errors at IP1\", \"ip5_misserror_bbx\": \"Horizontal beta-beating values from misalignment errors at IP5\", \"ip5_misserror_bby\": \"Vertical beta-beating values from misalignment errors at IP5\", \"max_misserror_bbx\": \"Maximal horizontal beta-beating values from misalignment errors\", \"max_misserror_bby\": \"Maximal vertical beta-beating values from misalignment errors\", \"\"\" tferror_bbx : List [ float ] = [] tferror_bby : List [ float ] = [] ip1_tferror_bbx : List [ float ] = [] ip1_tferror_bby : List [ float ] = [] ip5_tferror_bbx : List [ float ] = [] ip5_tferror_bby : List [ float ] = [] max_tferror_bbx : List [ float ] = [] max_tferror_bby : List [ float ] = [] misserror_bbx : List [ float ] = [] misserror_bby : List [ float ] = [] ip1_misserror_bbx : List [ float ] = [] ip1_misserror_bby : List [ float ] = [] ip5_misserror_bbx : List [ float ] = [] ip5_misserror_bby : List [ float ] = [] max_misserror_bbx : List [ float ] = [] max_misserror_bby : List [ float ] = [] def describe ( self ) -> None : \"\"\" Simple print statement of instance attributes. \"\"\" for attribute , value in self . dict () . items (): print ( f \" { attribute : <20 } { value } \" ) def update_tf_from_cpymad ( self , cpymad_betabeatings : pd . DataFrame ) -> None : \"\"\" This is to update a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Args: cpymad_betabeatings (pd.DataFrame): the beta-beatings from the simulation, compared to the nominal twiss from a reference run. \"\"\" logger . trace ( \"Getting rms and max values for betatron functions of provided run\" ) self . tferror_bbx . append ( _get_rms ( cpymad_betabeatings [ \"BETX\" ])) self . tferror_bby . append ( _get_rms ( cpymad_betabeatings [ \"BETY\" ])) self . max_tferror_bbx . append ( cpymad_betabeatings [ \"BETX\" ] . max ()) self . max_tferror_bby . append ( cpymad_betabeatings [ \"BETY\" ] . max ()) logger . trace ( \"Getting betatron functions at IP1 and IP5\" ) # cpymad naming: lowercase and appended with :beam_number self . ip1_tferror_bbx . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip1_tferror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip5_tferror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) self . ip5_tferror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) def update_tf_from_seeds ( self , temp_data ) -> None : \"\"\" Updates the error's beta-beatings values after having ran simulations for all seeds. Append computed rms values for a group of seeds, to field errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. \"\"\" self . tferror_bbx . append ( _get_rms ( temp_data . tferror_bbx )) self . tferror_bby . append ( _get_rms ( temp_data . tferror_bby )) self . max_tferror_bbx . append ( _get_rms ( temp_data . max_tferror_bbx )) self . max_tferror_bby . append ( _get_rms ( temp_data . max_tferror_bby )) self . ip1_tferror_bbx . append ( _get_rms ( temp_data . ip1_tferror_bbx )) self . ip1_tferror_bby . append ( _get_rms ( temp_data . ip1_tferror_bby )) self . ip5_tferror_bbx . append ( _get_rms ( temp_data . ip5_tferror_bbx )) self . ip5_tferror_bby . append ( _get_rms ( temp_data . ip5_tferror_bby )) def update_miss_from_cpymad ( self , cpymad_betabeatings : pd . DataFrame ) -> None : \"\"\" Updates a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Args: cpymad_betabeatings (pd.DataFrame): the beta-beatings from the simulation, compared to the nominal twiss from a reference run. \"\"\" logger . trace ( \"Getting rms and max values for betatron functions of provided run\" ) self . misserror_bbx . append ( _get_rms ( cpymad_betabeatings [ \"BETX\" ])) self . misserror_bby . append ( _get_rms ( cpymad_betabeatings [ \"BETY\" ])) self . max_misserror_bbx . append ( cpymad_betabeatings [ \"BETX\" ] . max ()) self . max_misserror_bby . append ( cpymad_betabeatings [ \"BETY\" ] . max ()) logger . trace ( \"Getting betatron functions at IP1 and IP5\" ) # cpymad naming: lowercase and appended with :beam_number self . ip1_misserror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip1_misserror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip5_misserror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) self . ip5_misserror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) def update_miss_from_seeds ( self , temp_data ) -> None : \"\"\" Append computed rms values for a group of seeds, to misalignment result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. \"\"\" self . misserror_bbx . append ( _get_rms ( temp_data . misserror_bbx )) self . misserror_bby . append ( _get_rms ( temp_data . misserror_bby )) self . max_misserror_bbx . append ( _get_rms ( temp_data . max_misserror_bbx )) self . max_misserror_bby . append ( _get_rms ( temp_data . max_misserror_bby )) self . ip1_misserror_bbx . append ( _get_rms ( temp_data . ip1_misserror_bbx )) self . ip1_misserror_bby . append ( _get_rms ( temp_data . ip1_misserror_bby )) self . ip5_misserror_bbx . append ( _get_rms ( temp_data . ip5_misserror_bbx )) self . ip5_misserror_bby . append ( _get_rms ( temp_data . ip5_misserror_bby )) def to_pandas ( self , * args , ** kwargs ) -> pd . DataFrame : \"\"\" Exports stored values as a pandas DataFrame. Returns: A `pandas.DataFrame` object with the instance's attributes as columns. \"\"\" return pd . DataFrame ( self . dict ( * args , ** kwargs )) class StdevValues ( BaseModel ): \"\"\" Simple class to store and transfer standard deviation values. Class attributes are as follows: \"stdev_tf_x\": \"Horizontal standard deviation values from field errors\", \"stdev_tf_y\": \"Vertical standard deviation values from field errors\", \"ip1_stdev_tf_x\": \"Horizontal standard deviation values from field errors at IP1\", \"ip1_stdev_tf_y\": \"Vertical standard deviation values from field errors at IP1\", \"ip5_stdev_tf_x\": \"Horizontal standard deviation values from field errors at IP5\", \"ip5_stdev_tf_y\": \"Vertical standard deviation values from field errors at IP5\", \"max_stdev_tf_x\": \"Maximal horizontal standard deviation values from field errors\", \"max_stdev_tf_y\": \"Maximal vertical standard deviation values from field errors\", \"stdev_miss_x\": \"Horizontal standard deviation values from misalignment errors\", \"stdev_miss_y\": \"Horizontal standard deviation values from misalignment errors\", \"ip1_stdev_miss_x\": \"Horizontal standard deviation values from misalignment errors at IP1\", \"ip1_stdev_miss_y\": \"Vertical standard deviation values from misalignment errors at IP1\", \"ip5_stdev_miss_x\": \"Horizontal standard deviation values from misalignment errors at IP5\", \"ip5_stdev_miss_y\": \"Vertical standard deviation values from misalignment errors at IP5\", \"max_stdev_miss_x\": \"Maximal horizontal standard deviation values from misalignment errors\", \"max_stdev_miss_y\": \"Maximal vertical standard deviation values from misalignment errors\", \"\"\" stdev_tf_x : List [ float ] = [] stdev_tf_y : List [ float ] = [] ip1_stdev_tf_x : List [ float ] = [] ip1_stdev_tf_y : List [ float ] = [] ip5_stdev_tf_x : List [ float ] = [] ip5_stdev_tf_y : List [ float ] = [] max_stdev_tf_x : List [ float ] = [] max_stdev_tf_y : List [ float ] = [] stdev_miss_x : List [ float ] = [] stdev_miss_y : List [ float ] = [] ip1_stdev_miss_x : List [ float ] = [] ip1_stdev_miss_y : List [ float ] = [] ip5_stdev_miss_x : List [ float ] = [] ip5_stdev_miss_y : List [ float ] = [] max_stdev_miss_x : List [ float ] = [] max_stdev_miss_y : List [ float ] = [] def describe ( self ) -> None : \"\"\" Simple print statement of instance attributes. \"\"\" for attribute , value in self . dict () . items (): print ( f \" { attribute : <20 } { value } \" ) def update_tf ( self , temp_data ) -> None : \"\"\" Append computed stdev values for a group of seeds, to field errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. Returns: Nothing, updates inplace. \"\"\" self . stdev_tf_x . append ( np . std ( temp_data . tferror_bbx )) self . stdev_tf_y . append ( np . std ( temp_data . tferror_bby )) self . max_stdev_tf_x . append ( np . std ( temp_data . max_tferror_bbx )) self . max_stdev_tf_y . append ( np . std ( temp_data . max_tferror_bby )) self . ip1_stdev_tf_x . append ( np . std ( temp_data . ip1_tferror_bbx )) self . ip1_stdev_tf_y . append ( np . std ( temp_data . ip1_tferror_bby )) self . ip5_stdev_tf_x . append ( np . std ( temp_data . ip5_tferror_bbx )) self . ip5_stdev_tf_y . append ( np . std ( temp_data . ip5_tferror_bby )) def update_miss ( self , temp_data ) -> None : \"\"\" Append computed rms values for a group of seeds, to misalignment errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. Returns: Nothing, updates inplace. \"\"\" self . stdev_miss_x . append ( np . std ( temp_data . misserror_bbx )) self . stdev_miss_y . append ( np . std ( temp_data . misserror_bby )) self . max_stdev_miss_x . append ( np . std ( temp_data . max_misserror_bbx )) self . max_stdev_miss_y . append ( np . std ( temp_data . max_misserror_bby )) self . ip1_stdev_miss_x . append ( np . std ( temp_data . ip1_misserror_bbx )) self . ip1_stdev_miss_y . append ( np . std ( temp_data . ip1_misserror_bby )) self . ip5_stdev_miss_x . append ( np . std ( temp_data . ip5_misserror_bbx )) self . ip5_stdev_miss_y . append ( np . std ( temp_data . ip5_misserror_bby )) def to_pandas ( self , * args , ** kwargs ) -> pd . DataFrame : \"\"\" Simple function to export stored values as a pandas dataframe. Returns: A `pandas.DataFrame` object with the instance's attributes as columns. \"\"\" return pd . DataFrame ( self . dict ( * args , ** kwargs )) def _get_rms ( values_list : List [ float ]) -> float : \"\"\" Get the root mean square of a list of values. Args: values_list (List[float]): a distribution of values. Returns: The root mean square of said distribution. \"\"\" try : return np . sqrt ( np . sum ( i ** 2 for i in values_list ) / len ( values_list )) except ZeroDivisionError as issue : logger . exception ( \"An empty list was provided, check the simulation logs to understand why.\" ) raise ZeroDivisionError ( \"No values were provided\" ) from issue Classes BetaBeatValues class BetaBeatValues ( __pydantic_self__ , ** data : Any ) View Source class BetaBeatValues ( BaseModel ) : \"\"\" Simple class to store and transfer beta-beating values. Class attributes are as follows: \" tferror_bbx \": \" Horizontal beta - beating values from field errors \", \" tferror_bby \": \" Vertical beta - beating values from field errors \", \" ip1_tferror_bbx \": \" Horizontal beta - beating values from field errors at IP1 \", \" ip1_tferror_bby \": \" Vertical beta - beating values from field errors at IP1 \", \" ip5_tferror_bbx \": \" Horizontal beta - beating values from field errors at IP5 \", \" ip5_tferror_bby \": \" Vertical beta - beating values from field errors at IP5 \", \" max_tferror_bbx \": \" Maximal horizontal beta - beating values from field errors \", \" max_tferror_bby \": \" Maximal vertical beta - beating values from field errors \", \" misserror_bbx \": \" Horizontal beta - beating values from misalignment errors \", \" misserror_bby \": \" Horizontal beta - beating values from misalignment errors \", \" ip1_misserror_bbx \": \" Horizontal beta - beating values from misalignment errors at IP1 \", \" ip1_misserror_bby \": \" Vertical beta - beating values from misalignment errors at IP1 \", \" ip5_misserror_bbx \": \" Horizontal beta - beating values from misalignment errors at IP5 \", \" ip5_misserror_bby \": \" Vertical beta - beating values from misalignment errors at IP5 \", \" max_misserror_bbx \": \" Maximal horizontal beta - beating values from misalignment errors \", \" max_misserror_bby \": \" Maximal vertical beta - beating values from misalignment errors \", \"\"\" tferror_bbx : List [ float ] = [] tferror_bby : List [ float ] = [] ip1_tferror_bbx : List [ float ] = [] ip1_tferror_bby : List [ float ] = [] ip5_tferror_bbx : List [ float ] = [] ip5_tferror_bby : List [ float ] = [] max_tferror_bbx : List [ float ] = [] max_tferror_bby : List [ float ] = [] misserror_bbx : List [ float ] = [] misserror_bby : List [ float ] = [] ip1_misserror_bbx : List [ float ] = [] ip1_misserror_bby : List [ float ] = [] ip5_misserror_bbx : List [ float ] = [] ip5_misserror_bby : List [ float ] = [] max_misserror_bbx : List [ float ] = [] max_misserror_bby : List [ float ] = [] def describe ( self ) -> None : \"\"\" Simple print statement of instance attributes. \"\"\" for attribute , value in self . dict (). items () : print ( f \"{attribute:<20} {value}\" ) def update_tf_from_cpymad ( self , cpymad_betabeatings : pd . DataFrame ) -> None : \"\"\" This is to update a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Args: cpymad_betabeatings (pd.DataFrame): the beta-beatings from the simulation, compared to the nominal twiss from a reference run. \"\"\" logger . trace ( \"Getting rms and max values for betatron functions of provided run\" ) self . tferror_bbx . append ( _get_rms ( cpymad_betabeatings [ \"BETX\" ] )) self . tferror_bby . append ( _get_rms ( cpymad_betabeatings [ \"BETY\" ] )) self . max_tferror_bbx . append ( cpymad_betabeatings [ \"BETX\" ] . max ()) self . max_tferror_bby . append ( cpymad_betabeatings [ \"BETY\" ] . max ()) logger . trace ( \"Getting betatron functions at IP1 and IP5\" ) # cpymad naming : lowercase and appended with : beam_number self . ip1_tferror_bbx . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings.NAME == \"ip1:1\" ][ 0 ] ) self . ip1_tferror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings.NAME == \"ip1:1\" ][ 0 ] ) self . ip5_tferror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings.NAME == \"ip5:1\" ][ 0 ] ) self . ip5_tferror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings.NAME == \"ip5:1\" ][ 0 ] ) def update_tf_from_seeds ( self , temp_data ) -> None : \"\"\" Updates the error's beta-beatings values after having ran simulations for all seeds. Append computed rms values for a group of seeds, to field errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. \"\"\" self . tferror_bbx . append ( _get_rms ( temp_data . tferror_bbx )) self . tferror_bby . append ( _get_rms ( temp_data . tferror_bby )) self . max_tferror_bbx . append ( _get_rms ( temp_data . max_tferror_bbx )) self . max_tferror_bby . append ( _get_rms ( temp_data . max_tferror_bby )) self . ip1_tferror_bbx . append ( _get_rms ( temp_data . ip1_tferror_bbx )) self . ip1_tferror_bby . append ( _get_rms ( temp_data . ip1_tferror_bby )) self . ip5_tferror_bbx . append ( _get_rms ( temp_data . ip5_tferror_bbx )) self . ip5_tferror_bby . append ( _get_rms ( temp_data . ip5_tferror_bby )) def update_miss_from_cpymad ( self , cpymad_betabeatings : pd . DataFrame ) -> None : \"\"\" Updates a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Args: cpymad_betabeatings (pd.DataFrame): the beta-beatings from the simulation, compared to the nominal twiss from a reference run. \"\"\" logger . trace ( \"Getting rms and max values for betatron functions of provided run\" ) self . misserror_bbx . append ( _get_rms ( cpymad_betabeatings [ \"BETX\" ] )) self . misserror_bby . append ( _get_rms ( cpymad_betabeatings [ \"BETY\" ] )) self . max_misserror_bbx . append ( cpymad_betabeatings [ \"BETX\" ] . max ()) self . max_misserror_bby . append ( cpymad_betabeatings [ \"BETY\" ] . max ()) logger . trace ( \"Getting betatron functions at IP1 and IP5\" ) # cpymad naming : lowercase and appended with : beam_number self . ip1_misserror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings.NAME == \"ip1:1\" ][ 0 ] ) self . ip1_misserror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings.NAME == \"ip1:1\" ][ 0 ] ) self . ip5_misserror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings.NAME == \"ip5:1\" ][ 0 ] ) self . ip5_misserror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings.NAME == \"ip5:1\" ][ 0 ] ) def update_miss_from_seeds ( self , temp_data ) -> None : \"\"\" Append computed rms values for a group of seeds, to misalignment result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. \"\"\" self . misserror_bbx . append ( _get_rms ( temp_data . misserror_bbx )) self . misserror_bby . append ( _get_rms ( temp_data . misserror_bby )) self . max_misserror_bbx . append ( _get_rms ( temp_data . max_misserror_bbx )) self . max_misserror_bby . append ( _get_rms ( temp_data . max_misserror_bby )) self . ip1_misserror_bbx . append ( _get_rms ( temp_data . ip1_misserror_bbx )) self . ip1_misserror_bby . append ( _get_rms ( temp_data . ip1_misserror_bby )) self . ip5_misserror_bbx . append ( _get_rms ( temp_data . ip5_misserror_bbx )) self . ip5_misserror_bby . append ( _get_rms ( temp_data . ip5_misserror_bby )) def to_pandas ( self , * args , ** kwargs ) -> pd . DataFrame : \"\"\" Exports stored values as a pandas DataFrame. Returns: A `pandas.DataFrame` object with the instance's attributes as columns. \"\"\" return pd . DataFrame ( self . dict ( * args , ** kwargs )) Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fields Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance describe def describe ( self ) -> None Simple print statement of instance attributes. View Source def describe ( self ) -> None : \"\"\" Simple print statement of instance attributes. \"\"\" for attribute , value in self . dict (). items (): print ( f \"{attribute:<20} {value}\" ) dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . to_pandas def to_pandas ( self , * args , ** kwargs ) -> pandas . core . frame . DataFrame Exports stored values as a pandas DataFrame. Returns: Type Description None A pandas.DataFrame object with the instance's attributes as columns. View Source def to_pandas ( self , * args , ** kwargs ) -> pd . DataFrame : \"\"\" Exports stored values as a pandas DataFrame. Returns: A `pandas.DataFrame` object with the instance's attributes as columns. \"\"\" return pd . DataFrame ( self . dict ( * args , ** kwargs )) to_string def to_string ( self , pretty : bool = False ) -> 'unicode' update_miss_from_cpymad def update_miss_from_cpymad ( self , cpymad_betabeatings : pandas . core . frame . DataFrame ) -> None Updates a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Parameters: Name Type Description Default cpymad_betabeatings pd.DataFrame the beta-beatings from the simulation, compared to the nominal twiss from a reference run. None View Source def update_miss_from_cpymad ( self , cpymad_betabeatings : pd . DataFrame ) -> None : \"\"\" Updates a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Args: cpymad_betabeatings (pd.DataFrame): the beta-beatings from the simulation, compared to the nominal twiss from a reference run. \"\"\" logger . trace ( \"Getting rms and max values for betatron functions of provided run\" ) self . misserror_bbx . append ( _get_rms ( cpymad_betabeatings [ \"BETX\" ])) self . misserror_bby . append ( _get_rms ( cpymad_betabeatings [ \"BETY\" ])) self . max_misserror_bbx . append ( cpymad_betabeatings [ \"BETX\" ]. max ()) self . max_misserror_bby . append ( cpymad_betabeatings [ \"BETY\" ]. max ()) logger . trace ( \"Getting betatron functions at IP1 and IP5\" ) # cpymad naming : lowercase and appended with : beam_number self . ip1_misserror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip1_misserror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip5_misserror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) self . ip5_misserror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) update_miss_from_seeds def update_miss_from_seeds ( self , temp_data ) -> None Append computed rms values for a group of seeds, to misalignment result values. Parameters: Name Type Description Default temp_data None a BetaBeatValues object with the seeds' results. None View Source def update_miss_from_seeds ( self , temp_data ) -> None : \" \"\" Append computed rms values for a group of seeds, to misalignment result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. \"\" \" self . misserror_bbx . append ( _get_rms ( temp_data . misserror_bbx )) self . misserror_bby . append ( _get_rms ( temp_data . misserror_bby )) self . max_misserror_bbx . append ( _get_rms ( temp_data . max_misserror_bbx )) self . max_misserror_bby . append ( _get_rms ( temp_data . max_misserror_bby )) self . ip1_misserror_bbx . append ( _get_rms ( temp_data . ip1_misserror_bbx )) self . ip1_misserror_bby . append ( _get_rms ( temp_data . ip1_misserror_bby )) self . ip5_misserror_bbx . append ( _get_rms ( temp_data . ip5_misserror_bbx )) self . ip5_misserror_bby . append ( _get_rms ( temp_data . ip5_misserror_bby )) update_tf_from_cpymad def update_tf_from_cpymad ( self , cpymad_betabeatings : pandas . core . frame . DataFrame ) -> None This is to update a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Parameters: Name Type Description Default cpymad_betabeatings pd.DataFrame the beta-beatings from the simulation, compared to the nominal twiss from a reference run. None View Source def update_tf_from_cpymad ( self , cpymad_betabeatings : pd . DataFrame ) -> None : \"\"\" This is to update a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Args: cpymad_betabeatings (pd.DataFrame): the beta-beatings from the simulation, compared to the nominal twiss from a reference run. \"\"\" logger . trace ( \"Getting rms and max values for betatron functions of provided run\" ) self . tferror_bbx . append ( _get_rms ( cpymad_betabeatings [ \"BETX\" ])) self . tferror_bby . append ( _get_rms ( cpymad_betabeatings [ \"BETY\" ])) self . max_tferror_bbx . append ( cpymad_betabeatings [ \"BETX\" ]. max ()) self . max_tferror_bby . append ( cpymad_betabeatings [ \"BETY\" ]. max ()) logger . trace ( \"Getting betatron functions at IP1 and IP5\" ) # cpymad naming : lowercase and appended with : beam_number self . ip1_tferror_bbx . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip1_tferror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip5_tferror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) self . ip5_tferror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) update_tf_from_seeds def update_tf_from_seeds ( self , temp_data ) -> None Updates the error's beta-beatings values after having ran simulations for all seeds. Append computed rms values for a group of seeds, to field errors result values. Parameters: Name Type Description Default temp_data None a BetaBeatValues object with the seeds' results. None View Source def update_tf_from_seeds ( self , temp_data ) -> None : \" \"\" Updates the error's beta-beatings values after having ran simulations for all seeds. Append computed rms values for a group of seeds, to field errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. \"\" \" self . tferror_bbx . append ( _get_rms ( temp_data . tferror_bbx )) self . tferror_bby . append ( _get_rms ( temp_data . tferror_bby )) self . max_tferror_bbx . append ( _get_rms ( temp_data . max_tferror_bbx )) self . max_tferror_bby . append ( _get_rms ( temp_data . max_tferror_bby )) self . ip1_tferror_bbx . append ( _get_rms ( temp_data . ip1_tferror_bbx )) self . ip1_tferror_bby . append ( _get_rms ( temp_data . ip1_tferror_bby )) self . ip5_tferror_bbx . append ( _get_rms ( temp_data . ip5_tferror_bbx )) self . ip5_tferror_bby . append ( _get_rms ( temp_data . ip5_tferror_bby )) StdevValues class StdevValues ( __pydantic_self__ , ** data : Any ) View Source class StdevValues ( BaseModel ) : \"\"\" Simple class to store and transfer standard deviation values. Class attributes are as follows: \" stdev_tf_x \": \" Horizontal standard deviation values from field errors \", \" stdev_tf_y \": \" Vertical standard deviation values from field errors \", \" ip1_stdev_tf_x \": \" Horizontal standard deviation values from field errors at IP1 \", \" ip1_stdev_tf_y \": \" Vertical standard deviation values from field errors at IP1 \", \" ip5_stdev_tf_x \": \" Horizontal standard deviation values from field errors at IP5 \", \" ip5_stdev_tf_y \": \" Vertical standard deviation values from field errors at IP5 \", \" max_stdev_tf_x \": \" Maximal horizontal standard deviation values from field errors \", \" max_stdev_tf_y \": \" Maximal vertical standard deviation values from field errors \", \" stdev_miss_x \": \" Horizontal standard deviation values from misalignment errors \", \" stdev_miss_y \": \" Horizontal standard deviation values from misalignment errors \", \" ip1_stdev_miss_x \": \" Horizontal standard deviation values from misalignment errors at IP1 \", \" ip1_stdev_miss_y \": \" Vertical standard deviation values from misalignment errors at IP1 \", \" ip5_stdev_miss_x \": \" Horizontal standard deviation values from misalignment errors at IP5 \", \" ip5_stdev_miss_y \": \" Vertical standard deviation values from misalignment errors at IP5 \", \" max_stdev_miss_x \": \" Maximal horizontal standard deviation values from misalignment errors \", \" max_stdev_miss_y \": \" Maximal vertical standard deviation values from misalignment errors \", \"\"\" stdev_tf_x : List [ float ] = [] stdev_tf_y : List [ float ] = [] ip1_stdev_tf_x : List [ float ] = [] ip1_stdev_tf_y : List [ float ] = [] ip5_stdev_tf_x : List [ float ] = [] ip5_stdev_tf_y : List [ float ] = [] max_stdev_tf_x : List [ float ] = [] max_stdev_tf_y : List [ float ] = [] stdev_miss_x : List [ float ] = [] stdev_miss_y : List [ float ] = [] ip1_stdev_miss_x : List [ float ] = [] ip1_stdev_miss_y : List [ float ] = [] ip5_stdev_miss_x : List [ float ] = [] ip5_stdev_miss_y : List [ float ] = [] max_stdev_miss_x : List [ float ] = [] max_stdev_miss_y : List [ float ] = [] def describe ( self ) -> None : \"\"\" Simple print statement of instance attributes. \"\"\" for attribute , value in self . dict (). items () : print ( f \"{attribute:<20} {value}\" ) def update_tf ( self , temp_data ) -> None : \"\"\" Append computed stdev values for a group of seeds, to field errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. Returns: Nothing, updates inplace. \"\"\" self . stdev_tf_x . append ( np . std ( temp_data . tferror_bbx )) self . stdev_tf_y . append ( np . std ( temp_data . tferror_bby )) self . max_stdev_tf_x . append ( np . std ( temp_data . max_tferror_bbx )) self . max_stdev_tf_y . append ( np . std ( temp_data . max_tferror_bby )) self . ip1_stdev_tf_x . append ( np . std ( temp_data . ip1_tferror_bbx )) self . ip1_stdev_tf_y . append ( np . std ( temp_data . ip1_tferror_bby )) self . ip5_stdev_tf_x . append ( np . std ( temp_data . ip5_tferror_bbx )) self . ip5_stdev_tf_y . append ( np . std ( temp_data . ip5_tferror_bby )) def update_miss ( self , temp_data ) -> None : \"\"\" Append computed rms values for a group of seeds, to misalignment errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. Returns: Nothing, updates inplace. \"\"\" self . stdev_miss_x . append ( np . std ( temp_data . misserror_bbx )) self . stdev_miss_y . append ( np . std ( temp_data . misserror_bby )) self . max_stdev_miss_x . append ( np . std ( temp_data . max_misserror_bbx )) self . max_stdev_miss_y . append ( np . std ( temp_data . max_misserror_bby )) self . ip1_stdev_miss_x . append ( np . std ( temp_data . ip1_misserror_bbx )) self . ip1_stdev_miss_y . append ( np . std ( temp_data . ip1_misserror_bby )) self . ip5_stdev_miss_x . append ( np . std ( temp_data . ip5_misserror_bbx )) self . ip5_stdev_miss_y . append ( np . std ( temp_data . ip5_misserror_bby )) def to_pandas ( self , * args , ** kwargs ) -> pd . DataFrame : \"\"\" Simple function to export stored values as a pandas dataframe. Returns: A `pandas.DataFrame` object with the instance's attributes as columns. \"\"\" return pd . DataFrame ( self . dict ( * args , ** kwargs )) Ancestors (in MRO) pydantic.main.BaseModel pydantic.utils.Representation Class variables Config Static methods construct def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed. from_orm def from_orm ( obj : Any ) -> 'Model' parse_file def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' parse_obj def parse_obj ( obj : Any ) -> 'Model' parse_raw def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model' schema def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny' schema_json def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode' update_forward_refs def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns. validate def validate ( value : Any ) -> 'Model' Instance variables fields Methods copy def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance describe def describe ( self ) -> None Simple print statement of instance attributes. View Source def describe ( self ) -> None : \"\"\" Simple print statement of instance attributes. \"\"\" for attribute , value in self . dict (). items (): print ( f \"{attribute:<20} {value}\" ) dict def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. json def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() . to_pandas def to_pandas ( self , * args , ** kwargs ) -> pandas . core . frame . DataFrame Simple function to export stored values as a pandas dataframe. Returns: Type Description None A pandas.DataFrame object with the instance's attributes as columns. View Source def to_pandas ( self , * args , ** kwargs ) -> pd . DataFrame : \"\"\" Simple function to export stored values as a pandas dataframe. Returns: A `pandas.DataFrame` object with the instance's attributes as columns. \"\"\" return pd . DataFrame ( self . dict ( * args , ** kwargs )) to_string def to_string ( self , pretty : bool = False ) -> 'unicode' update_miss def update_miss ( self , temp_data ) -> None Append computed rms values for a group of seeds, to misalignment errors result values. Parameters: Name Type Description Default temp_data None a BetaBeatValues object with the seeds' results. None Returns: Type Description None Nothing, updates inplace. View Source def update_miss ( self , temp_data ) -> None : \" \"\" Append computed rms values for a group of seeds, to misalignment errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. Returns: Nothing, updates inplace. \"\" \" self . stdev_miss_x . append ( np . std ( temp_data . misserror_bbx )) self . stdev_miss_y . append ( np . std ( temp_data . misserror_bby )) self . max_stdev_miss_x . append ( np . std ( temp_data . max_misserror_bbx )) self . max_stdev_miss_y . append ( np . std ( temp_data . max_misserror_bby )) self . ip1_stdev_miss_x . append ( np . std ( temp_data . ip1_misserror_bbx )) self . ip1_stdev_miss_y . append ( np . std ( temp_data . ip1_misserror_bby )) self . ip5_stdev_miss_x . append ( np . std ( temp_data . ip5_misserror_bbx )) self . ip5_stdev_miss_y . append ( np . std ( temp_data . ip5_misserror_bby )) update_tf def update_tf ( self , temp_data ) -> None Append computed stdev values for a group of seeds, to field errors result values. Parameters: Name Type Description Default temp_data None a BetaBeatValues object with the seeds' results. None Returns: Type Description None Nothing, updates inplace. View Source def update_tf ( self , temp_data ) -> None : \" \"\" Append computed stdev values for a group of seeds, to field errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. Returns: Nothing, updates inplace. \"\" \" self . stdev_tf_x . append ( np . std ( temp_data . tferror_bbx )) self . stdev_tf_y . append ( np . std ( temp_data . tferror_bby )) self . max_stdev_tf_x . append ( np . std ( temp_data . max_tferror_bbx )) self . max_stdev_tf_y . append ( np . std ( temp_data . max_tferror_bby )) self . ip1_stdev_tf_x . append ( np . std ( temp_data . ip1_tferror_bbx )) self . ip1_stdev_tf_y . append ( np . std ( temp_data . ip1_tferror_bby )) self . ip5_stdev_tf_x . append ( np . std ( temp_data . ip5_tferror_bbx )) self . ip5_stdev_tf_y . append ( np . std ( temp_data . ip5_tferror_bby ))","title":"Data Classes"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#module-pyhdtoolkitscriptstriplet_errorsdata_classes","text":"Module scripts.triplet_errors.data_classes Created on 2019.06.15 View Source \"\"\" Module scripts.triplet_errors.data_classes ------------------------------------------ Created on 2019.06.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A few classes that will be useful to store values calculated from the results of the GridCompute Algorithm. \"\"\" from typing import List import numpy as np import pandas as pd from loguru import logger from pydantic import BaseModel class BetaBeatValues ( BaseModel ): \"\"\" Simple class to store and transfer beta-beating values. Class attributes are as follows: \"tferror_bbx\": \"Horizontal beta-beating values from field errors\", \"tferror_bby\": \"Vertical beta-beating values from field errors\", \"ip1_tferror_bbx\": \"Horizontal beta-beating values from field errors at IP1\", \"ip1_tferror_bby\": \"Vertical beta-beating values from field errors at IP1\", \"ip5_tferror_bbx\": \"Horizontal beta-beating values from field errors at IP5\", \"ip5_tferror_bby\": \"Vertical beta-beating values from field errors at IP5\", \"max_tferror_bbx\": \"Maximal horizontal beta-beating values from field errors\", \"max_tferror_bby\": \"Maximal vertical beta-beating values from field errors\", \"misserror_bbx\": \"Horizontal beta-beating values from misalignment errors\", \"misserror_bby\": \"Horizontal beta-beating values from misalignment errors\", \"ip1_misserror_bbx\": \"Horizontal beta-beating values from misalignment errors at IP1\", \"ip1_misserror_bby\": \"Vertical beta-beating values from misalignment errors at IP1\", \"ip5_misserror_bbx\": \"Horizontal beta-beating values from misalignment errors at IP5\", \"ip5_misserror_bby\": \"Vertical beta-beating values from misalignment errors at IP5\", \"max_misserror_bbx\": \"Maximal horizontal beta-beating values from misalignment errors\", \"max_misserror_bby\": \"Maximal vertical beta-beating values from misalignment errors\", \"\"\" tferror_bbx : List [ float ] = [] tferror_bby : List [ float ] = [] ip1_tferror_bbx : List [ float ] = [] ip1_tferror_bby : List [ float ] = [] ip5_tferror_bbx : List [ float ] = [] ip5_tferror_bby : List [ float ] = [] max_tferror_bbx : List [ float ] = [] max_tferror_bby : List [ float ] = [] misserror_bbx : List [ float ] = [] misserror_bby : List [ float ] = [] ip1_misserror_bbx : List [ float ] = [] ip1_misserror_bby : List [ float ] = [] ip5_misserror_bbx : List [ float ] = [] ip5_misserror_bby : List [ float ] = [] max_misserror_bbx : List [ float ] = [] max_misserror_bby : List [ float ] = [] def describe ( self ) -> None : \"\"\" Simple print statement of instance attributes. \"\"\" for attribute , value in self . dict () . items (): print ( f \" { attribute : <20 } { value } \" ) def update_tf_from_cpymad ( self , cpymad_betabeatings : pd . DataFrame ) -> None : \"\"\" This is to update a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Args: cpymad_betabeatings (pd.DataFrame): the beta-beatings from the simulation, compared to the nominal twiss from a reference run. \"\"\" logger . trace ( \"Getting rms and max values for betatron functions of provided run\" ) self . tferror_bbx . append ( _get_rms ( cpymad_betabeatings [ \"BETX\" ])) self . tferror_bby . append ( _get_rms ( cpymad_betabeatings [ \"BETY\" ])) self . max_tferror_bbx . append ( cpymad_betabeatings [ \"BETX\" ] . max ()) self . max_tferror_bby . append ( cpymad_betabeatings [ \"BETY\" ] . max ()) logger . trace ( \"Getting betatron functions at IP1 and IP5\" ) # cpymad naming: lowercase and appended with :beam_number self . ip1_tferror_bbx . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip1_tferror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip5_tferror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) self . ip5_tferror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) def update_tf_from_seeds ( self , temp_data ) -> None : \"\"\" Updates the error's beta-beatings values after having ran simulations for all seeds. Append computed rms values for a group of seeds, to field errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. \"\"\" self . tferror_bbx . append ( _get_rms ( temp_data . tferror_bbx )) self . tferror_bby . append ( _get_rms ( temp_data . tferror_bby )) self . max_tferror_bbx . append ( _get_rms ( temp_data . max_tferror_bbx )) self . max_tferror_bby . append ( _get_rms ( temp_data . max_tferror_bby )) self . ip1_tferror_bbx . append ( _get_rms ( temp_data . ip1_tferror_bbx )) self . ip1_tferror_bby . append ( _get_rms ( temp_data . ip1_tferror_bby )) self . ip5_tferror_bbx . append ( _get_rms ( temp_data . ip5_tferror_bbx )) self . ip5_tferror_bby . append ( _get_rms ( temp_data . ip5_tferror_bby )) def update_miss_from_cpymad ( self , cpymad_betabeatings : pd . DataFrame ) -> None : \"\"\" Updates a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Args: cpymad_betabeatings (pd.DataFrame): the beta-beatings from the simulation, compared to the nominal twiss from a reference run. \"\"\" logger . trace ( \"Getting rms and max values for betatron functions of provided run\" ) self . misserror_bbx . append ( _get_rms ( cpymad_betabeatings [ \"BETX\" ])) self . misserror_bby . append ( _get_rms ( cpymad_betabeatings [ \"BETY\" ])) self . max_misserror_bbx . append ( cpymad_betabeatings [ \"BETX\" ] . max ()) self . max_misserror_bby . append ( cpymad_betabeatings [ \"BETY\" ] . max ()) logger . trace ( \"Getting betatron functions at IP1 and IP5\" ) # cpymad naming: lowercase and appended with :beam_number self . ip1_misserror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip1_misserror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip5_misserror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) self . ip5_misserror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) def update_miss_from_seeds ( self , temp_data ) -> None : \"\"\" Append computed rms values for a group of seeds, to misalignment result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. \"\"\" self . misserror_bbx . append ( _get_rms ( temp_data . misserror_bbx )) self . misserror_bby . append ( _get_rms ( temp_data . misserror_bby )) self . max_misserror_bbx . append ( _get_rms ( temp_data . max_misserror_bbx )) self . max_misserror_bby . append ( _get_rms ( temp_data . max_misserror_bby )) self . ip1_misserror_bbx . append ( _get_rms ( temp_data . ip1_misserror_bbx )) self . ip1_misserror_bby . append ( _get_rms ( temp_data . ip1_misserror_bby )) self . ip5_misserror_bbx . append ( _get_rms ( temp_data . ip5_misserror_bbx )) self . ip5_misserror_bby . append ( _get_rms ( temp_data . ip5_misserror_bby )) def to_pandas ( self , * args , ** kwargs ) -> pd . DataFrame : \"\"\" Exports stored values as a pandas DataFrame. Returns: A `pandas.DataFrame` object with the instance's attributes as columns. \"\"\" return pd . DataFrame ( self . dict ( * args , ** kwargs )) class StdevValues ( BaseModel ): \"\"\" Simple class to store and transfer standard deviation values. Class attributes are as follows: \"stdev_tf_x\": \"Horizontal standard deviation values from field errors\", \"stdev_tf_y\": \"Vertical standard deviation values from field errors\", \"ip1_stdev_tf_x\": \"Horizontal standard deviation values from field errors at IP1\", \"ip1_stdev_tf_y\": \"Vertical standard deviation values from field errors at IP1\", \"ip5_stdev_tf_x\": \"Horizontal standard deviation values from field errors at IP5\", \"ip5_stdev_tf_y\": \"Vertical standard deviation values from field errors at IP5\", \"max_stdev_tf_x\": \"Maximal horizontal standard deviation values from field errors\", \"max_stdev_tf_y\": \"Maximal vertical standard deviation values from field errors\", \"stdev_miss_x\": \"Horizontal standard deviation values from misalignment errors\", \"stdev_miss_y\": \"Horizontal standard deviation values from misalignment errors\", \"ip1_stdev_miss_x\": \"Horizontal standard deviation values from misalignment errors at IP1\", \"ip1_stdev_miss_y\": \"Vertical standard deviation values from misalignment errors at IP1\", \"ip5_stdev_miss_x\": \"Horizontal standard deviation values from misalignment errors at IP5\", \"ip5_stdev_miss_y\": \"Vertical standard deviation values from misalignment errors at IP5\", \"max_stdev_miss_x\": \"Maximal horizontal standard deviation values from misalignment errors\", \"max_stdev_miss_y\": \"Maximal vertical standard deviation values from misalignment errors\", \"\"\" stdev_tf_x : List [ float ] = [] stdev_tf_y : List [ float ] = [] ip1_stdev_tf_x : List [ float ] = [] ip1_stdev_tf_y : List [ float ] = [] ip5_stdev_tf_x : List [ float ] = [] ip5_stdev_tf_y : List [ float ] = [] max_stdev_tf_x : List [ float ] = [] max_stdev_tf_y : List [ float ] = [] stdev_miss_x : List [ float ] = [] stdev_miss_y : List [ float ] = [] ip1_stdev_miss_x : List [ float ] = [] ip1_stdev_miss_y : List [ float ] = [] ip5_stdev_miss_x : List [ float ] = [] ip5_stdev_miss_y : List [ float ] = [] max_stdev_miss_x : List [ float ] = [] max_stdev_miss_y : List [ float ] = [] def describe ( self ) -> None : \"\"\" Simple print statement of instance attributes. \"\"\" for attribute , value in self . dict () . items (): print ( f \" { attribute : <20 } { value } \" ) def update_tf ( self , temp_data ) -> None : \"\"\" Append computed stdev values for a group of seeds, to field errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. Returns: Nothing, updates inplace. \"\"\" self . stdev_tf_x . append ( np . std ( temp_data . tferror_bbx )) self . stdev_tf_y . append ( np . std ( temp_data . tferror_bby )) self . max_stdev_tf_x . append ( np . std ( temp_data . max_tferror_bbx )) self . max_stdev_tf_y . append ( np . std ( temp_data . max_tferror_bby )) self . ip1_stdev_tf_x . append ( np . std ( temp_data . ip1_tferror_bbx )) self . ip1_stdev_tf_y . append ( np . std ( temp_data . ip1_tferror_bby )) self . ip5_stdev_tf_x . append ( np . std ( temp_data . ip5_tferror_bbx )) self . ip5_stdev_tf_y . append ( np . std ( temp_data . ip5_tferror_bby )) def update_miss ( self , temp_data ) -> None : \"\"\" Append computed rms values for a group of seeds, to misalignment errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. Returns: Nothing, updates inplace. \"\"\" self . stdev_miss_x . append ( np . std ( temp_data . misserror_bbx )) self . stdev_miss_y . append ( np . std ( temp_data . misserror_bby )) self . max_stdev_miss_x . append ( np . std ( temp_data . max_misserror_bbx )) self . max_stdev_miss_y . append ( np . std ( temp_data . max_misserror_bby )) self . ip1_stdev_miss_x . append ( np . std ( temp_data . ip1_misserror_bbx )) self . ip1_stdev_miss_y . append ( np . std ( temp_data . ip1_misserror_bby )) self . ip5_stdev_miss_x . append ( np . std ( temp_data . ip5_misserror_bbx )) self . ip5_stdev_miss_y . append ( np . std ( temp_data . ip5_misserror_bby )) def to_pandas ( self , * args , ** kwargs ) -> pd . DataFrame : \"\"\" Simple function to export stored values as a pandas dataframe. Returns: A `pandas.DataFrame` object with the instance's attributes as columns. \"\"\" return pd . DataFrame ( self . dict ( * args , ** kwargs )) def _get_rms ( values_list : List [ float ]) -> float : \"\"\" Get the root mean square of a list of values. Args: values_list (List[float]): a distribution of values. Returns: The root mean square of said distribution. \"\"\" try : return np . sqrt ( np . sum ( i ** 2 for i in values_list ) / len ( values_list )) except ZeroDivisionError as issue : logger . exception ( \"An empty list was provided, check the simulation logs to understand why.\" ) raise ZeroDivisionError ( \"No values were provided\" ) from issue","title":"Module pyhdtoolkit.scripts.triplet_errors.data_classes"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#betabeatvalues","text":"class BetaBeatValues ( __pydantic_self__ , ** data : Any ) View Source class BetaBeatValues ( BaseModel ) : \"\"\" Simple class to store and transfer beta-beating values. Class attributes are as follows: \" tferror_bbx \": \" Horizontal beta - beating values from field errors \", \" tferror_bby \": \" Vertical beta - beating values from field errors \", \" ip1_tferror_bbx \": \" Horizontal beta - beating values from field errors at IP1 \", \" ip1_tferror_bby \": \" Vertical beta - beating values from field errors at IP1 \", \" ip5_tferror_bbx \": \" Horizontal beta - beating values from field errors at IP5 \", \" ip5_tferror_bby \": \" Vertical beta - beating values from field errors at IP5 \", \" max_tferror_bbx \": \" Maximal horizontal beta - beating values from field errors \", \" max_tferror_bby \": \" Maximal vertical beta - beating values from field errors \", \" misserror_bbx \": \" Horizontal beta - beating values from misalignment errors \", \" misserror_bby \": \" Horizontal beta - beating values from misalignment errors \", \" ip1_misserror_bbx \": \" Horizontal beta - beating values from misalignment errors at IP1 \", \" ip1_misserror_bby \": \" Vertical beta - beating values from misalignment errors at IP1 \", \" ip5_misserror_bbx \": \" Horizontal beta - beating values from misalignment errors at IP5 \", \" ip5_misserror_bby \": \" Vertical beta - beating values from misalignment errors at IP5 \", \" max_misserror_bbx \": \" Maximal horizontal beta - beating values from misalignment errors \", \" max_misserror_bby \": \" Maximal vertical beta - beating values from misalignment errors \", \"\"\" tferror_bbx : List [ float ] = [] tferror_bby : List [ float ] = [] ip1_tferror_bbx : List [ float ] = [] ip1_tferror_bby : List [ float ] = [] ip5_tferror_bbx : List [ float ] = [] ip5_tferror_bby : List [ float ] = [] max_tferror_bbx : List [ float ] = [] max_tferror_bby : List [ float ] = [] misserror_bbx : List [ float ] = [] misserror_bby : List [ float ] = [] ip1_misserror_bbx : List [ float ] = [] ip1_misserror_bby : List [ float ] = [] ip5_misserror_bbx : List [ float ] = [] ip5_misserror_bby : List [ float ] = [] max_misserror_bbx : List [ float ] = [] max_misserror_bby : List [ float ] = [] def describe ( self ) -> None : \"\"\" Simple print statement of instance attributes. \"\"\" for attribute , value in self . dict (). items () : print ( f \"{attribute:<20} {value}\" ) def update_tf_from_cpymad ( self , cpymad_betabeatings : pd . DataFrame ) -> None : \"\"\" This is to update a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Args: cpymad_betabeatings (pd.DataFrame): the beta-beatings from the simulation, compared to the nominal twiss from a reference run. \"\"\" logger . trace ( \"Getting rms and max values for betatron functions of provided run\" ) self . tferror_bbx . append ( _get_rms ( cpymad_betabeatings [ \"BETX\" ] )) self . tferror_bby . append ( _get_rms ( cpymad_betabeatings [ \"BETY\" ] )) self . max_tferror_bbx . append ( cpymad_betabeatings [ \"BETX\" ] . max ()) self . max_tferror_bby . append ( cpymad_betabeatings [ \"BETY\" ] . max ()) logger . trace ( \"Getting betatron functions at IP1 and IP5\" ) # cpymad naming : lowercase and appended with : beam_number self . ip1_tferror_bbx . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings.NAME == \"ip1:1\" ][ 0 ] ) self . ip1_tferror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings.NAME == \"ip1:1\" ][ 0 ] ) self . ip5_tferror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings.NAME == \"ip5:1\" ][ 0 ] ) self . ip5_tferror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings.NAME == \"ip5:1\" ][ 0 ] ) def update_tf_from_seeds ( self , temp_data ) -> None : \"\"\" Updates the error's beta-beatings values after having ran simulations for all seeds. Append computed rms values for a group of seeds, to field errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. \"\"\" self . tferror_bbx . append ( _get_rms ( temp_data . tferror_bbx )) self . tferror_bby . append ( _get_rms ( temp_data . tferror_bby )) self . max_tferror_bbx . append ( _get_rms ( temp_data . max_tferror_bbx )) self . max_tferror_bby . append ( _get_rms ( temp_data . max_tferror_bby )) self . ip1_tferror_bbx . append ( _get_rms ( temp_data . ip1_tferror_bbx )) self . ip1_tferror_bby . append ( _get_rms ( temp_data . ip1_tferror_bby )) self . ip5_tferror_bbx . append ( _get_rms ( temp_data . ip5_tferror_bbx )) self . ip5_tferror_bby . append ( _get_rms ( temp_data . ip5_tferror_bby )) def update_miss_from_cpymad ( self , cpymad_betabeatings : pd . DataFrame ) -> None : \"\"\" Updates a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Args: cpymad_betabeatings (pd.DataFrame): the beta-beatings from the simulation, compared to the nominal twiss from a reference run. \"\"\" logger . trace ( \"Getting rms and max values for betatron functions of provided run\" ) self . misserror_bbx . append ( _get_rms ( cpymad_betabeatings [ \"BETX\" ] )) self . misserror_bby . append ( _get_rms ( cpymad_betabeatings [ \"BETY\" ] )) self . max_misserror_bbx . append ( cpymad_betabeatings [ \"BETX\" ] . max ()) self . max_misserror_bby . append ( cpymad_betabeatings [ \"BETY\" ] . max ()) logger . trace ( \"Getting betatron functions at IP1 and IP5\" ) # cpymad naming : lowercase and appended with : beam_number self . ip1_misserror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings.NAME == \"ip1:1\" ][ 0 ] ) self . ip1_misserror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings.NAME == \"ip1:1\" ][ 0 ] ) self . ip5_misserror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings.NAME == \"ip5:1\" ][ 0 ] ) self . ip5_misserror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings.NAME == \"ip5:1\" ][ 0 ] ) def update_miss_from_seeds ( self , temp_data ) -> None : \"\"\" Append computed rms values for a group of seeds, to misalignment result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. \"\"\" self . misserror_bbx . append ( _get_rms ( temp_data . misserror_bbx )) self . misserror_bby . append ( _get_rms ( temp_data . misserror_bby )) self . max_misserror_bbx . append ( _get_rms ( temp_data . max_misserror_bbx )) self . max_misserror_bby . append ( _get_rms ( temp_data . max_misserror_bby )) self . ip1_misserror_bbx . append ( _get_rms ( temp_data . ip1_misserror_bbx )) self . ip1_misserror_bby . append ( _get_rms ( temp_data . ip1_misserror_bby )) self . ip5_misserror_bbx . append ( _get_rms ( temp_data . ip5_misserror_bbx )) self . ip5_misserror_bby . append ( _get_rms ( temp_data . ip5_misserror_bby )) def to_pandas ( self , * args , ** kwargs ) -> pd . DataFrame : \"\"\" Exports stored values as a pandas DataFrame. Returns: A `pandas.DataFrame` object with the instance's attributes as columns. \"\"\" return pd . DataFrame ( self . dict ( * args , ** kwargs ))","title":"BetaBeatValues"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#ancestors-in-mro","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#class-variables","text":"Config","title":"Class variables"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#construct","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed.","title":"construct"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#from_orm","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#parse_file","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#parse_obj","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#parse_raw","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#schema","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#schema_json","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#update_forward_refs","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#validate","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#instance-variables","text":"fields","title":"Instance variables"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#methods","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#copy","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#describe","text":"def describe ( self ) -> None Simple print statement of instance attributes. View Source def describe ( self ) -> None : \"\"\" Simple print statement of instance attributes. \"\"\" for attribute , value in self . dict (). items (): print ( f \"{attribute:<20} {value}\" )","title":"describe"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#dict","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#json","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#to_pandas","text":"def to_pandas ( self , * args , ** kwargs ) -> pandas . core . frame . DataFrame Exports stored values as a pandas DataFrame. Returns: Type Description None A pandas.DataFrame object with the instance's attributes as columns. View Source def to_pandas ( self , * args , ** kwargs ) -> pd . DataFrame : \"\"\" Exports stored values as a pandas DataFrame. Returns: A `pandas.DataFrame` object with the instance's attributes as columns. \"\"\" return pd . DataFrame ( self . dict ( * args , ** kwargs ))","title":"to_pandas"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#to_string","text":"def to_string ( self , pretty : bool = False ) -> 'unicode'","title":"to_string"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#update_miss_from_cpymad","text":"def update_miss_from_cpymad ( self , cpymad_betabeatings : pandas . core . frame . DataFrame ) -> None Updates a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Parameters: Name Type Description Default cpymad_betabeatings pd.DataFrame the beta-beatings from the simulation, compared to the nominal twiss from a reference run. None View Source def update_miss_from_cpymad ( self , cpymad_betabeatings : pd . DataFrame ) -> None : \"\"\" Updates a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Args: cpymad_betabeatings (pd.DataFrame): the beta-beatings from the simulation, compared to the nominal twiss from a reference run. \"\"\" logger . trace ( \"Getting rms and max values for betatron functions of provided run\" ) self . misserror_bbx . append ( _get_rms ( cpymad_betabeatings [ \"BETX\" ])) self . misserror_bby . append ( _get_rms ( cpymad_betabeatings [ \"BETY\" ])) self . max_misserror_bbx . append ( cpymad_betabeatings [ \"BETX\" ]. max ()) self . max_misserror_bby . append ( cpymad_betabeatings [ \"BETY\" ]. max ()) logger . trace ( \"Getting betatron functions at IP1 and IP5\" ) # cpymad naming : lowercase and appended with : beam_number self . ip1_misserror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip1_misserror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip5_misserror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) self . ip5_misserror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ])","title":"update_miss_from_cpymad"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#update_miss_from_seeds","text":"def update_miss_from_seeds ( self , temp_data ) -> None Append computed rms values for a group of seeds, to misalignment result values. Parameters: Name Type Description Default temp_data None a BetaBeatValues object with the seeds' results. None View Source def update_miss_from_seeds ( self , temp_data ) -> None : \" \"\" Append computed rms values for a group of seeds, to misalignment result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. \"\" \" self . misserror_bbx . append ( _get_rms ( temp_data . misserror_bbx )) self . misserror_bby . append ( _get_rms ( temp_data . misserror_bby )) self . max_misserror_bbx . append ( _get_rms ( temp_data . max_misserror_bbx )) self . max_misserror_bby . append ( _get_rms ( temp_data . max_misserror_bby )) self . ip1_misserror_bbx . append ( _get_rms ( temp_data . ip1_misserror_bbx )) self . ip1_misserror_bby . append ( _get_rms ( temp_data . ip1_misserror_bby )) self . ip5_misserror_bbx . append ( _get_rms ( temp_data . ip5_misserror_bbx )) self . ip5_misserror_bby . append ( _get_rms ( temp_data . ip5_misserror_bby ))","title":"update_miss_from_seeds"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#update_tf_from_cpymad","text":"def update_tf_from_cpymad ( self , cpymad_betabeatings : pandas . core . frame . DataFrame ) -> None This is to update a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Parameters: Name Type Description Default cpymad_betabeatings pd.DataFrame the beta-beatings from the simulation, compared to the nominal twiss from a reference run. None View Source def update_tf_from_cpymad ( self , cpymad_betabeatings : pd . DataFrame ) -> None : \"\"\" This is to update a temporary BetaBeatValues after having ran a simulation for a specific seed. Appends relevant values to the instance's attributes. Args: cpymad_betabeatings (pd.DataFrame): the beta-beatings from the simulation, compared to the nominal twiss from a reference run. \"\"\" logger . trace ( \"Getting rms and max values for betatron functions of provided run\" ) self . tferror_bbx . append ( _get_rms ( cpymad_betabeatings [ \"BETX\" ])) self . tferror_bby . append ( _get_rms ( cpymad_betabeatings [ \"BETY\" ])) self . max_tferror_bbx . append ( cpymad_betabeatings [ \"BETX\" ]. max ()) self . max_tferror_bby . append ( cpymad_betabeatings [ \"BETY\" ]. max ()) logger . trace ( \"Getting betatron functions at IP1 and IP5\" ) # cpymad naming : lowercase and appended with : beam_number self . ip1_tferror_bbx . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip1_tferror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip1:1\" ][ 0 ]) self . ip5_tferror_bbx . append ( cpymad_betabeatings . BETX [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ]) self . ip5_tferror_bby . append ( cpymad_betabeatings . BETY [ cpymad_betabeatings . NAME == \"ip5:1\" ][ 0 ])","title":"update_tf_from_cpymad"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#update_tf_from_seeds","text":"def update_tf_from_seeds ( self , temp_data ) -> None Updates the error's beta-beatings values after having ran simulations for all seeds. Append computed rms values for a group of seeds, to field errors result values. Parameters: Name Type Description Default temp_data None a BetaBeatValues object with the seeds' results. None View Source def update_tf_from_seeds ( self , temp_data ) -> None : \" \"\" Updates the error's beta-beatings values after having ran simulations for all seeds. Append computed rms values for a group of seeds, to field errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. \"\" \" self . tferror_bbx . append ( _get_rms ( temp_data . tferror_bbx )) self . tferror_bby . append ( _get_rms ( temp_data . tferror_bby )) self . max_tferror_bbx . append ( _get_rms ( temp_data . max_tferror_bbx )) self . max_tferror_bby . append ( _get_rms ( temp_data . max_tferror_bby )) self . ip1_tferror_bbx . append ( _get_rms ( temp_data . ip1_tferror_bbx )) self . ip1_tferror_bby . append ( _get_rms ( temp_data . ip1_tferror_bby )) self . ip5_tferror_bbx . append ( _get_rms ( temp_data . ip5_tferror_bbx )) self . ip5_tferror_bby . append ( _get_rms ( temp_data . ip5_tferror_bby ))","title":"update_tf_from_seeds"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#stdevvalues","text":"class StdevValues ( __pydantic_self__ , ** data : Any ) View Source class StdevValues ( BaseModel ) : \"\"\" Simple class to store and transfer standard deviation values. Class attributes are as follows: \" stdev_tf_x \": \" Horizontal standard deviation values from field errors \", \" stdev_tf_y \": \" Vertical standard deviation values from field errors \", \" ip1_stdev_tf_x \": \" Horizontal standard deviation values from field errors at IP1 \", \" ip1_stdev_tf_y \": \" Vertical standard deviation values from field errors at IP1 \", \" ip5_stdev_tf_x \": \" Horizontal standard deviation values from field errors at IP5 \", \" ip5_stdev_tf_y \": \" Vertical standard deviation values from field errors at IP5 \", \" max_stdev_tf_x \": \" Maximal horizontal standard deviation values from field errors \", \" max_stdev_tf_y \": \" Maximal vertical standard deviation values from field errors \", \" stdev_miss_x \": \" Horizontal standard deviation values from misalignment errors \", \" stdev_miss_y \": \" Horizontal standard deviation values from misalignment errors \", \" ip1_stdev_miss_x \": \" Horizontal standard deviation values from misalignment errors at IP1 \", \" ip1_stdev_miss_y \": \" Vertical standard deviation values from misalignment errors at IP1 \", \" ip5_stdev_miss_x \": \" Horizontal standard deviation values from misalignment errors at IP5 \", \" ip5_stdev_miss_y \": \" Vertical standard deviation values from misalignment errors at IP5 \", \" max_stdev_miss_x \": \" Maximal horizontal standard deviation values from misalignment errors \", \" max_stdev_miss_y \": \" Maximal vertical standard deviation values from misalignment errors \", \"\"\" stdev_tf_x : List [ float ] = [] stdev_tf_y : List [ float ] = [] ip1_stdev_tf_x : List [ float ] = [] ip1_stdev_tf_y : List [ float ] = [] ip5_stdev_tf_x : List [ float ] = [] ip5_stdev_tf_y : List [ float ] = [] max_stdev_tf_x : List [ float ] = [] max_stdev_tf_y : List [ float ] = [] stdev_miss_x : List [ float ] = [] stdev_miss_y : List [ float ] = [] ip1_stdev_miss_x : List [ float ] = [] ip1_stdev_miss_y : List [ float ] = [] ip5_stdev_miss_x : List [ float ] = [] ip5_stdev_miss_y : List [ float ] = [] max_stdev_miss_x : List [ float ] = [] max_stdev_miss_y : List [ float ] = [] def describe ( self ) -> None : \"\"\" Simple print statement of instance attributes. \"\"\" for attribute , value in self . dict (). items () : print ( f \"{attribute:<20} {value}\" ) def update_tf ( self , temp_data ) -> None : \"\"\" Append computed stdev values for a group of seeds, to field errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. Returns: Nothing, updates inplace. \"\"\" self . stdev_tf_x . append ( np . std ( temp_data . tferror_bbx )) self . stdev_tf_y . append ( np . std ( temp_data . tferror_bby )) self . max_stdev_tf_x . append ( np . std ( temp_data . max_tferror_bbx )) self . max_stdev_tf_y . append ( np . std ( temp_data . max_tferror_bby )) self . ip1_stdev_tf_x . append ( np . std ( temp_data . ip1_tferror_bbx )) self . ip1_stdev_tf_y . append ( np . std ( temp_data . ip1_tferror_bby )) self . ip5_stdev_tf_x . append ( np . std ( temp_data . ip5_tferror_bbx )) self . ip5_stdev_tf_y . append ( np . std ( temp_data . ip5_tferror_bby )) def update_miss ( self , temp_data ) -> None : \"\"\" Append computed rms values for a group of seeds, to misalignment errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. Returns: Nothing, updates inplace. \"\"\" self . stdev_miss_x . append ( np . std ( temp_data . misserror_bbx )) self . stdev_miss_y . append ( np . std ( temp_data . misserror_bby )) self . max_stdev_miss_x . append ( np . std ( temp_data . max_misserror_bbx )) self . max_stdev_miss_y . append ( np . std ( temp_data . max_misserror_bby )) self . ip1_stdev_miss_x . append ( np . std ( temp_data . ip1_misserror_bbx )) self . ip1_stdev_miss_y . append ( np . std ( temp_data . ip1_misserror_bby )) self . ip5_stdev_miss_x . append ( np . std ( temp_data . ip5_misserror_bbx )) self . ip5_stdev_miss_y . append ( np . std ( temp_data . ip5_misserror_bby )) def to_pandas ( self , * args , ** kwargs ) -> pd . DataFrame : \"\"\" Simple function to export stored values as a pandas dataframe. Returns: A `pandas.DataFrame` object with the instance's attributes as columns. \"\"\" return pd . DataFrame ( self . dict ( * args , ** kwargs ))","title":"StdevValues"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#ancestors-in-mro_1","text":"pydantic.main.BaseModel pydantic.utils.Representation","title":"Ancestors (in MRO)"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#class-variables_1","text":"Config","title":"Class variables"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#construct_1","text":"def construct ( _fields_set : Union [ ForwardRef ( 'SetStr' ), NoneType ] = None , ** values : Any ) -> 'Model' Creates a new model setting dict and fields_set from trusted or pre-validated data. Default values are respected, but no other validation is performed.","title":"construct"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#from_orm_1","text":"def from_orm ( obj : Any ) -> 'Model'","title":"from_orm"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#parse_file_1","text":"def parse_file ( path : Union [ str , pathlib . Path ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_file"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#parse_obj_1","text":"def parse_obj ( obj : Any ) -> 'Model'","title":"parse_obj"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#parse_raw_1","text":"def parse_raw ( b : Union [ str , bytes ], * , content_type : 'unicode' = None , encoding : 'unicode' = 'utf8' , proto : pydantic . parse . Protocol = None , allow_pickle : bool = False ) -> 'Model'","title":"parse_raw"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#schema_1","text":"def schema ( by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' ) -> 'DictStrAny'","title":"schema"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#schema_json_1","text":"def schema_json ( * , by_alias : bool = True , ref_template : 'unicode' = '#/definitions/ {model} ' , ** dumps_kwargs : Any ) -> 'unicode'","title":"schema_json"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#update_forward_refs_1","text":"def update_forward_refs ( ** localns : Any ) -> None Try to update ForwardRefs on fields based on this Model, globalns and localns.","title":"update_forward_refs"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#validate_1","text":"def validate ( value : Any ) -> 'Model'","title":"validate"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#instance-variables_1","text":"fields","title":"Instance variables"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#methods_1","text":"","title":"Methods"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#copy_1","text":"def copy ( self : 'Model' , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , update : 'DictStrAny' = None , deep : bool = False ) -> 'Model' Duplicate a model, optionally choose which fields to include, exclude and change. Parameters: Name Type Description Default include None fields to include in new model None exclude None fields to exclude from new model, as with values this takes precedence over include None update None values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data None deep None set to True to make a deep copy of the model None Returns: Type Description None new model instance","title":"copy"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#describe_1","text":"def describe ( self ) -> None Simple print statement of instance attributes. View Source def describe ( self ) -> None : \"\"\" Simple print statement of instance attributes. \"\"\" for attribute , value in self . dict (). items (): print ( f \"{attribute:<20} {value}\" )","title":"describe"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#dict_1","text":"def dict ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False ) -> 'DictStrAny' Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.","title":"dict"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#json_1","text":"def json ( self , * , include : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , exclude : Union [ ForwardRef ( 'AbstractSetIntStr' ), ForwardRef ( 'MappingIntStrAny' )] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , encoder : Union [ Callable [[ Any ], Any ], NoneType ] = None , ** dumps_kwargs : Any ) -> 'unicode' Generate a JSON representation of the model, include and exclude arguments as per dict() . encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps() .","title":"json"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#to_pandas_1","text":"def to_pandas ( self , * args , ** kwargs ) -> pandas . core . frame . DataFrame Simple function to export stored values as a pandas dataframe. Returns: Type Description None A pandas.DataFrame object with the instance's attributes as columns. View Source def to_pandas ( self , * args , ** kwargs ) -> pd . DataFrame : \"\"\" Simple function to export stored values as a pandas dataframe. Returns: A `pandas.DataFrame` object with the instance's attributes as columns. \"\"\" return pd . DataFrame ( self . dict ( * args , ** kwargs ))","title":"to_pandas"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#to_string_1","text":"def to_string ( self , pretty : bool = False ) -> 'unicode'","title":"to_string"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#update_miss","text":"def update_miss ( self , temp_data ) -> None Append computed rms values for a group of seeds, to misalignment errors result values. Parameters: Name Type Description Default temp_data None a BetaBeatValues object with the seeds' results. None Returns: Type Description None Nothing, updates inplace. View Source def update_miss ( self , temp_data ) -> None : \" \"\" Append computed rms values for a group of seeds, to misalignment errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. Returns: Nothing, updates inplace. \"\" \" self . stdev_miss_x . append ( np . std ( temp_data . misserror_bbx )) self . stdev_miss_y . append ( np . std ( temp_data . misserror_bby )) self . max_stdev_miss_x . append ( np . std ( temp_data . max_misserror_bbx )) self . max_stdev_miss_y . append ( np . std ( temp_data . max_misserror_bby )) self . ip1_stdev_miss_x . append ( np . std ( temp_data . ip1_misserror_bbx )) self . ip1_stdev_miss_y . append ( np . std ( temp_data . ip1_misserror_bby )) self . ip5_stdev_miss_x . append ( np . std ( temp_data . ip5_misserror_bbx )) self . ip5_stdev_miss_y . append ( np . std ( temp_data . ip5_misserror_bby ))","title":"update_miss"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/data_classes/#update_tf","text":"def update_tf ( self , temp_data ) -> None Append computed stdev values for a group of seeds, to field errors result values. Parameters: Name Type Description Default temp_data None a BetaBeatValues object with the seeds' results. None Returns: Type Description None Nothing, updates inplace. View Source def update_tf ( self , temp_data ) -> None : \" \"\" Append computed stdev values for a group of seeds, to field errors result values. Args: temp_data: a `BetaBeatValues` object with the seeds' results. Returns: Nothing, updates inplace. \"\" \" self . stdev_tf_x . append ( np . std ( temp_data . tferror_bbx )) self . stdev_tf_y . append ( np . std ( temp_data . tferror_bby )) self . max_stdev_tf_x . append ( np . std ( temp_data . max_tferror_bbx )) self . max_stdev_tf_y . append ( np . std ( temp_data . max_tferror_bby )) self . ip1_stdev_tf_x . append ( np . std ( temp_data . ip1_tferror_bbx )) self . ip1_stdev_tf_y . append ( np . std ( temp_data . ip1_tferror_bby )) self . ip5_stdev_tf_x . append ( np . std ( temp_data . ip5_tferror_bbx )) self . ip5_stdev_tf_y . append ( np . std ( temp_data . ip5_tferror_bby ))","title":"update_tf"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/plotting_functions/","text":"Module pyhdtoolkit.scripts.triplet_errors.plotting_functions Script scripts.triplet_errors.plotting_functions Created on 2019.06.15 View Source \"\"\" Script scripts.triplet_errors.plotting_functions ------------------------------------------------ Created on 2019.06.15 :author: Felix Soubelet A collection of functions that will be useful to plot the results from GridCompute Algorithm. \"\"\" import os import pathlib from typing import List import matplotlib import matplotlib.pyplot as plt import pandas as pd from loguru import logger if os . environ . get ( \"Display\" , \"\" ) == \"\" : logger . warning ( \"Display configuration error found. Using non-interactive Agg backend\" ) matplotlib . use ( \"Agg\" ) def plot_betas_across_machine ( s_values : List [ float ], betx_values : List [ float ], bety_values : List [ float ], error_type : str , error_value : str , ) -> None : \"\"\" Plot beta functions across the machine. Save according to simulation scenario. Creates a plot of the horizontal and vertical beta functions across the whole machine. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Args: s_values (List[float]): the values of the s axis. betx_values (List[float]): horizontal betatron function values accross the machine. bety_values (List[float]): vertical betatron function values accross the machine. error_type (str): which error you have simulated too get those results. error_value (str): the value of the error you used in your simulations. \"\"\" if error_type == \"TFERROR\" : title = f \"r'Beta values, hllhc1.3, 15cm optics, relative field error: {error_value}[$10^{-4}$]'\" elif error_type == \"MISERROR\" : title = f \"r'Beta values, hllhc1.3 15cm optics, misalignment: {error_value}[mm]'\" else : logger . warning ( f \"Invalid error parameter {error_type} provided, aborting plot\" ) raise ValueError ( \"Error parameter should be either `TFERROR` or `MISERROR`.\" ) output_dir = pathlib . Path ( \"beta_plots\" ) / f \"{error_type}\" / f \"{error_value}\" if not output_dir . is_dir (): logger . info ( f \"Creating directory {output_dir}\" ) output_dir . mkdir () plt . figure ( figsize = ( 18 , 10 )) plt . title ( title , fontsize = 21 ) plt . xlabel ( \"Position alongside s axis [m]\" , fontsize = 17 ) plt . ylabel ( r \"$\\beta$ value [m]\" , fontsize = 17 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . xlim ( 6500 , max ( s_values )) plt . plot ( s_values , betx_values , label = \"BETX\" ) plt . plot ( s_values , bety_values , label = \"BETY\" ) plt . legend ( loc = \"best\" , fontsize = \"xx-large\" ) plt . savefig ( f \"beta_plots/{error_type}/{error_value}/betas_across_machine.png\" , format = \"pdf\" , dpi = 300 ) logger . info ( f \"Plotted betas for {error_type} {error_value}\" ) def plot_bbing_max_errorbar ( errors : List [ float ], beta_beatings_df : pd . DataFrame , stdev_df : pd . DataFrame , plane : str , figname : str , ) -> None : \"\"\" Plot beta-beating values, with error bars, as a function of the error values. Save according to plotted plane. Creates a plot of the horizontal or vertical beta-beatings across the range of simulated error values. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Args: errors (List[float]): the different error values simulated. beta_beatings_df (pd.DataFrame): the resulting beta-beating values. stdev_df (pd.DataFrame): the standard deviations for those values. plane (str): the name of the plane to plot. figname (str): how to name the file when exporting the plot. \"\"\" if plane . lower () == \"horizontal\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bbx , yerr = stdev_df . stdev_tf_x , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bbx , yerr = stdev_df . stdev_miss_x , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . max_tferror_bbx , \"^\" , color = \"C0\" , label = \"Max Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . max_misserror_bbx , \"^\" , color = \"C1\" , label = \"Max Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"Relative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-Beating Against Triplet Errors, {plane} Plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) elif plane . lower () == \"vertical\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bby , yerr = stdev_df . stdev_tf_y , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bby , yerr = stdev_df . stdev_miss_y , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . max_tferror_bby , \"^\" , color = \"C0\" , label = \"Max Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . max_misserror_bby , \"^\" , color = \"C1\" , label = \"Max Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"Relative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-beating against triplet errors, {plane} plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) else : logger . warning ( f \"Invalid plane parameter {plane} provided, aborting plot\" ) raise ValueError ( \"Plane parameter should be either `Horizontal` or `Vertical`\" ) logger . info ( f \"Plotted beta-beatings with error bars for {plane.lower()} plane\" ) def plot_bbing_with_ips_errorbar ( errors : List [ float ], beta_beatings_df : pd . DataFrame , stdev_df : pd . DataFrame , plane : str , figname : str , ) -> None : \"\"\" Plot beta-beating values, with error bars, as a function of the error values. Save according to plotted plane. Creates a plot of the horizontal or vertical beta-beatings across the range of simulated error values, with the addition of the beta-beating value at IPs. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Args: errors (List[float]): list with different error values simulated. beta_beatings_df (pd.DataFrame): the resulting beta-beating values. stdev_df (pd.DataFrame): the standard deviations for those values. plane (str): the name of the plane to plot. figname (str): how to name the file when exporting the plot. \"\"\" if plane . lower () == \"horizontal\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bbx , yerr = stdev_df . stdev_tf_x , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bbx , yerr = stdev_df . stdev_miss_x , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_tferror_bbx , \"^\" , color = \"C0\" , label = \"IP1 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_misserror_bbx , \"^\" , color = \"C1\" , label = \"IP1 Beta-Beating Value from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_tferror_bbx , \"x\" , color = \"C0\" , label = \"IP5 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_misserror_bbx , \"x\" , color = \"C1\" , label = \"IP5 Beta-Beating Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"Relative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-Beating Against Triplet Errors, {plane} Plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) elif plane . lower () == \"vertical\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bby , yerr = stdev_df . stdev_tf_y , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bby , yerr = stdev_df . stdev_miss_y , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_tferror_bby , \"^\" , color = \"C0\" , label = \"IP1 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_misserror_bby , \"^\" , color = \"C1\" , label = \"IP1 Beta-Beating Value from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_tferror_bby , \"x\" , color = \"C0\" , label = \"IP5 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_misserror_bby , \"x\" , color = \"C1\" , label = \"IP5 Beta-Beating Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"TRelative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-Beating Against Triplet Errors, {plane} Plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) else : logger . warning ( f \"Invalid plane parameter {plane} provided, aborting plot\" ) raise ValueError ( \"Plane parameter should be either `Horizontal` or `Vertical`\" ) logger . info ( f \"Plotted beta-beatings (including IPs) with error bars for {plane.lower()} plane\" ) def plot_intermediate_beta_histograms ( betasx : List [ float ], betasy : List [ float ], error_val : float , title : str , outputname : str ) -> None : \"\"\" Plot histogram distribution for betas at seeds. Args: betasx (List[float]): horizontal beta values for all seeds for a specific error value. betasy (List[float]): vertical beta values for all seeds for a specific error value. error_val (float): the error value. title (str): the title to give the figure. outputname (str): the name to give the file saving the figure. \"\"\" plt . hist ( betasx , bins = 50 , label = f \"{error_val}, horizontal\" , alpha = 0.6 , density = True ) plt . hist ( betasy , bins = 50 , label = f \"{error_val}, vertical\" , alpha = 0.6 , density = True ) plt . legend ( loc = \"best\" ) plt . title ( title ) plt . savefig ( outputname , format = \"pdf\" , dpi = 300 ) logger . info ( f \"Plotted intermediate beta histogram, saved as {outputname}\" ) Functions plot_bbing_max_errorbar def plot_bbing_max_errorbar ( errors : List [ float ], beta_beatings_df : pandas . core . frame . DataFrame , stdev_df : pandas . core . frame . DataFrame , plane : str , figname : str ) -> None Plot beta-beating values, with error bars, as a function of the error values. Save according to plotted plane. Creates a plot of the horizontal or vertical beta-beatings across the range of simulated error values. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Parameters: Name Type Description Default errors List[float] the different error values simulated. None beta_beatings_df pd.DataFrame the resulting beta-beating values. None stdev_df pd.DataFrame the standard deviations for those values. None plane str the name of the plane to plot. None figname str how to name the file when exporting the plot. None View Source def plot_bbing_max_errorbar ( errors : List [ float ] , beta_beatings_df : pd . DataFrame , stdev_df : pd . DataFrame , plane : str , figname : str , ) -> None : \"\"\" Plot beta-beating values, with error bars, as a function of the error values. Save according to plotted plane. Creates a plot of the horizontal or vertical beta-beatings across the range of simulated error values. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Args: errors (List[float]): the different error values simulated. beta_beatings_df (pd.DataFrame): the resulting beta-beating values. stdev_df (pd.DataFrame): the standard deviations for those values. plane (str): the name of the plane to plot. figname (str): how to name the file when exporting the plot. \"\"\" if plane . lower () == \"horizontal\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bbx , yerr = stdev_df . stdev_tf_x , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bbx , yerr = stdev_df . stdev_miss_x , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . max_tferror_bbx , \"^\" , color = \"C0\" , label = \"Max Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . max_misserror_bbx , \"^\" , color = \"C1\" , label = \"Max Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"Relative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-Beating Against Triplet Errors, {plane} Plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) elif plane . lower () == \"vertical\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bby , yerr = stdev_df . stdev_tf_y , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bby , yerr = stdev_df . stdev_miss_y , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . max_tferror_bby , \"^\" , color = \"C0\" , label = \"Max Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . max_misserror_bby , \"^\" , color = \"C1\" , label = \"Max Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"Relative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-beating against triplet errors, {plane} plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) else : logger . warning ( f \"Invalid plane parameter {plane} provided, aborting plot\" ) raise ValueError ( \"Plane parameter should be either `Horizontal` or `Vertical`\" ) logger . info ( f \"Plotted beta-beatings with error bars for {plane.lower()} plane\" ) plot_bbing_with_ips_errorbar def plot_bbing_with_ips_errorbar ( errors : List [ float ], beta_beatings_df : pandas . core . frame . DataFrame , stdev_df : pandas . core . frame . DataFrame , plane : str , figname : str ) -> None Plot beta-beating values, with error bars, as a function of the error values. Save according to plotted plane. Creates a plot of the horizontal or vertical beta-beatings across the range of simulated error values, with the addition of the beta-beating value at IPs. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Parameters: Name Type Description Default errors List[float] list with different error values simulated. None beta_beatings_df pd.DataFrame the resulting beta-beating values. None stdev_df pd.DataFrame the standard deviations for those values. None plane str the name of the plane to plot. None figname str how to name the file when exporting the plot. None View Source def plot_bbing_with_ips_errorbar ( errors : List [ float ] , beta_beatings_df : pd . DataFrame , stdev_df : pd . DataFrame , plane : str , figname : str , ) -> None : \"\"\" Plot beta-beating values, with error bars, as a function of the error values. Save according to plotted plane. Creates a plot of the horizontal or vertical beta-beatings across the range of simulated error values, with the addition of the beta-beating value at IPs. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Args: errors (List[float]): list with different error values simulated. beta_beatings_df (pd.DataFrame): the resulting beta-beating values. stdev_df (pd.DataFrame): the standard deviations for those values. plane (str): the name of the plane to plot. figname (str): how to name the file when exporting the plot. \"\"\" if plane . lower () == \"horizontal\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bbx , yerr = stdev_df . stdev_tf_x , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bbx , yerr = stdev_df . stdev_miss_x , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_tferror_bbx , \"^\" , color = \"C0\" , label = \"IP1 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_misserror_bbx , \"^\" , color = \"C1\" , label = \"IP1 Beta-Beating Value from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_tferror_bbx , \"x\" , color = \"C0\" , label = \"IP5 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_misserror_bbx , \"x\" , color = \"C1\" , label = \"IP5 Beta-Beating Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"Relative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-Beating Against Triplet Errors, {plane} Plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) elif plane . lower () == \"vertical\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bby , yerr = stdev_df . stdev_tf_y , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bby , yerr = stdev_df . stdev_miss_y , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_tferror_bby , \"^\" , color = \"C0\" , label = \"IP1 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_misserror_bby , \"^\" , color = \"C1\" , label = \"IP1 Beta-Beating Value from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_tferror_bby , \"x\" , color = \"C0\" , label = \"IP5 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_misserror_bby , \"x\" , color = \"C1\" , label = \"IP5 Beta-Beating Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"TRelative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-Beating Against Triplet Errors, {plane} Plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) else : logger . warning ( f \"Invalid plane parameter {plane} provided, aborting plot\" ) raise ValueError ( \"Plane parameter should be either `Horizontal` or `Vertical`\" ) logger . info ( f \"Plotted beta-beatings (including IPs) with error bars for {plane.lower()} plane\" ) plot_betas_across_machine def plot_betas_across_machine ( s_values : List [ float ], betx_values : List [ float ], bety_values : List [ float ], error_type : str , error_value : str ) -> None Plot beta functions across the machine. Save according to simulation scenario. Creates a plot of the horizontal and vertical beta functions across the whole machine. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Parameters: Name Type Description Default s_values List[float] the values of the s axis. None betx_values List[float] horizontal betatron function values accross the machine. None bety_values List[float] vertical betatron function values accross the machine. None error_type str which error you have simulated too get those results. None error_value str the value of the error you used in your simulations. None View Source def plot_betas_across_machine ( s_values : List [ float ] , betx_values : List [ float ] , bety_values : List [ float ] , error_type : str , error_value : str , ) -> None : \"\"\" Plot beta functions across the machine. Save according to simulation scenario. Creates a plot of the horizontal and vertical beta functions across the whole machine. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Args: s_values (List[float]): the values of the s axis. betx_values (List[float]): horizontal betatron function values accross the machine. bety_values (List[float]): vertical betatron function values accross the machine. error_type (str): which error you have simulated too get those results. error_value (str): the value of the error you used in your simulations. \"\"\" if error_type == \"TFERROR\" : title = f \"r'Beta values, hllhc1.3, 15cm optics, relative field error: {error_value}[$10^{-4}$]'\" elif error_type == \"MISERROR\" : title = f \"r'Beta values, hllhc1.3 15cm optics, misalignment: {error_value}[mm]'\" else : logger . warning ( f \"Invalid error parameter {error_type} provided, aborting plot\" ) raise ValueError ( \"Error parameter should be either `TFERROR` or `MISERROR`.\" ) output_dir = pathlib . Path ( \"beta_plots\" ) / f \"{error_type}\" / f \"{error_value}\" if not output_dir . is_dir () : logger . info ( f \"Creating directory {output_dir}\" ) output_dir . mkdir () plt . figure ( figsize = ( 18 , 10 )) plt . title ( title , fontsize = 21 ) plt . xlabel ( \"Position alongside s axis [m]\" , fontsize = 17 ) plt . ylabel ( r \"$\\beta$ value [m]\" , fontsize = 17 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . xlim ( 6500 , max ( s_values )) plt . plot ( s_values , betx_values , label = \"BETX\" ) plt . plot ( s_values , bety_values , label = \"BETY\" ) plt . legend ( loc = \"best\" , fontsize = \"xx-large\" ) plt . savefig ( f \"beta_plots/{error_type}/{error_value}/betas_across_machine.png\" , format = \"pdf\" , dpi = 300 ) logger . info ( f \"Plotted betas for {error_type} {error_value}\" ) plot_intermediate_beta_histograms def plot_intermediate_beta_histograms ( betasx : List [ float ], betasy : List [ float ], error_val : float , title : str , outputname : str ) -> None Plot histogram distribution for betas at seeds. Parameters: Name Type Description Default betasx List[float] horizontal beta values for all seeds for a specific error value. None betasy List[float] vertical beta values for all seeds for a specific error value. None error_val float the error value. None title str the title to give the figure. None outputname str the name to give the file saving the figure. None View Source def plot_intermediate_beta_histograms ( betasx : List [ float ] , betasy : List [ float ] , error_val : float , title : str , outputname : str ) -> None : \"\"\" Plot histogram distribution for betas at seeds. Args: betasx (List[float]): horizontal beta values for all seeds for a specific error value. betasy (List[float]): vertical beta values for all seeds for a specific error value. error_val (float): the error value. title (str): the title to give the figure. outputname (str): the name to give the file saving the figure. \"\"\" plt . hist ( betasx , bins = 50 , label = f \"{error_val}, horizontal\" , alpha = 0.6 , density = True ) plt . hist ( betasy , bins = 50 , label = f \"{error_val}, vertical\" , alpha = 0.6 , density = True ) plt . legend ( loc = \"best\" ) plt . title ( title ) plt . savefig ( outputname , format = \"pdf\" , dpi = 300 ) logger . info ( f \"Plotted intermediate beta histogram, saved as {outputname}\" )","title":"Plotting Functions"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/plotting_functions/#module-pyhdtoolkitscriptstriplet_errorsplotting_functions","text":"Script scripts.triplet_errors.plotting_functions Created on 2019.06.15 View Source \"\"\" Script scripts.triplet_errors.plotting_functions ------------------------------------------------ Created on 2019.06.15 :author: Felix Soubelet A collection of functions that will be useful to plot the results from GridCompute Algorithm. \"\"\" import os import pathlib from typing import List import matplotlib import matplotlib.pyplot as plt import pandas as pd from loguru import logger if os . environ . get ( \"Display\" , \"\" ) == \"\" : logger . warning ( \"Display configuration error found. Using non-interactive Agg backend\" ) matplotlib . use ( \"Agg\" ) def plot_betas_across_machine ( s_values : List [ float ], betx_values : List [ float ], bety_values : List [ float ], error_type : str , error_value : str , ) -> None : \"\"\" Plot beta functions across the machine. Save according to simulation scenario. Creates a plot of the horizontal and vertical beta functions across the whole machine. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Args: s_values (List[float]): the values of the s axis. betx_values (List[float]): horizontal betatron function values accross the machine. bety_values (List[float]): vertical betatron function values accross the machine. error_type (str): which error you have simulated too get those results. error_value (str): the value of the error you used in your simulations. \"\"\" if error_type == \"TFERROR\" : title = f \"r'Beta values, hllhc1.3, 15cm optics, relative field error: {error_value}[$10^{-4}$]'\" elif error_type == \"MISERROR\" : title = f \"r'Beta values, hllhc1.3 15cm optics, misalignment: {error_value}[mm]'\" else : logger . warning ( f \"Invalid error parameter {error_type} provided, aborting plot\" ) raise ValueError ( \"Error parameter should be either `TFERROR` or `MISERROR`.\" ) output_dir = pathlib . Path ( \"beta_plots\" ) / f \"{error_type}\" / f \"{error_value}\" if not output_dir . is_dir (): logger . info ( f \"Creating directory {output_dir}\" ) output_dir . mkdir () plt . figure ( figsize = ( 18 , 10 )) plt . title ( title , fontsize = 21 ) plt . xlabel ( \"Position alongside s axis [m]\" , fontsize = 17 ) plt . ylabel ( r \"$\\beta$ value [m]\" , fontsize = 17 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . xlim ( 6500 , max ( s_values )) plt . plot ( s_values , betx_values , label = \"BETX\" ) plt . plot ( s_values , bety_values , label = \"BETY\" ) plt . legend ( loc = \"best\" , fontsize = \"xx-large\" ) plt . savefig ( f \"beta_plots/{error_type}/{error_value}/betas_across_machine.png\" , format = \"pdf\" , dpi = 300 ) logger . info ( f \"Plotted betas for {error_type} {error_value}\" ) def plot_bbing_max_errorbar ( errors : List [ float ], beta_beatings_df : pd . DataFrame , stdev_df : pd . DataFrame , plane : str , figname : str , ) -> None : \"\"\" Plot beta-beating values, with error bars, as a function of the error values. Save according to plotted plane. Creates a plot of the horizontal or vertical beta-beatings across the range of simulated error values. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Args: errors (List[float]): the different error values simulated. beta_beatings_df (pd.DataFrame): the resulting beta-beating values. stdev_df (pd.DataFrame): the standard deviations for those values. plane (str): the name of the plane to plot. figname (str): how to name the file when exporting the plot. \"\"\" if plane . lower () == \"horizontal\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bbx , yerr = stdev_df . stdev_tf_x , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bbx , yerr = stdev_df . stdev_miss_x , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . max_tferror_bbx , \"^\" , color = \"C0\" , label = \"Max Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . max_misserror_bbx , \"^\" , color = \"C1\" , label = \"Max Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"Relative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-Beating Against Triplet Errors, {plane} Plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) elif plane . lower () == \"vertical\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bby , yerr = stdev_df . stdev_tf_y , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bby , yerr = stdev_df . stdev_miss_y , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . max_tferror_bby , \"^\" , color = \"C0\" , label = \"Max Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . max_misserror_bby , \"^\" , color = \"C1\" , label = \"Max Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"Relative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-beating against triplet errors, {plane} plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) else : logger . warning ( f \"Invalid plane parameter {plane} provided, aborting plot\" ) raise ValueError ( \"Plane parameter should be either `Horizontal` or `Vertical`\" ) logger . info ( f \"Plotted beta-beatings with error bars for {plane.lower()} plane\" ) def plot_bbing_with_ips_errorbar ( errors : List [ float ], beta_beatings_df : pd . DataFrame , stdev_df : pd . DataFrame , plane : str , figname : str , ) -> None : \"\"\" Plot beta-beating values, with error bars, as a function of the error values. Save according to plotted plane. Creates a plot of the horizontal or vertical beta-beatings across the range of simulated error values, with the addition of the beta-beating value at IPs. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Args: errors (List[float]): list with different error values simulated. beta_beatings_df (pd.DataFrame): the resulting beta-beating values. stdev_df (pd.DataFrame): the standard deviations for those values. plane (str): the name of the plane to plot. figname (str): how to name the file when exporting the plot. \"\"\" if plane . lower () == \"horizontal\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bbx , yerr = stdev_df . stdev_tf_x , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bbx , yerr = stdev_df . stdev_miss_x , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_tferror_bbx , \"^\" , color = \"C0\" , label = \"IP1 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_misserror_bbx , \"^\" , color = \"C1\" , label = \"IP1 Beta-Beating Value from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_tferror_bbx , \"x\" , color = \"C0\" , label = \"IP5 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_misserror_bbx , \"x\" , color = \"C1\" , label = \"IP5 Beta-Beating Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"Relative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-Beating Against Triplet Errors, {plane} Plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) elif plane . lower () == \"vertical\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bby , yerr = stdev_df . stdev_tf_y , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bby , yerr = stdev_df . stdev_miss_y , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_tferror_bby , \"^\" , color = \"C0\" , label = \"IP1 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_misserror_bby , \"^\" , color = \"C1\" , label = \"IP1 Beta-Beating Value from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_tferror_bby , \"x\" , color = \"C0\" , label = \"IP5 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_misserror_bby , \"x\" , color = \"C1\" , label = \"IP5 Beta-Beating Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"TRelative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-Beating Against Triplet Errors, {plane} Plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) else : logger . warning ( f \"Invalid plane parameter {plane} provided, aborting plot\" ) raise ValueError ( \"Plane parameter should be either `Horizontal` or `Vertical`\" ) logger . info ( f \"Plotted beta-beatings (including IPs) with error bars for {plane.lower()} plane\" ) def plot_intermediate_beta_histograms ( betasx : List [ float ], betasy : List [ float ], error_val : float , title : str , outputname : str ) -> None : \"\"\" Plot histogram distribution for betas at seeds. Args: betasx (List[float]): horizontal beta values for all seeds for a specific error value. betasy (List[float]): vertical beta values for all seeds for a specific error value. error_val (float): the error value. title (str): the title to give the figure. outputname (str): the name to give the file saving the figure. \"\"\" plt . hist ( betasx , bins = 50 , label = f \"{error_val}, horizontal\" , alpha = 0.6 , density = True ) plt . hist ( betasy , bins = 50 , label = f \"{error_val}, vertical\" , alpha = 0.6 , density = True ) plt . legend ( loc = \"best\" ) plt . title ( title ) plt . savefig ( outputname , format = \"pdf\" , dpi = 300 ) logger . info ( f \"Plotted intermediate beta histogram, saved as {outputname}\" )","title":"Module pyhdtoolkit.scripts.triplet_errors.plotting_functions"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/plotting_functions/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/plotting_functions/#plot_bbing_max_errorbar","text":"def plot_bbing_max_errorbar ( errors : List [ float ], beta_beatings_df : pandas . core . frame . DataFrame , stdev_df : pandas . core . frame . DataFrame , plane : str , figname : str ) -> None Plot beta-beating values, with error bars, as a function of the error values. Save according to plotted plane. Creates a plot of the horizontal or vertical beta-beatings across the range of simulated error values. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Parameters: Name Type Description Default errors List[float] the different error values simulated. None beta_beatings_df pd.DataFrame the resulting beta-beating values. None stdev_df pd.DataFrame the standard deviations for those values. None plane str the name of the plane to plot. None figname str how to name the file when exporting the plot. None View Source def plot_bbing_max_errorbar ( errors : List [ float ] , beta_beatings_df : pd . DataFrame , stdev_df : pd . DataFrame , plane : str , figname : str , ) -> None : \"\"\" Plot beta-beating values, with error bars, as a function of the error values. Save according to plotted plane. Creates a plot of the horizontal or vertical beta-beatings across the range of simulated error values. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Args: errors (List[float]): the different error values simulated. beta_beatings_df (pd.DataFrame): the resulting beta-beating values. stdev_df (pd.DataFrame): the standard deviations for those values. plane (str): the name of the plane to plot. figname (str): how to name the file when exporting the plot. \"\"\" if plane . lower () == \"horizontal\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bbx , yerr = stdev_df . stdev_tf_x , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bbx , yerr = stdev_df . stdev_miss_x , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . max_tferror_bbx , \"^\" , color = \"C0\" , label = \"Max Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . max_misserror_bbx , \"^\" , color = \"C1\" , label = \"Max Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"Relative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-Beating Against Triplet Errors, {plane} Plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) elif plane . lower () == \"vertical\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bby , yerr = stdev_df . stdev_tf_y , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bby , yerr = stdev_df . stdev_miss_y , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . max_tferror_bby , \"^\" , color = \"C0\" , label = \"Max Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . max_misserror_bby , \"^\" , color = \"C1\" , label = \"Max Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"Relative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-beating against triplet errors, {plane} plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) else : logger . warning ( f \"Invalid plane parameter {plane} provided, aborting plot\" ) raise ValueError ( \"Plane parameter should be either `Horizontal` or `Vertical`\" ) logger . info ( f \"Plotted beta-beatings with error bars for {plane.lower()} plane\" )","title":"plot_bbing_max_errorbar"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/plotting_functions/#plot_bbing_with_ips_errorbar","text":"def plot_bbing_with_ips_errorbar ( errors : List [ float ], beta_beatings_df : pandas . core . frame . DataFrame , stdev_df : pandas . core . frame . DataFrame , plane : str , figname : str ) -> None Plot beta-beating values, with error bars, as a function of the error values. Save according to plotted plane. Creates a plot of the horizontal or vertical beta-beatings across the range of simulated error values, with the addition of the beta-beating value at IPs. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Parameters: Name Type Description Default errors List[float] list with different error values simulated. None beta_beatings_df pd.DataFrame the resulting beta-beating values. None stdev_df pd.DataFrame the standard deviations for those values. None plane str the name of the plane to plot. None figname str how to name the file when exporting the plot. None View Source def plot_bbing_with_ips_errorbar ( errors : List [ float ] , beta_beatings_df : pd . DataFrame , stdev_df : pd . DataFrame , plane : str , figname : str , ) -> None : \"\"\" Plot beta-beating values, with error bars, as a function of the error values. Save according to plotted plane. Creates a plot of the horizontal or vertical beta-beatings across the range of simulated error values, with the addition of the beta-beating value at IPs. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Args: errors (List[float]): list with different error values simulated. beta_beatings_df (pd.DataFrame): the resulting beta-beating values. stdev_df (pd.DataFrame): the standard deviations for those values. plane (str): the name of the plane to plot. figname (str): how to name the file when exporting the plot. \"\"\" if plane . lower () == \"horizontal\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bbx , yerr = stdev_df . stdev_tf_x , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bbx , yerr = stdev_df . stdev_miss_x , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_tferror_bbx , \"^\" , color = \"C0\" , label = \"IP1 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_misserror_bbx , \"^\" , color = \"C1\" , label = \"IP1 Beta-Beating Value from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_tferror_bbx , \"x\" , color = \"C0\" , label = \"IP5 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_misserror_bbx , \"x\" , color = \"C1\" , label = \"IP5 Beta-Beating Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"Relative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-Beating Against Triplet Errors, {plane} Plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) elif plane . lower () == \"vertical\" : _ , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) axes . errorbar ( errors , beta_beatings_df . tferror_bby , yerr = stdev_df . stdev_tf_y , color = \"C0\" , label = \"Global Beta-Beating from Field Errors\" , ) axes . errorbar ( errors , beta_beatings_df . misserror_bby , yerr = stdev_df . stdev_miss_y , color = \"C1\" , label = \"Global Beta-Beating from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_tferror_bby , \"^\" , color = \"C0\" , label = \"IP1 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip1_misserror_bby , \"^\" , color = \"C1\" , label = \"IP1 Beta-Beating Value from Misalignment Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_tferror_bby , \"x\" , color = \"C0\" , label = \"IP5 Beta-Beating Value from Field Errors\" , ) axes . plot ( errors , beta_beatings_df . ip5_misserror_bby , \"x\" , color = \"C1\" , label = \"IP5 Beta-Beating Value from Misalignment Errors\" , ) axes . set_xlabel ( r \"TRelative Field Error [$10^{-4}$] or Longitudinal Misalignment [mm]\" , fontsize = 15 ) axes . set_ylabel ( r \"$\\Delta \\beta / \\beta$ [%]\" , fontsize = 15 ) plt . tight_layout () plt . title ( f \"Beta-Beating Against Triplet Errors, {plane} Plane\" , fontsize = 15 ) plt . legend ( loc = \"best\" ) plt . savefig ( figname , format = \"pdf\" , dpi = 300 ) else : logger . warning ( f \"Invalid plane parameter {plane} provided, aborting plot\" ) raise ValueError ( \"Plane parameter should be either `Horizontal` or `Vertical`\" ) logger . info ( f \"Plotted beta-beatings (including IPs) with error bars for {plane.lower()} plane\" )","title":"plot_bbing_with_ips_errorbar"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/plotting_functions/#plot_betas_across_machine","text":"def plot_betas_across_machine ( s_values : List [ float ], betx_values : List [ float ], bety_values : List [ float ], error_type : str , error_value : str ) -> None Plot beta functions across the machine. Save according to simulation scenario. Creates a plot of the horizontal and vertical beta functions across the whole machine. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Parameters: Name Type Description Default s_values List[float] the values of the s axis. None betx_values List[float] horizontal betatron function values accross the machine. None bety_values List[float] vertical betatron function values accross the machine. None error_type str which error you have simulated too get those results. None error_value str the value of the error you used in your simulations. None View Source def plot_betas_across_machine ( s_values : List [ float ] , betx_values : List [ float ] , bety_values : List [ float ] , error_type : str , error_value : str , ) -> None : \"\"\" Plot beta functions across the machine. Save according to simulation scenario. Creates a plot of the horizontal and vertical beta functions across the whole machine. Gives a title generated according to the error type and error value. Saves in dedicated subfolder. Args: s_values (List[float]): the values of the s axis. betx_values (List[float]): horizontal betatron function values accross the machine. bety_values (List[float]): vertical betatron function values accross the machine. error_type (str): which error you have simulated too get those results. error_value (str): the value of the error you used in your simulations. \"\"\" if error_type == \"TFERROR\" : title = f \"r'Beta values, hllhc1.3, 15cm optics, relative field error: {error_value}[$10^{-4}$]'\" elif error_type == \"MISERROR\" : title = f \"r'Beta values, hllhc1.3 15cm optics, misalignment: {error_value}[mm]'\" else : logger . warning ( f \"Invalid error parameter {error_type} provided, aborting plot\" ) raise ValueError ( \"Error parameter should be either `TFERROR` or `MISERROR`.\" ) output_dir = pathlib . Path ( \"beta_plots\" ) / f \"{error_type}\" / f \"{error_value}\" if not output_dir . is_dir () : logger . info ( f \"Creating directory {output_dir}\" ) output_dir . mkdir () plt . figure ( figsize = ( 18 , 10 )) plt . title ( title , fontsize = 21 ) plt . xlabel ( \"Position alongside s axis [m]\" , fontsize = 17 ) plt . ylabel ( r \"$\\beta$ value [m]\" , fontsize = 17 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . xlim ( 6500 , max ( s_values )) plt . plot ( s_values , betx_values , label = \"BETX\" ) plt . plot ( s_values , bety_values , label = \"BETY\" ) plt . legend ( loc = \"best\" , fontsize = \"xx-large\" ) plt . savefig ( f \"beta_plots/{error_type}/{error_value}/betas_across_machine.png\" , format = \"pdf\" , dpi = 300 ) logger . info ( f \"Plotted betas for {error_type} {error_value}\" )","title":"plot_betas_across_machine"},{"location":"reference/pyhdtoolkit/scripts/triplet_errors/plotting_functions/#plot_intermediate_beta_histograms","text":"def plot_intermediate_beta_histograms ( betasx : List [ float ], betasy : List [ float ], error_val : float , title : str , outputname : str ) -> None Plot histogram distribution for betas at seeds. Parameters: Name Type Description Default betasx List[float] horizontal beta values for all seeds for a specific error value. None betasy List[float] vertical beta values for all seeds for a specific error value. None error_val float the error value. None title str the title to give the figure. None outputname str the name to give the file saving the figure. None View Source def plot_intermediate_beta_histograms ( betasx : List [ float ] , betasy : List [ float ] , error_val : float , title : str , outputname : str ) -> None : \"\"\" Plot histogram distribution for betas at seeds. Args: betasx (List[float]): horizontal beta values for all seeds for a specific error value. betasy (List[float]): vertical beta values for all seeds for a specific error value. error_val (float): the error value. title (str): the title to give the figure. outputname (str): the name to give the file saving the figure. \"\"\" plt . hist ( betasx , bins = 50 , label = f \"{error_val}, horizontal\" , alpha = 0.6 , density = True ) plt . hist ( betasy , bins = 50 , label = f \"{error_val}, vertical\" , alpha = 0.6 , density = True ) plt . legend ( loc = \"best\" ) plt . title ( title ) plt . savefig ( outputname , format = \"pdf\" , dpi = 300 ) logger . info ( f \"Plotted intermediate beta histogram, saved as {outputname}\" )","title":"plot_intermediate_beta_histograms"},{"location":"reference/pyhdtoolkit/tfstools/","text":"Module pyhdtoolkit.tfstools tfstools package ~ ~ ~ ~ ~ ~ ~ tfstools is a collection of utilities that integrate within my workflow when manipulating tfs files. View Source \"\"\" tfstools package ~~~~~~~~~~~~~~~~~~~ tfstools is a collection of utilities that integrate within my workflow when manipulating `tfs` files. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .latwiss import plot_latwiss Sub-modules pyhdtoolkit.tfstools.latwiss","title":"Index"},{"location":"reference/pyhdtoolkit/tfstools/#module-pyhdtoolkittfstools","text":"tfstools package ~ ~ ~ ~ ~ ~ ~ tfstools is a collection of utilities that integrate within my workflow when manipulating tfs files. View Source \"\"\" tfstools package ~~~~~~~~~~~~~~~~~~~ tfstools is a collection of utilities that integrate within my workflow when manipulating `tfs` files. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .latwiss import plot_latwiss","title":"Module pyhdtoolkit.tfstools"},{"location":"reference/pyhdtoolkit/tfstools/#sub-modules","text":"pyhdtoolkit.tfstools.latwiss","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/tfstools/latwiss/","text":"Module pyhdtoolkit.tfstools.latwiss Module tfstools.latwiss Created on 2020.10.15 View Source \"\"\" Module tfstools.latwiss -------------------------- Created on 2020.10.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions to elegantly plot the Twiss parameters output of a MADX twiss command, either from a file on disk or a loaded TfsDataFrame. \"\"\" from pathlib import Path from typing import Dict , List , Tuple , Union import matplotlib.axes import matplotlib.patches as patches import matplotlib.pyplot as plt import pandas as pd import tfs from loguru import logger from pyhdtoolkit.plotting.settings import PLOT_PARAMS plt . rcParams . update ( PLOT_PARAMS ) # ----- Plotting Functionality ----- def _plot_lattice_series ( ax : matplotlib . axes . Axes , series : pd . DataFrame , height : float = 1.0 , v_offset : float = 0.0 , color : str = \"r\" , alpha : float = 0.5 , lw : int = 3 , ) -> None : \"\"\" Plots the layout of your machine as a patch of rectangles for different element types. Original code from Guido Sterbini. Args: ax (matplotlib.axes.Axes): an existing matplotlib.axis `Axes` object to act on. series (pd.DataFrame): a dataframe with your elements' data. height (float): value to reach for the patch on the y axis. v_offset (float): vertical offset for the patch. color (str): color kwarg to transmit to pyplot. alpha (float): alpha kwarg to transmit to pyplot. lw (int): linewidth kwarg to transmit to pyplot. Returns: Nothing, acts directly on the provided axis. \"\"\" # pylint: disable=too-many-arguments ax . add_patch ( patches . Rectangle ( ( series . s - series . l , v_offset - height / 2.0 ), series . l , # width height , # height color = color , alpha = alpha , lw = lw , ) ) def plot_latwiss ( twiss_ouptut : Union [ str , Path , tfs . TfsDataFrame , pd . DataFrame ] = None , title : str = None , figsize : Tuple [ int , int ] = ( 16 , 10 ), savefig : str = None , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_sextupoles : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ), ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is heavily refactored code, but the original is from Guido Sterbini. WARNING: This will FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: twiss_ouptut (Union[str, Path, tfs.TfsDataFrame, pd.DataFrame]): the output of the MADX twiss command, either loaded as a tfs.TfsDataFrame or the location of the output file on disk. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_sextupoles (bool): if True, sextupole patches will be plotted on the layout subplot of the figure. Defaults to False. Sextupoles are plotted in yellow. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint: disable=too-many-arguments twiss_df : tfs . TfsDataFrame = _get_tfs_dataframe_from_input ( twiss_ouptut ) twiss_df . columns = [ colname . lower () for colname in twiss_df . columns ] _assert_necessary_columns ( twiss_df , columns = [ \"s\" , \"keyword\" , \"betx\" , \"bety\" , \"dx\" , \"dy\" ]) elements_df = _make_survey_groups ( twiss_df ) logger . info ( \"Setting up figure\" ) figure = plt . figure ( figsize = figsize ) # Create a subplot for the lattice patches (takes a third of figure) logger . trace ( \"Setting up element patches subplots\" ) quadrupole_patches_axis = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) dipole_patches_axis = quadrupole_patches_axis . twinx () quadrupole_patches_axis . set_ylabel ( r \"1/f=K1L [m$^{-1}$]\" , color = \"red\" ) # quadrupole in red quadrupole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"red\" ) dipole_patches_axis . set_ylabel ( r \"$\\theta$=K0L [rad]\" , color = \"blue\" ) # dipoles in blue dipole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"blue\" ) quadrupole_patches_axis . set_ylim ( k1l_lim ) dipole_patches_axis . set_ylim ( k0l_lim ) dipole_patches_axis . grid ( False ) quadrupole_patches_axis . set_title ( title ) quadrupole_patches_axis . plot ( twiss_df . s , 0 * twiss_df . s , \"k\" ) # Plotting dipole patches, beware 'sbend' and 'rbend' have an 'angle' value and not a 'k0l' if plot_dipoles : _assert_necessary_columns ( twiss_df , columns = [ \"k0l\" , \"angle\" ]) dipoles_df = elements_df [ \"dipoles\" ] logger . debug ( \"Plotting dipole patches\" ) for _ , dipole in dipoles_df . iterrows (): if dipole . k0l != 0 : logger . trace ( \"Plotting dipole element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . k0l , v_offset = dipole . k0l / 2 , color = \"b\" , ) elif dipole . angle != 0 : logger . trace ( \"Plotting 'sbend' / 'rbend' element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . angle , v_offset = dipole . angle / 2 , color = \"b\" , ) # Plotting the quadrupole patches if plot_quadrupoles : _assert_necessary_columns ( twiss_df , columns = [ \"k1l\" ]) quadrupoles_df = elements_df [ \"quadrupoles\" ] logger . debug ( \"Plotting quadrupole patches\" ) for _ , quad in quadrupoles_df . iterrows (): _plot_lattice_series ( quadrupole_patches_axis , quad , height = quad . k1l , v_offset = quad . k1l / 2 , color = \"r\" ) # Plotting the sextupole patches if plot_sextupoles : _assert_necessary_columns ( twiss_df , columns = [ \"k2l\" ]) sextupoles_df = elements_df [ \"sextupoles\" ] logger . debug ( \"Plotting sextupole patches\" ) for _ , sext in sextupoles_df . iterrows (): _plot_lattice_series ( dipole_patches_axis , sext , height = sext . k2l , v_offset = sext . k2l / 2 , color = \"y\" ) # Plotting beta functions on remaining two thirds of the figure logger . trace ( \"Setting up betatron functions subplot\" ) betatron_axis = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 2 , sharex = quadrupole_patches_axis ) betatron_axis . plot ( twiss_df . s , twiss_df . betx , label = r \"$\\beta_{x}$\" , lw = 1.5 ) betatron_axis . plot ( twiss_df . s , twiss_df . bety , label = r \"$\\beta_{y}$\" , lw = 1.5 ) betatron_axis . legend ( loc = 2 ) betatron_axis . set_ylabel ( r \"$\\beta$-functions [m]\" ) if beta_ylim : logger . debug ( \"Setting ylim for betatron functions plot\" ) betatron_axis . set_ylim ( beta_ylim ) betatron_axis . set_xlabel ( \"s [m]\" ) # Plotting the dispersion logger . trace ( \"Setting up dispersion functions subplot\" ) dispertion_axis = betatron_axis . twinx () dispertion_axis . plot ( twiss_df . s , twiss_df . dx , color = \"brown\" , label = r \"$D_{x}$\" , lw = 2 ) dispertion_axis . plot ( twiss_df . s , twiss_df . dy , ls = \"-.\" , color = \"sienna\" , label = r \"$D_{y}$\" , lw = 2 ) dispertion_axis . legend ( loc = 1 ) dispertion_axis . set_ylabel ( \"Dispersion [m]\" , color = \"brown\" ) dispertion_axis . tick_params ( axis = \"y\" , labelcolor = \"brown\" ) dispertion_axis . grid ( False ) if disp_ylim : logger . debug ( \"Setting ylim for dispersion plot\" ) dispertion_axis . set_ylim ( disp_ylim ) if xlimits : logger . debug ( \"Setting xlim for longitudinal coordinate\" ) plt . xlim ( xlimits ) if savefig : logger . info ( f \"Saving latwiss plot as {savefig}\" ) plt . savefig ( savefig , format = \"pdf\" , dpi = 350 ) return figure # ----- Helpers ----1- def _get_tfs_dataframe_from_input ( twiss_input : Union [ str , Path , tfs . TfsDataFrame , pd . DataFrame ] ) -> tfs . TfsDataFrame : \"\"\" Args: twiss_input (Union[str, Path, tfs.TfsDataFrame. pd.DataFrame]): the output of the MADX twiss command, either loaded as a tfs.TfsDataFrame or the location of the output file on disk. Returns: A TfsDataFrame or pd.DataFrame with the data. \"\"\" if isinstance ( twiss_input , ( Path , str )): logger . trace ( \"Loading Twiss dataframe from disk\" ) return tfs . read ( Path ( twiss_input )) if isinstance ( twiss_input , ( pd . DataFrame , tfs . TfsDataFrame )): logger . trace ( \"Copying input dataframe\" ) return twiss_input . copy () logger . error ( \"Expected either a string, Path object or TfsDataFrame, but provided input \" f \"was of type '{type(twiss_input)}'\" ) raise ValueError ( f \"Invalid input type for argument 'twiss_input': {type(twiss_input)}\" ) def _assert_necessary_columns ( dataframe : Union [ pd . DataFrame , tfs . TfsDataFrame ], columns : List [ str ]) -> None : \"\"\" Checks the presence of needed inputs for the latwiss plot in the provided dataframe. Will raise a KeyError if any of them is missing. Args: dataframe (Union[pd.DataFrame, tfs.TfsDataFrame]): the dataframe used for the data. columns (List[str]): list of column names to check for. \"\"\" if any ( colname not in dataframe . columns for colname in columns ): logger . error ( \"Some necessary columns are missing in the provided dataframe. \\n \" f \"The required columns are: {columns} \\n \" f \"The detected columns are: {dataframe.columns.to_numpy()}\" ) raise KeyError ( \"Missing columns in the provided dataframe\" ) def _make_survey_groups ( survey_df : pd . DataFrame ) -> Dict [ str , pd . DataFrame ]: \"\"\" Gets a survey dataframe and returns different sub-dataframes corresponding to different magnetic elements. Args: survey_df (pd.DataFrame): machine survey dataframe obtained from your Madx instance, with <instance>.table.survey.dframe(). Returns: A dictionary containing a dataframe for dipoles, focusing quadrupoles, defocusing quadrupoles, sextupoles and octupoles. The keys are self-explanatory. \"\"\" logger . debug ( \"Getting different element groups dframes from MAD-X survey\" ) return { \"dipoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"sbend\" , \"rbend\" ])) & ( survey_df . name . str . contains ( \"B\" , case = False )) ], \"quadrupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) ], \"quad_foc\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) & ( survey_df . name . str . contains ( \"F\" , case = False )) ], \"quad_defoc\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) & ( survey_df . name . str . contains ( \"D\" , case = False )) ], \"sextupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"sextupole\" ])) & ( survey_df . name . str . contains ( \"S\" , case = False )) ], \"octupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"octupole\" ])) & ( survey_df . name . str . contains ( \"O\" , case = False )) ], } Variables PLOT_PARAMS Functions plot_latwiss def plot_latwiss ( twiss_ouptut : Union [ str , pathlib . Path , tfs . handler . TfsDataFrame , pandas . core . frame . DataFrame ] = None , title : str = None , figsize : Tuple [ int , int ] = ( 16 , 10 ), savefig : str = None , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_sextupoles : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ) ) -> matplotlib . figure . Figure Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is heavily refactored code, but the original is from Guido Sterbini. WARNING: This will FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Parameters: Name Type Description Default twiss_ouptut Union[str, Path, tfs.TfsDataFrame, pd.DataFrame] the output of the MADX twiss command, either loaded as a tfs.TfsDataFrame or the location of the output file on disk. None title str title of your plot. None figsize Tuple[int, int] size of the figure, defaults to (16, 10). None savefig str will save the figure if this is not None, using the string value passed. None xlimits Tuple[float, float] will implement xlim (for the s coordinate) if this is not None, using the tuple passed. None plot_dipoles bool if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. None plot_quadrupoles bool if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. None plot_sextupoles bool if True, sextupole patches will be plotted on the layout subplot of the figure. Defaults to False. Sextupoles are plotted in yellow. None disp_ylim Tuple[float, float] vertical axis limits for the dispersion values. Defaults to (-10, 125). None beta_ylim Tuple[float, float] vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. None k0l_lim Tuple[float, float] vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). None k1l_lim Tuple[float, float] vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source def plot_latwiss ( twiss_ouptut : Union [ str, Path, tfs.TfsDataFrame, pd.DataFrame ] = None , title : str = None , figsize : Tuple [ int, int ] = ( 16 , 10 ), savefig : str = None , xlimits : Tuple [ float, float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_sextupoles : bool = False , disp_ylim : Tuple [ float, float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float, float ] = None , k0l_lim : Tuple [ float, float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float, float ] = ( - 0.08 , 0.08 ), ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is heavily refactored code, but the original is from Guido Sterbini. WARNING: This will FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: twiss_ouptut (Union[str, Path, tfs.TfsDataFrame, pd.DataFrame]): the output of the MADX twiss command, either loaded as a tfs.TfsDataFrame or the location of the output file on disk. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_sextupoles (bool): if True, sextupole patches will be plotted on the layout subplot of the figure. Defaults to False. Sextupoles are plotted in yellow. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint : disable = too - many - arguments twiss_df : tfs . TfsDataFrame = _get_tfs_dataframe_from_input ( twiss_ouptut ) twiss_df . columns = [ colname.lower() for colname in twiss_df.columns ] _assert_necessary_columns ( twiss_df , columns =[ \"s\", \"keyword\", \"betx\", \"bety\", \"dx\", \"dy\" ] ) elements_df = _make_survey_groups ( twiss_df ) logger . info ( \"Setting up figure\" ) figure = plt . figure ( figsize = figsize ) # Create a subplot for the lattice patches ( takes a third of figure ) logger . trace ( \"Setting up element patches subplots\" ) quadrupole_patches_axis = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) dipole_patches_axis = quadrupole_patches_axis . twinx () quadrupole_patches_axis . set_ylabel ( r \"1/f=K1L [m$^{-1}$]\" , color = \"red\" ) # quadrupole in red quadrupole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"red\" ) dipole_patches_axis . set_ylabel ( r \"$\\theta$=K0L [rad]\" , color = \"blue\" ) # dipoles in blue dipole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"blue\" ) quadrupole_patches_axis . set_ylim ( k1l_lim ) dipole_patches_axis . set_ylim ( k0l_lim ) dipole_patches_axis . grid ( False ) quadrupole_patches_axis . set_title ( title ) quadrupole_patches_axis . plot ( twiss_df . s , 0 * twiss_df . s , \"k\" ) # Plotting dipole patches , beware 'sbend' and 'rbend' have an 'angle' value and not a 'k0l' if plot_dipoles : _assert_necessary_columns ( twiss_df , columns =[ \"k0l\", \"angle\" ] ) dipoles_df = elements_df [ \"dipoles\" ] logger . debug ( \"Plotting dipole patches\" ) for _ , dipole in dipoles_df . iterrows () : if dipole . k0l != 0 : logger . trace ( \"Plotting dipole element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . k0l , v_offset = dipole . k0l / 2 , color = \"b\" , ) elif dipole . angle != 0 : logger . trace ( \"Plotting 'sbend' / 'rbend' element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . angle , v_offset = dipole . angle / 2 , color = \"b\" , ) # Plotting the quadrupole patches if plot_quadrupoles : _assert_necessary_columns ( twiss_df , columns =[ \"k1l\" ] ) quadrupoles_df = elements_df [ \"quadrupoles\" ] logger . debug ( \"Plotting quadrupole patches\" ) for _ , quad in quadrupoles_df . iterrows () : _plot_lattice_series ( quadrupole_patches_axis , quad , height = quad . k1l , v_offset = quad . k1l / 2 , color = \"r\" ) # Plotting the sextupole patches if plot_sextupoles : _assert_necessary_columns ( twiss_df , columns =[ \"k2l\" ] ) sextupoles_df = elements_df [ \"sextupoles\" ] logger . debug ( \"Plotting sextupole patches\" ) for _ , sext in sextupoles_df . iterrows () : _plot_lattice_series ( dipole_patches_axis , sext , height = sext . k2l , v_offset = sext . k2l / 2 , color = \"y\" ) # Plotting beta functions on remaining two thirds of the figure logger . trace ( \"Setting up betatron functions subplot\" ) betatron_axis = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 2 , sharex = quadrupole_patches_axis ) betatron_axis . plot ( twiss_df . s , twiss_df . betx , label = r \"$\\beta_{x}$\" , lw = 1.5 ) betatron_axis . plot ( twiss_df . s , twiss_df . bety , label = r \"$\\beta_{y}$\" , lw = 1.5 ) betatron_axis . legend ( loc = 2 ) betatron_axis . set_ylabel ( r \"$\\beta$-functions [m]\" ) if beta_ylim : logger . debug ( \"Setting ylim for betatron functions plot\" ) betatron_axis . set_ylim ( beta_ylim ) betatron_axis . set_xlabel ( \"s [m]\" ) # Plotting the dispersion logger . trace ( \"Setting up dispersion functions subplot\" ) dispertion_axis = betatron_axis . twinx () dispertion_axis . plot ( twiss_df . s , twiss_df . dx , color = \"brown\" , label = r \"$D_{x}$\" , lw = 2 ) dispertion_axis . plot ( twiss_df . s , twiss_df . dy , ls = \"-.\" , color = \"sienna\" , label = r \"$D_{y}$\" , lw = 2 ) dispertion_axis . legend ( loc = 1 ) dispertion_axis . set_ylabel ( \"Dispersion [m]\" , color = \"brown\" ) dispertion_axis . tick_params ( axis = \"y\" , labelcolor = \"brown\" ) dispertion_axis . grid ( False ) if disp_ylim : logger . debug ( \"Setting ylim for dispersion plot\" ) dispertion_axis . set_ylim ( disp_ylim ) if xlimits : logger . debug ( \"Setting xlim for longitudinal coordinate\" ) plt . xlim ( xlimits ) if savefig : logger . info ( f \"Saving latwiss plot as {savefig}\" ) plt . savefig ( savefig , format = \"pdf\" , dpi = 350 ) return figure","title":"Latwiss"},{"location":"reference/pyhdtoolkit/tfstools/latwiss/#module-pyhdtoolkittfstoolslatwiss","text":"Module tfstools.latwiss Created on 2020.10.15 View Source \"\"\" Module tfstools.latwiss -------------------------- Created on 2020.10.15 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection of functions to elegantly plot the Twiss parameters output of a MADX twiss command, either from a file on disk or a loaded TfsDataFrame. \"\"\" from pathlib import Path from typing import Dict , List , Tuple , Union import matplotlib.axes import matplotlib.patches as patches import matplotlib.pyplot as plt import pandas as pd import tfs from loguru import logger from pyhdtoolkit.plotting.settings import PLOT_PARAMS plt . rcParams . update ( PLOT_PARAMS ) # ----- Plotting Functionality ----- def _plot_lattice_series ( ax : matplotlib . axes . Axes , series : pd . DataFrame , height : float = 1.0 , v_offset : float = 0.0 , color : str = \"r\" , alpha : float = 0.5 , lw : int = 3 , ) -> None : \"\"\" Plots the layout of your machine as a patch of rectangles for different element types. Original code from Guido Sterbini. Args: ax (matplotlib.axes.Axes): an existing matplotlib.axis `Axes` object to act on. series (pd.DataFrame): a dataframe with your elements' data. height (float): value to reach for the patch on the y axis. v_offset (float): vertical offset for the patch. color (str): color kwarg to transmit to pyplot. alpha (float): alpha kwarg to transmit to pyplot. lw (int): linewidth kwarg to transmit to pyplot. Returns: Nothing, acts directly on the provided axis. \"\"\" # pylint: disable=too-many-arguments ax . add_patch ( patches . Rectangle ( ( series . s - series . l , v_offset - height / 2.0 ), series . l , # width height , # height color = color , alpha = alpha , lw = lw , ) ) def plot_latwiss ( twiss_ouptut : Union [ str , Path , tfs . TfsDataFrame , pd . DataFrame ] = None , title : str = None , figsize : Tuple [ int , int ] = ( 16 , 10 ), savefig : str = None , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_sextupoles : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ), ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is heavily refactored code, but the original is from Guido Sterbini. WARNING: This will FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: twiss_ouptut (Union[str, Path, tfs.TfsDataFrame, pd.DataFrame]): the output of the MADX twiss command, either loaded as a tfs.TfsDataFrame or the location of the output file on disk. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_sextupoles (bool): if True, sextupole patches will be plotted on the layout subplot of the figure. Defaults to False. Sextupoles are plotted in yellow. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint: disable=too-many-arguments twiss_df : tfs . TfsDataFrame = _get_tfs_dataframe_from_input ( twiss_ouptut ) twiss_df . columns = [ colname . lower () for colname in twiss_df . columns ] _assert_necessary_columns ( twiss_df , columns = [ \"s\" , \"keyword\" , \"betx\" , \"bety\" , \"dx\" , \"dy\" ]) elements_df = _make_survey_groups ( twiss_df ) logger . info ( \"Setting up figure\" ) figure = plt . figure ( figsize = figsize ) # Create a subplot for the lattice patches (takes a third of figure) logger . trace ( \"Setting up element patches subplots\" ) quadrupole_patches_axis = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) dipole_patches_axis = quadrupole_patches_axis . twinx () quadrupole_patches_axis . set_ylabel ( r \"1/f=K1L [m$^{-1}$]\" , color = \"red\" ) # quadrupole in red quadrupole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"red\" ) dipole_patches_axis . set_ylabel ( r \"$\\theta$=K0L [rad]\" , color = \"blue\" ) # dipoles in blue dipole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"blue\" ) quadrupole_patches_axis . set_ylim ( k1l_lim ) dipole_patches_axis . set_ylim ( k0l_lim ) dipole_patches_axis . grid ( False ) quadrupole_patches_axis . set_title ( title ) quadrupole_patches_axis . plot ( twiss_df . s , 0 * twiss_df . s , \"k\" ) # Plotting dipole patches, beware 'sbend' and 'rbend' have an 'angle' value and not a 'k0l' if plot_dipoles : _assert_necessary_columns ( twiss_df , columns = [ \"k0l\" , \"angle\" ]) dipoles_df = elements_df [ \"dipoles\" ] logger . debug ( \"Plotting dipole patches\" ) for _ , dipole in dipoles_df . iterrows (): if dipole . k0l != 0 : logger . trace ( \"Plotting dipole element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . k0l , v_offset = dipole . k0l / 2 , color = \"b\" , ) elif dipole . angle != 0 : logger . trace ( \"Plotting 'sbend' / 'rbend' element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . angle , v_offset = dipole . angle / 2 , color = \"b\" , ) # Plotting the quadrupole patches if plot_quadrupoles : _assert_necessary_columns ( twiss_df , columns = [ \"k1l\" ]) quadrupoles_df = elements_df [ \"quadrupoles\" ] logger . debug ( \"Plotting quadrupole patches\" ) for _ , quad in quadrupoles_df . iterrows (): _plot_lattice_series ( quadrupole_patches_axis , quad , height = quad . k1l , v_offset = quad . k1l / 2 , color = \"r\" ) # Plotting the sextupole patches if plot_sextupoles : _assert_necessary_columns ( twiss_df , columns = [ \"k2l\" ]) sextupoles_df = elements_df [ \"sextupoles\" ] logger . debug ( \"Plotting sextupole patches\" ) for _ , sext in sextupoles_df . iterrows (): _plot_lattice_series ( dipole_patches_axis , sext , height = sext . k2l , v_offset = sext . k2l / 2 , color = \"y\" ) # Plotting beta functions on remaining two thirds of the figure logger . trace ( \"Setting up betatron functions subplot\" ) betatron_axis = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 2 , sharex = quadrupole_patches_axis ) betatron_axis . plot ( twiss_df . s , twiss_df . betx , label = r \"$\\beta_{x}$\" , lw = 1.5 ) betatron_axis . plot ( twiss_df . s , twiss_df . bety , label = r \"$\\beta_{y}$\" , lw = 1.5 ) betatron_axis . legend ( loc = 2 ) betatron_axis . set_ylabel ( r \"$\\beta$-functions [m]\" ) if beta_ylim : logger . debug ( \"Setting ylim for betatron functions plot\" ) betatron_axis . set_ylim ( beta_ylim ) betatron_axis . set_xlabel ( \"s [m]\" ) # Plotting the dispersion logger . trace ( \"Setting up dispersion functions subplot\" ) dispertion_axis = betatron_axis . twinx () dispertion_axis . plot ( twiss_df . s , twiss_df . dx , color = \"brown\" , label = r \"$D_{x}$\" , lw = 2 ) dispertion_axis . plot ( twiss_df . s , twiss_df . dy , ls = \"-.\" , color = \"sienna\" , label = r \"$D_{y}$\" , lw = 2 ) dispertion_axis . legend ( loc = 1 ) dispertion_axis . set_ylabel ( \"Dispersion [m]\" , color = \"brown\" ) dispertion_axis . tick_params ( axis = \"y\" , labelcolor = \"brown\" ) dispertion_axis . grid ( False ) if disp_ylim : logger . debug ( \"Setting ylim for dispersion plot\" ) dispertion_axis . set_ylim ( disp_ylim ) if xlimits : logger . debug ( \"Setting xlim for longitudinal coordinate\" ) plt . xlim ( xlimits ) if savefig : logger . info ( f \"Saving latwiss plot as {savefig}\" ) plt . savefig ( savefig , format = \"pdf\" , dpi = 350 ) return figure # ----- Helpers ----1- def _get_tfs_dataframe_from_input ( twiss_input : Union [ str , Path , tfs . TfsDataFrame , pd . DataFrame ] ) -> tfs . TfsDataFrame : \"\"\" Args: twiss_input (Union[str, Path, tfs.TfsDataFrame. pd.DataFrame]): the output of the MADX twiss command, either loaded as a tfs.TfsDataFrame or the location of the output file on disk. Returns: A TfsDataFrame or pd.DataFrame with the data. \"\"\" if isinstance ( twiss_input , ( Path , str )): logger . trace ( \"Loading Twiss dataframe from disk\" ) return tfs . read ( Path ( twiss_input )) if isinstance ( twiss_input , ( pd . DataFrame , tfs . TfsDataFrame )): logger . trace ( \"Copying input dataframe\" ) return twiss_input . copy () logger . error ( \"Expected either a string, Path object or TfsDataFrame, but provided input \" f \"was of type '{type(twiss_input)}'\" ) raise ValueError ( f \"Invalid input type for argument 'twiss_input': {type(twiss_input)}\" ) def _assert_necessary_columns ( dataframe : Union [ pd . DataFrame , tfs . TfsDataFrame ], columns : List [ str ]) -> None : \"\"\" Checks the presence of needed inputs for the latwiss plot in the provided dataframe. Will raise a KeyError if any of them is missing. Args: dataframe (Union[pd.DataFrame, tfs.TfsDataFrame]): the dataframe used for the data. columns (List[str]): list of column names to check for. \"\"\" if any ( colname not in dataframe . columns for colname in columns ): logger . error ( \"Some necessary columns are missing in the provided dataframe. \\n \" f \"The required columns are: {columns} \\n \" f \"The detected columns are: {dataframe.columns.to_numpy()}\" ) raise KeyError ( \"Missing columns in the provided dataframe\" ) def _make_survey_groups ( survey_df : pd . DataFrame ) -> Dict [ str , pd . DataFrame ]: \"\"\" Gets a survey dataframe and returns different sub-dataframes corresponding to different magnetic elements. Args: survey_df (pd.DataFrame): machine survey dataframe obtained from your Madx instance, with <instance>.table.survey.dframe(). Returns: A dictionary containing a dataframe for dipoles, focusing quadrupoles, defocusing quadrupoles, sextupoles and octupoles. The keys are self-explanatory. \"\"\" logger . debug ( \"Getting different element groups dframes from MAD-X survey\" ) return { \"dipoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"sbend\" , \"rbend\" ])) & ( survey_df . name . str . contains ( \"B\" , case = False )) ], \"quadrupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) ], \"quad_foc\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) & ( survey_df . name . str . contains ( \"F\" , case = False )) ], \"quad_defoc\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"quadrupole\" ])) & ( survey_df . name . str . contains ( \"Q\" , case = False )) & ( survey_df . name . str . contains ( \"D\" , case = False )) ], \"sextupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"sextupole\" ])) & ( survey_df . name . str . contains ( \"S\" , case = False )) ], \"octupoles\" : survey_df [ ( survey_df . keyword . isin ([ \"multipole\" , \"octupole\" ])) & ( survey_df . name . str . contains ( \"O\" , case = False )) ], }","title":"Module pyhdtoolkit.tfstools.latwiss"},{"location":"reference/pyhdtoolkit/tfstools/latwiss/#variables","text":"PLOT_PARAMS","title":"Variables"},{"location":"reference/pyhdtoolkit/tfstools/latwiss/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/tfstools/latwiss/#plot_latwiss","text":"def plot_latwiss ( twiss_ouptut : Union [ str , pathlib . Path , tfs . handler . TfsDataFrame , pandas . core . frame . DataFrame ] = None , title : str = None , figsize : Tuple [ int , int ] = ( 16 , 10 ), savefig : str = None , xlimits : Tuple [ float , float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_sextupoles : bool = False , disp_ylim : Tuple [ float , float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float , float ] = None , k0l_lim : Tuple [ float , float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float , float ] = ( - 0.08 , 0.08 ) ) -> matplotlib . figure . Figure Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is heavily refactored code, but the original is from Guido Sterbini. WARNING: This will FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Parameters: Name Type Description Default twiss_ouptut Union[str, Path, tfs.TfsDataFrame, pd.DataFrame] the output of the MADX twiss command, either loaded as a tfs.TfsDataFrame or the location of the output file on disk. None title str title of your plot. None figsize Tuple[int, int] size of the figure, defaults to (16, 10). None savefig str will save the figure if this is not None, using the string value passed. None xlimits Tuple[float, float] will implement xlim (for the s coordinate) if this is not None, using the tuple passed. None plot_dipoles bool if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. None plot_quadrupoles bool if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. None plot_sextupoles bool if True, sextupole patches will be plotted on the layout subplot of the figure. Defaults to False. Sextupoles are plotted in yellow. None disp_ylim Tuple[float, float] vertical axis limits for the dispersion values. Defaults to (-10, 125). None beta_ylim Tuple[float, float] vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. None k0l_lim Tuple[float, float] vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). None k1l_lim Tuple[float, float] vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). None Returns: Type Description None The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. View Source def plot_latwiss ( twiss_ouptut : Union [ str, Path, tfs.TfsDataFrame, pd.DataFrame ] = None , title : str = None , figsize : Tuple [ int, int ] = ( 16 , 10 ), savefig : str = None , xlimits : Tuple [ float, float ] = None , plot_dipoles : bool = True , plot_quadrupoles : bool = True , plot_sextupoles : bool = False , disp_ylim : Tuple [ float, float ] = ( - 10 , 125 ), beta_ylim : Tuple [ float, float ] = None , k0l_lim : Tuple [ float, float ] = ( - 0.25 , 0.25 ), k1l_lim : Tuple [ float, float ] = ( - 0.08 , 0.08 ), ) -> matplotlib . figure . Figure : \"\"\" Provided with an active Cpymad class after having ran a script, will create a plot representing nicely the lattice layout and the beta functions along with the horizontal dispertion function. This is heavily refactored code, but the original is from Guido Sterbini. WARNING: This will FAIL if you have not included 'q' or 'Q' in your quadrupoles' names, and 'b' or 'B' in your dipoles' names when defining your MAD-X sequence. Args: twiss_ouptut (Union[str, Path, tfs.TfsDataFrame, pd.DataFrame]): the output of the MADX twiss command, either loaded as a tfs.TfsDataFrame or the location of the output file on disk. title (str): title of your plot. figsize (Tuple[int, int]): size of the figure, defaults to (16, 10). savefig (str): will save the figure if this is not None, using the string value passed. xlimits (Tuple[float, float]): will implement xlim (for the s coordinate) if this is not None, using the tuple passed. plot_dipoles (bool): if True, dipole patches will be plotted on the layout subplot of the figure. Defaults to True. Dipoles are plotted in blue. plot_quadrupoles (bool): if True, quadrupole patches will be plotted on the layout subplot of the figure. Defaults to True. Quadrupoles are plotted in red. plot_sextupoles (bool): if True, sextupole patches will be plotted on the layout subplot of the figure. Defaults to False. Sextupoles are plotted in yellow. disp_ylim (Tuple[float, float]): vertical axis limits for the dispersion values. Defaults to (-10, 125). beta_ylim (Tuple[float, float]): vertical axis limits for the betatron function values. Defaults to None, to be determined by matplotlib based on the provided beta values. k0l_lim (Tuple[float, float]): vertical axis limits for the k0l values used for the height of dipole patches. Defaults to (-0.25, 0.25). k1l_lim (Tuple[float, float]): vertical axis limits for the k1l values used for the height of quadrupole patches. Defaults to (-0.08, 0.08). Returns: The figure on which the plots are drawn. The underlying axes can be accessed with 'fig.get_axes()'. Eventually saves the figure as a file. \"\"\" # pylint : disable = too - many - arguments twiss_df : tfs . TfsDataFrame = _get_tfs_dataframe_from_input ( twiss_ouptut ) twiss_df . columns = [ colname.lower() for colname in twiss_df.columns ] _assert_necessary_columns ( twiss_df , columns =[ \"s\", \"keyword\", \"betx\", \"bety\", \"dx\", \"dy\" ] ) elements_df = _make_survey_groups ( twiss_df ) logger . info ( \"Setting up figure\" ) figure = plt . figure ( figsize = figsize ) # Create a subplot for the lattice patches ( takes a third of figure ) logger . trace ( \"Setting up element patches subplots\" ) quadrupole_patches_axis = plt . subplot2grid (( 3 , 3 ), ( 0 , 0 ), colspan = 3 , rowspan = 1 ) dipole_patches_axis = quadrupole_patches_axis . twinx () quadrupole_patches_axis . set_ylabel ( r \"1/f=K1L [m$^{-1}$]\" , color = \"red\" ) # quadrupole in red quadrupole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"red\" ) dipole_patches_axis . set_ylabel ( r \"$\\theta$=K0L [rad]\" , color = \"blue\" ) # dipoles in blue dipole_patches_axis . tick_params ( axis = \"y\" , labelcolor = \"blue\" ) quadrupole_patches_axis . set_ylim ( k1l_lim ) dipole_patches_axis . set_ylim ( k0l_lim ) dipole_patches_axis . grid ( False ) quadrupole_patches_axis . set_title ( title ) quadrupole_patches_axis . plot ( twiss_df . s , 0 * twiss_df . s , \"k\" ) # Plotting dipole patches , beware 'sbend' and 'rbend' have an 'angle' value and not a 'k0l' if plot_dipoles : _assert_necessary_columns ( twiss_df , columns =[ \"k0l\", \"angle\" ] ) dipoles_df = elements_df [ \"dipoles\" ] logger . debug ( \"Plotting dipole patches\" ) for _ , dipole in dipoles_df . iterrows () : if dipole . k0l != 0 : logger . trace ( \"Plotting dipole element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . k0l , v_offset = dipole . k0l / 2 , color = \"b\" , ) elif dipole . angle != 0 : logger . trace ( \"Plotting 'sbend' / 'rbend' element\" ) _plot_lattice_series ( dipole_patches_axis , dipole , height = dipole . angle , v_offset = dipole . angle / 2 , color = \"b\" , ) # Plotting the quadrupole patches if plot_quadrupoles : _assert_necessary_columns ( twiss_df , columns =[ \"k1l\" ] ) quadrupoles_df = elements_df [ \"quadrupoles\" ] logger . debug ( \"Plotting quadrupole patches\" ) for _ , quad in quadrupoles_df . iterrows () : _plot_lattice_series ( quadrupole_patches_axis , quad , height = quad . k1l , v_offset = quad . k1l / 2 , color = \"r\" ) # Plotting the sextupole patches if plot_sextupoles : _assert_necessary_columns ( twiss_df , columns =[ \"k2l\" ] ) sextupoles_df = elements_df [ \"sextupoles\" ] logger . debug ( \"Plotting sextupole patches\" ) for _ , sext in sextupoles_df . iterrows () : _plot_lattice_series ( dipole_patches_axis , sext , height = sext . k2l , v_offset = sext . k2l / 2 , color = \"y\" ) # Plotting beta functions on remaining two thirds of the figure logger . trace ( \"Setting up betatron functions subplot\" ) betatron_axis = plt . subplot2grid (( 3 , 3 ), ( 1 , 0 ), colspan = 3 , rowspan = 2 , sharex = quadrupole_patches_axis ) betatron_axis . plot ( twiss_df . s , twiss_df . betx , label = r \"$\\beta_{x}$\" , lw = 1.5 ) betatron_axis . plot ( twiss_df . s , twiss_df . bety , label = r \"$\\beta_{y}$\" , lw = 1.5 ) betatron_axis . legend ( loc = 2 ) betatron_axis . set_ylabel ( r \"$\\beta$-functions [m]\" ) if beta_ylim : logger . debug ( \"Setting ylim for betatron functions plot\" ) betatron_axis . set_ylim ( beta_ylim ) betatron_axis . set_xlabel ( \"s [m]\" ) # Plotting the dispersion logger . trace ( \"Setting up dispersion functions subplot\" ) dispertion_axis = betatron_axis . twinx () dispertion_axis . plot ( twiss_df . s , twiss_df . dx , color = \"brown\" , label = r \"$D_{x}$\" , lw = 2 ) dispertion_axis . plot ( twiss_df . s , twiss_df . dy , ls = \"-.\" , color = \"sienna\" , label = r \"$D_{y}$\" , lw = 2 ) dispertion_axis . legend ( loc = 1 ) dispertion_axis . set_ylabel ( \"Dispersion [m]\" , color = \"brown\" ) dispertion_axis . tick_params ( axis = \"y\" , labelcolor = \"brown\" ) dispertion_axis . grid ( False ) if disp_ylim : logger . debug ( \"Setting ylim for dispersion plot\" ) dispertion_axis . set_ylim ( disp_ylim ) if xlimits : logger . debug ( \"Setting xlim for longitudinal coordinate\" ) plt . xlim ( xlimits ) if savefig : logger . info ( f \"Saving latwiss plot as {savefig}\" ) plt . savefig ( savefig , format = \"pdf\" , dpi = 350 ) return figure","title":"plot_latwiss"},{"location":"reference/pyhdtoolkit/utils/","text":"Module pyhdtoolkit.utils utils package ~ ~ ~ ~ ~ ~ ~ These are miscellaneous modules with functions that sometime tunr out useful to my workflow. These are mainly wrappers around lower-level tools, or simply just additional niceties. View Source \"\"\" utils package ~~~~~~~~~~~~~~~~~~~ These are miscellaneous modules with functions that sometime tunr out useful to my workflow. These are mainly wrappers around lower-level tools, or simply just additional niceties. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .cmdline import CommandLine from .executors import MultiProcessor , MultiThreader from .printutil import END , Background , Foreground , Styles Sub-modules pyhdtoolkit.utils.cmdline pyhdtoolkit.utils.contexts pyhdtoolkit.utils.defaults pyhdtoolkit.utils.executors pyhdtoolkit.utils.operations pyhdtoolkit.utils.printutil Variables python3 END","title":"Index"},{"location":"reference/pyhdtoolkit/utils/#module-pyhdtoolkitutils","text":"utils package ~ ~ ~ ~ ~ ~ ~ These are miscellaneous modules with functions that sometime tunr out useful to my workflow. These are mainly wrappers around lower-level tools, or simply just additional niceties. View Source \"\"\" utils package ~~~~~~~~~~~~~~~~~~~ These are miscellaneous modules with functions that sometime tunr out useful to my workflow. These are mainly wrappers around lower-level tools, or simply just additional niceties. :copyright: (c) 2019 by Felix Soubelet. :license: MIT, see LICENSE for more details. \"\"\" from .cmdline import CommandLine from .executors import MultiProcessor , MultiThreader from .printutil import END , Background , Foreground , Styles","title":"Module pyhdtoolkit.utils"},{"location":"reference/pyhdtoolkit/utils/#sub-modules","text":"pyhdtoolkit.utils.cmdline pyhdtoolkit.utils.contexts pyhdtoolkit.utils.defaults pyhdtoolkit.utils.executors pyhdtoolkit.utils.operations pyhdtoolkit.utils.printutil","title":"Sub-modules"},{"location":"reference/pyhdtoolkit/utils/#variables","text":"python3 END","title":"Variables"},{"location":"reference/pyhdtoolkit/utils/cmdline/","text":"Module pyhdtoolkit.utils.cmdline Module utils.cmdline Created on 2019.11.06 View Source \"\"\" Module utils.cmdline -------------------- Created on 2019.11.06 :author: Felix Soubelet (felix.soubelet@cern.ch) Utility script to help run commands and access the commandline. \"\"\" import errno import os import signal import subprocess from typing import Mapping , Optional , Tuple from loguru import logger from pyhdtoolkit.utils.contexts import timeit class CommandLine : \"\"\" A high-level object to encapsulate the different methods for interacting with the commandline. \"\"\" @staticmethod def check_pid_exists ( pid : int ) -> bool : \"\"\" Check whether the given PID exists in the current process table. Args: pid (int): the Process ID you want to check. Returns: A boolean stating the result. \"\"\" if pid == 0 : # According to \"man 2 kill\", PID 0 refers to <<every process in the process group of # the calling process>>. Best not to go any further. logger . warning ( \"PID 0 refers to 'every process in calling processes', and should be untouched\" ) return True try : # Sending SIG 0 only checks if process has terminated, we're not actually terminating it os . kill ( pid , 0 ) except OSError as pid_checkout_error : if pid_checkout_error . errno == errno . ESRCH : # ERROR \"No such process\" return False if ( pid_checkout_error . errno == errno . EPERM ): # ERROR \"Operation not permitted\" -> there's a process to deny access to. return True # According to \"man 2 kill\" possible error values are (EINVAL, EPERM, ESRCH), therefore # we should never get here. If so let's be explicit in considering this an error. logger . exception ( \"Could not figure out the provided PID for some reason\" ) raise pid_checkout_error return True @staticmethod def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Optional [ int ], bytes ]: \"\"\" Run command based on `subprocess.Popen` and return the tuple of `(returncode, stdout)`. Note that `stderr` is redirected to `stdout`. `shell` is same to parameter of `Popen`. If the process does not terminate after `timeout` seconds, a `TimeoutExpired` exception will be raised. Args: command (str): string, the command you want to run. shell (bool): same as `Popen` argument. Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to run the command. In other words, using an intermediate shell means that variables, glob patterns, and other special shell features in the command string are processed before the command is ran. Defaults to True. env (Mapping): mapping that defines the environment variables for the new process. timeout (float): same as `Popen.communicate` argument, number of seconds to wait for a response before raising a TimeoutExpired exception. Returns: The tuple of (returncode, stdout). Beware, the stdout will be a byte array (id est b'some returned text'). This output, returned as stdout, needs to be decoded properly before you do anything with it, especially if you intend to log it into a file. While it will most likely be 'utf-8', the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on. Usage: CommandLine.run('echo hello') -> (0, b'hello\\r\\n') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') \"\"\" with timeit ( lambda spanned : logger . info ( f \"Ran command '{command}' in a subprocess, in: {spanned:.4f} seconds\" ) ): process = subprocess . Popen ( command , shell = shell , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , env = env ) stdout , _ = process . communicate ( timeout = timeout ) if process . poll () != 0 : logger . warning ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) else : logger . success ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) return process . poll (), stdout @staticmethod def terminate ( pid : int ) -> bool : \"\"\" Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Args: pid (int): the process ID to kill. Returns: A boolean stating the success of the operation. \"\"\" if CommandLine . check_pid_exists ( pid ): os . kill ( pid , signal . SIGTERM ) logger . debug ( f \"Process {pid} has successfully been terminated.\" ) return True logger . error ( f \"Process with ID {pid} could not be terminated.\" ) return False Classes CommandLine class CommandLine ( / , * args , ** kwargs ) View Source class CommandLine : \" \"\" A high-level object to encapsulate the different methods for interacting with the commandline. \"\" \" @staticmethod def check_pid_exists ( pid : int ) -> bool : \" \"\" Check whether the given PID exists in the current process table. Args: pid (int): the Process ID you want to check. Returns: A boolean stating the result. \"\" \" if pid == 0 : # According to \"man 2 kill\", PID 0 refers to <<every process in the process group of # the calling process>>. Best not to go any further. logger . warning ( \"PID 0 refers to 'every process in calling processes', and should be untouched\" ) return True try : # Sending SIG 0 only checks if process has terminated, we're not actually terminating it os . kill ( pid , 0 ) except OSError as pid_checkout_error : if pid_checkout_error . errno == errno . ESRCH : # ERROR \"No such process\" return False if ( pid_checkout_error . errno == errno . EPERM ) : # ERROR \"Operation not permitted\" -> there's a process to deny access to. return True # According to \"man 2 kill\" possible error values are (EINVAL, EPERM, ESRCH), therefore # we should never get here. If so let's be explicit in considering this an error. logger . exception ( \"Could not figure out the provided PID for some reason\" ) raise pid_checkout_error return True @staticmethod def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Optional [ int ] , bytes ] : \" \"\" Run command based on `subprocess.Popen` and return the tuple of `(returncode, stdout)`. Note that `stderr` is redirected to `stdout`. `shell` is same to parameter of `Popen`. If the process does not terminate after `timeout` seconds, a `TimeoutExpired` exception will be raised. Args: command (str): string, the command you want to run. shell (bool): same as `Popen` argument. Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to run the command. In other words, using an intermediate shell means that variables, glob patterns, and other special shell features in the command string are processed before the command is ran. Defaults to True. env (Mapping): mapping that defines the environment variables for the new process. timeout (float): same as `Popen.communicate` argument, number of seconds to wait for a response before raising a TimeoutExpired exception. Returns: The tuple of (returncode, stdout). Beware, the stdout will be a byte array (id est b'some returned text'). This output, returned as stdout, needs to be decoded properly before you do anything with it, especially if you intend to log it into a file. While it will most likely be 'utf-8', the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on. Usage: CommandLine.run('echo hello') -> (0, b'hello \\r\\n ') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') \"\" \" with timeit ( lambda spanned : logger . info ( f \"Ran command '{command}' in a subprocess, in: {spanned:.4f} seconds\" ) ) : process = subprocess . Popen ( command , shell = shell , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , env = env ) stdout , _ = process . communicate ( timeout = timeout ) if process . poll () != 0 : logger . warning ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) else : logger . success ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) return process . poll (), stdout @staticmethod def terminate ( pid : int ) -> bool : \" \"\" Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Args: pid (int): the process ID to kill. Returns: A boolean stating the success of the operation. \"\" \" if CommandLine . check_pid_exists ( pid ) : os . kill ( pid , signal . SIGTERM ) logger . debug ( f \"Process {pid} has successfully been terminated.\" ) return True logger . error ( f \"Process with ID {pid} could not be terminated.\" ) return False Static methods check_pid_exists def check_pid_exists ( pid : int ) -> bool Check whether the given PID exists in the current process table. Parameters: Name Type Description Default pid int the Process ID you want to check. None Returns: Type Description None A boolean stating the result. View Source @staticmethod def check_pid_exists ( pid : int ) -> bool : \"\"\" Check whether the given PID exists in the current process table. Args: pid (int): the Process ID you want to check. Returns: A boolean stating the result. \"\"\" if pid == 0 : # According to \"man 2 kill\" , PID 0 refers to << every process in the process group of # the calling process >> . Best not to go any further . logger . warning ( \"PID 0 refers to 'every process in calling processes', and should be untouched\" ) return True try : # Sending SIG 0 only checks if process has terminated , we 're not actually terminating it os.kill(pid, 0) except OSError as pid_checkout_error: if pid_checkout_error.errno == errno.ESRCH: # ERROR \"No such process\" return False if ( pid_checkout_error.errno == errno.EPERM ): # ERROR \"Operation not permitted\" -> there' s a process to deny access to . return True # According to \"man 2 kill\" possible error values are ( EINVAL , EPERM , ESRCH ), therefore # we should never get here . If so let ' s be explicit in considering this an error . logger . exception ( \"Could not figure out the provided PID for some reason\" ) raise pid_checkout_error return True run def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Union [ int , NoneType ], bytes ] Run command based on subprocess.Popen and return the tuple of (returncode, stdout) . Note that stderr is redirected to stdout . shell is same to parameter of Popen . If the process does not terminate after timeout seconds, a TimeoutExpired exception will be raised. Args : command ( str ) : string , the command you want to run . shell ( bool ) : same as `Popen` argument . Set ting the shell argument to a true value causes subprocess to spawn an intermediate shell process , and tell it to run the command . In other words , using an intermediate shell means that variables , glob patterns , and other special shell features in the command string are processed before the command is ran . Defaults to True . env ( Mapping ) : mapping that defines the environment variables for the new process . timeout ( float ) : same as `Popen.communicate` argument , number of seconds to wait for a response before raising a TimeoutExpired exception . Returns : The tuple of ( returncode , stdout ). Beware , the stdout will be a byte array ( id est b 'some returned text' ). This output , returned as stdout , needs to be decoded properly before you do anything with it , especially if you intend to log it into a file . While it will most likely be 'utf-8' , the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on . Usage : CommandLine . run ( 'echo hello' ) -> ( 0 , b 'hello ') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') View Source @staticmethod def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Optional [ int ] , bytes ] : \" \"\" Run command based on `subprocess.Popen` and return the tuple of `(returncode, stdout)`. Note that `stderr` is redirected to `stdout`. `shell` is same to parameter of `Popen`. If the process does not terminate after `timeout` seconds, a `TimeoutExpired` exception will be raised. Args: command (str): string, the command you want to run. shell (bool): same as `Popen` argument. Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to run the command. In other words, using an intermediate shell means that variables, glob patterns, and other special shell features in the command string are processed before the command is ran. Defaults to True. env (Mapping): mapping that defines the environment variables for the new process. timeout (float): same as `Popen.communicate` argument, number of seconds to wait for a response before raising a TimeoutExpired exception. Returns: The tuple of (returncode, stdout). Beware, the stdout will be a byte array (id est b'some returned text'). This output, returned as stdout, needs to be decoded properly before you do anything with it, especially if you intend to log it into a file. While it will most likely be 'utf-8', the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on. Usage: CommandLine.run('echo hello') -> (0, b'hello \\r\\n ') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') \"\" \" with timeit ( lambda spanned : logger . info ( f \"Ran command '{command}' in a subprocess, in: {spanned:.4f} seconds\" ) ) : process = subprocess . Popen ( command , shell = shell , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , env = env ) stdout , _ = process . communicate ( timeout = timeout ) if process . poll () != 0 : logger . warning ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) else : logger . success ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) return process . poll (), stdout terminate def terminate ( pid : int ) -> bool Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Parameters: Name Type Description Default pid int the process ID to kill. None Returns: Type Description None A boolean stating the success of the operation. View Source @ staticmethod def terminate ( pid : int ) -> bool : \"\"\" Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Args: pid (int): the process ID to kill. Returns: A boolean stating the success of the operation. \"\"\" if CommandLine . check_pid_exists ( pid ): os . kill ( pid , signal . SIGTERM ) logger . debug ( f \"Process {pid} has successfully been terminated.\" ) return True logger . error ( f \"Process with ID {pid} could not be terminated.\" ) return False","title":"Cmdline"},{"location":"reference/pyhdtoolkit/utils/cmdline/#module-pyhdtoolkitutilscmdline","text":"Module utils.cmdline Created on 2019.11.06 View Source \"\"\" Module utils.cmdline -------------------- Created on 2019.11.06 :author: Felix Soubelet (felix.soubelet@cern.ch) Utility script to help run commands and access the commandline. \"\"\" import errno import os import signal import subprocess from typing import Mapping , Optional , Tuple from loguru import logger from pyhdtoolkit.utils.contexts import timeit class CommandLine : \"\"\" A high-level object to encapsulate the different methods for interacting with the commandline. \"\"\" @staticmethod def check_pid_exists ( pid : int ) -> bool : \"\"\" Check whether the given PID exists in the current process table. Args: pid (int): the Process ID you want to check. Returns: A boolean stating the result. \"\"\" if pid == 0 : # According to \"man 2 kill\", PID 0 refers to <<every process in the process group of # the calling process>>. Best not to go any further. logger . warning ( \"PID 0 refers to 'every process in calling processes', and should be untouched\" ) return True try : # Sending SIG 0 only checks if process has terminated, we're not actually terminating it os . kill ( pid , 0 ) except OSError as pid_checkout_error : if pid_checkout_error . errno == errno . ESRCH : # ERROR \"No such process\" return False if ( pid_checkout_error . errno == errno . EPERM ): # ERROR \"Operation not permitted\" -> there's a process to deny access to. return True # According to \"man 2 kill\" possible error values are (EINVAL, EPERM, ESRCH), therefore # we should never get here. If so let's be explicit in considering this an error. logger . exception ( \"Could not figure out the provided PID for some reason\" ) raise pid_checkout_error return True @staticmethod def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Optional [ int ], bytes ]: \"\"\" Run command based on `subprocess.Popen` and return the tuple of `(returncode, stdout)`. Note that `stderr` is redirected to `stdout`. `shell` is same to parameter of `Popen`. If the process does not terminate after `timeout` seconds, a `TimeoutExpired` exception will be raised. Args: command (str): string, the command you want to run. shell (bool): same as `Popen` argument. Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to run the command. In other words, using an intermediate shell means that variables, glob patterns, and other special shell features in the command string are processed before the command is ran. Defaults to True. env (Mapping): mapping that defines the environment variables for the new process. timeout (float): same as `Popen.communicate` argument, number of seconds to wait for a response before raising a TimeoutExpired exception. Returns: The tuple of (returncode, stdout). Beware, the stdout will be a byte array (id est b'some returned text'). This output, returned as stdout, needs to be decoded properly before you do anything with it, especially if you intend to log it into a file. While it will most likely be 'utf-8', the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on. Usage: CommandLine.run('echo hello') -> (0, b'hello\\r\\n') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') \"\"\" with timeit ( lambda spanned : logger . info ( f \"Ran command '{command}' in a subprocess, in: {spanned:.4f} seconds\" ) ): process = subprocess . Popen ( command , shell = shell , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , env = env ) stdout , _ = process . communicate ( timeout = timeout ) if process . poll () != 0 : logger . warning ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) else : logger . success ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) return process . poll (), stdout @staticmethod def terminate ( pid : int ) -> bool : \"\"\" Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Args: pid (int): the process ID to kill. Returns: A boolean stating the success of the operation. \"\"\" if CommandLine . check_pid_exists ( pid ): os . kill ( pid , signal . SIGTERM ) logger . debug ( f \"Process {pid} has successfully been terminated.\" ) return True logger . error ( f \"Process with ID {pid} could not be terminated.\" ) return False","title":"Module pyhdtoolkit.utils.cmdline"},{"location":"reference/pyhdtoolkit/utils/cmdline/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/utils/cmdline/#commandline","text":"class CommandLine ( / , * args , ** kwargs ) View Source class CommandLine : \" \"\" A high-level object to encapsulate the different methods for interacting with the commandline. \"\" \" @staticmethod def check_pid_exists ( pid : int ) -> bool : \" \"\" Check whether the given PID exists in the current process table. Args: pid (int): the Process ID you want to check. Returns: A boolean stating the result. \"\" \" if pid == 0 : # According to \"man 2 kill\", PID 0 refers to <<every process in the process group of # the calling process>>. Best not to go any further. logger . warning ( \"PID 0 refers to 'every process in calling processes', and should be untouched\" ) return True try : # Sending SIG 0 only checks if process has terminated, we're not actually terminating it os . kill ( pid , 0 ) except OSError as pid_checkout_error : if pid_checkout_error . errno == errno . ESRCH : # ERROR \"No such process\" return False if ( pid_checkout_error . errno == errno . EPERM ) : # ERROR \"Operation not permitted\" -> there's a process to deny access to. return True # According to \"man 2 kill\" possible error values are (EINVAL, EPERM, ESRCH), therefore # we should never get here. If so let's be explicit in considering this an error. logger . exception ( \"Could not figure out the provided PID for some reason\" ) raise pid_checkout_error return True @staticmethod def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Optional [ int ] , bytes ] : \" \"\" Run command based on `subprocess.Popen` and return the tuple of `(returncode, stdout)`. Note that `stderr` is redirected to `stdout`. `shell` is same to parameter of `Popen`. If the process does not terminate after `timeout` seconds, a `TimeoutExpired` exception will be raised. Args: command (str): string, the command you want to run. shell (bool): same as `Popen` argument. Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to run the command. In other words, using an intermediate shell means that variables, glob patterns, and other special shell features in the command string are processed before the command is ran. Defaults to True. env (Mapping): mapping that defines the environment variables for the new process. timeout (float): same as `Popen.communicate` argument, number of seconds to wait for a response before raising a TimeoutExpired exception. Returns: The tuple of (returncode, stdout). Beware, the stdout will be a byte array (id est b'some returned text'). This output, returned as stdout, needs to be decoded properly before you do anything with it, especially if you intend to log it into a file. While it will most likely be 'utf-8', the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on. Usage: CommandLine.run('echo hello') -> (0, b'hello \\r\\n ') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') \"\" \" with timeit ( lambda spanned : logger . info ( f \"Ran command '{command}' in a subprocess, in: {spanned:.4f} seconds\" ) ) : process = subprocess . Popen ( command , shell = shell , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , env = env ) stdout , _ = process . communicate ( timeout = timeout ) if process . poll () != 0 : logger . warning ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) else : logger . success ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) return process . poll (), stdout @staticmethod def terminate ( pid : int ) -> bool : \" \"\" Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Args: pid (int): the process ID to kill. Returns: A boolean stating the success of the operation. \"\" \" if CommandLine . check_pid_exists ( pid ) : os . kill ( pid , signal . SIGTERM ) logger . debug ( f \"Process {pid} has successfully been terminated.\" ) return True logger . error ( f \"Process with ID {pid} could not be terminated.\" ) return False","title":"CommandLine"},{"location":"reference/pyhdtoolkit/utils/cmdline/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/cmdline/#check_pid_exists","text":"def check_pid_exists ( pid : int ) -> bool Check whether the given PID exists in the current process table. Parameters: Name Type Description Default pid int the Process ID you want to check. None Returns: Type Description None A boolean stating the result. View Source @staticmethod def check_pid_exists ( pid : int ) -> bool : \"\"\" Check whether the given PID exists in the current process table. Args: pid (int): the Process ID you want to check. Returns: A boolean stating the result. \"\"\" if pid == 0 : # According to \"man 2 kill\" , PID 0 refers to << every process in the process group of # the calling process >> . Best not to go any further . logger . warning ( \"PID 0 refers to 'every process in calling processes', and should be untouched\" ) return True try : # Sending SIG 0 only checks if process has terminated , we 're not actually terminating it os.kill(pid, 0) except OSError as pid_checkout_error: if pid_checkout_error.errno == errno.ESRCH: # ERROR \"No such process\" return False if ( pid_checkout_error.errno == errno.EPERM ): # ERROR \"Operation not permitted\" -> there' s a process to deny access to . return True # According to \"man 2 kill\" possible error values are ( EINVAL , EPERM , ESRCH ), therefore # we should never get here . If so let ' s be explicit in considering this an error . logger . exception ( \"Could not figure out the provided PID for some reason\" ) raise pid_checkout_error return True","title":"check_pid_exists"},{"location":"reference/pyhdtoolkit/utils/cmdline/#run","text":"def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Union [ int , NoneType ], bytes ] Run command based on subprocess.Popen and return the tuple of (returncode, stdout) . Note that stderr is redirected to stdout . shell is same to parameter of Popen . If the process does not terminate after timeout seconds, a TimeoutExpired exception will be raised. Args : command ( str ) : string , the command you want to run . shell ( bool ) : same as `Popen` argument . Set ting the shell argument to a true value causes subprocess to spawn an intermediate shell process , and tell it to run the command . In other words , using an intermediate shell means that variables , glob patterns , and other special shell features in the command string are processed before the command is ran . Defaults to True . env ( Mapping ) : mapping that defines the environment variables for the new process . timeout ( float ) : same as `Popen.communicate` argument , number of seconds to wait for a response before raising a TimeoutExpired exception . Returns : The tuple of ( returncode , stdout ). Beware , the stdout will be a byte array ( id est b 'some returned text' ). This output , returned as stdout , needs to be decoded properly before you do anything with it , especially if you intend to log it into a file . While it will most likely be 'utf-8' , the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on . Usage : CommandLine . run ( 'echo hello' ) -> ( 0 , b 'hello ') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') View Source @staticmethod def run ( command : str , shell : bool = True , env : Mapping = None , timeout : float = None ) -> Tuple [ Optional [ int ] , bytes ] : \" \"\" Run command based on `subprocess.Popen` and return the tuple of `(returncode, stdout)`. Note that `stderr` is redirected to `stdout`. `shell` is same to parameter of `Popen`. If the process does not terminate after `timeout` seconds, a `TimeoutExpired` exception will be raised. Args: command (str): string, the command you want to run. shell (bool): same as `Popen` argument. Setting the shell argument to a true value causes subprocess to spawn an intermediate shell process, and tell it to run the command. In other words, using an intermediate shell means that variables, glob patterns, and other special shell features in the command string are processed before the command is ran. Defaults to True. env (Mapping): mapping that defines the environment variables for the new process. timeout (float): same as `Popen.communicate` argument, number of seconds to wait for a response before raising a TimeoutExpired exception. Returns: The tuple of (returncode, stdout). Beware, the stdout will be a byte array (id est b'some returned text'). This output, returned as stdout, needs to be decoded properly before you do anything with it, especially if you intend to log it into a file. While it will most likely be 'utf-8', the encoding can vary from system to system so the standard output is returned in bytes format and should be decoded later on. Usage: CommandLine.run('echo hello') -> (0, b'hello \\r\\n ') modified_env = os.environ.copy() modified_env['ENV_VAR'] = new_value CommandLine.run('echo $ENV_VAR', env=modified_env) -> (0, b'new_value') \"\" \" with timeit ( lambda spanned : logger . info ( f \"Ran command '{command}' in a subprocess, in: {spanned:.4f} seconds\" ) ) : process = subprocess . Popen ( command , shell = shell , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , env = env ) stdout , _ = process . communicate ( timeout = timeout ) if process . poll () != 0 : logger . warning ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) else : logger . success ( f \"Subprocess command '{command}' finished with exit code: {process.poll()}\" ) return process . poll (), stdout","title":"run"},{"location":"reference/pyhdtoolkit/utils/cmdline/#terminate","text":"def terminate ( pid : int ) -> bool Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Parameters: Name Type Description Default pid int the process ID to kill. None Returns: Type Description None A boolean stating the success of the operation. View Source @ staticmethod def terminate ( pid : int ) -> bool : \"\"\" Terminate process by given pid. On Other platforms, using os.kill with signal.SIGTERM to kill. Args: pid (int): the process ID to kill. Returns: A boolean stating the success of the operation. \"\"\" if CommandLine . check_pid_exists ( pid ): os . kill ( pid , signal . SIGTERM ) logger . debug ( f \"Process {pid} has successfully been terminated.\" ) return True logger . error ( f \"Process with ID {pid} could not be terminated.\" ) return False","title":"terminate"},{"location":"reference/pyhdtoolkit/utils/contexts/","text":"Module pyhdtoolkit.utils.contexts Module utils.contexts Provides contexts to use functions in. View Source \"\"\" Module utils.contexts --------------------- Provides contexts to use functions in. \"\"\" import time from contextlib import contextmanager from typing import Callable , Iterator @contextmanager def timeit ( function : Callable ) -> Iterator [ None ]: \"\"\" Returns the time elapsed when executing code in the context via `function`. Original code from @jaimecp89 Args: function (Callable): any callable taking one argument. Was conceived with a lambda in mind. Returns: The elapsed time as an argument for the provided function. Usage: with timeit(lambda spanned: logger.debug(f'Did some stuff in {spanned} seconds')): some_stuff() some_other_stuff() \"\"\" start_time = time . time () try : yield finally : time_used = time . time () - start_time function ( time_used ) Functions timeit def timeit ( function : Callable ) -> Iterator [ NoneType ] Returns the time elapsed when executing code in the context via function . Original code from @jaimecp89 Parameters: Name Type Description Default function Callable any callable taking one argument. Was conceived with a lambda in mind. None Returns: Type Description None The elapsed time as an argument for the provided function. Usage: with timeit(lambda spanned: logger.debug(f'Did some stuff in {spanned} seconds')): some_stuff() some_other_stuff() | View Source @contextmanager def timeit ( function : Callable ) -> Iterator [ None ] : \"\"\" Returns the time elapsed when executing code in the context via `function`. Original code from @jaimecp89 Args: function (Callable): any callable taking one argument. Was conceived with a lambda in mind. Returns: The elapsed time as an argument for the provided function. Usage: with timeit(lambda spanned: logger.debug(f'Did some stuff in {spanned} seconds')): some_stuff() some_other_stuff() \"\"\" start_time = time . time () try : yield finally : time_used = time . time () - start_time function ( time_used )","title":"Contexts"},{"location":"reference/pyhdtoolkit/utils/contexts/#module-pyhdtoolkitutilscontexts","text":"Module utils.contexts Provides contexts to use functions in. View Source \"\"\" Module utils.contexts --------------------- Provides contexts to use functions in. \"\"\" import time from contextlib import contextmanager from typing import Callable , Iterator @contextmanager def timeit ( function : Callable ) -> Iterator [ None ]: \"\"\" Returns the time elapsed when executing code in the context via `function`. Original code from @jaimecp89 Args: function (Callable): any callable taking one argument. Was conceived with a lambda in mind. Returns: The elapsed time as an argument for the provided function. Usage: with timeit(lambda spanned: logger.debug(f'Did some stuff in {spanned} seconds')): some_stuff() some_other_stuff() \"\"\" start_time = time . time () try : yield finally : time_used = time . time () - start_time function ( time_used )","title":"Module pyhdtoolkit.utils.contexts"},{"location":"reference/pyhdtoolkit/utils/contexts/#functions","text":"","title":"Functions"},{"location":"reference/pyhdtoolkit/utils/contexts/#timeit","text":"def timeit ( function : Callable ) -> Iterator [ NoneType ] Returns the time elapsed when executing code in the context via function . Original code from @jaimecp89 Parameters: Name Type Description Default function Callable any callable taking one argument. Was conceived with a lambda in mind. None Returns: Type Description None The elapsed time as an argument for the provided function. Usage: with timeit(lambda spanned: logger.debug(f'Did some stuff in {spanned} seconds')): some_stuff() some_other_stuff() | View Source @contextmanager def timeit ( function : Callable ) -> Iterator [ None ] : \"\"\" Returns the time elapsed when executing code in the context via `function`. Original code from @jaimecp89 Args: function (Callable): any callable taking one argument. Was conceived with a lambda in mind. Returns: The elapsed time as an argument for the provided function. Usage: with timeit(lambda spanned: logger.debug(f'Did some stuff in {spanned} seconds')): some_stuff() some_other_stuff() \"\"\" start_time = time . time () try : yield finally : time_used = time . time () - start_time function ( time_used )","title":"timeit"},{"location":"reference/pyhdtoolkit/utils/defaults/","text":"Module pyhdtoolkit.utils.defaults Module utils.defaults Created on 2019.11.12 View Source \"\"\" Module utils.defaults --------------------- Created on 2019.11.12 :author: Felix Soubelet (felix.soubelet@cern.ch) Provides defaults to import for different settings. \"\"\" from pathlib import Path ANACONDA_INSTALL = Path () . home () / \"anaconda3\" OMC_PYTHON = ANACONDA_INSTALL / \"envs\" / \"OMC\" / \"bin\" / \"python\" WORK_REPOSITORIES = Path . home () / \"Repositories\" / \"Work\" BETABEAT_REPO = WORK_REPOSITORIES / \"Beta-Beat.src\" OMC3_REPO = WORK_REPOSITORIES / \"omc3\" TBT_CONVERTER_SCRIPT = OMC3_REPO / \"omc3\" / \"tbt_converter.py\" LOGURU_FORMAT = ( \"<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | \" \"<level>{level: <8}</level> | \" \"<cyan>{name}</cyan>:<cyan>{line}</cyan> - \" \"<level>{message}</level>\" ) Variables ANACONDA_INSTALL BETABEAT_REPO LOGURU_FORMAT OMC3_REPO OMC_PYTHON TBT_CONVERTER_SCRIPT WORK_REPOSITORIES","title":"Defaults"},{"location":"reference/pyhdtoolkit/utils/defaults/#module-pyhdtoolkitutilsdefaults","text":"Module utils.defaults Created on 2019.11.12 View Source \"\"\" Module utils.defaults --------------------- Created on 2019.11.12 :author: Felix Soubelet (felix.soubelet@cern.ch) Provides defaults to import for different settings. \"\"\" from pathlib import Path ANACONDA_INSTALL = Path () . home () / \"anaconda3\" OMC_PYTHON = ANACONDA_INSTALL / \"envs\" / \"OMC\" / \"bin\" / \"python\" WORK_REPOSITORIES = Path . home () / \"Repositories\" / \"Work\" BETABEAT_REPO = WORK_REPOSITORIES / \"Beta-Beat.src\" OMC3_REPO = WORK_REPOSITORIES / \"omc3\" TBT_CONVERTER_SCRIPT = OMC3_REPO / \"omc3\" / \"tbt_converter.py\" LOGURU_FORMAT = ( \"<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | \" \"<level>{level: <8}</level> | \" \"<cyan>{name}</cyan>:<cyan>{line}</cyan> - \" \"<level>{message}</level>\" )","title":"Module pyhdtoolkit.utils.defaults"},{"location":"reference/pyhdtoolkit/utils/defaults/#variables","text":"ANACONDA_INSTALL BETABEAT_REPO LOGURU_FORMAT OMC3_REPO OMC_PYTHON TBT_CONVERTER_SCRIPT WORK_REPOSITORIES","title":"Variables"},{"location":"reference/pyhdtoolkit/utils/executors/","text":"Module pyhdtoolkit.utils.executors Module utils.executors Created on 2019.12.09 View Source \"\"\" Module utils.executors ---------------------- Created on 2019.12.09 :author: Felix Soubelet A module providing two classes to execute functions + arguments couples through either a multiprocessing approach, or a multithreading approach. Here are a few tidbits to keep in mind: - There can only be one thread running at any given time in a python process because of the GIL. - Multiprocessing is parallelism. Multithreading is concurrency. - Multiprocessing is for increasing speed. Multithreading is for hiding latency. - Multiprocessing is best for computations. Multithreading is best for IO. - If you have CPU heavy tasks, use multiprocessing with n_process = n_cores and never more. Never. - If you have IO heavy tasks, use multithreading with n_threads = m * n_cores with m a number bigger than 1 that you can tweak on your own. Try many values and choose the one with the best speedup because there isn\u2019t a general rule. For instance the default value of m in ThreadPoolExecutor is set to 5 which I think is quite random. \"\"\" from concurrent import futures from typing import Callable , List from loguru import logger class MultiProcessor : \"\"\" A class to easily wrap a multi-processing context manager call to a function. Reminder: multiprocessing is good for cpu-heavy tasks. Reminder: only picklable objects can be executed and returned. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_cpu_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ] : \"\"\" Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_processes (int): the number of processes to fire up. No more than your number of cores! If n_processes is `None` or not given, ProcessPoolExecutor will default it to the number of processors on the machine. Returns: A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multiprocessing with {n_processes} processes\" ) with futures . ProcessPoolExecutor ( n_processes ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_processes} processes finished\" ) return list ( results ) class MultiThreader : \"\"\" A class to easily wrap a multi-threading context manager call to a function. Reminder: multithreading is good for IO-heavy tasks. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_io_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ] : \"\"\" Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_threads (int): the number of threads to fire up. If n_threads is `None` or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. Returns: A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multithreading with {n_threads} threads\" ) with futures . ThreadPoolExecutor ( n_threads ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_threads} threads finished\" ) return list ( results ) Classes MultiProcessor class MultiProcessor ( / , * args , ** kwargs ) View Source class MultiProcessor : \"\"\" A class to easily wrap a multi-processing context manager call to a function. Reminder: multiprocessing is good for cpu-heavy tasks. Reminder: only picklable objects can be executed and returned. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_cpu_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_processes (int): the number of processes to fire up. No more than your number of cores! If n_processes is `None` or not given, ProcessPoolExecutor will default it to the number of processors on the machine. Returns: A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multiprocessing with {n_processes} processes\" ) with futures . ProcessPoolExecutor ( n_processes ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_processes} processes finished\" ) return list ( results ) Static methods execute_function def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ] Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Parameters: Name Type Description Default func Callable the function to call. None func_args list list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. None n_processes int the number of processes to fire up. No more than your number of cores! If n_processes is None or not given, ProcessPoolExecutor will default it to the number of processors on the machine. None Returns: Type Description None A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. View Source @ staticmethod def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_processes (int): the number of processes to fire up. No more than your number of cores! If n_processes is `None` or not given, ProcessPoolExecutor will default it to the number of processors on the machine. Returns: A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multiprocessing with {n_processes} processes\" ) with futures . ProcessPoolExecutor ( n_processes ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_processes} processes finished\" ) return list ( results ) MultiThreader class MultiThreader ( / , * args , ** kwargs ) View Source class MultiThreader : \"\"\" A class to easily wrap a multi-threading context manager call to a function. Reminder: multithreading is good for IO-heavy tasks. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_io_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_threads (int): the number of threads to fire up. If n_threads is `None` or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. Returns: A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multithreading with {n_threads} threads\" ) with futures . ThreadPoolExecutor ( n_threads ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_threads} threads finished\" ) return list ( results ) Static methods execute_function def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ] Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Parameters: Name Type Description Default func Callable the function to call. None func_args list list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. None n_threads int the number of threads to fire up. If n_threads is None or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. None Returns: Type Description None A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. View Source @ staticmethod def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_threads (int): the number of threads to fire up. If n_threads is `None` or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. Returns: A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multithreading with {n_threads} threads\" ) with futures . ThreadPoolExecutor ( n_threads ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_threads} threads finished\" ) return list ( results )","title":"Executors"},{"location":"reference/pyhdtoolkit/utils/executors/#module-pyhdtoolkitutilsexecutors","text":"Module utils.executors Created on 2019.12.09 View Source \"\"\" Module utils.executors ---------------------- Created on 2019.12.09 :author: Felix Soubelet A module providing two classes to execute functions + arguments couples through either a multiprocessing approach, or a multithreading approach. Here are a few tidbits to keep in mind: - There can only be one thread running at any given time in a python process because of the GIL. - Multiprocessing is parallelism. Multithreading is concurrency. - Multiprocessing is for increasing speed. Multithreading is for hiding latency. - Multiprocessing is best for computations. Multithreading is best for IO. - If you have CPU heavy tasks, use multiprocessing with n_process = n_cores and never more. Never. - If you have IO heavy tasks, use multithreading with n_threads = m * n_cores with m a number bigger than 1 that you can tweak on your own. Try many values and choose the one with the best speedup because there isn\u2019t a general rule. For instance the default value of m in ThreadPoolExecutor is set to 5 which I think is quite random. \"\"\" from concurrent import futures from typing import Callable , List from loguru import logger class MultiProcessor : \"\"\" A class to easily wrap a multi-processing context manager call to a function. Reminder: multiprocessing is good for cpu-heavy tasks. Reminder: only picklable objects can be executed and returned. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_cpu_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ] : \"\"\" Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_processes (int): the number of processes to fire up. No more than your number of cores! If n_processes is `None` or not given, ProcessPoolExecutor will default it to the number of processors on the machine. Returns: A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multiprocessing with {n_processes} processes\" ) with futures . ProcessPoolExecutor ( n_processes ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_processes} processes finished\" ) return list ( results ) class MultiThreader : \"\"\" A class to easily wrap a multi-threading context manager call to a function. Reminder: multithreading is good for IO-heavy tasks. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_io_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ] : \"\"\" Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_threads (int): the number of threads to fire up. If n_threads is `None` or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. Returns: A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multithreading with {n_threads} threads\" ) with futures . ThreadPoolExecutor ( n_threads ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_threads} threads finished\" ) return list ( results )","title":"Module pyhdtoolkit.utils.executors"},{"location":"reference/pyhdtoolkit/utils/executors/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/utils/executors/#multiprocessor","text":"class MultiProcessor ( / , * args , ** kwargs ) View Source class MultiProcessor : \"\"\" A class to easily wrap a multi-processing context manager call to a function. Reminder: multiprocessing is good for cpu-heavy tasks. Reminder: only picklable objects can be executed and returned. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_cpu_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_processes (int): the number of processes to fire up. No more than your number of cores! If n_processes is `None` or not given, ProcessPoolExecutor will default it to the number of processors on the machine. Returns: A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multiprocessing with {n_processes} processes\" ) with futures . ProcessPoolExecutor ( n_processes ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_processes} processes finished\" ) return list ( results )","title":"MultiProcessor"},{"location":"reference/pyhdtoolkit/utils/executors/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/executors/#execute_function","text":"def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ] Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Parameters: Name Type Description Default func Callable the function to call. None func_args list list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. None n_processes int the number of processes to fire up. No more than your number of cores! If n_processes is None or not given, ProcessPoolExecutor will default it to the number of processors on the machine. None Returns: Type Description None A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. View Source @ staticmethod def execute_function ( func : Callable , func_args : list , n_processes : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple processes. Do not fire up more processes than you have cores! Never! Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_processes (int): the number of processes to fire up. No more than your number of cores! If n_processes is `None` or not given, ProcessPoolExecutor will default it to the number of processors on the machine. Returns: A list of tuples, each tuple being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multiprocessing with {n_processes} processes\" ) with futures . ProcessPoolExecutor ( n_processes ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_processes} processes finished\" ) return list ( results )","title":"execute_function"},{"location":"reference/pyhdtoolkit/utils/executors/#multithreader","text":"class MultiThreader ( / , * args , ** kwargs ) View Source class MultiThreader : \"\"\" A class to easily wrap a multi-threading context manager call to a function. Reminder: multithreading is good for IO-heavy tasks. Usage: Processor = MultiProcessor() results_one_tuple_per_run = Processor.execute_function( func=your_io_heavy_function, func_args=list_of_args_for_each_call, n_processes=some_int_up_to_you, ) \"\"\" @ staticmethod def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_threads (int): the number of threads to fire up. If n_threads is `None` or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. Returns: A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multithreading with {n_threads} threads\" ) with futures . ThreadPoolExecutor ( n_threads ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_threads} threads finished\" ) return list ( results )","title":"MultiThreader"},{"location":"reference/pyhdtoolkit/utils/executors/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/executors/#execute_function_1","text":"def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ] Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Parameters: Name Type Description Default func Callable the function to call. None func_args list list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. None n_threads int the number of threads to fire up. If n_threads is None or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. None Returns: Type Description None A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. View Source @ staticmethod def execute_function ( func : Callable , func_args : list , n_threads : int ) -> List [ tuple ]: \"\"\" Executes the function with the provided arguments as multiple threads. Remember there is no point of having more threads than the number calls to be executed, the excess threads would be idle and you'd lose the time spent to fire them up. Args: func (Callable): the function to call. func_args (list): list of the different parameters for each call. If your function takes more than one parameter, wrap them up in tuples, e.g.: [(params, run, one), (params, run, two), (params, run, three)]. n_threads (int): the number of threads to fire up. If n_threads is `None` or not given, ThreadPoolExecutor will default it to the number of processors on the machine multiplied by 5, assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for a ProcessPoolExecutor. Returns: A list of tuples, each tuples being the returned value(s) of your function for the given call, for instance [(results, run, one), (results, run, two), (results, run, three)]. \"\"\" logger . debug ( f \"Starting multithreading with {n_threads} threads\" ) with futures . ThreadPoolExecutor ( n_threads ) as ex : results = ex . map ( func , func_args ) logger . debug ( f \"All {n_threads} threads finished\" ) return list ( results )","title":"execute_function"},{"location":"reference/pyhdtoolkit/utils/operations/","text":"Module pyhdtoolkit.utils.operations Module utils.operations Created on 2019.11.12 View Source \"\"\" Module utils.operations ----------------------- Created on 2019.11.12 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection classes with utility functions to perform common / convenient operations on the classic Python structures. \"\"\" import copy import itertools import math import random import re from functools import reduce from typing import Callable , Dict , List , Sequence , Tuple , Union class ListOperations : \"\"\" A class to group some common / useful operations on lists. \"\"\" @staticmethod def all_unique ( sequence : Sequence ) -> bool : \"\"\" Returns True if all the values in a flat list are unique, False otherwise. Args: sequence (Sequence): a sequence of elements. Returns: True if all elements are unique, False otherwise. \"\"\" return len ( sequence ) == len ( set ( sequence )) @staticmethod def average_by ( sequence : Sequence , function : Callable = lambda x : x ) -> float : \"\"\" Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Args: sequence (Sequence): a sequence of elements. function (Callable): function to apply to elements of the sequence. Returns: The average of each element's result through `function`. Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 \"\"\" return float ( sum ( map ( function , sequence ), 0.0 ) / len ( sequence )) @staticmethod def bifurcate ( sequence : Sequence , filters : List [ bool ]) -> Sequence : \"\"\" Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Args: sequence (Sequence): a sequence of elements. filters (List[bool]): a list of booleans. Returns: A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] \"\"\" return [ [ x for i , x in enumerate ( sequence ) if filters [ i ]], [ x for i , x in enumerate ( sequence ) if not filters [ i ]], ] @staticmethod def bifurcate_by ( sequence : Sequence , function : Callable ) -> list : \"\"\" Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of lst, that should return a boolean. Returns: A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] \"\"\" return [[ x for x in sequence if function ( x )], [ x for x in sequence if not function ( x )]] @staticmethod def chunk_list ( sequence : Sequence , size : int ) -> Sequence : \"\"\" Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Args: sequence (Sequence): a sequence of elements. size (int): the size of the wanted sublists. Returns: A list of lists of length `size` (except maybe the last element), with elements from `lst`. Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] \"\"\" if size > len ( sequence ) : return sequence return list ( map ( lambda x : sequence [ x * size : x * size + size ], list ( range ( math . ceil ( len ( sequence ) / size ))),) ) @staticmethod def deep_flatten ( sequence : Sequence ) -> list : \"\"\" Deep flattens a list, no matter the nesting levels. This is a recursive approach. Args: sequence (Sequence): a sequence of elements. Returns: A list with all elements of `lst`, but flattened. Usage: deep_flatten([[\" a \", \" b \"], [1, 2], None, [True, False]]) -> [\" a \", \" b \", 1, 2, None True, False] \"\"\" return ( [ elem for sublist in sequence for elem in ListOperations . deep_flatten ( sublist )] if isinstance ( sequence , list ) else [ sequence ] ) @staticmethod def eval_none ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False \"\"\" return not any ( map ( function , sequence )) @staticmethod def eval_some ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False \"\"\" return any ( map ( function , sequence )) @staticmethod def get_indices ( element , sequence : Sequence ) -> List [ int ] : \"\"\" Return all array indices at which number is located. Args: element: any reference element to check. sequence (Sequence): a sequence containing objects comparable to `elements`. A string can be compared to an int in Python, custom objects probably won't be comparable. Returns: A list of all indices at which `element` is found in `sequence`. Empty list if `element` is not present in `sequence` at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] \"\"\" return [ i for ( y , i ) in zip ( sequence , range ( len ( sequence ))) if element == y ] @staticmethod def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ] : \"\"\" Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of `sequence` that should return a boolean. Returns: A dict with keys \" True \" and \" False \", each having as value a list of all elements of `lst` that were evaluated to respectively `True` or `False` through `function`. Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} \"\"\" groups = {} for key in list ( map ( function , sequence )) : groups [ key ] = [ item for item in sequence if function ( item ) == key ] return groups @staticmethod def has_duplicates ( sequence : Sequence ) -> bool : \"\"\" Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Args: sequence (Sequence): a sequence of elements. Returns: A boolean indicating the presence of duplicates in `lst`. Usage: has_duplicates([1, 2, 1]) -> True \"\"\" return len ( sequence ) ! = len ( set ( sequence )) @staticmethod def sample ( sequence : Sequence ) -> list : \"\"\" Returns a random element from an array. Args: sequence (Sequence): a sequence of elements. Returns: A random element from `lst` in a list (to manage potentially nested lists as input). \"\"\" return sequence [ random . randint ( 0 , len ( sequence ) - 1 )] @staticmethod def sanitize_list ( sequence : Sequence ) -> list : \"\"\" Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Args: sequence (Sequence): a sequence of elements. Returns: The sequence without falsy values. Usage: sanitize_list([1, False, \" a \", 2, \"\", None, 6, 0]) -> [1, \" a \", 2, 6] \"\"\" return list ( filter ( bool , sequence )) @staticmethod def shuffle ( sequence : Sequence ) -> Sequence : \"\"\" Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm (https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) to reorder the elements. Args: sequence (Sequence): a sequence of elements. Returns: The `lst` with original elements at a random index. \"\"\" temp_list = copy . deepcopy ( sequence ) amount_to_shuffle = len ( temp_list ) while amount_to_shuffle > 1 : rand_index = int ( math . floor ( random . random () * amount_to_shuffle )) amount_to_shuffle -= 1 temp_list [ rand_index ], temp_list [ amount_to_shuffle ] = ( temp_list [ amount_to_shuffle ], temp_list [ rand_index ], ) return temp_list @staticmethod def spread ( sequence : Sequence ) -> list : \"\"\" Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in `lst` are iterables! Args: sequence (Sequence): a sequence of elements. Returns: The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] \"\"\" return list ( itertools . chain . from_iterable ( sequence )) @staticmethod def symmetric_difference_by ( lst_1: Sequence , lst_2: Sequence , function : Callable ) -> list : \"\"\" Returns the symmetric difference (https://en.wikipedia.org/wiki/Symmetric_difference) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] \"\"\" _ lst_1 , _ lst_2 = set ( map ( function , lst_1 )), set ( map ( function , lst_2 )) return [ item for item in lst_1 if function ( item ) not in _ lst_2 ] + [ item for item in lst_2 if function ( item ) not in _ lst_1 ] @staticmethod def union_by ( lst_1: Sequence , lst_2: Sequence , function : Callable ) -> list : \"\"\" Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union (https://en.wikipedia.org/wiki/Union_(set_theory)) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] \"\"\" _ lst_1 = set ( map ( function , lst_1 )) return sorted ( list ( set ( lst_1 + [ item for item in lst_2 if function ( item ) not in _ lst_1 ]))) @staticmethod def zipper ( * args , fillvalue = None ) -> list : \"\"\" Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Args: *args: a number (>= 2) of different iterables. fillvalue: value to use in case of length mismatch. Returns: A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\" a \", \" b \", \" c \"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] \"\"\" max_length = max ( len ( lst ) for lst in args ) return [ [ args [ k ][ i ] if i < len ( args [ k ]) else fillvalue for k in range ( len ( args ))] for i in range ( max_length ) ] class MiscellaneousOperations : \"\"\" A class to group some misc. operations that don't pertain to classic structures. \"\"\" @staticmethod def longest_item ( * args ) : \"\"\" Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Args: *args: any number (>= 2) of iterables. Returns: The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) \"\"\" return max ( args , key = len ) @staticmethod def map_values ( obj : dict , function : Callable ) -> dict : \"\"\" Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Args: obj: a dictionary. function (Callable): a callable on values of `obj`. Returns: A new dictionary with the results. Usage: map_values( {\" a \": list(range(5)), \" b \": list(range(10)), \" c \": list(range(15))}, lambda x: len(x) ) -> {\" a \": 5, \" b \": 10, \" c \": 15} \"\"\" ret = {} for key in obj : ret [ key ] = function ( obj [ key ]) return ret class NumberOperations : \"\"\" A class to group some common / useful operations on numbers. \"\"\" @staticmethod def clamp_number ( num : Union [ int , float ], a_val: Union [ int , float ], b_val: Union [ int , float ] ) -> Union [ int , float ] : \"\"\" Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Args: num (Union[int, float]): a number (float / int) a_val (Union[int, float]): a number (float / int) b_val (Union[int, float]): a number (float / int) Returns: A number (float / int), being the nearest to `num` in the range [`a_val`, `b_val`]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 \"\"\" return max ( min ( num , max ( a_val , b_val )), min ( a_val , b_val )) @staticmethod def degrees_to_radians ( deg_value: Union [ int , float ], decompose : bool = False ) -> Union [ Tuple [ float , str , str ], int , float ] : \"\"\" Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Args: deg_value (Union[int, float]): angle value in degrees. decompose (bool): boolean option to return a more verbose result. Defaults to False. Returns: The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \" pi \", \" rad \") \"\"\" if decompose : return deg_value / 180 , \"pi\" , \"rad\" return ( deg_value * math . pi ) / 180.0 @staticmethod def greatest_common_divisor ( numbers_list: Sequence ) -> Union [ int , float ] : \"\"\" Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in `numbers_list`. Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 \"\"\" return reduce ( math . gcd , numbers_list ) @staticmethod def is_divisible_by ( dividend : Union [ int , float ], divisor : Union [ int , float ]) -> bool : \"\"\" Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Args: dividend (Union[int, float]): a number. divisor (Union[int, float]): a number. Returns: A boolean stating if `dividend` can be divided by `divisor`. \"\"\" return dividend % divisor == 0 @staticmethod def least_common_multiple ( * args ) -> int : \"\"\" Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 \"\"\" numbers = list ( ListOperations . spread ( list ( args ))) def _ lcm ( number1 , number2 ) : \"\"\"A least common multiple method for two numbers only\"\"\" return int ( number1 * number2 / math . gcd ( number1 , number2 )) return reduce ( lambda x , y : _ lcm ( x , y ), numbers ) @staticmethod def radians_to_degrees ( rad_value: Union [ int , float ]) -> Union [ int , float ] : \"\"\" Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Args: rad_value (Union[int, float]): angle value in degrees. Returns: The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 \"\"\" return ( rad_value * 180.0 ) / math . pi class StringOperations : \"\"\" A class to group some common / useful operations on strings. \"\"\" @staticmethod def camel_case ( text : str ) -> str : \"\"\" Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Args: text (str): a string. Returns: The same string best adapted to camel_case. Usage: camel_case(\" a_snake_case_name \") -> \" aSnakeCaseName \" camel_case(\" A Title Case Name \") -> \" aTitleCaseName \" \"\"\" text = re . sub ( r \"(\\s|_|-)+\" , \" \" , text ). title (). replace ( \" \" , \"\" ) return text [ 0 ]. lower () + text [ 1 : ] @staticmethod def capitalize ( text : str , lower_rest: bool = False ) -> str : \"\"\" Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Args: text (str): a string. lower_rest (bool): boolean option to lower all elements starting from the second. Returns: The `string`, capitalized. Usage: capitalize(\" astringtocapitalize \") -> \" Astringtocapitalize \" capitalize(\" astRIngTocApItalizE \", lower_rest=True) -> \" Astringtocapitalize \" \"\"\" return text [ : 1 ]. upper () + ( text [ 1 : ]. lower () if lower_rest else text [ 1 : ]) @staticmethod def is_anagram ( str_1: str , str_2: str ) -> bool : \"\"\" Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Args: str_1 (str): a string. str_2 (str): a string. Returns: A boolean stating whether `str_1` is an anagram of `str_2` or not. Usage: is_anagram(\" Tom Marvolo Riddle \", \" I am Lord Voldemort \") -> True is_anagram(\" A first string \", \" Definitely not an anagram \") -> False \"\"\" _ str1 , _ str2 = ( str_1 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), str_2 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), ) return sorted ( _ str1 . lower ()) == sorted ( _ str2 . lower ()) @staticmethod def is_palindrome ( text : str ) -> bool : \"\"\" Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Args: text (str): a string. Returns: A boolean stating whether `string` is a palindrome or not. Usage: is_palindrome(\" racecar \") -> True is_palindrome(\" definitelynot \") -> False \"\"\" s_reverse = re . sub ( r \" [ \\ W_ ] \", \"\", text.lower()) return s_reverse == s_reverse[::-1] @staticmethod def kebab_case(text: str) -> str: \"\"\" Converts a string to kebab - case . Break the string into words and combine them adding - as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to kebab_case . Usage : kebab_case ( \"camel Case\" ) -> \"camel-case\" kebab_case ( \"snake_case\" ) -> \"snake-case\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" - \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \", lambda mo: mo.group(0).lower(), text, ), ) @staticmethod def snake_case(text: str) -> str: \"\"\" Converts a string to snake_case . Break the string into words and combine them adding _ as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to snake_case . Usage : snake_case ( \"A bunch of words\" ) -> \"a_bunch_of_words\" snake_case ( \"camelCase\" ) -> \"camelcase\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" _ \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \" , lambda mo : mo . group ( 0 ). lower (), text , ), ) Classes ListOperations class ListOperations ( / , * args , ** kwargs ) View Source class ListOperations : \"\"\" A class to group some common / useful operations on lists. \"\"\" @ staticmethod def all_unique ( sequence : Sequence ) -> bool : \"\"\" Returns True if all the values in a flat list are unique, False otherwise. Args: sequence (Sequence): a sequence of elements. Returns: True if all elements are unique, False otherwise. \"\"\" return len ( sequence ) == len ( set ( sequence )) @ staticmethod def average_by ( sequence : Sequence , function : Callable = lambda x : x ) -> float : \"\"\" Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Args: sequence (Sequence): a sequence of elements. function (Callable): function to apply to elements of the sequence. Returns: The average of each element's result through `function`. Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 \"\"\" return float ( sum ( map ( function , sequence ), 0.0 ) / len ( sequence )) @ staticmethod def bifurcate ( sequence : Sequence , filters : List [ bool ]) -> Sequence : \"\"\" Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Args: sequence (Sequence): a sequence of elements. filters (List[bool]): a list of booleans. Returns: A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] \"\"\" return [ [ x for i , x in enumerate ( sequence ) if filters [ i ]], [ x for i , x in enumerate ( sequence ) if not filters [ i ]], ] @ staticmethod def bifurcate_by ( sequence : Sequence , function : Callable ) -> list : \"\"\" Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of lst, that should return a boolean. Returns: A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] \"\"\" return [[ x for x in sequence if function ( x )], [ x for x in sequence if not function ( x )]] @ staticmethod def chunk_list ( sequence : Sequence , size : int ) -> Sequence : \"\"\" Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Args: sequence (Sequence): a sequence of elements. size (int): the size of the wanted sublists. Returns: A list of lists of length `size` (except maybe the last element), with elements from `lst`. Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] \"\"\" if size > len ( sequence ): return sequence return list ( map ( lambda x : sequence [ x * size : x * size + size ], list ( range ( math . ceil ( len ( sequence ) / size ))),) ) @ staticmethod def deep_flatten ( sequence : Sequence ) -> list : \"\"\" Deep flattens a list, no matter the nesting levels. This is a recursive approach. Args: sequence (Sequence): a sequence of elements. Returns: A list with all elements of `lst`, but flattened. Usage: deep_flatten([[\"a\", \"b\"], [1, 2], None, [True, False]]) -> [\"a\", \"b\", 1, 2, None True, False] \"\"\" return ( [ elem for sublist in sequence for elem in ListOperations . deep_flatten ( sublist )] if isinstance ( sequence , list ) else [ sequence ] ) @ staticmethod def eval_none ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False \"\"\" return not any ( map ( function , sequence )) @ staticmethod def eval_some ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False \"\"\" return any ( map ( function , sequence )) @ staticmethod def get_indices ( element , sequence : Sequence ) -> List [ int ]: \"\"\" Return all array indices at which number is located. Args: element: any reference element to check. sequence (Sequence): a sequence containing objects comparable to `elements`. A string can be compared to an int in Python, custom objects probably won't be comparable. Returns: A list of all indices at which `element` is found in `sequence`. Empty list if `element` is not present in `sequence` at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] \"\"\" return [ i for ( y , i ) in zip ( sequence , range ( len ( sequence ))) if element == y ] @ staticmethod def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ]: \"\"\" Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of `sequence` that should return a boolean. Returns: A dict with keys \"True\" and \"False\", each having as value a list of all elements of `lst` that were evaluated to respectively `True` or `False` through `function`. Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} \"\"\" groups = {} for key in list ( map ( function , sequence )): groups [ key ] = [ item for item in sequence if function ( item ) == key ] return groups @ staticmethod def has_duplicates ( sequence : Sequence ) -> bool : \"\"\" Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Args: sequence (Sequence): a sequence of elements. Returns: A boolean indicating the presence of duplicates in `lst`. Usage: has_duplicates([1, 2, 1]) -> True \"\"\" return len ( sequence ) != len ( set ( sequence )) @ staticmethod def sample ( sequence : Sequence ) -> list : \"\"\" Returns a random element from an array. Args: sequence (Sequence): a sequence of elements. Returns: A random element from `lst` in a list (to manage potentially nested lists as input). \"\"\" return sequence [ random . randint ( 0 , len ( sequence ) - 1 )] @ staticmethod def sanitize_list ( sequence : Sequence ) -> list : \"\"\" Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Args: sequence (Sequence): a sequence of elements. Returns: The sequence without falsy values. Usage: sanitize_list([1, False, \"a\", 2, \"\", None, 6, 0]) -> [1, \"a\", 2, 6] \"\"\" return list ( filter ( bool , sequence )) @ staticmethod def shuffle ( sequence : Sequence ) -> Sequence : \"\"\" Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm (https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) to reorder the elements. Args: sequence (Sequence): a sequence of elements. Returns: The `lst` with original elements at a random index. \"\"\" temp_list = copy . deepcopy ( sequence ) amount_to_shuffle = len ( temp_list ) while amount_to_shuffle > 1 : rand_index = int ( math . floor ( random . random () * amount_to_shuffle )) amount_to_shuffle -= 1 temp_list [ rand_index ], temp_list [ amount_to_shuffle ] = ( temp_list [ amount_to_shuffle ], temp_list [ rand_index ], ) return temp_list @ staticmethod def spread ( sequence : Sequence ) -> list : \"\"\" Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in `lst` are iterables! Args: sequence (Sequence): a sequence of elements. Returns: The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] \"\"\" return list ( itertools . chain . from_iterable ( sequence )) @ staticmethod def symmetric_difference_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \"\"\" Returns the symmetric difference (https://en.wikipedia.org/wiki/Symmetric_difference) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] \"\"\" _lst_1 , _lst_2 = set ( map ( function , lst_1 )), set ( map ( function , lst_2 )) return [ item for item in lst_1 if function ( item ) not in _lst_2 ] + [ item for item in lst_2 if function ( item ) not in _lst_1 ] @ staticmethod def union_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \"\"\" Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union (https://en.wikipedia.org/wiki/Union_(set_theory)) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] \"\"\" _lst_1 = set ( map ( function , lst_1 )) return sorted ( list ( set ( lst_1 + [ item for item in lst_2 if function ( item ) not in _lst_1 ]))) @ staticmethod def zipper ( * args , fillvalue = None ) -> list : \"\"\" Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Args: *args: a number (>= 2) of different iterables. fillvalue: value to use in case of length mismatch. Returns: A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\"a\", \"b\", \"c\"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] \"\"\" max_length = max ( len ( lst ) for lst in args ) return [ [ args [ k ][ i ] if i < len ( args [ k ]) else fillvalue for k in range ( len ( args ))] for i in range ( max_length ) ] Static methods all_unique def all_unique ( sequence : Sequence ) -> bool Returns True if all the values in a flat list are unique, False otherwise. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None True if all elements are unique, False otherwise. View Source @staticmethod def all_unique ( sequence : Sequence ) -> bool : \"\"\" Returns True if all the values in a flat list are unique, False otherwise. Args: sequence (Sequence): a sequence of elements. Returns: True if all elements are unique, False otherwise. \"\"\" return len ( sequence ) == len ( set ( sequence )) average_by def average_by ( sequence : Sequence , function : Callable = < function ListOperations .< lambda > at 0x7fdeb06e45f0 > ) -> float Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable function to apply to elements of the sequence. None Returns: Type Description None The average of each element's result through function . Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 | View Source @staticmethod def average_by ( sequence : Sequence , function : Callable = lambda x : x ) -> float : \" \"\" Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Args: sequence (Sequence): a sequence of elements. function (Callable): function to apply to elements of the sequence. Returns: The average of each element's result through `function`. Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 \"\" \" return float ( sum ( map ( function , sequence ), 0.0 ) / len ( sequence )) bifurcate def bifurcate ( sequence : Sequence , filters : List [ bool ] ) -> Sequence Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None filters List[bool] a list of booleans. None Returns: Type Description None A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] | View Source @staticmethod def bifurcate ( sequence : Sequence , filters : List [ bool ] ) -> Sequence : \"\"\" Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Args: sequence (Sequence): a sequence of elements. filters (List[bool]): a list of booleans. Returns: A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] \"\"\" return [ [x for i, x in enumerate(sequence) if filters[i ] ] , [ x for i, x in enumerate(sequence) if not filters[i ] ] , ] bifurcate_by def bifurcate_by ( sequence : Sequence , function : Callable ) -> list Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on the elements of lst, that should return a boolean. None Returns: Type Description None A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] | View Source @staticmethod def bifurcate_by ( sequence : Sequence , function : Callable ) -> list : \"\"\" Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of lst, that should return a boolean. Returns: A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] \"\"\" return [ [x for x in sequence if function(x) ] , [ x for x in sequence if not function(x) ] ] chunk_list def chunk_list ( sequence : Sequence , size : int ) -> Sequence Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None size int the size of the wanted sublists. None Returns: Type Description None A list of lists of length size (except maybe the last element), with elements from lst . Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] | View Source @staticmethod def chunk_list ( sequence : Sequence , size : int ) -> Sequence : \" \"\" Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Args: sequence (Sequence): a sequence of elements. size (int): the size of the wanted sublists. Returns: A list of lists of length `size` (except maybe the last element), with elements from `lst`. Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] \"\" \" if size > len ( sequence ) : return sequence return list ( map ( lambda x : sequence [ x * size : x * size + size ] , list ( range ( math . ceil ( len ( sequence ) / size ))),) ) deep_flatten def deep_flatten ( sequence : Sequence ) -> list Deep flattens a list, no matter the nesting levels. This is a recursive approach. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None A list with all elements of lst , but flattened. Usage: deep_flatten([[\"a\", \"b\"], [1, 2], None, [True, False]]) -> [\"a\", \"b\", 1, 2, None True, False] | View Source @staticmethod def deep_flatten ( sequence : Sequence ) -> list : \"\"\" Deep flattens a list, no matter the nesting levels. This is a recursive approach. Args: sequence (Sequence): a sequence of elements. Returns: A list with all elements of `lst`, but flattened. Usage: deep_flatten([[\" a \", \" b \"], [1, 2], None, [True, False]]) -> [\" a \", \" b \", 1, 2, None True, False] \"\"\" return ( [ elem for sublist in sequence for elem in ListOperations.deep_flatten(sublist) ] if isinstance ( sequence , list ) else [ sequence ] ) eval_none def eval_none ( sequence : Sequence , function : Callable = < function ListOperations .< lambda > at 0x7fdeb06e4950 > ) -> bool Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on elements of sequence that should return a boolean. None Returns: Type Description None A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False | View Source @staticmethod def eval_none ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \" \"\" Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False \"\" \" return not any ( map ( function , sequence )) eval_some def eval_some ( sequence : Sequence , function : Callable = < function ListOperations .< lambda > at 0x7fdeb06e4a70 > ) -> bool Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on elements of sequence that should return a boolean. None Returns: Type Description None A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False | View Source @staticmethod def eval_some ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \" \"\" Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False \"\" \" return any ( map ( function , sequence )) get_indices def get_indices ( element , sequence : Sequence ) -> List [ int ] Return all array indices at which number is located. Parameters: Name Type Description Default element None any reference element to check. None sequence Sequence a sequence containing objects comparable to elements . A string can be compared to an int in Python, custom objects probably won't be comparable. None Returns: Type Description None A list of all indices at which element is found in sequence . Empty list if element is not present in sequence at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] | View Source @staticmethod def get_indices ( element , sequence : Sequence ) -> List [ int ] : \" \"\" Return all array indices at which number is located. Args: element: any reference element to check. sequence (Sequence): a sequence containing objects comparable to `elements`. A string can be compared to an int in Python, custom objects probably won't be comparable. Returns: A list of all indices at which `element` is found in `sequence`. Empty list if `element` is not present in `sequence` at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] \"\" \" return [ i for ( y , i ) in zip ( sequence , range ( len ( sequence ))) if element == y ] group_by def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ] Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on the elements of sequence that should return a boolean. None Returns: Type Description None A dict with keys \"True\" and \"False\", each having as value a list of all elements of lst that were evaluated to respectively True or False through function . Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} | View Source @staticmethod def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ] : \" \"\" Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of `sequence` that should return a boolean. Returns: A dict with keys \" True \" and \" False \", each having as value a list of all elements of `lst` that were evaluated to respectively `True` or `False` through `function`. Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} \"\" \" groups = {} for key in list ( map ( function , sequence )) : groups [ key ] = [ item for item in sequence if function ( item ) == key ] return groups has_duplicates def has_duplicates ( sequence : Sequence ) -> bool Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None A boolean indicating the presence of duplicates in lst . Usage: has_duplicates([1, 2, 1]) -> True | View Source @staticmethod def has_duplicates ( sequence : Sequence ) -> bool : \" \"\" Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Args: sequence (Sequence): a sequence of elements. Returns: A boolean indicating the presence of duplicates in `lst`. Usage: has_duplicates([1, 2, 1]) -> True \"\" \" return len ( sequence ) != len ( set ( sequence )) sample def sample ( sequence : Sequence ) -> list Returns a random element from an array. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None A random element from lst in a list (to manage potentially nested lists as input). View Source @staticmethod def sample ( sequence : Sequence ) -> list : \" \"\" Returns a random element from an array. Args: sequence (Sequence): a sequence of elements. Returns: A random element from `lst` in a list (to manage potentially nested lists as input). \"\" \" return sequence [ random . randint ( 0 , len ( sequence ) - 1 ) ] sanitize_list def sanitize_list ( sequence : Sequence ) -> list Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None The sequence without falsy values. Usage: sanitize_list([1, False, \"a\", 2, \"\", None, 6, 0]) -> [1, \"a\", 2, 6] | View Source @staticmethod def sanitize_list ( sequence : Sequence ) -> list : \"\"\" Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Args: sequence (Sequence): a sequence of elements. Returns: The sequence without falsy values. Usage: sanitize_list([1, False, \" a \", 2, \"\", None, 6, 0]) -> [1, \" a \", 2, 6] \"\"\" return list ( filter ( bool , sequence )) shuffle def shuffle ( sequence : Sequence ) -> Sequence Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm ( https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle ) to reorder the elements. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None The lst with original elements at a random index. View Source @staticmethod def shuffle ( sequence : Sequence ) -> Sequence : \"\"\" Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm (https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) to reorder the elements. Args: sequence (Sequence): a sequence of elements. Returns: The `lst` with original elements at a random index. \"\"\" temp_list = copy . deepcopy ( sequence ) amount_to_shuffle = len ( temp_list ) while amount_to_shuffle > 1 : rand_index = int ( math . floor ( random . random () * amount_to_shuffle )) amount_to_shuffle -= 1 temp_list [ rand_index ] , temp_list [ amount_to_shuffle ] = ( temp_list [ amount_to_shuffle ] , temp_list [ rand_index ] , ) return temp_list spread def spread ( sequence : Sequence ) -> list Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in lst are iterables! Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] | View Source @staticmethod def spread ( sequence : Sequence ) -> list : \" \"\" Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in `lst` are iterables! Args: sequence (Sequence): a sequence of elements. Returns: The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] \"\" \" return list ( itertools . chain . from_iterable ( sequence )) symmetric_difference_by def symmetric_difference_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list Returns the symmetric difference ( https://en.wikipedia.org/wiki/Symmetric_difference ) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Parameters: Name Type Description Default lst_1 Sequence a sequence of elements. None lst_2 Sequence a sequence of elements. None function Callable a callable on elements of lst_1 and lst_2 . None Returns: Type Description None A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] | View Source @staticmethod def symmetric_difference_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \" \"\" Returns the symmetric difference (https://en.wikipedia.org/wiki/Symmetric_difference) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] \"\" \" _lst_1 , _lst_2 = set ( map ( function , lst_1 )), set ( map ( function , lst_2 )) return [ item for item in lst_1 if function ( item ) not in _lst_2 ] + [ item for item in lst_2 if function ( item ) not in _lst_1 ] union_by def union_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union ( https://en.wikipedia.org/wiki/Union_(set_theory )) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Parameters: Name Type Description Default lst_1 Sequence a sequence of elements. None lst_2 Sequence a sequence of elements. None function Callable a callable on elements of lst_1 and lst_2 . None Returns: Type Description None A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] | View Source @staticmethod def union_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \" \"\" Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union (https://en.wikipedia.org/wiki/Union_(set_theory)) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] \"\" \" _lst_1 = set ( map ( function , lst_1 )) return sorted ( list ( set ( lst_1 + [ item for item in lst_2 if function ( item ) not in _lst_1 ] ))) zipper def zipper ( * args , fillvalue = None ) -> list Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Parameters: Name Type Description Default *args None a number (>= 2) of different iterables. None fillvalue None value to use in case of length mismatch. None Returns: Type Description None A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\"a\", \"b\", \"c\"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] | View Source @staticmethod def zipper ( * args , fillvalue = None ) -> list : \"\"\" Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Args: *args: a number (>= 2) of different iterables. fillvalue: value to use in case of length mismatch. Returns: A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\" a \", \" b \", \" c \"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] \"\"\" max_length = max ( len ( lst ) for lst in args ) return [ [args[k ][ i ] if i < len ( args [ k ] ) else fillvalue for k in range ( len ( args )) ] for i in range ( max_length ) ] MiscellaneousOperations class MiscellaneousOperations ( / , * args , ** kwargs ) View Source class MiscellaneousOperations : \"\"\" A class to group some misc. operations that don't pertain to classic structures. \"\"\" @staticmethod def longest_item ( * args ) : \"\"\" Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Args: *args: any number (>= 2) of iterables. Returns: The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) \"\"\" return max ( args , key = len ) @staticmethod def map_values ( obj : dict , function : Callable ) -> dict : \"\"\" Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Args: obj: a dictionary. function (Callable): a callable on values of `obj`. Returns: A new dictionary with the results. Usage: map_values( {\" a \": list(range(5)), \" b \": list(range(10)), \" c \": list(range(15))}, lambda x: len(x) ) -> {\" a \": 5, \" b \": 10, \" c \": 15} \"\"\" ret = {} for key in obj : ret [ key ] = function ( obj [ key ] ) return ret Static methods longest_item def longest_item ( * args ) Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Parameters: Name Type Description Default *args None any number (>= 2) of iterables. None Returns: Type Description None The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) | View Source @staticmethod def longest_item ( * args ) : \"\"\" Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Args: *args: any number (>= 2) of iterables. Returns: The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) \"\"\" return max ( args , key = len ) map_values def map_values ( obj : dict , function : Callable ) -> dict Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Parameters: Name Type Description Default obj None a dictionary. None function Callable a callable on values of obj . None Returns: Type Description None A new dictionary with the results. Usage: map_values( {\"a\": list(range(5)), \"b\": list(range(10)), \"c\": list(range(15))}, lambda x: len(x) ) -> {\"a\": 5, \"b\": 10, \"c\": 15} | View Source @staticmethod def map_values ( obj : dict , function : Callable ) -> dict : \"\"\" Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Args: obj: a dictionary. function (Callable): a callable on values of `obj`. Returns: A new dictionary with the results. Usage: map_values( {\" a \": list(range(5)), \" b \": list(range(10)), \" c \": list(range(15))}, lambda x: len(x) ) -> {\" a \": 5, \" b \": 10, \" c \": 15} \"\"\" ret = {} for key in obj : ret [ key ] = function ( obj [ key ] ) return ret NumberOperations class NumberOperations ( / , * args , ** kwargs ) View Source class NumberOperations : \" \"\" A class to group some common / useful operations on numbers. \"\" \" @staticmethod def clamp_number ( num : Union [ int , float ] , a_val : Union [ int , float ] , b_val : Union [ int , float ] ) -> Union [ int , float ] : \" \"\" Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Args: num (Union[int, float]): a number (float / int) a_val (Union[int, float]): a number (float / int) b_val (Union[int, float]): a number (float / int) Returns: A number (float / int), being the nearest to `num` in the range [`a_val`, `b_val`]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 \"\" \" return max ( min ( num , max ( a_val , b_val )), min ( a_val , b_val )) @staticmethod def degrees_to_radians ( deg_value : Union [ int , float ] , decompose : bool = False ) -> Union [ Tuple [ float , str , str ] , int , float ] : \" \"\" Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Args: deg_value (Union[int, float]): angle value in degrees. decompose (bool): boolean option to return a more verbose result. Defaults to False. Returns: The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \" pi \", \" rad \") \"\" \" if decompose : return deg_value / 180 , \"pi\" , \"rad\" return ( deg_value * math . pi ) / 180.0 @staticmethod def greatest_common_divisor ( numbers_list : Sequence ) -> Union [ int , float ] : \" \"\" Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in `numbers_list`. Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 \"\" \" return reduce ( math . gcd , numbers_list ) @staticmethod def is_divisible_by ( dividend : Union [ int , float ] , divisor : Union [ int , float ] ) -> bool : \" \"\" Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Args: dividend (Union[int, float]): a number. divisor (Union[int, float]): a number. Returns: A boolean stating if `dividend` can be divided by `divisor`. \"\" \" return dividend % divisor == 0 @staticmethod def least_common_multiple ( * args ) -> int : \" \"\" Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 \"\" \" numbers = list ( ListOperations . spread ( list ( args ))) def _lcm ( number1 , number2 ) : \" \"\" A least common multiple method for two numbers only \"\" \" return int ( number1 * number2 / math . gcd ( number1 , number2 )) return reduce ( lambda x , y : _lcm ( x , y ), numbers ) @staticmethod def radians_to_degrees ( rad_value : Union [ int , float ] ) -> Union [ int , float ] : \" \"\" Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Args: rad_value (Union[int, float]): angle value in degrees. Returns: The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 \"\" \" return ( rad_value * 180.0 ) / math . pi Static methods clamp_number def clamp_number ( num : Union [ int , float ], a_val : Union [ int , float ], b_val : Union [ int , float ] ) -> Union [ int , float ] Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Parameters: Name Type Description Default num Union[int, float] a number (float / int) None a_val Union[int, float] a number (float / int) None b_val Union[int, float] a number (float / int) None Returns: Type Description None A number (float / int), being the nearest to num in the range [ a_val , b_val ]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 | View Source @staticmethod def clamp_number ( num : Union [ int , float ] , a_val : Union [ int , float ] , b_val : Union [ int , float ] ) -> Union [ int , float ] : \" \"\" Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Args: num (Union[int, float]): a number (float / int) a_val (Union[int, float]): a number (float / int) b_val (Union[int, float]): a number (float / int) Returns: A number (float / int), being the nearest to `num` in the range [`a_val`, `b_val`]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 \"\" \" return max ( min ( num , max ( a_val , b_val )), min ( a_val , b_val )) degrees_to_radians def degrees_to_radians ( deg_value : Union [ int , float ], decompose : bool = False ) -> Union [ Tuple [ float , str , str ], int , float ] Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Parameters: Name Type Description Default deg_value Union[int, float] angle value in degrees. None decompose bool boolean option to return a more verbose result. Defaults to False. False Returns: Type Description None The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \"pi\", \"rad\") | View Source @staticmethod def degrees_to_radians ( deg_value : Union [ int, float ] , decompose : bool = False ) -> Union [ Tuple[float, str, str ] , int , float ]: \"\"\" Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Args: deg_value (Union[int, float]): angle value in degrees. decompose (bool): boolean option to return a more verbose result. Defaults to False. Returns: The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \" pi \", \" rad \") \"\"\" if decompose : return deg_value / 180 , \"pi\" , \"rad\" return ( deg_value * math . pi ) / 180.0 greatest_common_divisor def greatest_common_divisor ( numbers_list : Sequence ) -> Union [ int , float ] Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in numbers_list . Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 View Source @staticmethod def greatest_common_divisor ( numbers_list : Sequence ) -> Union [ int , float ] : \" \"\" Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in `numbers_list`. Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 \"\" \" return reduce ( math . gcd , numbers_list ) is_divisible_by def is_divisible_by ( dividend : Union [ int , float ], divisor : Union [ int , float ] ) -> bool Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Parameters: Name Type Description Default dividend Union[int, float] a number. None divisor Union[int, float] a number. None Returns: Type Description None A boolean stating if dividend can be divided by divisor . View Source @staticmethod def is_divisible_by ( dividend : Union [ int , float ] , divisor : Union [ int , float ] ) -> bool : \" \"\" Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Args: dividend (Union[int, float]): a number. divisor (Union[int, float]): a number. Returns: A boolean stating if `dividend` can be divided by `divisor`. \"\" \" return dividend % divisor == 0 least_common_multiple def least_common_multiple ( * args ) -> int Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 View Source @staticmethod def least_common_multiple ( * args ) -> int : \"\"\" Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 \"\"\" numbers = list ( ListOperations . spread ( list ( args ))) def _lcm ( number1 , number2 ) : \"\"\"A least common multiple method for two numbers only\"\"\" return int ( number1 * number2 / math . gcd ( number1 , number2 )) return reduce ( lambda x , y : _lcm ( x , y ), numbers ) radians_to_degrees def radians_to_degrees ( rad_value : Union [ int , float ] ) -> Union [ int , float ] Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Parameters: Name Type Description Default rad_value Union[int, float] angle value in degrees. None Returns: Type Description None The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 | View Source @staticmethod def radians_to_degrees ( rad_value : Union [ int, float ] ) -> Union [ int, float ] : \"\"\" Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Args: rad_value (Union[int, float]): angle value in degrees. Returns: The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 \"\"\" return ( rad_value * 180.0 ) / math . pi StringOperations class StringOperations ( / , * args , ** kwargs ) View Source class StringOperations : \"\"\" A class to group some common / useful operations on strings. \"\"\" @staticmethod def camel_case ( text : str ) -> str : \"\"\" Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Args: text (str): a string. Returns: The same string best adapted to camel_case. Usage: camel_case(\" a_snake_case_name \") -> \" aSnakeCaseName \" camel_case(\" A Title Case Name \") -> \" aTitleCaseName \" \"\"\" text = re . sub ( r \"(\\s|_|-)+\" , \" \" , text ). title (). replace ( \" \" , \"\" ) return text [ 0 ]. lower () + text [ 1 : ] @staticmethod def capitalize ( text : str , lower_rest: bool = False ) -> str : \"\"\" Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Args: text (str): a string. lower_rest (bool): boolean option to lower all elements starting from the second. Returns: The `string`, capitalized. Usage: capitalize(\" astringtocapitalize \") -> \" Astringtocapitalize \" capitalize(\" astRIngTocApItalizE \", lower_rest=True) -> \" Astringtocapitalize \" \"\"\" return text [ : 1 ]. upper () + ( text [ 1 : ]. lower () if lower_rest else text [ 1 : ]) @staticmethod def is_anagram ( str_1: str , str_2: str ) -> bool : \"\"\" Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Args: str_1 (str): a string. str_2 (str): a string. Returns: A boolean stating whether `str_1` is an anagram of `str_2` or not. Usage: is_anagram(\" Tom Marvolo Riddle \", \" I am Lord Voldemort \") -> True is_anagram(\" A first string \", \" Definitely not an anagram \") -> False \"\"\" _ str1 , _ str2 = ( str_1 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), str_2 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), ) return sorted ( _ str1 . lower ()) == sorted ( _ str2 . lower ()) @staticmethod def is_palindrome ( text : str ) -> bool : \"\"\" Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Args: text (str): a string. Returns: A boolean stating whether `string` is a palindrome or not. Usage: is_palindrome(\" racecar \") -> True is_palindrome(\" definitelynot \") -> False \"\"\" s_reverse = re . sub ( r \" [ \\ W_ ] \", \"\", text.lower()) return s_reverse == s_reverse[::-1] @staticmethod def kebab_case(text: str) -> str: \"\"\" Converts a string to kebab - case . Break the string into words and combine them adding - as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to kebab_case . Usage : kebab_case ( \"camel Case\" ) -> \"camel-case\" kebab_case ( \"snake_case\" ) -> \"snake-case\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" - \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \", lambda mo: mo.group(0).lower(), text, ), ) @staticmethod def snake_case(text: str) -> str: \"\"\" Converts a string to snake_case . Break the string into words and combine them adding _ as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to snake_case . Usage : snake_case ( \"A bunch of words\" ) -> \"a_bunch_of_words\" snake_case ( \"camelCase\" ) -> \"camelcase\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" _ \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \" , lambda mo : mo . group ( 0 ). lower (), text , ), ) Static methods camel_case def camel_case ( text : str ) -> str Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Parameters: Name Type Description Default text str a string. None Returns: Type Description None The same string best adapted to camel_case. Usage: camel_case(\"a_snake_case_name\") -> \"aSnakeCaseName\" camel_case(\"A Title Case Name\") -> \"aTitleCaseName\" | View Source @staticmethod def camel_case ( text : str ) -> str : \"\"\" Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Args: text (str): a string. Returns: The same string best adapted to camel_case. Usage: camel_case(\" a_snake_case_name \") -> \" aSnakeCaseName \" camel_case(\" A Title Case Name \") -> \" aTitleCaseName \" \"\"\" text = re . sub ( r \"(\\s|_|-)+\" , \" \" , text ). title (). replace ( \" \" , \"\" ) return text [ 0 ] . lower () + text [ 1: ] capitalize def capitalize ( text : str , lower_rest : bool = False ) -> str Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Parameters: Name Type Description Default text str a string. None lower_rest bool boolean option to lower all elements starting from the second. None Returns: Type Description None The string , capitalized. Usage: capitalize(\"astringtocapitalize\") -> \"Astringtocapitalize\" capitalize(\"astRIngTocApItalizE\", lower_rest=True) -> \"Astringtocapitalize\" | View Source @staticmethod def capitalize ( text : str , lower_rest : bool = False ) -> str : \" \"\" Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Args: text (str): a string. lower_rest (bool): boolean option to lower all elements starting from the second. Returns: The `string`, capitalized. Usage: capitalize(\" astringtocapitalize \") -> \" Astringtocapitalize \" capitalize(\" astRIngTocApItalizE \", lower_rest=True) -> \" Astringtocapitalize \" \"\" \" return text [ : 1 ] . upper () + ( text [ 1 : ] . lower () if lower_rest else text [ 1 : ] ) is_anagram def is_anagram ( str_1 : str , str_2 : str ) -> bool Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Parameters: Name Type Description Default str_1 str a string. None str_2 str a string. None Returns: Type Description None A boolean stating whether str_1 is an anagram of str_2 or not. Usage: is_anagram(\"Tom Marvolo Riddle\", \"I am Lord Voldemort\") -> True is_anagram(\"A first string\", \"Definitely not an anagram\") -> False | View Source @staticmethod def is_anagram ( str_1 : str , str_2 : str ) -> bool : \" \"\" Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Args: str_1 (str): a string. str_2 (str): a string. Returns: A boolean stating whether `str_1` is an anagram of `str_2` or not. Usage: is_anagram(\" Tom Marvolo Riddle \", \" I am Lord Voldemort \") -> True is_anagram(\" A first string \", \" Definitely not an anagram \") -> False \"\" \" _str1 , _str2 = ( str_1 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), str_2 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), ) return sorted ( _str1 . lower ()) == sorted ( _str2 . lower ()) is_palindrome def is_palindrome ( text : str ) -> bool Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Parameters: Name Type Description Default text str a string. None Returns: Type Description None A boolean stating whether string is a palindrome or not. Usage: is_palindrome(\"racecar\") -> True is_palindrome(\"definitelynot\") -> False | View Source @staticmethod def is_palindrome ( text : str ) -> bool : \"\"\" Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Args: text (str): a string. Returns: A boolean stating whether `string` is a palindrome or not. Usage: is_palindrome(\" racecar \") -> True is_palindrome(\" definitelynot \") -> False \"\"\" s_reverse = re . sub ( r \" [ \\ W_ ] \", \" \" , text . lower ()) return s_reverse == s_reverse [ ::- 1 ] kebab_case def kebab_case ( text : str ) -> str Converts a string to kebab-case. Break the string into words and combine them adding - as a separator, using a regexp. Parameters: Name Type Description Default text str a string. None Returns: Type Description None The same string best adapted to kebab_case. Usage: kebab_case(\"camel Case\") -> \"camel-case\" kebab_case(\"snake_case\") -> \"snake-case\" | View Source @staticmethod def kebab_case ( text : str ) -> str : \"\"\" Converts a string to kebab-case. Break the string into words and combine them adding - as a separator, using a regexp. Args: text (str): a string. Returns: The same string best adapted to kebab_case. Usage: kebab_case(\" camel Case \") -> \" camel - case \" kebab_case(\" snake_case \") -> \" snake - case \" \"\"\" return re . sub ( r \"(\\s|_|-)+\" , \"-\" , re . sub ( r \"[A-Z]{2,}(?=[A-Z][a-z]+[0-9]*|\\b)|[A-Z]?[a-z]+[0-9]*|[A-Z]|[0-9]+\" , lambda mo : mo . group ( 0 ). lower (), text , ), ) snake_case def snake_case ( text : str ) -> str Converts a string to snake_case. Break the string into words and combine them adding _ as a separator, using a regexp. Parameters: Name Type Description Default text str a string. None Returns: Type Description None The same string best adapted to snake_case. Usage: snake_case(\"A bunch of words\") -> \"a_bunch_of_words\" snake_case(\"camelCase\") -> \"camelcase\" | View Source @staticmethod def snake_case ( text : str ) -> str : \"\"\" Converts a string to snake_case. Break the string into words and combine them adding _ as a separator, using a regexp. Args: text (str): a string. Returns: The same string best adapted to snake_case. Usage: snake_case(\" A bunch of words \") -> \" a_bunch_of_words \" snake_case(\" camelCase \") -> \" camelcase \" \"\"\" return re . sub ( r \"(\\s|_|-)+\" , \"_\" , re . sub ( r \"[A-Z]{2,}(?=[A-Z][a-z]+[0-9]*|\\b)|[A-Z]?[a-z]+[0-9]*|[A-Z]|[0-9]+\" , lambda mo : mo . group ( 0 ). lower (), text , ), )","title":"Operations"},{"location":"reference/pyhdtoolkit/utils/operations/#module-pyhdtoolkitutilsoperations","text":"Module utils.operations Created on 2019.11.12 View Source \"\"\" Module utils.operations ----------------------- Created on 2019.11.12 :author: Felix Soubelet (felix.soubelet@cern.ch) A collection classes with utility functions to perform common / convenient operations on the classic Python structures. \"\"\" import copy import itertools import math import random import re from functools import reduce from typing import Callable , Dict , List , Sequence , Tuple , Union class ListOperations : \"\"\" A class to group some common / useful operations on lists. \"\"\" @staticmethod def all_unique ( sequence : Sequence ) -> bool : \"\"\" Returns True if all the values in a flat list are unique, False otherwise. Args: sequence (Sequence): a sequence of elements. Returns: True if all elements are unique, False otherwise. \"\"\" return len ( sequence ) == len ( set ( sequence )) @staticmethod def average_by ( sequence : Sequence , function : Callable = lambda x : x ) -> float : \"\"\" Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Args: sequence (Sequence): a sequence of elements. function (Callable): function to apply to elements of the sequence. Returns: The average of each element's result through `function`. Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 \"\"\" return float ( sum ( map ( function , sequence ), 0.0 ) / len ( sequence )) @staticmethod def bifurcate ( sequence : Sequence , filters : List [ bool ]) -> Sequence : \"\"\" Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Args: sequence (Sequence): a sequence of elements. filters (List[bool]): a list of booleans. Returns: A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] \"\"\" return [ [ x for i , x in enumerate ( sequence ) if filters [ i ]], [ x for i , x in enumerate ( sequence ) if not filters [ i ]], ] @staticmethod def bifurcate_by ( sequence : Sequence , function : Callable ) -> list : \"\"\" Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of lst, that should return a boolean. Returns: A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] \"\"\" return [[ x for x in sequence if function ( x )], [ x for x in sequence if not function ( x )]] @staticmethod def chunk_list ( sequence : Sequence , size : int ) -> Sequence : \"\"\" Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Args: sequence (Sequence): a sequence of elements. size (int): the size of the wanted sublists. Returns: A list of lists of length `size` (except maybe the last element), with elements from `lst`. Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] \"\"\" if size > len ( sequence ) : return sequence return list ( map ( lambda x : sequence [ x * size : x * size + size ], list ( range ( math . ceil ( len ( sequence ) / size ))),) ) @staticmethod def deep_flatten ( sequence : Sequence ) -> list : \"\"\" Deep flattens a list, no matter the nesting levels. This is a recursive approach. Args: sequence (Sequence): a sequence of elements. Returns: A list with all elements of `lst`, but flattened. Usage: deep_flatten([[\" a \", \" b \"], [1, 2], None, [True, False]]) -> [\" a \", \" b \", 1, 2, None True, False] \"\"\" return ( [ elem for sublist in sequence for elem in ListOperations . deep_flatten ( sublist )] if isinstance ( sequence , list ) else [ sequence ] ) @staticmethod def eval_none ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False \"\"\" return not any ( map ( function , sequence )) @staticmethod def eval_some ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False \"\"\" return any ( map ( function , sequence )) @staticmethod def get_indices ( element , sequence : Sequence ) -> List [ int ] : \"\"\" Return all array indices at which number is located. Args: element: any reference element to check. sequence (Sequence): a sequence containing objects comparable to `elements`. A string can be compared to an int in Python, custom objects probably won't be comparable. Returns: A list of all indices at which `element` is found in `sequence`. Empty list if `element` is not present in `sequence` at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] \"\"\" return [ i for ( y , i ) in zip ( sequence , range ( len ( sequence ))) if element == y ] @staticmethod def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ] : \"\"\" Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of `sequence` that should return a boolean. Returns: A dict with keys \" True \" and \" False \", each having as value a list of all elements of `lst` that were evaluated to respectively `True` or `False` through `function`. Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} \"\"\" groups = {} for key in list ( map ( function , sequence )) : groups [ key ] = [ item for item in sequence if function ( item ) == key ] return groups @staticmethod def has_duplicates ( sequence : Sequence ) -> bool : \"\"\" Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Args: sequence (Sequence): a sequence of elements. Returns: A boolean indicating the presence of duplicates in `lst`. Usage: has_duplicates([1, 2, 1]) -> True \"\"\" return len ( sequence ) ! = len ( set ( sequence )) @staticmethod def sample ( sequence : Sequence ) -> list : \"\"\" Returns a random element from an array. Args: sequence (Sequence): a sequence of elements. Returns: A random element from `lst` in a list (to manage potentially nested lists as input). \"\"\" return sequence [ random . randint ( 0 , len ( sequence ) - 1 )] @staticmethod def sanitize_list ( sequence : Sequence ) -> list : \"\"\" Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Args: sequence (Sequence): a sequence of elements. Returns: The sequence without falsy values. Usage: sanitize_list([1, False, \" a \", 2, \"\", None, 6, 0]) -> [1, \" a \", 2, 6] \"\"\" return list ( filter ( bool , sequence )) @staticmethod def shuffle ( sequence : Sequence ) -> Sequence : \"\"\" Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm (https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) to reorder the elements. Args: sequence (Sequence): a sequence of elements. Returns: The `lst` with original elements at a random index. \"\"\" temp_list = copy . deepcopy ( sequence ) amount_to_shuffle = len ( temp_list ) while amount_to_shuffle > 1 : rand_index = int ( math . floor ( random . random () * amount_to_shuffle )) amount_to_shuffle -= 1 temp_list [ rand_index ], temp_list [ amount_to_shuffle ] = ( temp_list [ amount_to_shuffle ], temp_list [ rand_index ], ) return temp_list @staticmethod def spread ( sequence : Sequence ) -> list : \"\"\" Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in `lst` are iterables! Args: sequence (Sequence): a sequence of elements. Returns: The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] \"\"\" return list ( itertools . chain . from_iterable ( sequence )) @staticmethod def symmetric_difference_by ( lst_1: Sequence , lst_2: Sequence , function : Callable ) -> list : \"\"\" Returns the symmetric difference (https://en.wikipedia.org/wiki/Symmetric_difference) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] \"\"\" _ lst_1 , _ lst_2 = set ( map ( function , lst_1 )), set ( map ( function , lst_2 )) return [ item for item in lst_1 if function ( item ) not in _ lst_2 ] + [ item for item in lst_2 if function ( item ) not in _ lst_1 ] @staticmethod def union_by ( lst_1: Sequence , lst_2: Sequence , function : Callable ) -> list : \"\"\" Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union (https://en.wikipedia.org/wiki/Union_(set_theory)) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] \"\"\" _ lst_1 = set ( map ( function , lst_1 )) return sorted ( list ( set ( lst_1 + [ item for item in lst_2 if function ( item ) not in _ lst_1 ]))) @staticmethod def zipper ( * args , fillvalue = None ) -> list : \"\"\" Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Args: *args: a number (>= 2) of different iterables. fillvalue: value to use in case of length mismatch. Returns: A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\" a \", \" b \", \" c \"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] \"\"\" max_length = max ( len ( lst ) for lst in args ) return [ [ args [ k ][ i ] if i < len ( args [ k ]) else fillvalue for k in range ( len ( args ))] for i in range ( max_length ) ] class MiscellaneousOperations : \"\"\" A class to group some misc. operations that don't pertain to classic structures. \"\"\" @staticmethod def longest_item ( * args ) : \"\"\" Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Args: *args: any number (>= 2) of iterables. Returns: The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) \"\"\" return max ( args , key = len ) @staticmethod def map_values ( obj : dict , function : Callable ) -> dict : \"\"\" Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Args: obj: a dictionary. function (Callable): a callable on values of `obj`. Returns: A new dictionary with the results. Usage: map_values( {\" a \": list(range(5)), \" b \": list(range(10)), \" c \": list(range(15))}, lambda x: len(x) ) -> {\" a \": 5, \" b \": 10, \" c \": 15} \"\"\" ret = {} for key in obj : ret [ key ] = function ( obj [ key ]) return ret class NumberOperations : \"\"\" A class to group some common / useful operations on numbers. \"\"\" @staticmethod def clamp_number ( num : Union [ int , float ], a_val: Union [ int , float ], b_val: Union [ int , float ] ) -> Union [ int , float ] : \"\"\" Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Args: num (Union[int, float]): a number (float / int) a_val (Union[int, float]): a number (float / int) b_val (Union[int, float]): a number (float / int) Returns: A number (float / int), being the nearest to `num` in the range [`a_val`, `b_val`]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 \"\"\" return max ( min ( num , max ( a_val , b_val )), min ( a_val , b_val )) @staticmethod def degrees_to_radians ( deg_value: Union [ int , float ], decompose : bool = False ) -> Union [ Tuple [ float , str , str ], int , float ] : \"\"\" Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Args: deg_value (Union[int, float]): angle value in degrees. decompose (bool): boolean option to return a more verbose result. Defaults to False. Returns: The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \" pi \", \" rad \") \"\"\" if decompose : return deg_value / 180 , \"pi\" , \"rad\" return ( deg_value * math . pi ) / 180.0 @staticmethod def greatest_common_divisor ( numbers_list: Sequence ) -> Union [ int , float ] : \"\"\" Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in `numbers_list`. Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 \"\"\" return reduce ( math . gcd , numbers_list ) @staticmethod def is_divisible_by ( dividend : Union [ int , float ], divisor : Union [ int , float ]) -> bool : \"\"\" Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Args: dividend (Union[int, float]): a number. divisor (Union[int, float]): a number. Returns: A boolean stating if `dividend` can be divided by `divisor`. \"\"\" return dividend % divisor == 0 @staticmethod def least_common_multiple ( * args ) -> int : \"\"\" Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 \"\"\" numbers = list ( ListOperations . spread ( list ( args ))) def _ lcm ( number1 , number2 ) : \"\"\"A least common multiple method for two numbers only\"\"\" return int ( number1 * number2 / math . gcd ( number1 , number2 )) return reduce ( lambda x , y : _ lcm ( x , y ), numbers ) @staticmethod def radians_to_degrees ( rad_value: Union [ int , float ]) -> Union [ int , float ] : \"\"\" Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Args: rad_value (Union[int, float]): angle value in degrees. Returns: The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 \"\"\" return ( rad_value * 180.0 ) / math . pi class StringOperations : \"\"\" A class to group some common / useful operations on strings. \"\"\" @staticmethod def camel_case ( text : str ) -> str : \"\"\" Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Args: text (str): a string. Returns: The same string best adapted to camel_case. Usage: camel_case(\" a_snake_case_name \") -> \" aSnakeCaseName \" camel_case(\" A Title Case Name \") -> \" aTitleCaseName \" \"\"\" text = re . sub ( r \"(\\s|_|-)+\" , \" \" , text ). title (). replace ( \" \" , \"\" ) return text [ 0 ]. lower () + text [ 1 : ] @staticmethod def capitalize ( text : str , lower_rest: bool = False ) -> str : \"\"\" Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Args: text (str): a string. lower_rest (bool): boolean option to lower all elements starting from the second. Returns: The `string`, capitalized. Usage: capitalize(\" astringtocapitalize \") -> \" Astringtocapitalize \" capitalize(\" astRIngTocApItalizE \", lower_rest=True) -> \" Astringtocapitalize \" \"\"\" return text [ : 1 ]. upper () + ( text [ 1 : ]. lower () if lower_rest else text [ 1 : ]) @staticmethod def is_anagram ( str_1: str , str_2: str ) -> bool : \"\"\" Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Args: str_1 (str): a string. str_2 (str): a string. Returns: A boolean stating whether `str_1` is an anagram of `str_2` or not. Usage: is_anagram(\" Tom Marvolo Riddle \", \" I am Lord Voldemort \") -> True is_anagram(\" A first string \", \" Definitely not an anagram \") -> False \"\"\" _ str1 , _ str2 = ( str_1 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), str_2 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), ) return sorted ( _ str1 . lower ()) == sorted ( _ str2 . lower ()) @staticmethod def is_palindrome ( text : str ) -> bool : \"\"\" Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Args: text (str): a string. Returns: A boolean stating whether `string` is a palindrome or not. Usage: is_palindrome(\" racecar \") -> True is_palindrome(\" definitelynot \") -> False \"\"\" s_reverse = re . sub ( r \" [ \\ W_ ] \", \"\", text.lower()) return s_reverse == s_reverse[::-1] @staticmethod def kebab_case(text: str) -> str: \"\"\" Converts a string to kebab - case . Break the string into words and combine them adding - as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to kebab_case . Usage : kebab_case ( \"camel Case\" ) -> \"camel-case\" kebab_case ( \"snake_case\" ) -> \"snake-case\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" - \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \", lambda mo: mo.group(0).lower(), text, ), ) @staticmethod def snake_case(text: str) -> str: \"\"\" Converts a string to snake_case . Break the string into words and combine them adding _ as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to snake_case . Usage : snake_case ( \"A bunch of words\" ) -> \"a_bunch_of_words\" snake_case ( \"camelCase\" ) -> \"camelcase\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" _ \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \" , lambda mo : mo . group ( 0 ). lower (), text , ), )","title":"Module pyhdtoolkit.utils.operations"},{"location":"reference/pyhdtoolkit/utils/operations/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/utils/operations/#listoperations","text":"class ListOperations ( / , * args , ** kwargs ) View Source class ListOperations : \"\"\" A class to group some common / useful operations on lists. \"\"\" @ staticmethod def all_unique ( sequence : Sequence ) -> bool : \"\"\" Returns True if all the values in a flat list are unique, False otherwise. Args: sequence (Sequence): a sequence of elements. Returns: True if all elements are unique, False otherwise. \"\"\" return len ( sequence ) == len ( set ( sequence )) @ staticmethod def average_by ( sequence : Sequence , function : Callable = lambda x : x ) -> float : \"\"\" Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Args: sequence (Sequence): a sequence of elements. function (Callable): function to apply to elements of the sequence. Returns: The average of each element's result through `function`. Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 \"\"\" return float ( sum ( map ( function , sequence ), 0.0 ) / len ( sequence )) @ staticmethod def bifurcate ( sequence : Sequence , filters : List [ bool ]) -> Sequence : \"\"\" Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Args: sequence (Sequence): a sequence of elements. filters (List[bool]): a list of booleans. Returns: A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] \"\"\" return [ [ x for i , x in enumerate ( sequence ) if filters [ i ]], [ x for i , x in enumerate ( sequence ) if not filters [ i ]], ] @ staticmethod def bifurcate_by ( sequence : Sequence , function : Callable ) -> list : \"\"\" Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of lst, that should return a boolean. Returns: A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] \"\"\" return [[ x for x in sequence if function ( x )], [ x for x in sequence if not function ( x )]] @ staticmethod def chunk_list ( sequence : Sequence , size : int ) -> Sequence : \"\"\" Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Args: sequence (Sequence): a sequence of elements. size (int): the size of the wanted sublists. Returns: A list of lists of length `size` (except maybe the last element), with elements from `lst`. Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] \"\"\" if size > len ( sequence ): return sequence return list ( map ( lambda x : sequence [ x * size : x * size + size ], list ( range ( math . ceil ( len ( sequence ) / size ))),) ) @ staticmethod def deep_flatten ( sequence : Sequence ) -> list : \"\"\" Deep flattens a list, no matter the nesting levels. This is a recursive approach. Args: sequence (Sequence): a sequence of elements. Returns: A list with all elements of `lst`, but flattened. Usage: deep_flatten([[\"a\", \"b\"], [1, 2], None, [True, False]]) -> [\"a\", \"b\", 1, 2, None True, False] \"\"\" return ( [ elem for sublist in sequence for elem in ListOperations . deep_flatten ( sublist )] if isinstance ( sequence , list ) else [ sequence ] ) @ staticmethod def eval_none ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False \"\"\" return not any ( map ( function , sequence )) @ staticmethod def eval_some ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \"\"\" Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False \"\"\" return any ( map ( function , sequence )) @ staticmethod def get_indices ( element , sequence : Sequence ) -> List [ int ]: \"\"\" Return all array indices at which number is located. Args: element: any reference element to check. sequence (Sequence): a sequence containing objects comparable to `elements`. A string can be compared to an int in Python, custom objects probably won't be comparable. Returns: A list of all indices at which `element` is found in `sequence`. Empty list if `element` is not present in `sequence` at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] \"\"\" return [ i for ( y , i ) in zip ( sequence , range ( len ( sequence ))) if element == y ] @ staticmethod def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ]: \"\"\" Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of `sequence` that should return a boolean. Returns: A dict with keys \"True\" and \"False\", each having as value a list of all elements of `lst` that were evaluated to respectively `True` or `False` through `function`. Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} \"\"\" groups = {} for key in list ( map ( function , sequence )): groups [ key ] = [ item for item in sequence if function ( item ) == key ] return groups @ staticmethod def has_duplicates ( sequence : Sequence ) -> bool : \"\"\" Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Args: sequence (Sequence): a sequence of elements. Returns: A boolean indicating the presence of duplicates in `lst`. Usage: has_duplicates([1, 2, 1]) -> True \"\"\" return len ( sequence ) != len ( set ( sequence )) @ staticmethod def sample ( sequence : Sequence ) -> list : \"\"\" Returns a random element from an array. Args: sequence (Sequence): a sequence of elements. Returns: A random element from `lst` in a list (to manage potentially nested lists as input). \"\"\" return sequence [ random . randint ( 0 , len ( sequence ) - 1 )] @ staticmethod def sanitize_list ( sequence : Sequence ) -> list : \"\"\" Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Args: sequence (Sequence): a sequence of elements. Returns: The sequence without falsy values. Usage: sanitize_list([1, False, \"a\", 2, \"\", None, 6, 0]) -> [1, \"a\", 2, 6] \"\"\" return list ( filter ( bool , sequence )) @ staticmethod def shuffle ( sequence : Sequence ) -> Sequence : \"\"\" Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm (https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) to reorder the elements. Args: sequence (Sequence): a sequence of elements. Returns: The `lst` with original elements at a random index. \"\"\" temp_list = copy . deepcopy ( sequence ) amount_to_shuffle = len ( temp_list ) while amount_to_shuffle > 1 : rand_index = int ( math . floor ( random . random () * amount_to_shuffle )) amount_to_shuffle -= 1 temp_list [ rand_index ], temp_list [ amount_to_shuffle ] = ( temp_list [ amount_to_shuffle ], temp_list [ rand_index ], ) return temp_list @ staticmethod def spread ( sequence : Sequence ) -> list : \"\"\" Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in `lst` are iterables! Args: sequence (Sequence): a sequence of elements. Returns: The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] \"\"\" return list ( itertools . chain . from_iterable ( sequence )) @ staticmethod def symmetric_difference_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \"\"\" Returns the symmetric difference (https://en.wikipedia.org/wiki/Symmetric_difference) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] \"\"\" _lst_1 , _lst_2 = set ( map ( function , lst_1 )), set ( map ( function , lst_2 )) return [ item for item in lst_1 if function ( item ) not in _lst_2 ] + [ item for item in lst_2 if function ( item ) not in _lst_1 ] @ staticmethod def union_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \"\"\" Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union (https://en.wikipedia.org/wiki/Union_(set_theory)) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] \"\"\" _lst_1 = set ( map ( function , lst_1 )) return sorted ( list ( set ( lst_1 + [ item for item in lst_2 if function ( item ) not in _lst_1 ]))) @ staticmethod def zipper ( * args , fillvalue = None ) -> list : \"\"\" Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Args: *args: a number (>= 2) of different iterables. fillvalue: value to use in case of length mismatch. Returns: A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\"a\", \"b\", \"c\"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] \"\"\" max_length = max ( len ( lst ) for lst in args ) return [ [ args [ k ][ i ] if i < len ( args [ k ]) else fillvalue for k in range ( len ( args ))] for i in range ( max_length ) ]","title":"ListOperations"},{"location":"reference/pyhdtoolkit/utils/operations/#static-methods","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/operations/#all_unique","text":"def all_unique ( sequence : Sequence ) -> bool Returns True if all the values in a flat list are unique, False otherwise. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None True if all elements are unique, False otherwise. View Source @staticmethod def all_unique ( sequence : Sequence ) -> bool : \"\"\" Returns True if all the values in a flat list are unique, False otherwise. Args: sequence (Sequence): a sequence of elements. Returns: True if all elements are unique, False otherwise. \"\"\" return len ( sequence ) == len ( set ( sequence ))","title":"all_unique"},{"location":"reference/pyhdtoolkit/utils/operations/#average_by","text":"def average_by ( sequence : Sequence , function : Callable = < function ListOperations .< lambda > at 0x7fdeb06e45f0 > ) -> float Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable function to apply to elements of the sequence. None Returns: Type Description None The average of each element's result through function . Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 | View Source @staticmethod def average_by ( sequence : Sequence , function : Callable = lambda x : x ) -> float : \" \"\" Returns the average of lst after mapping each element to a value using the provided function. Use map() to map each element to the value returned by function. Use sum() to sum all of the mapped values, divide by len(lst). Args: sequence (Sequence): a sequence of elements. function (Callable): function to apply to elements of the sequence. Returns: The average of each element's result through `function`. Usage: average_by([{'n': 4}, {'n': 2}, {'n': 8}, {'n': 6}], lambda x: x['n']) -> 5.0 \"\" \" return float ( sum ( map ( function , sequence ), 0.0 ) / len ( sequence ))","title":"average_by"},{"location":"reference/pyhdtoolkit/utils/operations/#bifurcate","text":"def bifurcate ( sequence : Sequence , filters : List [ bool ] ) -> Sequence Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None filters List[bool] a list of booleans. None Returns: Type Description None A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] | View Source @staticmethod def bifurcate ( sequence : Sequence , filters : List [ bool ] ) -> Sequence : \"\"\" Splits values into two groups. If an element in filter is True, the corresponding element in the collection belongs to the first group; otherwise, it belongs to the second group. Use list comprehension and enumerate() to add elements to groups, based on filter. Args: sequence (Sequence): a sequence of elements. filters (List[bool]): a list of booleans. Returns: A list of two lists, one for each boolean output of the filters Usage: bifurcate(['beep', 'boop', 'foo', 'bar'], [True, True, False, True]) -> [['beep', 'boop', 'bar'], ['foo']] \"\"\" return [ [x for i, x in enumerate(sequence) if filters[i ] ] , [ x for i, x in enumerate(sequence) if not filters[i ] ] , ]","title":"bifurcate"},{"location":"reference/pyhdtoolkit/utils/operations/#bifurcate_by","text":"def bifurcate_by ( sequence : Sequence , function : Callable ) -> list Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on the elements of lst, that should return a boolean. None Returns: Type Description None A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] | View Source @staticmethod def bifurcate_by ( sequence : Sequence , function : Callable ) -> list : \"\"\" Splits values into two groups according to a function, which specifies which group an element in the input list belongs to. If the function returns True, the element belongs to the first group; otherwise it belongs to the second group. Use list comprehension to add elements to groups, based on function. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of lst, that should return a boolean. Returns: A list of two lists, as groups of elements of lst classified depending on their result through function. Usage: bifurcate_by(list(range(5)), lambda x: x % 2 == 0) -> [[0, 2, 4], [1, 3]] \"\"\" return [ [x for x in sequence if function(x) ] , [ x for x in sequence if not function(x) ] ]","title":"bifurcate_by"},{"location":"reference/pyhdtoolkit/utils/operations/#chunk_list","text":"def chunk_list ( sequence : Sequence , size : int ) -> Sequence Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None size int the size of the wanted sublists. None Returns: Type Description None A list of lists of length size (except maybe the last element), with elements from lst . Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] | View Source @staticmethod def chunk_list ( sequence : Sequence , size : int ) -> Sequence : \" \"\" Chunks a list into smaller lists of a specified size. If the size is bigger than initial list, return the initial list to avoid unnecessary nesting. Use list() and range() to create a list of the desired size. Use map() on the list and fill it with splices of the given list. Finally, return use created list. Args: sequence (Sequence): a sequence of elements. size (int): the size of the wanted sublists. Returns: A list of lists of length `size` (except maybe the last element), with elements from `lst`. Usage: chunk_list(list(range(10)), 3) -> [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] \"\" \" if size > len ( sequence ) : return sequence return list ( map ( lambda x : sequence [ x * size : x * size + size ] , list ( range ( math . ceil ( len ( sequence ) / size ))),) )","title":"chunk_list"},{"location":"reference/pyhdtoolkit/utils/operations/#deep_flatten","text":"def deep_flatten ( sequence : Sequence ) -> list Deep flattens a list, no matter the nesting levels. This is a recursive approach. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None A list with all elements of lst , but flattened. Usage: deep_flatten([[\"a\", \"b\"], [1, 2], None, [True, False]]) -> [\"a\", \"b\", 1, 2, None True, False] | View Source @staticmethod def deep_flatten ( sequence : Sequence ) -> list : \"\"\" Deep flattens a list, no matter the nesting levels. This is a recursive approach. Args: sequence (Sequence): a sequence of elements. Returns: A list with all elements of `lst`, but flattened. Usage: deep_flatten([[\" a \", \" b \"], [1, 2], None, [True, False]]) -> [\" a \", \" b \", 1, 2, None True, False] \"\"\" return ( [ elem for sublist in sequence for elem in ListOperations.deep_flatten(sublist) ] if isinstance ( sequence , list ) else [ sequence ] )","title":"deep_flatten"},{"location":"reference/pyhdtoolkit/utils/operations/#eval_none","text":"def eval_none ( sequence : Sequence , function : Callable = < function ListOperations .< lambda > at 0x7fdeb06e4950 > ) -> bool Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on elements of sequence that should return a boolean. None Returns: Type Description None A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False | View Source @staticmethod def eval_none ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \" \"\" Returns False if the provided function returns True for at least one element in the list, True otherwise. Iterate over the elements of the list to test if every element in the list returns False based on function. Omit the seconds argument, function, to check if all elements are False. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_none([0, 0, 1, 0], lambda x: x >= 2) -> True eval_none([0, 1, 2, 0], lambda x: x >= 2) -> False \"\" \" return not any ( map ( function , sequence ))","title":"eval_none"},{"location":"reference/pyhdtoolkit/utils/operations/#eval_some","text":"def eval_some ( sequence : Sequence , function : Callable = < function ListOperations .< lambda > at 0x7fdeb06e4a70 > ) -> bool Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on elements of sequence that should return a boolean. None Returns: Type Description None A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False | View Source @staticmethod def eval_some ( sequence : Sequence , function : Callable = lambda x : not not x ) -> bool : \" \"\" Returns True if the provided function returns True for at least one element in the list, False otherwise. Iterate over the elements of the list to test if every element in the list returns True based on function. Omit the seconds argument, function, to check if all elements are True. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on elements of `sequence` that should return a boolean. Returns: A boolean. See first line of docstring. Usage: eval_some([0, 1, 2, 0], lambda x: x >= 2) -> True eval_some([0, 0, 1, 0], lambda x: x >= 2) -> False \"\" \" return any ( map ( function , sequence ))","title":"eval_some"},{"location":"reference/pyhdtoolkit/utils/operations/#get_indices","text":"def get_indices ( element , sequence : Sequence ) -> List [ int ] Return all array indices at which number is located. Parameters: Name Type Description Default element None any reference element to check. None sequence Sequence a sequence containing objects comparable to elements . A string can be compared to an int in Python, custom objects probably won't be comparable. None Returns: Type Description None A list of all indices at which element is found in sequence . Empty list if element is not present in sequence at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] | View Source @staticmethod def get_indices ( element , sequence : Sequence ) -> List [ int ] : \" \"\" Return all array indices at which number is located. Args: element: any reference element to check. sequence (Sequence): a sequence containing objects comparable to `elements`. A string can be compared to an int in Python, custom objects probably won't be comparable. Returns: A list of all indices at which `element` is found in `sequence`. Empty list if `element` is not present in `sequence` at all. Usage: get_indices(0, [0, 1, 3, 5, 7, 3, 9, 0, 0, 5, 3, 2]) -> [0, 7, 8] \"\" \" return [ i for ( y , i ) in zip ( sequence , range ( len ( sequence ))) if element == y ]","title":"get_indices"},{"location":"reference/pyhdtoolkit/utils/operations/#group_by","text":"def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ] Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None function Callable a callable on the elements of sequence that should return a boolean. None Returns: Type Description None A dict with keys \"True\" and \"False\", each having as value a list of all elements of lst that were evaluated to respectively True or False through function . Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} | View Source @staticmethod def group_by ( sequence : Sequence , function : Callable ) -> Dict [ str , list ] : \" \"\" Groups the elements of a list based on the given function. Use list() in combination with map() and function to map the values of the list to the keys of an object. Use list comprehension to map each element to the appropriate key. Args: sequence (Sequence): a sequence of elements. function (Callable): a callable on the elements of `sequence` that should return a boolean. Returns: A dict with keys \" True \" and \" False \", each having as value a list of all elements of `lst` that were evaluated to respectively `True` or `False` through `function`. Usage: group_by(list(range(5)), lambda x: x % 2 == 0) -> {True: [0, 2, 4], False: [1, 3]} \"\" \" groups = {} for key in list ( map ( function , sequence )) : groups [ key ] = [ item for item in sequence if function ( item ) == key ] return groups","title":"group_by"},{"location":"reference/pyhdtoolkit/utils/operations/#has_duplicates","text":"def has_duplicates ( sequence : Sequence ) -> bool Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None A boolean indicating the presence of duplicates in lst . Usage: has_duplicates([1, 2, 1]) -> True | View Source @staticmethod def has_duplicates ( sequence : Sequence ) -> bool : \" \"\" Returns True if there are duplicate values in a fast list, False otherwise. Use set() on the given list to remove duplicates, then compare its length with the length of the list. Args: sequence (Sequence): a sequence of elements. Returns: A boolean indicating the presence of duplicates in `lst`. Usage: has_duplicates([1, 2, 1]) -> True \"\" \" return len ( sequence ) != len ( set ( sequence ))","title":"has_duplicates"},{"location":"reference/pyhdtoolkit/utils/operations/#sample","text":"def sample ( sequence : Sequence ) -> list Returns a random element from an array. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None A random element from lst in a list (to manage potentially nested lists as input). View Source @staticmethod def sample ( sequence : Sequence ) -> list : \" \"\" Returns a random element from an array. Args: sequence (Sequence): a sequence of elements. Returns: A random element from `lst` in a list (to manage potentially nested lists as input). \"\" \" return sequence [ random . randint ( 0 , len ( sequence ) - 1 ) ]","title":"sample"},{"location":"reference/pyhdtoolkit/utils/operations/#sanitize_list","text":"def sanitize_list ( sequence : Sequence ) -> list Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None The sequence without falsy values. Usage: sanitize_list([1, False, \"a\", 2, \"\", None, 6, 0]) -> [1, \"a\", 2, 6] | View Source @staticmethod def sanitize_list ( sequence : Sequence ) -> list : \"\"\" Removes falsey values from a list. Use filter() to filter out falsey values (False, None, 0, and \"\"). Args: sequence (Sequence): a sequence of elements. Returns: The sequence without falsy values. Usage: sanitize_list([1, False, \" a \", 2, \"\", None, 6, 0]) -> [1, \" a \", 2, 6] \"\"\" return list ( filter ( bool , sequence ))","title":"sanitize_list"},{"location":"reference/pyhdtoolkit/utils/operations/#shuffle","text":"def shuffle ( sequence : Sequence ) -> Sequence Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm ( https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle ) to reorder the elements. Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None The lst with original elements at a random index. View Source @staticmethod def shuffle ( sequence : Sequence ) -> Sequence : \"\"\" Randomizes the order of the values of an list, returning a new list. Uses an improved version of the Fisher-Yates algorithm (https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) to reorder the elements. Args: sequence (Sequence): a sequence of elements. Returns: The `lst` with original elements at a random index. \"\"\" temp_list = copy . deepcopy ( sequence ) amount_to_shuffle = len ( temp_list ) while amount_to_shuffle > 1 : rand_index = int ( math . floor ( random . random () * amount_to_shuffle )) amount_to_shuffle -= 1 temp_list [ rand_index ] , temp_list [ amount_to_shuffle ] = ( temp_list [ amount_to_shuffle ] , temp_list [ rand_index ] , ) return temp_list","title":"shuffle"},{"location":"reference/pyhdtoolkit/utils/operations/#spread","text":"def spread ( sequence : Sequence ) -> list Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in lst are iterables! Parameters: Name Type Description Default sequence Sequence a sequence of elements. None Returns: Type Description None The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] | View Source @staticmethod def spread ( sequence : Sequence ) -> list : \" \"\" Flattens a list, by spreading its elements into a new list. Loop over elements, use list.extend() if the element is a list, list.append() otherwise. This might look like deep_flatten but is a subset of its functionality, and is used in deep_flatten. This only works if all elements in `lst` are iterables! Args: sequence (Sequence): a sequence of elements. Returns: The sequence flattened, see first docstring sentence. Usage: spread([list(range(5)), list(range(5))]) -> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4] \"\" \" return list ( itertools . chain . from_iterable ( sequence ))","title":"spread"},{"location":"reference/pyhdtoolkit/utils/operations/#symmetric_difference_by","text":"def symmetric_difference_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list Returns the symmetric difference ( https://en.wikipedia.org/wiki/Symmetric_difference ) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Parameters: Name Type Description Default lst_1 Sequence a sequence of elements. None lst_2 Sequence a sequence of elements. None function Callable a callable on elements of lst_1 and lst_2 . None Returns: Type Description None A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] | View Source @staticmethod def symmetric_difference_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \" \"\" Returns the symmetric difference (https://en.wikipedia.org/wiki/Symmetric_difference) of lists, after applying the provided function to each list element of both. Create a set by applying the function to each element in every list, then use list comprehension in combination with function on each one to only keep values not contained in the previously created set of the other. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: symmetric_difference_by([2.1, 1.2], [2.3, 3.4], math.floor) -> [1.2, 3.4] symmetric_difference_by([2.1, 1.2], [0.5, 1.2], lambda x: x >= 2) -> [2.1] \"\" \" _lst_1 , _lst_2 = set ( map ( function , lst_1 )), set ( map ( function , lst_2 )) return [ item for item in lst_1 if function ( item ) not in _lst_2 ] + [ item for item in lst_2 if function ( item ) not in _lst_1 ]","title":"symmetric_difference_by"},{"location":"reference/pyhdtoolkit/utils/operations/#union_by","text":"def union_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union ( https://en.wikipedia.org/wiki/Union_(set_theory )) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Parameters: Name Type Description Default lst_1 Sequence a sequence of elements. None lst_2 Sequence a sequence of elements. None function Callable a callable on elements of lst_1 and lst_2 . None Returns: Type Description None A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] | View Source @staticmethod def union_by ( lst_1 : Sequence , lst_2 : Sequence , function : Callable ) -> list : \" \"\" Returns every element that exists in any of the two lists once, after applying the provided function to each element of both. This is the set theory union (https://en.wikipedia.org/wiki/Union_(set_theory)) of the two lists, but based on the results of applying the function to each list. Python's set() is strange in how is gives output, so this function sorts the final list before returning it, in order to give it predictable behavior. Create a set by applying the function to each element in lst_1, then use list comprehension in combination with function on lst_2 to only keep values not contained in the previously created set, _lst_1. Finally, create a set from the previous result and _lst_1 and transform it into a list. Args: lst_1 (Sequence): a sequence of elements. lst_2 (Sequence): a sequence of elements. function (Callable): a callable on elements of `lst_1` and `lst_2`. Returns: A list, see first docstring sentence reference. Usage: union_by([2.1], [1.2, 2.3], math.floor) -> [1.2, 2.1] \"\" \" _lst_1 = set ( map ( function , lst_1 )) return sorted ( list ( set ( lst_1 + [ item for item in lst_2 if function ( item ) not in _lst_1 ] )))","title":"union_by"},{"location":"reference/pyhdtoolkit/utils/operations/#zipper","text":"def zipper ( * args , fillvalue = None ) -> list Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Parameters: Name Type Description Default *args None a number (>= 2) of different iterables. None fillvalue None value to use in case of length mismatch. None Returns: Type Description None A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\"a\", \"b\", \"c\"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] | View Source @staticmethod def zipper ( * args , fillvalue = None ) -> list : \"\"\" Creates a list of lists of elements, each internal list being a grouping based on the position of elements in the original lists. Essentially, a list containing: a first list with all first elements, then a second list with all second elements, etc. Use max combined with list comprehension to get the length of the longest list in the arguments. Loop for max_length times grouping elements. If lengths of lists vary, use fill_value (defaults to None). Args: *args: a number (>= 2) of different iterables. fillvalue: value to use in case of length mismatch. Returns: A list with the proper level of nesting, and original elements zipped. Usage: zipper([1, 2, 3], [2, 5, 3, 7], [\" a \", \" b \", \" c \"]) -> [[1, 2, 'a'], [2, 5, 'b'], [3, 3, 'c'], [None, 7, None]] \"\"\" max_length = max ( len ( lst ) for lst in args ) return [ [args[k ][ i ] if i < len ( args [ k ] ) else fillvalue for k in range ( len ( args )) ] for i in range ( max_length ) ]","title":"zipper"},{"location":"reference/pyhdtoolkit/utils/operations/#miscellaneousoperations","text":"class MiscellaneousOperations ( / , * args , ** kwargs ) View Source class MiscellaneousOperations : \"\"\" A class to group some misc. operations that don't pertain to classic structures. \"\"\" @staticmethod def longest_item ( * args ) : \"\"\" Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Args: *args: any number (>= 2) of iterables. Returns: The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) \"\"\" return max ( args , key = len ) @staticmethod def map_values ( obj : dict , function : Callable ) -> dict : \"\"\" Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Args: obj: a dictionary. function (Callable): a callable on values of `obj`. Returns: A new dictionary with the results. Usage: map_values( {\" a \": list(range(5)), \" b \": list(range(10)), \" c \": list(range(15))}, lambda x: len(x) ) -> {\" a \": 5, \" b \": 10, \" c \": 15} \"\"\" ret = {} for key in obj : ret [ key ] = function ( obj [ key ] ) return ret","title":"MiscellaneousOperations"},{"location":"reference/pyhdtoolkit/utils/operations/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/operations/#longest_item","text":"def longest_item ( * args ) Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Parameters: Name Type Description Default *args None any number (>= 2) of iterables. None Returns: Type Description None The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) | View Source @staticmethod def longest_item ( * args ) : \"\"\" Takes any number of iterable objects or objects with a length property and returns the longest one. If multiple objects have the same length, the first one will be returned. Use max() with len as the key to return the item with the greatest length. Args: *args: any number (>= 2) of iterables. Returns: The longest elements of provided iterables. Usage: longest_item(list(range(5)), list(range(100)), list(range(50))) -> list(range(100)) \"\"\" return max ( args , key = len )","title":"longest_item"},{"location":"reference/pyhdtoolkit/utils/operations/#map_values","text":"def map_values ( obj : dict , function : Callable ) -> dict Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Parameters: Name Type Description Default obj None a dictionary. None function Callable a callable on values of obj . None Returns: Type Description None A new dictionary with the results. Usage: map_values( {\"a\": list(range(5)), \"b\": list(range(10)), \"c\": list(range(15))}, lambda x: len(x) ) -> {\"a\": 5, \"b\": 10, \"c\": 15} | View Source @staticmethod def map_values ( obj : dict , function : Callable ) -> dict : \"\"\" Creates an new dict with the same keys as the provided dict, and values generated by running the provided function on the provided dict's values. Use dict.keys() to iterate over the object's keys, assigning the values produced by function to each key of a new object. Args: obj: a dictionary. function (Callable): a callable on values of `obj`. Returns: A new dictionary with the results. Usage: map_values( {\" a \": list(range(5)), \" b \": list(range(10)), \" c \": list(range(15))}, lambda x: len(x) ) -> {\" a \": 5, \" b \": 10, \" c \": 15} \"\"\" ret = {} for key in obj : ret [ key ] = function ( obj [ key ] ) return ret","title":"map_values"},{"location":"reference/pyhdtoolkit/utils/operations/#numberoperations","text":"class NumberOperations ( / , * args , ** kwargs ) View Source class NumberOperations : \" \"\" A class to group some common / useful operations on numbers. \"\" \" @staticmethod def clamp_number ( num : Union [ int , float ] , a_val : Union [ int , float ] , b_val : Union [ int , float ] ) -> Union [ int , float ] : \" \"\" Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Args: num (Union[int, float]): a number (float / int) a_val (Union[int, float]): a number (float / int) b_val (Union[int, float]): a number (float / int) Returns: A number (float / int), being the nearest to `num` in the range [`a_val`, `b_val`]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 \"\" \" return max ( min ( num , max ( a_val , b_val )), min ( a_val , b_val )) @staticmethod def degrees_to_radians ( deg_value : Union [ int , float ] , decompose : bool = False ) -> Union [ Tuple [ float , str , str ] , int , float ] : \" \"\" Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Args: deg_value (Union[int, float]): angle value in degrees. decompose (bool): boolean option to return a more verbose result. Defaults to False. Returns: The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \" pi \", \" rad \") \"\" \" if decompose : return deg_value / 180 , \"pi\" , \"rad\" return ( deg_value * math . pi ) / 180.0 @staticmethod def greatest_common_divisor ( numbers_list : Sequence ) -> Union [ int , float ] : \" \"\" Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in `numbers_list`. Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 \"\" \" return reduce ( math . gcd , numbers_list ) @staticmethod def is_divisible_by ( dividend : Union [ int , float ] , divisor : Union [ int , float ] ) -> bool : \" \"\" Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Args: dividend (Union[int, float]): a number. divisor (Union[int, float]): a number. Returns: A boolean stating if `dividend` can be divided by `divisor`. \"\" \" return dividend % divisor == 0 @staticmethod def least_common_multiple ( * args ) -> int : \" \"\" Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 \"\" \" numbers = list ( ListOperations . spread ( list ( args ))) def _lcm ( number1 , number2 ) : \" \"\" A least common multiple method for two numbers only \"\" \" return int ( number1 * number2 / math . gcd ( number1 , number2 )) return reduce ( lambda x , y : _lcm ( x , y ), numbers ) @staticmethod def radians_to_degrees ( rad_value : Union [ int , float ] ) -> Union [ int , float ] : \" \"\" Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Args: rad_value (Union[int, float]): angle value in degrees. Returns: The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 \"\" \" return ( rad_value * 180.0 ) / math . pi","title":"NumberOperations"},{"location":"reference/pyhdtoolkit/utils/operations/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/operations/#clamp_number","text":"def clamp_number ( num : Union [ int , float ], a_val : Union [ int , float ], b_val : Union [ int , float ] ) -> Union [ int , float ] Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Parameters: Name Type Description Default num Union[int, float] a number (float / int) None a_val Union[int, float] a number (float / int) None b_val Union[int, float] a number (float / int) None Returns: Type Description None A number (float / int), being the nearest to num in the range [ a_val , b_val ]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 | View Source @staticmethod def clamp_number ( num : Union [ int , float ] , a_val : Union [ int , float ] , b_val : Union [ int , float ] ) -> Union [ int , float ] : \" \"\" Clamps num within the inclusive range specified by the boundary values a and b. If num falls within the range, return num. Otherwise, return the nearest number in the range. Args: num (Union[int, float]): a number (float / int) a_val (Union[int, float]): a number (float / int) b_val (Union[int, float]): a number (float / int) Returns: A number (float / int), being the nearest to `num` in the range [`a_val`, `b_val`]. Usage: clamp_number(17, 4, 5) -> 5 clamp_number(23, 20, 30) -> 23 \"\" \" return max ( min ( num , max ( a_val , b_val )), min ( a_val , b_val ))","title":"clamp_number"},{"location":"reference/pyhdtoolkit/utils/operations/#degrees_to_radians","text":"def degrees_to_radians ( deg_value : Union [ int , float ], decompose : bool = False ) -> Union [ Tuple [ float , str , str ], int , float ] Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Parameters: Name Type Description Default deg_value Union[int, float] angle value in degrees. None decompose bool boolean option to return a more verbose result. Defaults to False. False Returns: Type Description None The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \"pi\", \"rad\") | View Source @staticmethod def degrees_to_radians ( deg_value : Union [ int, float ] , decompose : bool = False ) -> Union [ Tuple[float, str, str ] , int , float ]: \"\"\" Converts an angle from degrees to radians. Use math.pi and the degree to radian formula to convert the angle from degrees to radians. Args: deg_value (Union[int, float]): angle value in degrees. decompose (bool): boolean option to return a more verbose result. Defaults to False. Returns: The angle value in radians. Usage: degrees_to_radians(160) -> 2.792526803190927 degrees_to_radians(360, decompose=True) -> (2, \" pi \", \" rad \") \"\"\" if decompose : return deg_value / 180 , \"pi\" , \"rad\" return ( deg_value * math . pi ) / 180.0","title":"degrees_to_radians"},{"location":"reference/pyhdtoolkit/utils/operations/#greatest_common_divisor","text":"def greatest_common_divisor ( numbers_list : Sequence ) -> Union [ int , float ] Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in numbers_list . Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 View Source @staticmethod def greatest_common_divisor ( numbers_list : Sequence ) -> Union [ int , float ] : \" \"\" Calculates the greatest common divisor of a list of numbers. Use reduce() and math.gcd over the given list. Args: numbers_list (Sequence): a list of numbers (floats are advised against as this would become a very heavy computation). Returns: The greatest common divisor of all elements in `numbers_list`. Usage: greatest_common_divisor([54, 24]) -> greatest_common_divisor([30, 132, 378, 582, 738]) -> 6 \"\" \" return reduce ( math . gcd , numbers_list )","title":"greatest_common_divisor"},{"location":"reference/pyhdtoolkit/utils/operations/#is_divisible_by","text":"def is_divisible_by ( dividend : Union [ int , float ], divisor : Union [ int , float ] ) -> bool Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Parameters: Name Type Description Default dividend Union[int, float] a number. None divisor Union[int, float] a number. None Returns: Type Description None A boolean stating if dividend can be divided by divisor . View Source @staticmethod def is_divisible_by ( dividend : Union [ int , float ] , divisor : Union [ int , float ] ) -> bool : \" \"\" Checks if the first numeric argument is divisible by the second one. Use the modulo operator (%) to check if the remainder is equal to 0. Args: dividend (Union[int, float]): a number. divisor (Union[int, float]): a number. Returns: A boolean stating if `dividend` can be divided by `divisor`. \"\" \" return dividend % divisor == 0","title":"is_divisible_by"},{"location":"reference/pyhdtoolkit/utils/operations/#least_common_multiple","text":"def least_common_multiple ( * args ) -> int Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 View Source @staticmethod def least_common_multiple ( * args ) -> int : \"\"\" Returns the least common multiple of two or more numbers. Define a function, spread, that uses either list.extend() or list.append() on each element in a list to flatten it. Use math.gcd() and lcm(x,y) = x * y / gcd(x,y) to determine the least common multiple. Args: *args: any number (>= 2) of numbers (floats are advised against as this would become a very heavy computation). Returns: The least common multiple of all provided numbers. Usage: least_common_multiple(4, 5) -> 20 least_common_multiple(2, 5, 17, 632) -> 53720 \"\"\" numbers = list ( ListOperations . spread ( list ( args ))) def _lcm ( number1 , number2 ) : \"\"\"A least common multiple method for two numbers only\"\"\" return int ( number1 * number2 / math . gcd ( number1 , number2 )) return reduce ( lambda x , y : _lcm ( x , y ), numbers )","title":"least_common_multiple"},{"location":"reference/pyhdtoolkit/utils/operations/#radians_to_degrees","text":"def radians_to_degrees ( rad_value : Union [ int , float ] ) -> Union [ int , float ] Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Parameters: Name Type Description Default rad_value Union[int, float] angle value in degrees. None Returns: Type Description None The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 | View Source @staticmethod def radians_to_degrees ( rad_value : Union [ int, float ] ) -> Union [ int, float ] : \"\"\" Converts an angle from radians to degrees. Use math.pi and the radian to degree formula to convert the angle from radians to degrees. Args: rad_value (Union[int, float]): angle value in degrees. Returns: The angle value in degrees. Usage: radians_to_degrees(2* math.pi) -> 360 radians_to_degrees(2.710) -> 155.2715624804531 \"\"\" return ( rad_value * 180.0 ) / math . pi","title":"radians_to_degrees"},{"location":"reference/pyhdtoolkit/utils/operations/#stringoperations","text":"class StringOperations ( / , * args , ** kwargs ) View Source class StringOperations : \"\"\" A class to group some common / useful operations on strings. \"\"\" @staticmethod def camel_case ( text : str ) -> str : \"\"\" Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Args: text (str): a string. Returns: The same string best adapted to camel_case. Usage: camel_case(\" a_snake_case_name \") -> \" aSnakeCaseName \" camel_case(\" A Title Case Name \") -> \" aTitleCaseName \" \"\"\" text = re . sub ( r \"(\\s|_|-)+\" , \" \" , text ). title (). replace ( \" \" , \"\" ) return text [ 0 ]. lower () + text [ 1 : ] @staticmethod def capitalize ( text : str , lower_rest: bool = False ) -> str : \"\"\" Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Args: text (str): a string. lower_rest (bool): boolean option to lower all elements starting from the second. Returns: The `string`, capitalized. Usage: capitalize(\" astringtocapitalize \") -> \" Astringtocapitalize \" capitalize(\" astRIngTocApItalizE \", lower_rest=True) -> \" Astringtocapitalize \" \"\"\" return text [ : 1 ]. upper () + ( text [ 1 : ]. lower () if lower_rest else text [ 1 : ]) @staticmethod def is_anagram ( str_1: str , str_2: str ) -> bool : \"\"\" Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Args: str_1 (str): a string. str_2 (str): a string. Returns: A boolean stating whether `str_1` is an anagram of `str_2` or not. Usage: is_anagram(\" Tom Marvolo Riddle \", \" I am Lord Voldemort \") -> True is_anagram(\" A first string \", \" Definitely not an anagram \") -> False \"\"\" _ str1 , _ str2 = ( str_1 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), str_2 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), ) return sorted ( _ str1 . lower ()) == sorted ( _ str2 . lower ()) @staticmethod def is_palindrome ( text : str ) -> bool : \"\"\" Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Args: text (str): a string. Returns: A boolean stating whether `string` is a palindrome or not. Usage: is_palindrome(\" racecar \") -> True is_palindrome(\" definitelynot \") -> False \"\"\" s_reverse = re . sub ( r \" [ \\ W_ ] \", \"\", text.lower()) return s_reverse == s_reverse[::-1] @staticmethod def kebab_case(text: str) -> str: \"\"\" Converts a string to kebab - case . Break the string into words and combine them adding - as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to kebab_case . Usage : kebab_case ( \"camel Case\" ) -> \"camel-case\" kebab_case ( \"snake_case\" ) -> \"snake-case\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" - \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \", lambda mo: mo.group(0).lower(), text, ), ) @staticmethod def snake_case(text: str) -> str: \"\"\" Converts a string to snake_case . Break the string into words and combine them adding _ as a separator , using a regexp . Args : text ( str ) : a string . Returns : The same string best adapted to snake_case . Usage : snake_case ( \"A bunch of words\" ) -> \"a_bunch_of_words\" snake_case ( \"camelCase\" ) -> \"camelcase\" \"\"\" return re.sub( r\" ( \\s | _ | - ) + \", \" _ \", re.sub( r\" [ A - Z ]{ 2 ,}( ?= [ A - Z ][ a - z ] + [ 0 - 9 ] * | \\b )|[ A - Z ] ? [ a - z ] + [ 0 - 9 ] * |[ A - Z ]|[ 0 - 9 ] + \" , lambda mo : mo . group ( 0 ). lower (), text , ), )","title":"StringOperations"},{"location":"reference/pyhdtoolkit/utils/operations/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/pyhdtoolkit/utils/operations/#camel_case","text":"def camel_case ( text : str ) -> str Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Parameters: Name Type Description Default text str a string. None Returns: Type Description None The same string best adapted to camel_case. Usage: camel_case(\"a_snake_case_name\") -> \"aSnakeCaseName\" camel_case(\"A Title Case Name\") -> \"aTitleCaseName\" | View Source @staticmethod def camel_case ( text : str ) -> str : \"\"\" Converts a string to camelCase. Break the string into words and combine them capitalizing the first letter of each word, using a regexp, title() and lower. Args: text (str): a string. Returns: The same string best adapted to camel_case. Usage: camel_case(\" a_snake_case_name \") -> \" aSnakeCaseName \" camel_case(\" A Title Case Name \") -> \" aTitleCaseName \" \"\"\" text = re . sub ( r \"(\\s|_|-)+\" , \" \" , text ). title (). replace ( \" \" , \"\" ) return text [ 0 ] . lower () + text [ 1: ]","title":"camel_case"},{"location":"reference/pyhdtoolkit/utils/operations/#capitalize","text":"def capitalize ( text : str , lower_rest : bool = False ) -> str Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Parameters: Name Type Description Default text str a string. None lower_rest bool boolean option to lower all elements starting from the second. None Returns: Type Description None The string , capitalized. Usage: capitalize(\"astringtocapitalize\") -> \"Astringtocapitalize\" capitalize(\"astRIngTocApItalizE\", lower_rest=True) -> \"Astringtocapitalize\" | View Source @staticmethod def capitalize ( text : str , lower_rest : bool = False ) -> str : \" \"\" Capitalizes the first letter of a string, eventually lowers the rest of it. Capitalize the first letter of the string and then add it with rest of the string. Omit the lower_rest parameter to keep the rest of the string intact, or set it to True to convert to lowercase. Args: text (str): a string. lower_rest (bool): boolean option to lower all elements starting from the second. Returns: The `string`, capitalized. Usage: capitalize(\" astringtocapitalize \") -> \" Astringtocapitalize \" capitalize(\" astRIngTocApItalizE \", lower_rest=True) -> \" Astringtocapitalize \" \"\" \" return text [ : 1 ] . upper () + ( text [ 1 : ] . lower () if lower_rest else text [ 1 : ] )","title":"capitalize"},{"location":"reference/pyhdtoolkit/utils/operations/#is_anagram","text":"def is_anagram ( str_1 : str , str_2 : str ) -> bool Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Parameters: Name Type Description Default str_1 str a string. None str_2 str a string. None Returns: Type Description None A boolean stating whether str_1 is an anagram of str_2 or not. Usage: is_anagram(\"Tom Marvolo Riddle\", \"I am Lord Voldemort\") -> True is_anagram(\"A first string\", \"Definitely not an anagram\") -> False | View Source @staticmethod def is_anagram ( str_1 : str , str_2 : str ) -> bool : \" \"\" Checks if a string is an anagram of another string (case-insensitive, ignores spaces, punctuation and special characters). Use str.replace() to remove spaces from both strings. Compare the lengths of the two strings, return False if they are not equal. Use sorted() on both strings and compare the results. Args: str_1 (str): a string. str_2 (str): a string. Returns: A boolean stating whether `str_1` is an anagram of `str_2` or not. Usage: is_anagram(\" Tom Marvolo Riddle \", \" I am Lord Voldemort \") -> True is_anagram(\" A first string \", \" Definitely not an anagram \") -> False \"\" \" _str1 , _str2 = ( str_1 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), str_2 . replace ( \" \" , \"\" ). replace ( \"'\" , \"\" ), ) return sorted ( _str1 . lower ()) == sorted ( _str2 . lower ())","title":"is_anagram"},{"location":"reference/pyhdtoolkit/utils/operations/#is_palindrome","text":"def is_palindrome ( text : str ) -> bool Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Parameters: Name Type Description Default text str a string. None Returns: Type Description None A boolean stating whether string is a palindrome or not. Usage: is_palindrome(\"racecar\") -> True is_palindrome(\"definitelynot\") -> False | View Source @staticmethod def is_palindrome ( text : str ) -> bool : \"\"\" Returns True if the given string is a palindrome, False otherwise. Use str.lower() and re.sub() to convert to lowercase and remove non-alphanumeric characters from the given string. Then compare the new string with its reverse. Args: text (str): a string. Returns: A boolean stating whether `string` is a palindrome or not. Usage: is_palindrome(\" racecar \") -> True is_palindrome(\" definitelynot \") -> False \"\"\" s_reverse = re . sub ( r \" [ \\ W_ ] \", \" \" , text . lower ()) return s_reverse == s_reverse [ ::- 1 ]","title":"is_palindrome"},{"location":"reference/pyhdtoolkit/utils/operations/#kebab_case","text":"def kebab_case ( text : str ) -> str Converts a string to kebab-case. Break the string into words and combine them adding - as a separator, using a regexp. Parameters: Name Type Description Default text str a string. None Returns: Type Description None The same string best adapted to kebab_case. Usage: kebab_case(\"camel Case\") -> \"camel-case\" kebab_case(\"snake_case\") -> \"snake-case\" | View Source @staticmethod def kebab_case ( text : str ) -> str : \"\"\" Converts a string to kebab-case. Break the string into words and combine them adding - as a separator, using a regexp. Args: text (str): a string. Returns: The same string best adapted to kebab_case. Usage: kebab_case(\" camel Case \") -> \" camel - case \" kebab_case(\" snake_case \") -> \" snake - case \" \"\"\" return re . sub ( r \"(\\s|_|-)+\" , \"-\" , re . sub ( r \"[A-Z]{2,}(?=[A-Z][a-z]+[0-9]*|\\b)|[A-Z]?[a-z]+[0-9]*|[A-Z]|[0-9]+\" , lambda mo : mo . group ( 0 ). lower (), text , ), )","title":"kebab_case"},{"location":"reference/pyhdtoolkit/utils/operations/#snake_case","text":"def snake_case ( text : str ) -> str Converts a string to snake_case. Break the string into words and combine them adding _ as a separator, using a regexp. Parameters: Name Type Description Default text str a string. None Returns: Type Description None The same string best adapted to snake_case. Usage: snake_case(\"A bunch of words\") -> \"a_bunch_of_words\" snake_case(\"camelCase\") -> \"camelcase\" | View Source @staticmethod def snake_case ( text : str ) -> str : \"\"\" Converts a string to snake_case. Break the string into words and combine them adding _ as a separator, using a regexp. Args: text (str): a string. Returns: The same string best adapted to snake_case. Usage: snake_case(\" A bunch of words \") -> \" a_bunch_of_words \" snake_case(\" camelCase \") -> \" camelcase \" \"\"\" return re . sub ( r \"(\\s|_|-)+\" , \"_\" , re . sub ( r \"[A-Z]{2,}(?=[A-Z][a-z]+[0-9]*|\\b)|[A-Z]?[a-z]+[0-9]*|[A-Z]|[0-9]+\" , lambda mo : mo . group ( 0 ). lower (), text , ), )","title":"snake_case"},{"location":"reference/pyhdtoolkit/utils/printutil/","text":"Module pyhdtoolkit.utils.printutil Module utils.printutil Created on 2019.12.11 View Source \"\"\" Module utils.printutil ---------------------- Created on 2019.12.11 :author: Felix Soubelet (felix.soubelet@cern.ch) A class utility class to allow me printing text in color, bold, etc. \"\"\" END = \"\\033[0m\" class Background : \"\"\" ANSI color escape sequences for the background of a terminal output. \"\"\" black = \"\\033[40m\" blue = \"\\033[44m\" cyan = \"\\033[46m\" green = \"\\033[42m\" grey = \"\\033[47m\" magenta = \"\\033[45m\" red = \"\\033[41m\" yellow = \"\\033[43m\" class Foreground : \"\"\" ANSI color escape sequences for the foreground of a terminal output. \"\"\" blue = \"\\033[94m\" cyan = \"\\033[96m\" dark_blue = \"\\033[34m\" dark_cyan = \"\\033[36m\" dark_green = \"\\033[32m\" dark_grey = \"\\033[90m\" dark_red = \"\\033[31m\" dark_yellow = \"\\033[33m\" green = \"\\033[92m\" grey = \"\\033[37m\" magenta = \"\\033[35m\" pink = \"\\033[95m\" red = \"\\033[91m\" yellow = \"\\033[93m\" white = \"\\033[30m\" class Styles : \"\"\" ANSI style escape sequences for a terminal output. \"\"\" all_off = \"\\033[0m\" bold = \"\\033[1m\" concealed = \"\\033[7m\" disable = \"\\033[02m\" reverse = \"\\033[7m\" strikethrough = \"\\033[09m\" underscore = \"\\033[4m\" Variables END Classes Background class Background ( / , * args , ** kwargs ) View Source class Background: \"\"\" ANSI color escape sequences for the background of a terminal output. \"\"\" black = \"\\033[40m\" blue = \"\\033[44m\" cyan = \"\\033[46m\" green = \"\\033[42m\" grey = \"\\033[47m\" magenta = \"\\033[45m\" red = \"\\033[41m\" yellow = \"\\033[43m\" Class variables black blue cyan green grey magenta red yellow Foreground class Foreground ( / , * args , ** kwargs ) View Source class Foreground: \"\"\" ANSI color escape sequences for the foreground of a terminal output . \"\"\" blue = \" \\033 [94m\" cyan = \" \\033 [96m\" dark_blue = \" \\033 [34m\" dark_cyan = \" \\033 [36m\" dark_green = \" \\033 [32m\" dark_grey = \" \\033 [90m\" dark_red = \" \\033 [31m\" dark_yellow = \" \\033 [33m\" green = \" \\033 [92m\" grey = \" \\033 [37m\" magenta = \" \\033 [35m\" pink = \" \\033 [95m\" red = \" \\033 [91m\" yellow = \" \\033 [93m\" white = \" \\033 [30m\" Class variables blue cyan dark_blue dark_cyan dark_green dark_grey dark_red dark_yellow green grey magenta pink red white yellow Styles class Styles ( / , * args , ** kwargs ) View Source class Styles: \"\"\" ANSI style escape sequences for a terminal output. \"\"\" all_off = \"\\033[0m\" bold = \"\\033[1m\" concealed = \"\\033[7m\" disable = \"\\033[02m\" reverse = \"\\033[7m\" strikethrough = \"\\033[09m\" underscore = \"\\033[4m\" Class variables all_off bold concealed disable reverse strikethrough underscore","title":"Printutil"},{"location":"reference/pyhdtoolkit/utils/printutil/#module-pyhdtoolkitutilsprintutil","text":"Module utils.printutil Created on 2019.12.11 View Source \"\"\" Module utils.printutil ---------------------- Created on 2019.12.11 :author: Felix Soubelet (felix.soubelet@cern.ch) A class utility class to allow me printing text in color, bold, etc. \"\"\" END = \"\\033[0m\" class Background : \"\"\" ANSI color escape sequences for the background of a terminal output. \"\"\" black = \"\\033[40m\" blue = \"\\033[44m\" cyan = \"\\033[46m\" green = \"\\033[42m\" grey = \"\\033[47m\" magenta = \"\\033[45m\" red = \"\\033[41m\" yellow = \"\\033[43m\" class Foreground : \"\"\" ANSI color escape sequences for the foreground of a terminal output. \"\"\" blue = \"\\033[94m\" cyan = \"\\033[96m\" dark_blue = \"\\033[34m\" dark_cyan = \"\\033[36m\" dark_green = \"\\033[32m\" dark_grey = \"\\033[90m\" dark_red = \"\\033[31m\" dark_yellow = \"\\033[33m\" green = \"\\033[92m\" grey = \"\\033[37m\" magenta = \"\\033[35m\" pink = \"\\033[95m\" red = \"\\033[91m\" yellow = \"\\033[93m\" white = \"\\033[30m\" class Styles : \"\"\" ANSI style escape sequences for a terminal output. \"\"\" all_off = \"\\033[0m\" bold = \"\\033[1m\" concealed = \"\\033[7m\" disable = \"\\033[02m\" reverse = \"\\033[7m\" strikethrough = \"\\033[09m\" underscore = \"\\033[4m\"","title":"Module pyhdtoolkit.utils.printutil"},{"location":"reference/pyhdtoolkit/utils/printutil/#variables","text":"END","title":"Variables"},{"location":"reference/pyhdtoolkit/utils/printutil/#classes","text":"","title":"Classes"},{"location":"reference/pyhdtoolkit/utils/printutil/#background","text":"class Background ( / , * args , ** kwargs ) View Source class Background: \"\"\" ANSI color escape sequences for the background of a terminal output. \"\"\" black = \"\\033[40m\" blue = \"\\033[44m\" cyan = \"\\033[46m\" green = \"\\033[42m\" grey = \"\\033[47m\" magenta = \"\\033[45m\" red = \"\\033[41m\" yellow = \"\\033[43m\"","title":"Background"},{"location":"reference/pyhdtoolkit/utils/printutil/#class-variables","text":"black blue cyan green grey magenta red yellow","title":"Class variables"},{"location":"reference/pyhdtoolkit/utils/printutil/#foreground","text":"class Foreground ( / , * args , ** kwargs ) View Source class Foreground: \"\"\" ANSI color escape sequences for the foreground of a terminal output . \"\"\" blue = \" \\033 [94m\" cyan = \" \\033 [96m\" dark_blue = \" \\033 [34m\" dark_cyan = \" \\033 [36m\" dark_green = \" \\033 [32m\" dark_grey = \" \\033 [90m\" dark_red = \" \\033 [31m\" dark_yellow = \" \\033 [33m\" green = \" \\033 [92m\" grey = \" \\033 [37m\" magenta = \" \\033 [35m\" pink = \" \\033 [95m\" red = \" \\033 [91m\" yellow = \" \\033 [93m\" white = \" \\033 [30m\"","title":"Foreground"},{"location":"reference/pyhdtoolkit/utils/printutil/#class-variables_1","text":"blue cyan dark_blue dark_cyan dark_green dark_grey dark_red dark_yellow green grey magenta pink red white yellow","title":"Class variables"},{"location":"reference/pyhdtoolkit/utils/printutil/#styles","text":"class Styles ( / , * args , ** kwargs ) View Source class Styles: \"\"\" ANSI style escape sequences for a terminal output. \"\"\" all_off = \"\\033[0m\" bold = \"\\033[1m\" concealed = \"\\033[7m\" disable = \"\\033[02m\" reverse = \"\\033[7m\" strikethrough = \"\\033[09m\" underscore = \"\\033[4m\"","title":"Styles"},{"location":"reference/pyhdtoolkit/utils/printutil/#class-variables_2","text":"all_off bold concealed disable reverse strikethrough underscore","title":"Class variables"}]}